   1                             	# 1 "../src/wolfcrypt/src/aes_asm.asm"
   1                             	; /* aes_asm.asm
   0                             	
   0                             	
   0                             	
   2                             	;  *
   3                             	;  * Copyright (C) 2006-2021 wolfSSL Inc.
   4                             	;  *
   5                             	;  * This file is part of wolfSSL.
   6                             	;  *
   7                             	;  * wolfSSL is free software; you can redistribute it and/or modify
   8                             	;  * it under the terms of the GNU General Public License as published by
   9                             	;  * the Free Software Foundation; either version 2 of the License, or
  10                             	;  * (at your option) any later version.
  11                             	;  *
  12                             	;  * wolfSSL is distributed in the hope that it will be useful,
  13                             	;  * but WITHOUT ANY WARRANTY; without even the implied warranty of
  14                             	;  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  15                             	;  * GNU General Public License for more details.
  16                             	;  *
  17                             	;  * You should have received a copy of the GNU General Public License
  18                             	;  * along with this program; if not, write to the Free Software
  19                             	;  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1335, USA
  20                             	;  */
  21                             	
  22                             	
  23                             	;
  24                             	;
  25                             	;  /* See Intel Advanced Encryption Standard (AES) Instructions Set White Paper
  26                             	;   * by Israel, Intel Mobility Group Development Center, Israel Shay Gueron
  27                             	;   */
  28                             	;
  29                             	;   /* This file is in intel asm syntax, see .s for at&t syntax */
  30                             	;
  31                             	
  32                             	
  33                             	fips_version = 0
  34                             	IFDEF HAVE_FIPS
  35                             	  fips_version = 1
  36                             	  IFDEF HAVE_FIPS_VERSION
  37                             	    fips_version = HAVE_FIPS_VERSION
  38                             	  ENDIF
  39                             	ENDIF
  40                             	
  41                             	IF fips_version GE 2
  42                             	  fipsAh SEGMENT ALIAS(".fipsA$h") 'CODE'
  43                             	ELSE
  44                             	  _text SEGMENT
  45                             	ENDIF
  46                             	
  47                             	
  48                             	;	/*
  49                             	;	AES_CBC_encrypt[const	,unsigned	char*in
  50                             	;	unsigned	,char*out
  51                             	;	unsigned	,char	ivec+16
  52                             	;	unsigned	,long	length
  53                             	;	const	,unsigned	char*KS
  54                             	;	int	nr]
  55                             	;	*/
  56                             	AES_CBC_encrypt PROC
  57                             	;#	parameter	1:	rdi
  58                             	;#	parameter	2:	rsi
  59                             	;#	parameter	3:	rdx
  60                             	;#	parameter	4:	rcx
  61                             	;#	parameter	5:	r8
  62                             	;#	parameter	6:	r9d
  63                             	
  64                             	; save rdi and rsi to rax and r11, restore before ret
  65                             		mov rax,rdi
  66                             		mov r11,rsi
  67                             	
  68                             	; convert to what we had for att&t convention
  69                             		mov rdi,rcx
  70                             		mov rsi,rdx
  71                             		mov rdx,r8
  72                             		mov rcx,r9
  73                             		mov r8,[rsp+40]
  74                             		mov r9d,[rsp+48]
  75                             	
  76                             		mov	r10,rcx
  77                             		shr	rcx,4
  78                             		shl	r10,60
  79                             		je	NO_PARTS
  80                             		add	rcx,1
  81                             	NO_PARTS:
  82                             		sub	rsi,16
  83                             		movdqa	xmm1,[rdx]
  84                             	LOOP_1:
  85                             		pxor	xmm1,[rdi]
  86                             		pxor	xmm1,[r8]
  87                             		add	rsi,16
  88                             		add	rdi,16
  89                             		cmp	r9d,12
  90                             		aesenc	xmm1,16[r8]
  91                             		aesenc	xmm1,32[r8]
  92                             		aesenc	xmm1,48[r8]
  93                             		aesenc	xmm1,64[r8]
  94                             		aesenc	xmm1,80[r8]
  95                             		aesenc	xmm1,96[r8]
  96                             		aesenc	xmm1,112[r8]
  97                             		aesenc	xmm1,128[r8]
  98                             		aesenc	xmm1,144[r8]
  99                             		movdqa	xmm2,160[r8]
 100                             		jb	LAST
 101                             		cmp	r9d,14
 102                             	
 103                             		aesenc	xmm1,160[r8]
 104                             		aesenc	xmm1,176[r8]
 105                             		movdqa	xmm2,192[r8]
 106                             		jb	LAST
 107                             		aesenc	xmm1,192[r8]
 108                             		aesenc	xmm1,208[r8]
 109                             		movdqa	xmm2,224[r8]
 110                             	LAST:
 111                             		dec	rcx
 112                             		aesenclast	xmm1,xmm2
 113                             		movdqu	[rsi],xmm1
 114                             		jne	LOOP_1
 115                             		; restore non volatile rdi,rsi
 116                             		mov rdi,rax
 117                             		mov rsi,r11
 118                             		ret
 119                             	AES_CBC_encrypt ENDP
 120                             	
 121                             	
 122                             	; void AES_CBC_decrypt_by4(const unsigned char* in,
 123                             	;                          unsigned char* out,
 124                             	;                          unsigned char ivec[16],
 125                             	;                          unsigned long length,
 126                             	;                          const unsigned char* KS,
 127                             	;                          int nr)
 128                             	AES_CBC_decrypt_by4 PROC
 129                             	; parameter 1: rdi
 130                             	; parameter 2: rsi
 131                             	; parameter 3: rdx
 132                             	; parameter 4: rcx
 133                             	; parameter 5: r8
 134                             	; parameter 6: r9d
 135                             	
 136                             	        ; save rdi and rsi to rax and r11, restore before ret
 137                             	        mov         rax, rdi
 138                             	        mov         r11, rsi
 139                             	        ; convert to what we had for att&t convention
 140                             	        mov         rdi, rcx
 141                             	        mov         rsi, rdx
 142                             	        mov         rdx, r8
 143                             	        mov         rcx,r9
 144                             	        mov         r8, [rsp+40]
 145                             	        mov         r9d, [rsp+48]
 146                             	        ; on microsoft xmm6-xmm15 are non volatile,
 147                             	        ; let's save on stack and restore at end
 148                             	        sub         rsp, 8+8*16  ; 8 = align stack , 8 xmm6-12,15 16 bytes each
 149                             	        movdqa      [rsp+0], xmm6
 150                             	        movdqa      [rsp+16], xmm7
 151                             	        movdqa      [rsp+32], xmm8
 152                             	        movdqa      [rsp+48], xmm9
 153                             	        movdqa      [rsp+64], xmm10
 154                             	        movdqa      [rsp+80], xmm11
 155                             	        movdqa      [rsp+96], xmm12
 156                             	        movdqa      [rsp+112], xmm15
 157                             	        ; back to our original code, more or less
 158                             	        mov         r10, rcx
 159                             	        shr         rcx, 4
 160                             	        shl         r10, 60
 161                             	        je          DNO_PARTS_4
 162                             	        add         rcx, 1
 163                             	DNO_PARTS_4:
 164                             	        mov         r10, rcx
 165                             	        shl         r10, 62
 166                             	        shr         r10, 62
 167                             	        shr         rcx, 2
 168                             	        movdqu      xmm5, [rdx]
 169                             	        je          DREMAINDER_4
 170                             	        sub         rsi, 64
 171                             	DLOOP_4:
 172                             	        movdqu      xmm1, [rdi]
 173                             	        movdqu      xmm2, 16[rdi]
 174                             	        movdqu      xmm3, 32[rdi]
 175                             	        movdqu      xmm4, 48[rdi]
 176                             	        movdqa      xmm6, xmm1
 177                             	        movdqa      xmm7, xmm2
 178                             	        movdqa      xmm8, xmm3
 179                             	        movdqa      xmm15, xmm4
 180                             	        movdqa      xmm9, [r8]
 181                             	        movdqa      xmm10, 16[r8]
 182                             	        movdqa      xmm11, 32[r8]
 183                             	        movdqa      xmm12, 48[r8]
 184                             	        pxor        xmm1, xmm9
 185                             	        pxor        xmm2, xmm9
 186                             	        pxor        xmm3, xmm9
 187                             	        pxor        xmm4, xmm9
 188                             	        aesdec      xmm1, xmm10
 189                             	        aesdec      xmm2, xmm10
 190                             	        aesdec      xmm3, xmm10
 191                             	        aesdec      xmm4, xmm10
 192                             	        aesdec      xmm1, xmm11
 193                             	        aesdec      xmm2, xmm11
 194                             	        aesdec      xmm3, xmm11
 195                             	        aesdec      xmm4, xmm11
 196                             	        aesdec      xmm1, xmm12
 197                             	        aesdec      xmm2, xmm12
 198                             	        aesdec      xmm3, xmm12
 199                             	        aesdec      xmm4, xmm12
 200                             	        movdqa      xmm9, 64[r8]
 201                             	        movdqa      xmm10, 80[r8]
 202                             	        movdqa      xmm11, 96[r8]
 203                             	        movdqa      xmm12, 112[r8]
 204                             	        aesdec      xmm1, xmm9
 205                             	        aesdec      xmm2, xmm9
 206                             	        aesdec      xmm3, xmm9
 207                             	        aesdec      xmm4, xmm9
 208                             	        aesdec      xmm1, xmm10
 209                             	        aesdec      xmm2, xmm10
 210                             	        aesdec      xmm3, xmm10
 211                             	        aesdec      xmm4, xmm10
 212                             	        aesdec      xmm1, xmm11
 213                             	        aesdec      xmm2, xmm11
 214                             	        aesdec      xmm3, xmm11
 215                             	        aesdec      xmm4, xmm11
 216                             	        aesdec      xmm1, xmm12
 217                             	        aesdec      xmm2, xmm12
 218                             	        aesdec      xmm3, xmm12
 219                             	        aesdec      xmm4, xmm12
 220                             	        movdqa      xmm9, 128[r8]
 221                             	        movdqa      xmm10, 144[r8]
 222                             	        movdqa      xmm11, 160[r8]
 223                             	        cmp         r9d, 12
 224                             	        aesdec      xmm1, xmm9
 225                             	        aesdec      xmm2, xmm9
 226                             	        aesdec      xmm3, xmm9
 227                             	        aesdec      xmm4, xmm9
 228                             	        aesdec      xmm1, xmm10
 229                             	        aesdec      xmm2, xmm10
 230                             	        aesdec      xmm3, xmm10
 231                             	        aesdec      xmm4, xmm10
 232                             	        jb          DLAST_4
 233                             	        movdqa      xmm9, 160[r8]
 234                             	        movdqa      xmm10, 176[r8]
 235                             	        movdqa      xmm11, 192[r8]
 236                             	        cmp         r9d, 14
 237                             	        aesdec      xmm1, xmm9
 238                             	        aesdec      xmm2, xmm9
 239                             	        aesdec      xmm3, xmm9
 240                             	        aesdec      xmm4, xmm9
 241                             	        aesdec      xmm1, xmm10
 242                             	        aesdec      xmm2, xmm10
 243                             	        aesdec      xmm3, xmm10
 244                             	        aesdec      xmm4, xmm10
 245                             	        jb          DLAST_4
 246                             	        movdqa      xmm9, 192[r8]
 247                             	        movdqa      xmm10, 208[r8]
 248                             	        movdqa      xmm11, 224[r8]
 249                             	        aesdec      xmm1, xmm9
 250                             	        aesdec      xmm2, xmm9
 251                             	        aesdec      xmm3, xmm9
 252                             	        aesdec      xmm4, xmm9
 253                             	        aesdec      xmm1, xmm10
 254                             	        aesdec      xmm2, xmm10
 255                             	        aesdec      xmm3, xmm10
 256                             	        aesdec      xmm4, xmm10
 257                             	DLAST_4:
 258                             	        add         rdi, 64
 259                             	        add         rsi, 64
 260                             	        dec         rcx
 261                             	        aesdeclast  xmm1, xmm11
 262                             	        aesdeclast  xmm2, xmm11
 263                             	        aesdeclast  xmm3, xmm11
 264                             	        aesdeclast  xmm4, xmm11
 265                             	        pxor        xmm1, xmm5
 266                             	        pxor        xmm2, xmm6
 267                             	        pxor        xmm3, xmm7
 268                             	        pxor        xmm4, xmm8
 269                             	        movdqu      [rsi], xmm1
 270                             	        movdqu      16[rsi], xmm2
 271                             	        movdqu      32[rsi], xmm3
 272                             	        movdqu      48[rsi], xmm4
 273                             	        movdqa      xmm5, xmm15
 274                             	        jne         DLOOP_4
 275                             	        add         rsi, 64
 276                             	DREMAINDER_4:
 277                             	        cmp         r10, 0
 278                             	        je          DEND_4
 279                             	DLOOP_4_2:
 280                             	        movdqu      xmm1, [rdi]
 281                             	        movdqa      xmm15, xmm1
 282                             	        add         rdi, 16
 283                             	        pxor        xmm1, [r8]
 284                             	        movdqu      xmm2, 160[r8]
 285                             	        cmp         r9d, 12
 286                             	        aesdec      xmm1, 16[r8]
 287                             	        aesdec      xmm1, 32[r8]
 288                             	        aesdec      xmm1, 48[r8]
 289                             	        aesdec      xmm1, 64[r8]
 290                             	        aesdec      xmm1, 80[r8]
 291                             	        aesdec      xmm1, 96[r8]
 292                             	        aesdec      xmm1, 112[r8]
 293                             	        aesdec      xmm1, 128[r8]
 294                             	        aesdec      xmm1, 144[r8]
 295                             	        jb          DLAST_4_2
 296                             	        movdqu      xmm2, 192[r8]
 297                             	        cmp         r9d, 14
 298                             	        aesdec      xmm1, 160[r8]
 299                             	        aesdec      xmm1, 176[r8]
 300                             	        jb          DLAST_4_2
 301                             	        movdqu      xmm2, 224[r8]
 302                             	        aesdec      xmm1, 192[r8]
 303                             	        aesdec      xmm1, 208[r8]
 304                             	DLAST_4_2:
 305                             	        aesdeclast  xmm1, xmm2
 306                             	        pxor        xmm1, xmm5
 307                             	        movdqa      xmm5, xmm15
 308                             	        movdqu      [rsi], xmm1
 309                             	        add         rsi, 16
 310                             	        dec         r10
 311                             	        jne         DLOOP_4_2
 312                             	DEND_4:
 313                             	        ; restore non volatile rdi,rsi
 314                             	        mov         rdi, rax
 315                             	        mov         rsi, r11
 316                             	        ; restore non volatile xmms from stack
 317                             	        movdqa      xmm6, [rsp+0]
 318                             	        movdqa      xmm7, [rsp+16]
 319                             	        movdqa      xmm8, [rsp+32]
 320                             	        movdqa      xmm9, [rsp+48]
 321                             	        movdqa      xmm10, [rsp+64]
 322                             	        movdqa      xmm11, [rsp+80]
 323                             	        movdqa      xmm12, [rsp+96]
 324                             	        movdqa      xmm15, [rsp+112]
 325                             	        add         rsp, 8+8*16 ; 8 = align stack , 8 xmm6-12,15 16 bytes each
 326                             	        ret
 327                             	AES_CBC_decrypt_by4 ENDP
 328                             	
 329                             	
 330                             	; void AES_CBC_decrypt_by6(const unsigned char *in,
 331                             	;                          unsigned char *out,
 332                             	;                          unsigned char ivec[16],
 333                             	;                          unsigned long length,
 334                             	;                          const unsigned char *KS,
 335                             	;                          int nr)
 336                             	AES_CBC_decrypt_by6 PROC
 337                             	; parameter 1: rdi - in
 338                             	; parameter 2: rsi - out
 339                             	; parameter 3: rdx - ivec
 340                             	; parameter 4: rcx - length
 341                             	; parameter 5: r8  - KS
 342                             	; parameter 6: r9d - nr
 343                             	
 344                             	        ; save rdi and rsi to rax and r11, restore before ret
 345                             	        mov         rax, rdi
 346                             	        mov         r11, rsi
 347                             	        ; convert to what we had for att&t convention
 348                             	        mov         rdi, rcx
 349                             	        mov         rsi, rdx
 350                             	        mov         rdx, r8
 351                             	        mov         rcx, r9
 352                             	        mov         r8, [rsp+40]
 353                             	        mov         r9d, [rsp+48]
 354                             	        ; on microsoft xmm6-xmm15 are non volatile,
 355                             	        ; let's save on stack and restore at end
 356                             	        sub         rsp, 8+9*16  ; 8 = align stack , 9 xmm6-14 16 bytes each
 357                             	        movdqa      [rsp+0], xmm6
 358                             	        movdqa      [rsp+16], xmm7
 359                             	        movdqa      [rsp+32], xmm8
 360                             	        movdqa      [rsp+48], xmm9
 361                             	        movdqa      [rsp+64], xmm10
 362                             	        movdqa      [rsp+80], xmm11
 363                             	        movdqa      [rsp+96], xmm12
 364                             	        movdqa      [rsp+112], xmm13
 365                             	        movdqa      [rsp+128], xmm14
 366                             	        ; back to our original code, more or less
 367                             	        mov         r10, rcx
 368                             	        shr         rcx, 4
 369                             	        shl         r10, 60
 370                             	        je          DNO_PARTS_6
 371                             	        add         rcx, 1
 372                             	DNO_PARTS_6:
 373                             	        mov         r12, rax
 374                             	        mov         r13, rdx
 375                             	        mov         r14, rbx
 376                             	        mov         rdx, 0
 377                             	        mov         rax, rcx
 378                             	        mov         rbx, 6
 379                             	        div         rbx
 380                             	        mov         rcx, rax
 381                             	        mov         r10, rdx
 382                             	        mov         rax, r12
 383                             	        mov         rdx, r13
 384                             	        mov         rbx, r14
 385                             	        cmp         rcx, 0
 386                             	        movdqu      xmm7, [rdx]
 387                             	        je          DREMAINDER_6
 388                             	        sub         rsi, 96
 389                             	DLOOP_6:
 390                             	        movdqu      xmm1, [rdi]
 391                             	        movdqu      xmm2, 16[rdi]
 392                             	        movdqu      xmm3, 32[rdi]
 393                             	        movdqu      xmm4, 48[rdi]
 394                             	        movdqu      xmm5, 64[rdi]
 395                             	        movdqu      xmm6, 80[rdi]
 396                             	        movdqa      xmm8, [r8]
 397                             	        movdqa      xmm9, 16[r8]
 398                             	        movdqa      xmm10, 32[r8]
 399                             	        movdqa      xmm11, 48[r8]
 400                             	        pxor        xmm1, xmm8
 401                             	        pxor        xmm2, xmm8
 402                             	        pxor        xmm3, xmm8
 403                             	        pxor        xmm4, xmm8
 404                             	        pxor        xmm5, xmm8
 405                             	        pxor        xmm6, xmm8
 406                             	        aesdec      xmm1, xmm9
 407                             	        aesdec      xmm2, xmm9
 408                             	        aesdec      xmm3, xmm9
 409                             	        aesdec      xmm4, xmm9
 410                             	        aesdec      xmm5, xmm9
 411                             	        aesdec      xmm6, xmm9
 412                             	        aesdec      xmm1, xmm10
 413                             	        aesdec      xmm2, xmm10
 414                             	        aesdec      xmm3, xmm10
 415                             	        aesdec      xmm4, xmm10
 416                             	        aesdec      xmm5, xmm10
 417                             	        aesdec      xmm6, xmm10
 418                             	        aesdec      xmm1, xmm11
 419                             	        aesdec      xmm2, xmm11
 420                             	        aesdec      xmm3, xmm11
 421                             	        aesdec      xmm4, xmm11
 422                             	        aesdec      xmm5, xmm11
 423                             	        aesdec      xmm6, xmm11
 424                             	        movdqa      xmm8, 64[r8]
 425                             	        movdqa      xmm9, 80[r8]
 426                             	        movdqa      xmm10, 96[r8]
 427                             	        movdqa      xmm11, 112[r8]
 428                             	        aesdec      xmm1, xmm8
 429                             	        aesdec      xmm2, xmm8
 430                             	        aesdec      xmm3, xmm8
 431                             	        aesdec      xmm4, xmm8
 432                             	        aesdec      xmm5, xmm8
 433                             	        aesdec      xmm6, xmm8
 434                             	        aesdec      xmm1, xmm9
 435                             	        aesdec      xmm2, xmm9
 436                             	        aesdec      xmm3, xmm9
 437                             	        aesdec      xmm4, xmm9
 438                             	        aesdec      xmm5, xmm9
 439                             	        aesdec      xmm6, xmm9
 440                             	        aesdec      xmm1, xmm10
 441                             	        aesdec      xmm2, xmm10
 442                             	        aesdec      xmm3, xmm10
 443                             	        aesdec      xmm4, xmm10
 444                             	        aesdec      xmm5, xmm10
 445                             	        aesdec      xmm6, xmm10
 446                             	        aesdec      xmm1, xmm11
 447                             	        aesdec      xmm2, xmm11
 448                             	        aesdec      xmm3, xmm11
 449                             	        aesdec      xmm4, xmm11
 450                             	        aesdec      xmm5, xmm11
 451                             	        aesdec      xmm6, xmm11
 452                             	        movdqa      xmm8, 128[r8]
 453                             	        movdqa      xmm9, 144[r8]
 454                             	        movdqa      xmm10, 160[r8]
 455                             	        cmp         r9d, 12
 456                             	        aesdec      xmm1, xmm8
 457                             	        aesdec      xmm2, xmm8
 458                             	        aesdec      xmm3, xmm8
 459                             	        aesdec      xmm4, xmm8
 460                             	        aesdec      xmm5, xmm8
 461                             	        aesdec      xmm6, xmm8
 462                             	        aesdec      xmm1, xmm9
 463                             	        aesdec      xmm2, xmm9
 464                             	        aesdec      xmm3, xmm9
 465                             	        aesdec      xmm4, xmm9
 466                             	        aesdec      xmm5, xmm9
 467                             	        aesdec      xmm6, xmm9
 468                             	        jb          DLAST_6
 469                             	        movdqa      xmm8, 160[r8]
 470                             	        movdqa      xmm9, 176[r8]
 471                             	        movdqa      xmm10, 192[r8]
 472                             	        cmp         r9d, 14
 473                             	        aesdec      xmm1, xmm8
 474                             	        aesdec      xmm2, xmm8
 475                             	        aesdec      xmm3, xmm8
 476                             	        aesdec      xmm4, xmm8
 477                             	        aesdec      xmm5, xmm8
 478                             	        aesdec      xmm6, xmm8
 479                             	        aesdec      xmm1, xmm9
 480                             	        aesdec      xmm2, xmm9
 481                             	        aesdec      xmm3, xmm9
 482                             	        aesdec      xmm4, xmm9
 483                             	        aesdec      xmm5, xmm9
 484                             	        aesdec      xmm6, xmm9
 485                             	        jb          DLAST_6
 486                             	        movdqa      xmm8, 192[r8]
 487                             	        movdqa      xmm9, 208[r8]
 488                             	        movdqa      xmm10, 224[r8]
 489                             	        aesdec      xmm1, xmm8
 490                             	        aesdec      xmm2, xmm8
 491                             	        aesdec      xmm3, xmm8
 492                             	        aesdec      xmm4, xmm8
 493                             	        aesdec      xmm5, xmm8
 494                             	        aesdec      xmm6, xmm8
 495                             	        aesdec      xmm1, xmm9
 496                             	        aesdec      xmm2, xmm9
 497                             	        aesdec      xmm3, xmm9
 498                             	        aesdec      xmm4, xmm9
 499                             	        aesdec      xmm5, xmm9
 500                             	        aesdec      xmm6, xmm9
 501                             	DLAST_6:
 502                             	        add         rsi, 96
 503                             	        aesdeclast  xmm1, xmm10
 504                             	        aesdeclast  xmm2, xmm10
 505                             	        aesdeclast  xmm3, xmm10
 506                             	        aesdeclast  xmm4, xmm10
 507                             	        aesdeclast  xmm5, xmm10
 508                             	        aesdeclast  xmm6, xmm10
 509                             	        movdqu      xmm8, [rdi]
 510                             	        movdqu      xmm9, 16[rdi]
 511                             	        movdqu      xmm10, 32[rdi]
 512                             	        movdqu      xmm11, 48[rdi]
 513                             	        movdqu      xmm12, 64[rdi]
 514                             	        movdqu      xmm13, 80[rdi]
 515                             	        pxor        xmm1, xmm7
 516                             	        pxor        xmm2, xmm8
 517                             	        pxor        xmm3, xmm9
 518                             	        pxor        xmm4, xmm10
 519                             	        pxor        xmm5, xmm11
 520                             	        pxor        xmm6, xmm12
 521                             	        movdqu      xmm7, xmm13
 522                             	        movdqu      [rsi], xmm1
 523                             	        movdqu      16[rsi], xmm2
 524                             	        movdqu      32[rsi], xmm3
 525                             	        movdqu      48[rsi], xmm4
 526                             	        movdqu      64[rsi], xmm5
 527                             	        movdqu      80[rsi], xmm6
 528                             	        add         rdi, 96
 529                             	        dec         rcx
 530                             	        jne         DLOOP_6
 531                             	        add         rsi, 96
 532                             	DREMAINDER_6:
 533                             	        cmp         r10, 0
 534                             	        je          DEND_6
 535                             	DLOOP_6_2:
 536                             	        movdqu      xmm1, [rdi]
 537                             	        movdqa      xmm10, xmm1
 538                             	        add         rdi, 16
 539                             	        pxor        xmm1, [r8]
 540                             	        movdqu      xmm2, 160[r8]
 541                             	        cmp         r9d, 12
 542                             	        aesdec      xmm1, 16[r8]
 543                             	        aesdec      xmm1, 32[r8]
 544                             	        aesdec      xmm1, 48[r8]
 545                             	        aesdec      xmm1, 64[r8]
 546                             	        aesdec      xmm1, 80[r8]
 547                             	        aesdec      xmm1, 96[r8]
 548                             	        aesdec      xmm1, 112[r8]
 549                             	        aesdec      xmm1, 128[r8]
 550                             	        aesdec      xmm1, 144[r8]
 551                             	        jb          DLAST_6_2
 552                             	        movdqu      xmm2, 192[r8]
 553                             	        cmp         r9d, 14
 554                             	        aesdec      xmm1, 160[r8]
 555                             	        aesdec      xmm1, 176[r8]
 556                             	        jb          DLAST_6_2
 557                             	        movdqu      xmm2, 224[r8]
 558                             	        aesdec      xmm1, 192[r8]
 559                             	        aesdec      xmm1, 208[r8]
 560                             	DLAST_6_2:
 561                             	        aesdeclast  xmm1, xmm2
 562                             	        pxor        xmm1, xmm7
 563                             	        movdqa      xmm7, xmm10
 564                             	        movdqu      [rsi], xmm1
 565                             	        add         rsi, 16
 566                             	        dec         r10
 567                             	        jne         DLOOP_6_2
 568                             	DEND_6:
 569                             	        ; restore non volatile rdi,rsi
 570                             	        mov         rdi, rax
 571                             	        mov         rsi, r11
 572                             	        ; restore non volatile xmms from stack
 573                             	        movdqa      xmm6, [rsp+0]
 574                             	        movdqa      xmm7, [rsp+16]
 575                             	        movdqa      xmm8, [rsp+32]
 576                             	        movdqa      xmm9, [rsp+48]
 577                             	        movdqa      xmm10, [rsp+64]
 578                             	        movdqa      xmm11, [rsp+80]
 579                             	        movdqa      xmm12, [rsp+96]
 580                             	        movdqa      xmm13, [rsp+112]
 581                             	        movdqa      xmm14, [rsp+128]
 582                             	        add         rsp, 8+9*16 ; 8 = align stack , 9 xmm6-14 16 bytes each
 583                             	        ret
 584                             	AES_CBC_decrypt_by6 ENDP
 585                             	
 586                             	
 587                             	; void AES_CBC_decrypt_by8(const unsigned char *in,
 588                             	;                          unsigned char *out,
 589                             	;                          unsigned char ivec[16],
 590                             	;                          unsigned long length,
 591                             	;                          const unsigned char *KS,
 592                             	;                          int nr)
 593                             	AES_CBC_decrypt_by8 PROC
 594                             	; parameter 1: rdi - in
 595                             	; parameter 2: rsi - out
 596                             	; parameter 3: rdx - ivec
 597                             	; parameter 4: rcx - length
 598                             	; parameter 5: r8  - KS
 599                             	; parameter 6: r9d - nr
 600                             	
 601                             	        ; save rdi and rsi to rax and r11, restore before ret
 602                             	        mov         rax, rdi
 603                             	        mov         r11, rsi
 604                             	        ; convert to what we had for att&t convention
 605                             	        mov         rdi, rcx
 606                             	        mov         rsi, rdx
 607                             	        mov         rdx, r8
 608                             	        mov         rcx,r9
 609                             	        mov         r8, [rsp+40]
 610                             	        mov         r9d, [rsp+48]
 611                             	        ; on microsoft xmm6-xmm15 are non volatile,
 612                             	        ; let's save on stack and restore at end
 613                             	        sub         rsp, 8+8*16  ; 8 = align stack , 8 xmm6-13 16 bytes each
 614                             	        movdqa      [rsp+0], xmm6
 615                             	        movdqa      [rsp+16], xmm7
 616                             	        movdqa      [rsp+32], xmm8
 617                             	        movdqa      [rsp+48], xmm9
 618                             	        movdqa      [rsp+64], xmm10
 619                             	        movdqa      [rsp+80], xmm11
 620                             	        movdqa      [rsp+96], xmm12
 621                             	        movdqa      [rsp+112], xmm13
 622                             	        ; back to our original code, more or less
 623                             	        mov         r10, rcx
 624                             	        shr         rcx, 4
 625                             	        shl         r10, 60
 626                             	        je          DNO_PARTS_8
 627                             	        add         rcx, 1
 628                             	DNO_PARTS_8:
 629                             	        mov         r10, rcx
 630                             	        shl         r10, 61
 631                             	        shr         r10, 61
 632                             	        shr         rcx, 3
 633                             	        movdqu      xmm9, [rdx]
 634                             	        je          DREMAINDER_8
 635                             	        sub         rsi, 128
 636                             	DLOOP_8:
 637                             	        movdqu      xmm1, [rdi]
 638                             	        movdqu      xmm2, 16[rdi]
 639                             	        movdqu      xmm3, 32[rdi]
 640                             	        movdqu      xmm4, 48[rdi]
 641                             	        movdqu      xmm5, 64[rdi]
 642                             	        movdqu      xmm6, 80[rdi]
 643                             	        movdqu      xmm7, 96[rdi]
 644                             	        movdqu      xmm8, 112[rdi]
 645                             	        movdqa      xmm10, [r8]
 646                             	        movdqa      xmm11, 16[r8]
 647                             	        movdqa      xmm12, 32[r8]
 648                             	        movdqa      xmm13, 48[r8]
 649                             	        pxor        xmm1, xmm10
 650                             	        pxor        xmm2, xmm10
 651                             	        pxor        xmm3, xmm10
 652                             	        pxor        xmm4, xmm10
 653                             	        pxor        xmm5, xmm10
 654                             	        pxor        xmm6, xmm10
 655                             	        pxor        xmm7, xmm10
 656                             	        pxor        xmm8, xmm10
 657                             	        aesdec      xmm1, xmm11
 658                             	        aesdec      xmm2, xmm11
 659                             	        aesdec      xmm3, xmm11
 660                             	        aesdec      xmm4, xmm11
 661                             	        aesdec      xmm5, xmm11
 662                             	        aesdec      xmm6, xmm11
 663                             	        aesdec      xmm7, xmm11
 664                             	        aesdec      xmm8, xmm11
 665                             	        aesdec      xmm1, xmm12
 666                             	        aesdec      xmm2, xmm12
 667                             	        aesdec      xmm3, xmm12
 668                             	        aesdec      xmm4, xmm12
 669                             	        aesdec      xmm5, xmm12
 670                             	        aesdec      xmm6, xmm12
 671                             	        aesdec      xmm7, xmm12
 672                             	        aesdec      xmm8, xmm12
 673                             	        aesdec      xmm1, xmm13
 674                             	        aesdec      xmm2, xmm13
 675                             	        aesdec      xmm3, xmm13
 676                             	        aesdec      xmm4, xmm13
 677                             	        aesdec      xmm5, xmm13
 678                             	        aesdec      xmm6, xmm13
 679                             	        aesdec      xmm7, xmm13
 680                             	        aesdec      xmm8, xmm13
 681                             	        movdqa      xmm10, 64[r8]
 682                             	        movdqa      xmm11, 80[r8]
 683                             	        movdqa      xmm12, 96[r8]
 684                             	        movdqa      xmm13, 112[r8]
 685                             	        aesdec      xmm1, xmm10
 686                             	        aesdec      xmm2, xmm10
 687                             	        aesdec      xmm3, xmm10
 688                             	        aesdec      xmm4, xmm10
 689                             	        aesdec      xmm5, xmm10
 690                             	        aesdec      xmm6, xmm10
 691                             	        aesdec      xmm7, xmm10
 692                             	        aesdec      xmm8, xmm10
 693                             	        aesdec      xmm1, xmm11
 694                             	        aesdec      xmm2, xmm11
 695                             	        aesdec      xmm3, xmm11
 696                             	        aesdec      xmm4, xmm11
 697                             	        aesdec      xmm5, xmm11
 698                             	        aesdec      xmm6, xmm11
 699                             	        aesdec      xmm7, xmm11
 700                             	        aesdec      xmm8, xmm11
 701                             	        aesdec      xmm1, xmm12
 702                             	        aesdec      xmm2, xmm12
 703                             	        aesdec      xmm3, xmm12
 704                             	        aesdec      xmm4, xmm12
 705                             	        aesdec      xmm5, xmm12
 706                             	        aesdec      xmm6, xmm12
 707                             	        aesdec      xmm7, xmm12
 708                             	        aesdec      xmm8, xmm12
 709                             	        aesdec      xmm1, xmm13
 710                             	        aesdec      xmm2, xmm13
 711                             	        aesdec      xmm3, xmm13
 712                             	        aesdec      xmm4, xmm13
 713                             	        aesdec      xmm5, xmm13
 714                             	        aesdec      xmm6, xmm13
 715                             	        aesdec      xmm7, xmm13
 716                             	        aesdec      xmm8, xmm13
 717                             	        movdqa      xmm10, 128[r8]
 718                             	        movdqa      xmm11, 144[r8]
 719                             	        movdqa      xmm12, 160[r8]
 720                             	        cmp         r9d, 12
 721                             	        aesdec      xmm1, xmm10
 722                             	        aesdec      xmm2, xmm10
 723                             	        aesdec      xmm3, xmm10
 724                             	        aesdec      xmm4, xmm10
 725                             	        aesdec      xmm5, xmm10
 726                             	        aesdec      xmm6, xmm10
 727                             	        aesdec      xmm7, xmm10
 728                             	        aesdec      xmm8, xmm10
 729                             	        aesdec      xmm1, xmm11
 730                             	        aesdec      xmm2, xmm11
 731                             	        aesdec      xmm3, xmm11
 732                             	        aesdec      xmm4, xmm11
 733                             	        aesdec      xmm5, xmm11
 734                             	        aesdec      xmm6, xmm11
 735                             	        aesdec      xmm7, xmm11
 736                             	        aesdec      xmm8, xmm11
 737                             	        jb          DLAST_8
 738                             	        movdqa      xmm10, 160[r8]
 739                             	        movdqa      xmm11, 176[r8]
 740                             	        movdqa      xmm12, 192[r8]
 741                             	        cmp         r9d, 14
 742                             	        aesdec      xmm1, xmm10
 743                             	        aesdec      xmm2, xmm10
 744                             	        aesdec      xmm3, xmm10
 745                             	        aesdec      xmm4, xmm10
 746                             	        aesdec      xmm5, xmm10
 747                             	        aesdec      xmm6, xmm10
 748                             	        aesdec      xmm7, xmm10
 749                             	        aesdec      xmm8, xmm10
 750                             	        aesdec      xmm1, xmm11
 751                             	        aesdec      xmm2, xmm11
 752                             	        aesdec      xmm3, xmm11
 753                             	        aesdec      xmm4, xmm11
 754                             	        aesdec      xmm5, xmm11
 755                             	        aesdec      xmm6, xmm11
 756                             	        aesdec      xmm7, xmm11
 757                             	        aesdec      xmm8, xmm11
 758                             	        jb          DLAST_8
 759                             	        movdqa      xmm10, 192[r8]
 760                             	        movdqa      xmm11, 208[r8]
 761                             	        movdqa      xmm12, 224[r8]
 762                             	        aesdec      xmm1, xmm10
 763                             	        aesdec      xmm2, xmm10
 764                             	        aesdec      xmm3, xmm10
 765                             	        aesdec      xmm4, xmm10
 766                             	        aesdec      xmm5, xmm10
 767                             	        aesdec      xmm6, xmm10
 768                             	        aesdec      xmm7, xmm10
 769                             	        aesdec      xmm8, xmm10
 770                             	        aesdec      xmm1, xmm11
 771                             	        aesdec      xmm2, xmm11
 772                             	        aesdec      xmm3, xmm11
 773                             	        aesdec      xmm4, xmm11
 774                             	        aesdec      xmm5, xmm11
 775                             	        aesdec      xmm6, xmm11
 776                             	        aesdec      xmm7, xmm11
 777                             	        aesdec      xmm8, xmm11
 778                             	DLAST_8:
 779                             	        add         rsi, 128
 780                             	        aesdeclast  xmm1, xmm12
 781                             	        aesdeclast  xmm2, xmm12
 782                             	        aesdeclast  xmm3, xmm12
 783                             	        aesdeclast  xmm4, xmm12
 784                             	        aesdeclast  xmm5, xmm12
 785                             	        aesdeclast  xmm6, xmm12
 786                             	        aesdeclast  xmm7, xmm12
 787                             	        aesdeclast  xmm8, xmm12
 788                             	        movdqu      xmm10, [rdi]
 789                             	        movdqu      xmm11, 16[rdi]
 790                             	        movdqu      xmm12, 32[rdi]
 791                             	        movdqu      xmm13, 48[rdi]
 792                             	        pxor        xmm1, xmm9
 793                             	        pxor        xmm2, xmm10
 794                             	        pxor        xmm3, xmm11
 795                             	        pxor        xmm4, xmm12
 796                             	        pxor        xmm5, xmm13
 797                             	        movdqu      xmm10, 64[rdi]
 798                             	        movdqu      xmm11, 80[rdi]
 799                             	        movdqu      xmm12, 96[rdi]
 800                             	        movdqu      xmm9, 112[rdi]
 801                             	        pxor        xmm6, xmm10
 802                             	        pxor        xmm7, xmm11
 803                             	        pxor        xmm8, xmm12
 804                             	        movdqu      [rsi], xmm1
 805                             	        movdqu      16[rsi], xmm2
 806                             	        movdqu      32[rsi], xmm3
 807                             	        movdqu      48[rsi], xmm4
 808                             	        movdqu      64[rsi], xmm5
 809                             	        movdqu      80[rsi], xmm6
 810                             	        movdqu      96[rsi], xmm7
 811                             	        movdqu      112[rsi], xmm8
 812                             	        add         rdi, 128
 813                             	        dec         rcx
 814                             	        jne         DLOOP_8
 815                             	        add         rsi, 128
 816                             	DREMAINDER_8:
 817                             	        cmp         r10, 0 
 818                             	        je          DEND_8
 819                             	DLOOP_8_2:
 820                             	        movdqu      xmm1, [rdi]
 821                             	        movdqa      xmm10, xmm1
 822                             	        add         rdi, 16
 823                             	        pxor        xmm1, [r8]
 824                             	        movdqu      xmm2, 160[r8]
 825                             	        cmp         r9d, 12
 826                             	        aesdec      xmm1, 16[r8]
 827                             	        aesdec      xmm1, 32[r8]
 828                             	        aesdec      xmm1, 48[r8]
 829                             	        aesdec      xmm1, 64[r8]
 830                             	        aesdec      xmm1, 80[r8]
 831                             	        aesdec      xmm1, 96[r8]
 832                             	        aesdec      xmm1, 112[r8]
 833                             	        aesdec      xmm1, 128[r8]
 834                             	        aesdec      xmm1, 144[r8]
 835                             	        jb          DLAST_8_2
 836                             	        movdqu      xmm2, 192[r8]
 837                             	        cmp         r9d, 14
 838                             	        aesdec      xmm1, 160[r8]
 839                             	        aesdec      xmm1, 176[r8]
 840                             	        jb          DLAST_8_2
 841                             	        movdqu      xmm2, 224[r8]
 842                             	        aesdec      xmm1, 192[r8]
 843                             	        aesdec      xmm1, 208[r8]
 844                             	DLAST_8_2:
 845                             	        aesdeclast  xmm1, xmm2
 846                             	        pxor        xmm1, xmm9
 847                             	        movdqa      xmm9, xmm10
 848                             	        movdqu      [rsi], xmm1
 849                             	        add         rsi, 16
 850                             	        dec         r10
 851                             	        jne         DLOOP_8_2
 852                             	DEND_8:
 853                             	        ; restore non volatile rdi,rsi
 854                             	        mov         rdi, rax
 855                             	        mov         rsi, r11
 856                             	        ; restore non volatile xmms from stack
 857                             	        movdqa      xmm6, [rsp+0]
 858                             	        movdqa      xmm7, [rsp+16]
 859                             	        movdqa      xmm8, [rsp+32]
 860                             	        movdqa      xmm9, [rsp+48]
 861                             	        movdqa      xmm10, [rsp+64]
 862                             	        movdqa      xmm11, [rsp+80]
 863                             	        movdqa      xmm12, [rsp+96]
 864                             	        movdqa      xmm13, [rsp+112]
 865                             	        add         rsp, 8+8*16 ; 8 = align stack , 8 xmm6-13 16 bytes each
 866                             	        ret
 867                             	AES_CBC_decrypt_by8 ENDP
 868                             	
 869                             	
 870                             	;	/*
 871                             	;	AES_ECB_encrypt[const	,unsigned	char*in
 872                             	;	unsigned	,char*out
 873                             	;	unsigned	,long	length
 874                             	;	const	,unsigned	char*KS
 875                             	;	int	nr]
 876                             	;	*/
 877                             	;	.	globl	AES_ECB_encrypt
 878                             	AES_ECB_encrypt PROC
 879                             	;#	parameter	1:	rdi
 880                             	;#	parameter	2:	rsi
 881                             	;#	parameter	3:	rdx
 882                             	;#	parameter	4:	rcx
 883                             	;#	parameter	5:	r8d
 884                             	
 885                             	; save rdi and rsi to rax and r11, restore before ret
 886                             		mov rax,rdi
 887                             		mov r11,rsi
 888                             	
 889                             	; convert to what we had for att&t convention
 890                             	    mov rdi,rcx
 891                             		mov rsi,rdx
 892                             		mov rdx,r8
 893                             		mov rcx,r9
 894                             		mov r8d,[rsp+40]
 895                             	
 896                             	; on microsoft xmm6-xmm15 are non volaitle, let's save on stack and restore at end
 897                             		sub rsp,8+4*16  ; 8 = align stack , 4 xmm9-12, 16 bytes each
 898                             		movdqa [rsp+0], xmm9
 899                             		movdqa [rsp+16], xmm10
 900                             		movdqa [rsp+32], xmm11
 901                             		movdqa [rsp+48], xmm12
 902                             	
 903                             	
 904                             		mov	r10,rdx
 905                             		shr	rdx,4
 906                             		shl	r10,60
 907                             		je	EECB_NO_PARTS_4
 908                             		add	rdx,1
 909                             	EECB_NO_PARTS_4:
 910                             		mov	r10,rdx
 911                             		shl	r10,62
 912                             		shr	r10,62
 913                             		shr	rdx,2
 914                             		je	EECB_REMAINDER_4
 915                             		sub	rsi,64
 916                             	EECB_LOOP_4:
 917                             		movdqu  xmm1,[rdi]
 918                             		movdqu	xmm2,16[rdi]
 919                             		movdqu	xmm3,32[rdi]
 920                             		movdqu	xmm4,48[rdi]
 921                             		movdqa  xmm9,[rcx]
 922                             		movdqa	xmm10,16[rcx]
 923                             		movdqa	xmm11,32[rcx]
 924                             		movdqa	xmm12,48[rcx]
 925                             		pxor	xmm1,xmm9
 926                             		pxor	xmm2,xmm9
 927                             		pxor	xmm3,xmm9
 928                             		pxor	xmm4,xmm9
 929                             		aesenc	xmm1,xmm10
 930                             		aesenc	xmm2,xmm10
 931                             		aesenc	xmm3,xmm10
 932                             		aesenc	xmm4,xmm10
 933                             		aesenc	xmm1,xmm11
 934                             		aesenc	xmm2,xmm11
 935                             		aesenc	xmm3,xmm11
 936                             		aesenc	xmm4,xmm11
 937                             		aesenc	xmm1,xmm12
 938                             		aesenc	xmm2,xmm12
 939                             		aesenc	xmm3,xmm12
 940                             		aesenc	xmm4,xmm12
 941                             		movdqa	xmm9,64[rcx]
 942                             		movdqa	xmm10,80[rcx]
 943                             		movdqa	xmm11,96[rcx]
 944                             		movdqa	xmm12,112[rcx]
 945                             		aesenc	xmm1,xmm9
 946                             		aesenc	xmm2,xmm9
 947                             		aesenc	xmm3,xmm9
 948                             		aesenc	xmm4,xmm9
 949                             		aesenc	xmm1,xmm10
 950                             		aesenc	xmm2,xmm10
 951                             		aesenc	xmm3,xmm10
 952                             		aesenc	xmm4,xmm10
 953                             		aesenc	xmm1,xmm11
 954                             		aesenc	xmm2,xmm11
 955                             		aesenc	xmm3,xmm11
 956                             		aesenc	xmm4,xmm11
 957                             		aesenc	xmm1,xmm12
 958                             		aesenc	xmm2,xmm12
 959                             		aesenc	xmm3,xmm12
 960                             		aesenc	xmm4,xmm12
 961                             		movdqa	xmm9,128[rcx]
 962                             		movdqa	xmm10,144[rcx]
 963                             		movdqa	xmm11,160[rcx]
 964                             		cmp	r8d,12
 965                             		aesenc	xmm1,xmm9
 966                             		aesenc	xmm2,xmm9
 967                             		aesenc	xmm3,xmm9
 968                             		aesenc	xmm4,xmm9
 969                             		aesenc	xmm1,xmm10
 970                             		aesenc	xmm2,xmm10
 971                             		aesenc	xmm3,xmm10
 972                             		aesenc	xmm4,xmm10
 973                             		jb	EECB_LAST_4
 974                             		movdqa	xmm9,160[rcx]
 975                             		movdqa	xmm10,176[rcx]
 976                             		movdqa	xmm11,192[rcx]
 977                             		cmp	r8d,14
 978                             		aesenc	xmm1,xmm9
 979                             		aesenc	xmm2,xmm9
 980                             		aesenc	xmm3,xmm9
 981                             		aesenc	xmm4,xmm9
 982                             		aesenc	xmm1,xmm10
 983                             		aesenc	xmm2,xmm10
 984                             		aesenc	xmm3,xmm10
 985                             		aesenc	xmm4,xmm10
 986                             		jb	EECB_LAST_4
 987                             		movdqa	xmm9,192[rcx]
 988                             		movdqa	xmm10,208[rcx]
 989                             		movdqa	xmm11,224[rcx]
 990                             		aesenc	xmm1,xmm9
 991                             		aesenc	xmm2,xmm9
 992                             		aesenc	xmm3,xmm9
 993                             		aesenc	xmm4,xmm9
 994                             		aesenc	xmm1,xmm10
 995                             		aesenc	xmm2,xmm10
 996                             		aesenc	xmm3,xmm10
 997                             		aesenc	xmm4,xmm10
 998                             	EECB_LAST_4:
 999                             		add	rdi,64
 1000                             		add	rsi,64
 1001                             		dec	rdx
 1002                             		aesenclast	xmm1,xmm11
 1003                             		aesenclast	xmm2,xmm11
 1004                             		aesenclast	xmm3,xmm11
 1005                             		aesenclast	xmm4,xmm11
 1006                             		movdqu	[rsi],xmm1
 1007                             		movdqu	16[rsi],xmm2
 1008                             		movdqu	32[rsi],xmm3
 1009                             		movdqu	48[rsi],xmm4
 1010                             		jne	EECB_LOOP_4
 1011                             		add	rsi,64
 1012                             	EECB_REMAINDER_4:
 1013                             		cmp	r10,0
 1014                             		je	EECB_END_4
 1015                             	EECB_LOOP_4_2:
 1016                             		movdqu  xmm1,[rdi]
 1017                             		add	rdi,16
 1018                             		pxor	xmm1,[rcx]
 1019                             		movdqu	xmm2,160[rcx]
 1020                             		aesenc	xmm1,16[rcx]
 1021                             		aesenc	xmm1,32[rcx]
 1022                             		aesenc	xmm1,48[rcx]
 1023                             		aesenc	xmm1,64[rcx]
 1024                             		aesenc	xmm1,80[rcx]
 1025                             		aesenc	xmm1,96[rcx]
 1026                             		aesenc	xmm1,112[rcx]
 1027                             		aesenc	xmm1,128[rcx]
 1028                             		aesenc	xmm1,144[rcx]
 1029                             		cmp	r8d,12
 1030                             		jb	EECB_LAST_4_2
 1031                             		movdqu	xmm2,192[rcx]
 1032                             		aesenc	xmm1,160[rcx]
 1033                             		aesenc	xmm1,176[rcx]
 1034                             		cmp	r8d,14
 1035                             		jb	EECB_LAST_4_2
 1036                             		movdqu	xmm2,224[rcx]
 1037                             		aesenc	xmm1,192[rcx]
 1038                             		aesenc	xmm1,208[rcx]
 1039                             	EECB_LAST_4_2:
 1040                             		aesenclast	xmm1,xmm2
 1041                             		movdqu	[rsi],xmm1
 1042                             		add	rsi,16
 1043                             		dec	r10
 1044                             		jne	EECB_LOOP_4_2
 1045                             	EECB_END_4:
 1046                             		; restore non volatile rdi,rsi
 1047                             		mov rdi,rax
 1048                             		mov rsi,r11
 1049                             		; restore non volatile xmms from stack
 1050                             		movdqa xmm9, [rsp+0]
 1051                             		movdqa xmm10, [rsp+16]
 1052                             		movdqa xmm11, [rsp+32]
 1053                             		movdqa xmm12, [rsp+48]
 1054                             		add rsp,8+4*16 ; 8 = align stack , 4 xmm9-12 16 bytes each
 1055                             		ret
 1056                             	AES_ECB_encrypt ENDP
 1057                             	
 1058                             	;	/*
 1059                             	;	AES_ECB_decrypt[const	,unsigned	char*in
 1060                             	;	unsigned	,char*out
 1061                             	;	unsigned	,long	length
 1062                             	;	const	,unsigned	char*KS
 1063                             	;	int	nr]
 1064                             	;	*/
 1065                             	;	.	globl	AES_ECB_decrypt
 1066                             	AES_ECB_decrypt PROC
 1067                             	;#	parameter	1:	rdi
 1068                             	;#	parameter	2:	rsi
 1069                             	;#	parameter	3:	rdx
 1070                             	;#	parameter	4:	rcx
 1071                             	;#	parameter	5:	r8d
 1072                             	
 1073                             	; save rdi and rsi to rax and r11, restore before ret
 1074                             		mov rax,rdi
 1075                             		mov r11,rsi
 1076                             	
 1077                             	; convert to what we had for att&t convention
 1078                             		mov rdi,rcx
 1079                             		mov rsi,rdx
 1080                             		mov rdx,r8
 1081                             		mov rcx,r9
 1082                             		mov r8d,[rsp+40]
 1083                             	
 1084                             	; on microsoft xmm6-xmm15 are non volaitle, let's save on stack and restore at end
 1085                             		sub rsp,8+4*16  ; 8 = align stack , 4 xmm9-12, 16 bytes each
 1086                             		movdqa [rsp+0], xmm9
 1087                             		movdqa [rsp+16], xmm10
 1088                             		movdqa [rsp+32], xmm11
 1089                             		movdqa [rsp+48], xmm12
 1090                             	
 1091                             		mov	r10,rdx
 1092                             		shr	rdx,4
 1093                             		shl	r10,60
 1094                             		je	DECB_NO_PARTS_4
 1095                             		add	rdx,1
 1096                             	DECB_NO_PARTS_4:
 1097                             		mov	r10,rdx
 1098                             		shl	r10,62
 1099                             		shr	r10,62
 1100                             		shr	rdx,2
 1101                             		je	DECB_REMAINDER_4
 1102                             		sub	rsi,64
 1103                             	DECB_LOOP_4:
 1104                             		movdqu  xmm1,[rdi]
 1105                             		movdqu	xmm2,16[rdi]
 1106                             		movdqu	xmm3,32[rdi]
 1107                             		movdqu	xmm4,48[rdi]
 1108                             		movdqa  xmm9,[rcx]
 1109                             		movdqa	xmm10,16[rcx]
 1110                             		movdqa	xmm11,32[rcx]
 1111                             		movdqa	xmm12,48[rcx]
 1112                             		pxor	xmm1,xmm9
 1113                             		pxor	xmm2,xmm9
 1114                             		pxor	xmm3,xmm9
 1115                             		pxor	xmm4,xmm9
 1116                             		aesdec	xmm1,xmm10
 1117                             		aesdec	xmm2,xmm10
 1118                             		aesdec	xmm3,xmm10
 1119                             		aesdec	xmm4,xmm10
 1120                             		aesdec	xmm1,xmm11
 1121                             		aesdec	xmm2,xmm11
 1122                             		aesdec	xmm3,xmm11
 1123                             		aesdec	xmm4,xmm11
 1124                             		aesdec	xmm1,xmm12
 1125                             		aesdec	xmm2,xmm12
 1126                             		aesdec	xmm3,xmm12
 1127                             		aesdec	xmm4,xmm12
 1128                             		movdqa	xmm9,64[rcx]
 1129                             		movdqa	xmm10,80[rcx]
 1130                             		movdqa	xmm11,96[rcx]
 1131                             		movdqa	xmm12,112[rcx]
 1132                             		aesdec	xmm1,xmm9
 1133                             		aesdec	xmm2,xmm9
 1134                             		aesdec	xmm3,xmm9
 1135                             		aesdec	xmm4,xmm9
 1136                             		aesdec	xmm1,xmm10
 1137                             		aesdec	xmm2,xmm10
 1138                             		aesdec	xmm3,xmm10
 1139                             		aesdec	xmm4,xmm10
 1140                             		aesdec	xmm1,xmm11
 1141                             		aesdec	xmm2,xmm11
 1142                             		aesdec	xmm3,xmm11
 1143                             		aesdec	xmm4,xmm11
 1144                             		aesdec	xmm1,xmm12
 1145                             		aesdec	xmm2,xmm12
 1146                             		aesdec	xmm3,xmm12
 1147                             		aesdec	xmm4,xmm12
 1148                             		movdqa	xmm9,128[rcx]
 1149                             		movdqa	xmm10,144[rcx]
 1150                             		movdqa	xmm11,160[rcx]
 1151                             		cmp	r8d,12
 1152                             		aesdec	xmm1,xmm9
 1153                             		aesdec	xmm2,xmm9
 1154                             		aesdec	xmm3,xmm9
 1155                             		aesdec	xmm4,xmm9
 1156                             		aesdec	xmm1,xmm10
 1157                             		aesdec	xmm2,xmm10
 1158                             		aesdec	xmm3,xmm10
 1159                             		aesdec	xmm4,xmm10
 1160                             		jb	DECB_LAST_4
 1161                             		movdqa	xmm9,160[rcx]
 1162                             		movdqa	xmm10,176[rcx]
 1163                             		movdqa	xmm11,192[rcx]
 1164                             		cmp	r8d,14
 1165                             		aesdec	xmm1,xmm9
 1166                             		aesdec	xmm2,xmm9
 1167                             		aesdec	xmm3,xmm9
 1168                             		aesdec	xmm4,xmm9
 1169                             		aesdec	xmm1,xmm10
 1170                             		aesdec	xmm2,xmm10
 1171                             		aesdec	xmm3,xmm10
 1172                             		aesdec	xmm4,xmm10
 1173                             		jb	DECB_LAST_4
 1174                             		movdqa	xmm9,192[rcx]
 1175                             		movdqa	xmm10,208[rcx]
 1176                             		movdqa	xmm11,224[rcx]
 1177                             		aesdec	xmm1,xmm9
 1178                             		aesdec	xmm2,xmm9
 1179                             		aesdec	xmm3,xmm9
 1180                             		aesdec	xmm4,xmm9
 1181                             		aesdec	xmm1,xmm10
 1182                             		aesdec	xmm2,xmm10
 1183                             		aesdec	xmm3,xmm10
 1184                             		aesdec	xmm4,xmm10
 1185                             	DECB_LAST_4:
 1186                             		add	rdi,64
 1187                             		add	rsi,64
 1188                             		dec	rdx
 1189                             		aesdeclast	xmm1,xmm11
 1190                             		aesdeclast	xmm2,xmm11
 1191                             		aesdeclast	xmm3,xmm11
 1192                             		aesdeclast	xmm4,xmm11
 1193                             		movdqu	[rsi],xmm1
 1194                             		movdqu	16[rsi],xmm2
 1195                             		movdqu	32[rsi],xmm3
 1196                             		movdqu	48[rsi],xmm4
 1197                             		jne	DECB_LOOP_4
 1198                             		add	rsi,64
 1199                             	DECB_REMAINDER_4:
 1200                             		cmp	r10,0
 1201                             		je	DECB_END_4
 1202                             	DECB_LOOP_4_2:
 1203                             		movdqu  xmm1,[rdi]
 1204                             		add	rdi,16
 1205                             		pxor	xmm1,[rcx]
 1206                             		movdqu	xmm2,160[rcx]
 1207                             		cmp	r8d,12
 1208                             		aesdec	xmm1,16[rcx]
 1209                             		aesdec	xmm1,32[rcx]
 1210                             		aesdec	xmm1,48[rcx]
 1211                             		aesdec	xmm1,64[rcx]
 1212                             		aesdec	xmm1,80[rcx]
 1213                             		aesdec	xmm1,96[rcx]
 1214                             		aesdec	xmm1,112[rcx]
 1215                             		aesdec	xmm1,128[rcx]
 1216                             		aesdec	xmm1,144[rcx]
 1217                             		jb	DECB_LAST_4_2
 1218                             		cmp	r8d,14
 1219                             		movdqu	xmm2,192[rcx]
 1220                             		aesdec	xmm1,160[rcx]
 1221                             		aesdec	xmm1,176[rcx]
 1222                             		jb	DECB_LAST_4_2
 1223                             		movdqu	xmm2,224[rcx]
 1224                             		aesdec	xmm1,192[rcx]
 1225                             		aesdec	xmm1,208[rcx]
 1226                             	DECB_LAST_4_2:
 1227                             		aesdeclast	xmm1,xmm2
 1228                             		movdqu	[rsi],xmm1
 1229                             		add	rsi,16
 1230                             		dec	r10
 1231                             		jne	DECB_LOOP_4_2
 1232                             	DECB_END_4:
 1233                             		; restore non volatile rdi,rsi
 1234                             		mov rdi,rax
 1235                             		mov rsi,r11
 1236                             		; restore non volatile xmms from stack
 1237                             		movdqa xmm9, [rsp+0]
 1238                             		movdqa xmm10, [rsp+16]
 1239                             		movdqa xmm11, [rsp+32]
 1240                             		movdqa xmm12, [rsp+48]
 1241                             		add rsp,8+4*16 ; 8 = align stack , 4 xmm9-12 16 bytes each
 1242                             		ret
 1243                             	AES_ECB_decrypt ENDP
 1244                             	
 1245                             	
 1246                             	
 1247                             	;	/*
 1248                             	;	void	,AES_128_Key_Expansion[const	unsigned	char*userkey
 1249                             	;	unsigned	char*key_schedule]/
 1250                             	;	*/
 1251                             	;	.	align	16,0x90
 1252                             	;	.	globl	AES_128_Key_Expansion
 1253                             	AES_128_Key_Expansion PROC
 1254                             	;#	parameter	1:	rdi
 1255                             	;#	parameter	2:	rsi
 1256                             	
 1257                             	; save rdi and rsi to rax and r11, restore before ret
 1258                             		mov rax,rdi
 1259                             		mov r11,rsi
 1260                             	
 1261                             	; convert to what we had for att&t convention
 1262                             		mov rdi,rcx
 1263                             		mov rsi,rdx
 1264                             	
 1265                             		mov	dword ptr 240[rsi],10
 1266                             	
 1267                             		movdqu	xmm1,[rdi]
 1268                             		movdqa	[rsi],xmm1
 1269                             	
 1270                             	
 1271                             	ASSISTS:
 1272                             		aeskeygenassist	xmm2,xmm1,1
 1273                             		call	PREPARE_ROUNDKEY_128
 1274                             		movdqa	16[rsi],xmm1
 1275                             	
 1276                             		aeskeygenassist	xmm2,xmm1,2
 1277                             		call	PREPARE_ROUNDKEY_128
 1278                             		movdqa	32[rsi],xmm1
 1279                             	
 1280                             		aeskeygenassist	xmm2,xmm1,4
 1281                             		call	PREPARE_ROUNDKEY_128
 1282                             		movdqa	48[rsi],xmm1
 1283                             	
 1284                             		aeskeygenassist	xmm2,xmm1,8
 1285                             		call	PREPARE_ROUNDKEY_128
 1286                             		movdqa	64[rsi],xmm1
 1287                             	
 1288                             		aeskeygenassist	xmm2,xmm1,16
 1289                             		call	PREPARE_ROUNDKEY_128
 1290                             		movdqa	80[rsi],xmm1
 1291                             	
 1292                             		aeskeygenassist	xmm2,xmm1,32
 1293                             		call	PREPARE_ROUNDKEY_128
 1294                             		movdqa	96[rsi],xmm1
 1295                             	
 1296                             		aeskeygenassist	xmm2,xmm1,64
 1297                             		call	PREPARE_ROUNDKEY_128
 1298                             		movdqa	112[rsi],xmm1
 1299                             		aeskeygenassist	xmm2,xmm1,80h
 1300                             		call	PREPARE_ROUNDKEY_128
 1301                             		movdqa	128[rsi],xmm1
 1302                             		aeskeygenassist	xmm2,xmm1,1bh
 1303                             		call	PREPARE_ROUNDKEY_128
 1304                             		movdqa	144[rsi],xmm1
 1305                             		aeskeygenassist	xmm2,xmm1,36h
 1306                             		call	PREPARE_ROUNDKEY_128
 1307                             		movdqa	160[rsi],xmm1
 1308                             		; restore non volatile rdi,rsi
 1309                             		mov rdi,rax
 1310                             		mov rsi,r11
 1311                             		ret
 1312                             	
 1313                             	PREPARE_ROUNDKEY_128:
 1314                             		pshufd	xmm2,xmm2,255
 1315                             		movdqa	xmm3,xmm1
 1316                             		pslldq	xmm3,4
 1317                             		pxor	xmm1,xmm3
 1318                             		pslldq	xmm3,4
 1319                             		pxor	xmm1,xmm3
 1320                             		pslldq	xmm3,4
 1321                             		pxor	xmm1,xmm3
 1322                             		pxor	xmm1,xmm2
 1323                             		ret
 1324                             	AES_128_Key_Expansion ENDP
 1325                             	
 1326                             	;	/*
 1327                             	;	void	,AES_192_Key_Expansion[const	unsigned	char*userkey
 1328                             	;	unsigned	char*key]
 1329                             	;	*/
 1330                             	;	.	globl	AES_192_Key_Expansion
 1331                             	AES_192_Key_Expansion PROC
 1332                             	;#	parameter	1:	rdi
 1333                             	;#	parameter	2:	rsi
 1334                             	
 1335                             	; save rdi and rsi to rax and r11, restore before ret
 1336                             		mov rax,rdi
 1337                             		mov r11,rsi
 1338                             	
 1339                             	; convert to what we had for att&t convention
 1340                             	    mov rdi,rcx
 1341                             		mov rsi,rdx
 1342                             	
 1343                             	; on microsoft xmm6-xmm15 are non volaitle, let's save on stack and restore at end
 1344                             		sub rsp,8+1*16  ; 8 = align stack , 1 xmm6, 16 bytes each
 1345                             		movdqa [rsp+0], xmm6
 1346                             	
 1347                             		movdqu  xmm1,[rdi]
 1348                             		movq	xmm3,qword ptr 16[rdi]
 1349                             		movdqa	[rsi],xmm1
 1350                             		movdqa	xmm5,xmm3
 1351                             	
 1352                             		aeskeygenassist	xmm2,xmm3,1h
 1353                             		call	PREPARE_ROUNDKEY_192
 1354                             		shufpd	xmm5,xmm1,0
 1355                             		movdqa	16[rsi],xmm5
 1356                             		movdqa	xmm6,xmm1
 1357                             		shufpd	xmm6,xmm3,1
 1358                             		movdqa	32[rsi],xmm6
 1359                             	
 1360                             		aeskeygenassist	xmm2,xmm3,2h
 1361                             		call	PREPARE_ROUNDKEY_192
 1362                             		movdqa	48[rsi],xmm1
 1363                             		movdqa	xmm5,xmm3
 1364                             	
 1365                             		aeskeygenassist	xmm2,xmm3,4h
 1366                             		call	PREPARE_ROUNDKEY_192
 1367                             		shufpd	xmm5,xmm1,0
 1368                             		movdqa	64[rsi],xmm5
 1369                             		movdqa	xmm6,xmm1
 1370                             		shufpd	xmm6,xmm3,1
 1371                             		movdqa	80[rsi],xmm6
 1372                             	
 1373                             		aeskeygenassist	xmm2,xmm3,8h
 1374                             		call	PREPARE_ROUNDKEY_192
 1375                             		movdqa	96[rsi],xmm1
 1376                             		movdqa	xmm5,xmm3
 1377                             	
 1378                             		aeskeygenassist	xmm2,xmm3,10h
 1379                             		call	PREPARE_ROUNDKEY_192
 1380                             		shufpd	xmm5,xmm1,0
 1381                             		movdqa	112[rsi],xmm5
 1382                             		movdqa	xmm6,xmm1
 1383                             		shufpd	xmm6,xmm3,1
 1384                             		movdqa	128[rsi],xmm6
 1385                             	
 1386                             		aeskeygenassist	xmm2,xmm3,20h
 1387                             		call	PREPARE_ROUNDKEY_192
 1388                             		movdqa	144[rsi],xmm1
 1389                             		movdqa	xmm5,xmm3
 1390                             	
 1391                             		aeskeygenassist	xmm2,xmm3,40h
 1392                             		call	PREPARE_ROUNDKEY_192
 1393                             		shufpd	xmm5,xmm1,0
 1394                             		movdqa	160[rsi],xmm5
 1395                             		movdqa	xmm6,xmm1
 1396                             		shufpd	xmm6,xmm3,1
 1397                             		movdqa	176[rsi],xmm6
 1398                             	
 1399                             		aeskeygenassist	xmm2,xmm3,80h
 1400                             		call	PREPARE_ROUNDKEY_192
 1401                             		movdqa	192[rsi],xmm1
 1402                             		movdqa	208[rsi],xmm3
 1403                             		; restore non volatile rdi,rsi
 1404                             		mov rdi,rax
 1405                             		mov rsi,r11
 1406                             	; restore non volatile xmms from stack
 1407                             		movdqa xmm6, [rsp+0]
 1408                             		add rsp,8+1*16 ; 8 = align stack , 1 xmm6 16 bytes each
 1409                             		ret
 1410                             	
 1411                             	PREPARE_ROUNDKEY_192:
 1412                             		pshufd	xmm2,xmm2,55h
 1413                             		movdqu	xmm4,xmm1
 1414                             		pslldq	xmm4,4
 1415                             		pxor	xmm1,xmm4
 1416                             	
 1417                             		pslldq	xmm4,4
 1418                             		pxor	xmm1,xmm4
 1419                             		pslldq	xmm4,4
 1420                             		pxor	xmm1,xmm4
 1421                             		pxor	xmm1,xmm2
 1422                             		pshufd	xmm2,xmm1,0ffh
 1423                             		movdqu	xmm4,xmm3
 1424                             		pslldq	xmm4,4
 1425                             		pxor	xmm3,xmm4
 1426                             		pxor	xmm3,xmm2
 1427                             		ret
 1428                             	AES_192_Key_Expansion ENDP
 1429                             	
 1430                             	;	/*
 1431                             	;	void	,AES_256_Key_Expansion[const	unsigned	char*userkey
 1432                             	;	unsigned	char*key]
 1433                             	;	*/
 1434                             	;	.	globl	AES_256_Key_Expansion
 1435                             	AES_256_Key_Expansion PROC
 1436                             	;#	parameter	1:	rdi
 1437                             	;#	parameter	2:	rsi
 1438                             	
 1439                             	; save rdi and rsi to rax and r11, restore before ret
 1440                             		mov rax,rdi
 1441                             		mov r11,rsi
 1442                             	
 1443                             	; convert to what we had for att&t convention
 1444                             	    mov rdi,rcx
 1445                             		mov rsi,rdx
 1446                             	
 1447                             		movdqu  xmm1,[rdi]
 1448                             		movdqu	xmm3,16[rdi]
 1449                             		movdqa	[rsi],xmm1
 1450                             		movdqa	16[rsi],xmm3
 1451                             	
 1452                             		aeskeygenassist	xmm2,xmm3,1h
 1453                             		call	MAKE_RK256_a
 1454                             		movdqa	32[rsi],xmm1
 1455                             		aeskeygenassist	xmm2,xmm1,0h
 1456                             		call	MAKE_RK256_b
 1457                             		movdqa	48[rsi],xmm3
 1458                             		aeskeygenassist	xmm2,xmm3,2h
 1459                             		call	MAKE_RK256_a
 1460                             		movdqa	64[rsi],xmm1
 1461                             		aeskeygenassist	xmm2,xmm1,0h
 1462                             		call	MAKE_RK256_b
 1463                             		movdqa	80[rsi],xmm3
 1464                             		aeskeygenassist	xmm2,xmm3,4h
 1465                             		call	MAKE_RK256_a
 1466                             		movdqa	96[rsi],xmm1
 1467                             		aeskeygenassist	xmm2,xmm1,0h
 1468                             		call	MAKE_RK256_b
 1469                             		movdqa	112[rsi],xmm3
 1470                             		aeskeygenassist	xmm2,xmm3,8h
 1471                             		call	MAKE_RK256_a
 1472                             		movdqa	128[rsi],xmm1
 1473                             		aeskeygenassist	xmm2,xmm1,0h
 1474                             		call	MAKE_RK256_b
 1475                             		movdqa	144[rsi],xmm3
 1476                             		aeskeygenassist	xmm2,xmm3,10h
 1477                             		call	MAKE_RK256_a
 1478                             		movdqa	160[rsi],xmm1
 1479                             		aeskeygenassist	xmm2,xmm1,0h
 1480                             		call	MAKE_RK256_b
 1481                             		movdqa	176[rsi],xmm3
 1482                             		aeskeygenassist	xmm2,xmm3,20h
 1483                             		call	MAKE_RK256_a
 1484                             		movdqa	192[rsi],xmm1
 1485                             	
 1486                             		aeskeygenassist	xmm2,xmm1,0h
 1487                             		call	MAKE_RK256_b
 1488                             		movdqa	208[rsi],xmm3
 1489                             		aeskeygenassist	xmm2,xmm3,40h
 1490                             		call	MAKE_RK256_a
 1491                             		movdqa	224[rsi],xmm1
 1492                             	
 1493                             		; restore non volatile rdi,rsi
 1494                             		mov rdi,rax
 1495                             		mov rsi,r11
 1496                             		ret
 1497                             	AES_256_Key_Expansion ENDP
 1498                             	
 1499                             	MAKE_RK256_a:
 1500                             		pshufd	xmm2,xmm2,0ffh
 1501                             		movdqa	xmm4,xmm1
 1502                             		pslldq	xmm4,4
 1503                             		pxor	xmm1,xmm4
 1504                             		pslldq	xmm4,4
 1505                             		pxor	xmm1,xmm4
 1506                             		pslldq	xmm4,4
 1507                             		pxor	xmm1,xmm4
 1508                             		pxor	xmm1,xmm2
 1509                             		ret
 1510                             	
 1511                             	MAKE_RK256_b:
 1512                             		pshufd	xmm2,xmm2,0aah
 1513                             		movdqa	xmm4,xmm3
 1514                             		pslldq	xmm4,4
 1515                             		pxor	xmm3,xmm4
 1516                             		pslldq	xmm4,4
 1517                             		pxor	xmm3,xmm4
 1518                             		pslldq	xmm4,4
 1519                             		pxor	xmm3,xmm4
 1520                             		pxor	xmm3,xmm2
 1521                             		ret
 1522                             	
 1523                             	
 1524                             	IF fips_version GE 2
 1525                             	  fipsAh ENDS
 1526                             	ELSE
 1527                             	  _text ENDS
 1528                             	ENDIF
 1529                             	
