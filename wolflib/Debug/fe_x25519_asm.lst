   1                             	# 1 "../src/wolfcrypt/src/fe_x25519_asm.S"
   1                             	/* fe_x25519_asm
   0                             	
   0                             	
   0                             	
   2                             	 *
   3                             	 * Copyright (C) 2006-2021 wolfSSL Inc.
   4                             	 *
   5                             	 * This file is part of wolfSSL.
   6                             	 *
   7                             	 * wolfSSL is free software; you can redistribute it and/or modify
   8                             	 * it under the terms of the GNU General Public License as published by
   9                             	 * the Free Software Foundation; either version 2 of the License, or
  10                             	 * (at your option) any later version.
  11                             	 *
  12                             	 * wolfSSL is distributed in the hope that it will be useful,
  13                             	 * but WITHOUT ANY WARRANTY; without even the implied warranty of
  14                             	 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  15                             	 * GNU General Public License for more details.
  16                             	 *
  17                             	 * You should have received a copy of the GNU General Public License
  18                             	 * along with this program; if not, write to the Free Software
  19                             	 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1335, USA
  20                             	 */
  21                             	
  22                             	#ifndef HAVE_INTEL_AVX1
  23                             	#define HAVE_INTEL_AVX1
  24                             	#endif /* HAVE_INTEL_AVX1 */
  25                             	#ifndef NO_AVX2_SUPPORT
  26                             	#define HAVE_INTEL_AVX2
  27                             	#endif /* NO_AVX2_SUPPORT */
  28                             	
  29                             	#ifndef __APPLE__
  30                             	.text
  31                             	.globl	fe_init
  33                             	.align	16
  34                             	fe_init:
  35                             	#else
  36                             	.section	__TEXT,__text
  37                             	.globl	_fe_init
  38                             	.p2align	4
  39                             	_fe_init:
  40                             	#endif /* __APPLE__ */
  41                             	#ifdef HAVE_INTEL_AVX2
  42                             	#ifndef __APPLE__
  43                             	        movq	cpuFlagsSet@GOTPCREL(%rip), %rax
  44                             	        movl	(%rax), %eax
  45                             	#else
  46                             	        movl	_cpuFlagsSet(%rip), %eax
  47                             	#endif /* __APPLE__ */
  48                             	        testl	%eax, %eax
  49                             	        je	L_fe_init_get_flags
  50                             	        repz retq
  51                             	L_fe_init_get_flags:
  52                             	#ifndef __APPLE__
  53                             	        callq	cpuid_get_flags@plt
  54                             	#else
  55                             	        callq	_cpuid_get_flags
  56                             	#endif /* __APPLE__ */
  57                             	#ifndef __APPLE__
  58                             	        movq	intelFlags@GOTPCREL(%rip), %rdx
  59                             	        movl	%eax, (%rdx)
  60                             	#else
  61                             	        movl	%eax, _intelFlags(%rip)
  62                             	#endif /* __APPLE__ */
  63                             	        andl	$0x50, %eax
  64                             	        cmpl	$0x50, %eax
  65                             	        jne	L_fe_init_flags_done
  66                             	#ifndef __APPLE__
  67                             	        movq	fe_mul_avx2@GOTPCREL(%rip), %rax
  68                             	#else
  69                             	        leaq	_fe_mul_avx2(%rip), %rax
  70                             	#endif /* __APPLE__ */
  71                             	#ifndef __APPLE__
  72                             	        movq	fe_mul_p@GOTPCREL(%rip), %rdx
  73                             	        movq	%rax, (%rdx)
  74                             	#else
  75                             	        movq	%rax, _fe_mul_p(%rip)
  76                             	#endif /* __APPLE__ */
  77                             	#ifndef __APPLE__
  78                             	        movq	fe_sq_avx2@GOTPCREL(%rip), %rax
  79                             	#else
  80                             	        leaq	_fe_sq_avx2(%rip), %rax
  81                             	#endif /* __APPLE__ */
  82                             	#ifndef __APPLE__
  83                             	        movq	fe_sq_p@GOTPCREL(%rip), %rdx
  84                             	        movq	%rax, (%rdx)
  85                             	#else
  86                             	        movq	%rax, _fe_sq_p(%rip)
  87                             	#endif /* __APPLE__ */
  88                             	#ifndef __APPLE__
  89                             	        movq	fe_mul121666_avx2@GOTPCREL(%rip), %rax
  90                             	#else
  91                             	        leaq	_fe_mul121666_avx2(%rip), %rax
  92                             	#endif /* __APPLE__ */
  93                             	#ifndef __APPLE__
  94                             	        movq	fe_mul121666_p@GOTPCREL(%rip), %rdx
  95                             	        movq	%rax, (%rdx)
  96                             	#else
  97                             	        movq	%rax, _fe_mul121666_p(%rip)
  98                             	#endif /* __APPLE__ */
  99                             	#ifndef __APPLE__
 100                             	        movq	fe_sq2_avx2@GOTPCREL(%rip), %rax
 101                             	#else
 102                             	        leaq	_fe_sq2_avx2(%rip), %rax
 103                             	#endif /* __APPLE__ */
 104                             	#ifndef __APPLE__
 105                             	        movq	fe_sq2_p@GOTPCREL(%rip), %rdx
 106                             	        movq	%rax, (%rdx)
 107                             	#else
 108                             	        movq	%rax, _fe_sq2_p(%rip)
 109                             	#endif /* __APPLE__ */
 110                             	#ifndef __APPLE__
 111                             	        movq	fe_invert_avx2@GOTPCREL(%rip), %rax
 112                             	#else
 113                             	        leaq	_fe_invert_avx2(%rip), %rax
 114                             	#endif /* __APPLE__ */
 115                             	#ifndef __APPLE__
 116                             	        movq	fe_invert_p@GOTPCREL(%rip), %rdx
 117                             	        movq	%rax, (%rdx)
 118                             	#else
 119                             	        movq	%rax, _fe_invert_p(%rip)
 120                             	#endif /* __APPLE__ */
 121                             	#ifndef __APPLE__
 122                             	        movq	curve25519_avx2@GOTPCREL(%rip), %rax
 123                             	#else
 124                             	        leaq	_curve25519_avx2(%rip), %rax
 125                             	#endif /* __APPLE__ */
 126                             	#ifndef __APPLE__
 127                             	        movq	curve25519_p@GOTPCREL(%rip), %rdx
 128                             	        movq	%rax, (%rdx)
 129                             	#else
 130                             	        movq	%rax, _curve25519_p(%rip)
 131                             	#endif /* __APPLE__ */
 132                             	#ifndef __APPLE__
 133                             	        movq	fe_pow22523_avx2@GOTPCREL(%rip), %rax
 134                             	#else
 135                             	        leaq	_fe_pow22523_avx2(%rip), %rax
 136                             	#endif /* __APPLE__ */
 137                             	#ifndef __APPLE__
 138                             	        movq	fe_pow22523_p@GOTPCREL(%rip), %rdx
 139                             	        movq	%rax, (%rdx)
 140                             	#else
 141                             	        movq	%rax, _fe_pow22523_p(%rip)
 142                             	#endif /* __APPLE__ */
 143                             	#ifndef __APPLE__
 144                             	        movq	fe_ge_to_p2_avx2@GOTPCREL(%rip), %rax
 145                             	#else
 146                             	        leaq	_fe_ge_to_p2_avx2(%rip), %rax
 147                             	#endif /* __APPLE__ */
 148                             	#ifndef __APPLE__
 149                             	        movq	fe_ge_to_p2_p@GOTPCREL(%rip), %rdx
 150                             	        movq	%rax, (%rdx)
 151                             	#else
 152                             	        movq	%rax, _fe_ge_to_p2_p(%rip)
 153                             	#endif /* __APPLE__ */
 154                             	#ifndef __APPLE__
 155                             	        movq	fe_ge_to_p3_avx2@GOTPCREL(%rip), %rax
 156                             	#else
 157                             	        leaq	_fe_ge_to_p3_avx2(%rip), %rax
 158                             	#endif /* __APPLE__ */
 159                             	#ifndef __APPLE__
 160                             	        movq	fe_ge_to_p3_p@GOTPCREL(%rip), %rdx
 161                             	        movq	%rax, (%rdx)
 162                             	#else
 163                             	        movq	%rax, _fe_ge_to_p3_p(%rip)
 164                             	#endif /* __APPLE__ */
 165                             	#ifndef __APPLE__
 166                             	        movq	fe_ge_dbl_avx2@GOTPCREL(%rip), %rax
 167                             	#else
 168                             	        leaq	_fe_ge_dbl_avx2(%rip), %rax
 169                             	#endif /* __APPLE__ */
 170                             	#ifndef __APPLE__
 171                             	        movq	fe_ge_dbl_p@GOTPCREL(%rip), %rdx
 172                             	        movq	%rax, (%rdx)
 173                             	#else
 174                             	        movq	%rax, _fe_ge_dbl_p(%rip)
 175                             	#endif /* __APPLE__ */
 176                             	#ifndef __APPLE__
 177                             	        movq	fe_ge_madd_avx2@GOTPCREL(%rip), %rax
 178                             	#else
 179                             	        leaq	_fe_ge_madd_avx2(%rip), %rax
 180                             	#endif /* __APPLE__ */
 181                             	#ifndef __APPLE__
 182                             	        movq	fe_ge_madd_p@GOTPCREL(%rip), %rdx
 183                             	        movq	%rax, (%rdx)
 184                             	#else
 185                             	        movq	%rax, _fe_ge_madd_p(%rip)
 186                             	#endif /* __APPLE__ */
 187                             	#ifndef __APPLE__
 188                             	        movq	fe_ge_msub_avx2@GOTPCREL(%rip), %rax
 189                             	#else
 190                             	        leaq	_fe_ge_msub_avx2(%rip), %rax
 191                             	#endif /* __APPLE__ */
 192                             	#ifndef __APPLE__
 193                             	        movq	fe_ge_msub_p@GOTPCREL(%rip), %rdx
 194                             	        movq	%rax, (%rdx)
 195                             	#else
 196                             	        movq	%rax, _fe_ge_msub_p(%rip)
 197                             	#endif /* __APPLE__ */
 198                             	#ifndef __APPLE__
 199                             	        movq	fe_ge_add_avx2@GOTPCREL(%rip), %rax
 200                             	#else
 201                             	        leaq	_fe_ge_add_avx2(%rip), %rax
 202                             	#endif /* __APPLE__ */
 203                             	#ifndef __APPLE__
 204                             	        movq	fe_ge_add_p@GOTPCREL(%rip), %rdx
 205                             	        movq	%rax, (%rdx)
 206                             	#else
 207                             	        movq	%rax, _fe_ge_add_p(%rip)
 208                             	#endif /* __APPLE__ */
 209                             	#ifndef __APPLE__
 210                             	        movq	fe_ge_sub_avx2@GOTPCREL(%rip), %rax
 211                             	#else
 212                             	        leaq	_fe_ge_sub_avx2(%rip), %rax
 213                             	#endif /* __APPLE__ */
 214                             	#ifndef __APPLE__
 215                             	        movq	fe_ge_sub_p@GOTPCREL(%rip), %rdx
 216                             	        movq	%rax, (%rdx)
 217                             	#else
 218                             	        movq	%rax, _fe_ge_sub_p(%rip)
 219                             	#endif /* __APPLE__ */
 220                             	L_fe_init_flags_done:
 221                             	#ifndef __APPLE__
 222                             	        movq	cpuFlagsSet@GOTPCREL(%rip), %rdx
 223                             	        movl	$0x1, (%rdx)
 224                             	#else
 225                             	        movl	$0x1, _cpuFlagsSet(%rip)
 226                             	#endif /* __APPLE__ */
 227                             	#endif /* HAVE_INTEL_AVX2 */
 228                             	        repz retq
 229                             	#ifndef __APPLE__
 231                             	#endif /* __APPLE__ */
 232                             	#ifndef __APPLE__
 233                             	.text
 234                             	.globl	fe_frombytes
 236                             	.align	16
 237                             	fe_frombytes:
 238                             	#else
 239                             	.section	__TEXT,__text
 240                             	.globl	_fe_frombytes
 241                             	.p2align	4
 242                             	_fe_frombytes:
 243                             	#endif /* __APPLE__ */
 244                             	        movq	$0x7fffffffffffffff, %r9
 245                             	        movq	(%rsi), %rdx
 246                             	        movq	8(%rsi), %rax
 247                             	        movq	16(%rsi), %rcx
 248                             	        movq	24(%rsi), %r8
 249                             	        andq	%r9, %r8
 250                             	        movq	%rdx, (%rdi)
 251                             	        movq	%rax, 8(%rdi)
 252                             	        movq	%rcx, 16(%rdi)
 253                             	        movq	%r8, 24(%rdi)
 254                             	        repz retq
 255                             	#ifndef __APPLE__
 257                             	#endif /* __APPLE__ */
 258                             	#ifndef __APPLE__
 259                             	.text
 260                             	.globl	fe_tobytes
 262                             	.align	16
 263                             	fe_tobytes:
 264                             	#else
 265                             	.section	__TEXT,__text
 266                             	.globl	_fe_tobytes
 267                             	.p2align	4
 268                             	_fe_tobytes:
 269                             	#endif /* __APPLE__ */
 270                             	        movq	$0x7fffffffffffffff, %r10
 271                             	        movq	(%rsi), %rdx
 272                             	        movq	8(%rsi), %rax
 273                             	        movq	16(%rsi), %rcx
 274                             	        movq	24(%rsi), %r8
 275                             	        addq	$19, %rdx
 276                             	        adcq	$0x00, %rax
 277                             	        adcq	$0x00, %rcx
 278                             	        adcq	$0x00, %r8
 279                             	        shrq	$63, %r8
 280                             	        imulq	$19, %r8, %r9
 281                             	        movq	(%rsi), %rdx
 282                             	        movq	8(%rsi), %rax
 283                             	        movq	16(%rsi), %rcx
 284                             	        movq	24(%rsi), %r8
 285                             	        addq	%r9, %rdx
 286                             	        adcq	$0x00, %rax
 287                             	        adcq	$0x00, %rcx
 288                             	        adcq	$0x00, %r8
 289                             	        andq	%r10, %r8
 290                             	        movq	%rdx, (%rdi)
 291                             	        movq	%rax, 8(%rdi)
 292                             	        movq	%rcx, 16(%rdi)
 293                             	        movq	%r8, 24(%rdi)
 294                             	        repz retq
 295                             	#ifndef __APPLE__
 297                             	#endif /* __APPLE__ */
 298                             	#ifndef __APPLE__
 299                             	.text
 300                             	.globl	fe_1
 302                             	.align	16
 303                             	fe_1:
 304                             	#else
 305                             	.section	__TEXT,__text
 306                             	.globl	_fe_1
 307                             	.p2align	4
 308                             	_fe_1:
 309                             	#endif /* __APPLE__ */
 310                             	        # Set one
 311                             	        movq	$0x01, (%rdi)
 312                             	        movq	$0x00, 8(%rdi)
 313                             	        movq	$0x00, 16(%rdi)
 314                             	        movq	$0x00, 24(%rdi)
 315                             	        repz retq
 316                             	#ifndef __APPLE__
 318                             	#endif /* __APPLE__ */
 319                             	#ifndef __APPLE__
 320                             	.text
 321                             	.globl	fe_0
 323                             	.align	16
 324                             	fe_0:
 325                             	#else
 326                             	.section	__TEXT,__text
 327                             	.globl	_fe_0
 328                             	.p2align	4
 329                             	_fe_0:
 330                             	#endif /* __APPLE__ */
 331                             	        # Set zero
 332                             	        movq	$0x00, (%rdi)
 333                             	        movq	$0x00, 8(%rdi)
 334                             	        movq	$0x00, 16(%rdi)
 335                             	        movq	$0x00, 24(%rdi)
 336                             	        repz retq
 337                             	#ifndef __APPLE__
 339                             	#endif /* __APPLE__ */
 340                             	#ifndef __APPLE__
 341                             	.text
 342                             	.globl	fe_copy
 344                             	.align	16
 345                             	fe_copy:
 346                             	#else
 347                             	.section	__TEXT,__text
 348                             	.globl	_fe_copy
 349                             	.p2align	4
 350                             	_fe_copy:
 351                             	#endif /* __APPLE__ */
 352                             	        # Copy
 353                             	        movq	(%rsi), %rdx
 354                             	        movq	8(%rsi), %rax
 355                             	        movq	16(%rsi), %rcx
 356                             	        movq	24(%rsi), %r8
 357                             	        movq	%rdx, (%rdi)
 358                             	        movq	%rax, 8(%rdi)
 359                             	        movq	%rcx, 16(%rdi)
 360                             	        movq	%r8, 24(%rdi)
 361                             	        repz retq
 362                             	#ifndef __APPLE__
 364                             	#endif /* __APPLE__ */
 365                             	#ifndef __APPLE__
 366                             	.text
 367                             	.globl	fe_sub
 369                             	.align	16
 370                             	fe_sub:
 371                             	#else
 372                             	.section	__TEXT,__text
 373                             	.globl	_fe_sub
 374                             	.p2align	4
 375                             	_fe_sub:
 376                             	#endif /* __APPLE__ */
 377                             	        pushq	%r12
 378                             	        # Sub
 379                             	        movq	(%rsi), %rax
 380                             	        movq	8(%rsi), %rcx
 381                             	        movq	16(%rsi), %r8
 382                             	        movq	24(%rsi), %r9
 383                             	        subq	(%rdx), %rax
 384                             	        movq	$0x00, %r10
 385                             	        sbbq	8(%rdx), %rcx
 386                             	        movq	$-19, %r11
 387                             	        sbbq	16(%rdx), %r8
 388                             	        movq	$0x7fffffffffffffff, %r12
 389                             	        sbbq	24(%rdx), %r9
 390                             	        sbbq	$0x00, %r10
 391                             	        #   Mask the modulus
 392                             	        andq	%r10, %r11
 393                             	        andq	%r10, %r12
 394                             	        #   Add modulus (if underflow)
 395                             	        addq	%r11, %rax
 396                             	        adcq	%r10, %rcx
 397                             	        adcq	%r10, %r8
 398                             	        adcq	%r12, %r9
 399                             	        movq	%rax, (%rdi)
 400                             	        movq	%rcx, 8(%rdi)
 401                             	        movq	%r8, 16(%rdi)
 402                             	        movq	%r9, 24(%rdi)
 403                             	        popq	%r12
 404                             	        repz retq
 405                             	#ifndef __APPLE__
 407                             	#endif /* __APPLE__ */
 408                             	#ifndef __APPLE__
 409                             	.text
 410                             	.globl	fe_add
 412                             	.align	16
 413                             	fe_add:
 414                             	#else
 415                             	.section	__TEXT,__text
 416                             	.globl	_fe_add
 417                             	.p2align	4
 418                             	_fe_add:
 419                             	#endif /* __APPLE__ */
 420                             	        pushq	%r12
 421                             	        # Add
 422                             	        movq	(%rsi), %rax
 423                             	        movq	8(%rsi), %rcx
 424                             	        addq	(%rdx), %rax
 425                             	        movq	16(%rsi), %r8
 426                             	        adcq	8(%rdx), %rcx
 427                             	        movq	24(%rsi), %r10
 428                             	        adcq	16(%rdx), %r8
 429                             	        movq	$-19, %r11
 430                             	        adcq	24(%rdx), %r10
 431                             	        movq	$0x7fffffffffffffff, %r12
 432                             	        movq	%r10, %r9
 433                             	        sarq	$63, %r10
 434                             	        #   Mask the modulus
 435                             	        andq	%r10, %r11
 436                             	        andq	%r10, %r12
 437                             	        #   Sub modulus (if overflow)
 438                             	        subq	%r11, %rax
 439                             	        sbbq	%r10, %rcx
 440                             	        sbbq	%r10, %r8
 441                             	        sbbq	%r12, %r9
 442                             	        movq	%rax, (%rdi)
 443                             	        movq	%rcx, 8(%rdi)
 444                             	        movq	%r8, 16(%rdi)
 445                             	        movq	%r9, 24(%rdi)
 446                             	        popq	%r12
 447                             	        repz retq
 448                             	#ifndef __APPLE__
 450                             	#endif /* __APPLE__ */
 451                             	#ifndef __APPLE__
 452                             	.text
 453                             	.globl	fe_neg
 455                             	.align	16
 456                             	fe_neg:
 457                             	#else
 458                             	.section	__TEXT,__text
 459                             	.globl	_fe_neg
 460                             	.p2align	4
 461                             	_fe_neg:
 462                             	#endif /* __APPLE__ */
 463                             	        movq	$-19, %rdx
 464                             	        movq	$-1, %rax
 465                             	        movq	$-1, %rcx
 466                             	        movq	$0x7fffffffffffffff, %r8
 467                             	        subq	(%rsi), %rdx
 468                             	        sbbq	8(%rsi), %rax
 469                             	        sbbq	16(%rsi), %rcx
 470                             	        sbbq	24(%rsi), %r8
 471                             	        movq	%rdx, (%rdi)
 472                             	        movq	%rax, 8(%rdi)
 473                             	        movq	%rcx, 16(%rdi)
 474                             	        movq	%r8, 24(%rdi)
 475                             	        repz retq
 476                             	#ifndef __APPLE__
 478                             	#endif /* __APPLE__ */
 479                             	#ifndef __APPLE__
 480                             	.text
 481                             	.globl	fe_cmov
 483                             	.align	16
 484                             	fe_cmov:
 485                             	#else
 486                             	.section	__TEXT,__text
 487                             	.globl	_fe_cmov
 488                             	.p2align	4
 489                             	_fe_cmov:
 490                             	#endif /* __APPLE__ */
 491                             	        cmpl	$0x01, %edx
 492                             	        movq	(%rdi), %rcx
 493                             	        movq	8(%rdi), %r8
 494                             	        movq	16(%rdi), %r9
 495                             	        movq	24(%rdi), %r10
 496                             	        cmoveq	(%rsi), %rcx
 497                             	        cmoveq	8(%rsi), %r8
 498                             	        cmoveq	16(%rsi), %r9
 499                             	        cmoveq	24(%rsi), %r10
 500                             	        movq	%rcx, (%rdi)
 501                             	        movq	%r8, 8(%rdi)
 502                             	        movq	%r9, 16(%rdi)
 503                             	        movq	%r10, 24(%rdi)
 504                             	        repz retq
 505                             	#ifndef __APPLE__
 507                             	#endif /* __APPLE__ */
 508                             	#ifndef __APPLE__
 509                             	.text
 510                             	.globl	fe_isnonzero
 512                             	.align	16
 513                             	fe_isnonzero:
 514                             	#else
 515                             	.section	__TEXT,__text
 516                             	.globl	_fe_isnonzero
 517                             	.p2align	4
 518                             	_fe_isnonzero:
 519                             	#endif /* __APPLE__ */
 520                             	        movq	$0x7fffffffffffffff, %r10
 521                             	        movq	(%rdi), %rax
 522                             	        movq	8(%rdi), %rdx
 523                             	        movq	16(%rdi), %rcx
 524                             	        movq	24(%rdi), %r8
 525                             	        addq	$19, %rax
 526                             	        adcq	$0x00, %rdx
 527                             	        adcq	$0x00, %rcx
 528                             	        adcq	$0x00, %r8
 529                             	        shrq	$63, %r8
 530                             	        imulq	$19, %r8, %r9
 531                             	        movq	(%rdi), %rax
 532                             	        movq	8(%rdi), %rdx
 533                             	        movq	16(%rdi), %rcx
 534                             	        movq	24(%rdi), %r8
 535                             	        addq	%r9, %rax
 536                             	        adcq	$0x00, %rdx
 537                             	        adcq	$0x00, %rcx
 538                             	        adcq	$0x00, %r8
 539                             	        andq	%r10, %r8
 540                             	        orq	%rdx, %rax
 541                             	        orq	%rcx, %rax
 542                             	        orq	%r8, %rax
 543                             	        repz retq
 544                             	#ifndef __APPLE__
 546                             	#endif /* __APPLE__ */
 547                             	#ifndef __APPLE__
 548                             	.text
 549                             	.globl	fe_isnegative
 551                             	.align	16
 552                             	fe_isnegative:
 553                             	#else
 554                             	.section	__TEXT,__text
 555                             	.globl	_fe_isnegative
 556                             	.p2align	4
 557                             	_fe_isnegative:
 558                             	#endif /* __APPLE__ */
 559                             	        movq	$0x7fffffffffffffff, %r11
 560                             	        movq	(%rdi), %rdx
 561                             	        movq	8(%rdi), %rcx
 562                             	        movq	16(%rdi), %r8
 563                             	        movq	24(%rdi), %r9
 564                             	        movq	%rdx, %rax
 565                             	        addq	$19, %rdx
 566                             	        adcq	$0x00, %rcx
 567                             	        adcq	$0x00, %r8
 568                             	        adcq	$0x00, %r9
 569                             	        shrq	$63, %r9
 570                             	        imulq	$19, %r9, %r10
 571                             	        addq	%r10, %rax
 572                             	        andq	$0x01, %rax
 573                             	        repz retq
 574                             	#ifndef __APPLE__
 576                             	#endif /* __APPLE__ */
 577                             	#ifndef __APPLE__
 578                             	.text
 579                             	.globl	fe_cmov_table
 581                             	.align	16
 582                             	fe_cmov_table:
 583                             	#else
 584                             	.section	__TEXT,__text
 585                             	.globl	_fe_cmov_table
 586                             	.p2align	4
 587                             	_fe_cmov_table:
 588                             	#endif /* __APPLE__ */
 589                             	        pushq	%r12
 590                             	        pushq	%r13
 591                             	        pushq	%r14
 592                             	        pushq	%r15
 593                             	        movq	%rdx, %rcx
 594                             	        movsbq	%cl, %rax
 595                             	        cdq
 596                             	        xorb	%dl, %al
 597                             	        subb	%dl, %al
 598                             	        movb	%al, %r15b
 599                             	        movq	$0x01, %rax
 600                             	        xorq	%rdx, %rdx
 601                             	        xorq	%r8, %r8
 602                             	        xorq	%r9, %r9
 603                             	        movq	$0x01, %r10
 604                             	        xorq	%r11, %r11
 605                             	        xorq	%r12, %r12
 606                             	        xorq	%r13, %r13
 607                             	        cmpb	$0x01, %r15b
 608                             	        movq	(%rsi), %r14
 609                             	        cmoveq	%r14, %rax
 610                             	        movq	8(%rsi), %r14
 611                             	        cmoveq	%r14, %rdx
 612                             	        movq	16(%rsi), %r14
 613                             	        cmoveq	%r14, %r8
 614                             	        movq	24(%rsi), %r14
 615                             	        cmoveq	%r14, %r9
 616                             	        movq	32(%rsi), %r14
 617                             	        cmoveq	%r14, %r10
 618                             	        movq	40(%rsi), %r14
 619                             	        cmoveq	%r14, %r11
 620                             	        movq	48(%rsi), %r14
 621                             	        cmoveq	%r14, %r12
 622                             	        movq	56(%rsi), %r14
 623                             	        cmoveq	%r14, %r13
 624                             	        cmpb	$2, %r15b
 625                             	        movq	96(%rsi), %r14
 626                             	        cmoveq	%r14, %rax
 627                             	        movq	104(%rsi), %r14
 628                             	        cmoveq	%r14, %rdx
 629                             	        movq	112(%rsi), %r14
 630                             	        cmoveq	%r14, %r8
 631                             	        movq	120(%rsi), %r14
 632                             	        cmoveq	%r14, %r9
 633                             	        movq	128(%rsi), %r14
 634                             	        cmoveq	%r14, %r10
 635                             	        movq	136(%rsi), %r14
 636                             	        cmoveq	%r14, %r11
 637                             	        movq	144(%rsi), %r14
 638                             	        cmoveq	%r14, %r12
 639                             	        movq	152(%rsi), %r14
 640                             	        cmoveq	%r14, %r13
 641                             	        cmpb	$3, %r15b
 642                             	        movq	192(%rsi), %r14
 643                             	        cmoveq	%r14, %rax
 644                             	        movq	200(%rsi), %r14
 645                             	        cmoveq	%r14, %rdx
 646                             	        movq	208(%rsi), %r14
 647                             	        cmoveq	%r14, %r8
 648                             	        movq	216(%rsi), %r14
 649                             	        cmoveq	%r14, %r9
 650                             	        movq	224(%rsi), %r14
 651                             	        cmoveq	%r14, %r10
 652                             	        movq	232(%rsi), %r14
 653                             	        cmoveq	%r14, %r11
 654                             	        movq	240(%rsi), %r14
 655                             	        cmoveq	%r14, %r12
 656                             	        movq	248(%rsi), %r14
 657                             	        cmoveq	%r14, %r13
 658                             	        cmpb	$4, %r15b
 659                             	        movq	288(%rsi), %r14
 660                             	        cmoveq	%r14, %rax
 661                             	        movq	296(%rsi), %r14
 662                             	        cmoveq	%r14, %rdx
 663                             	        movq	304(%rsi), %r14
 664                             	        cmoveq	%r14, %r8
 665                             	        movq	312(%rsi), %r14
 666                             	        cmoveq	%r14, %r9
 667                             	        movq	320(%rsi), %r14
 668                             	        cmoveq	%r14, %r10
 669                             	        movq	328(%rsi), %r14
 670                             	        cmoveq	%r14, %r11
 671                             	        movq	336(%rsi), %r14
 672                             	        cmoveq	%r14, %r12
 673                             	        movq	344(%rsi), %r14
 674                             	        cmoveq	%r14, %r13
 675                             	        cmpb	$5, %r15b
 676                             	        movq	384(%rsi), %r14
 677                             	        cmoveq	%r14, %rax
 678                             	        movq	392(%rsi), %r14
 679                             	        cmoveq	%r14, %rdx
 680                             	        movq	400(%rsi), %r14
 681                             	        cmoveq	%r14, %r8
 682                             	        movq	408(%rsi), %r14
 683                             	        cmoveq	%r14, %r9
 684                             	        movq	416(%rsi), %r14
 685                             	        cmoveq	%r14, %r10
 686                             	        movq	424(%rsi), %r14
 687                             	        cmoveq	%r14, %r11
 688                             	        movq	432(%rsi), %r14
 689                             	        cmoveq	%r14, %r12
 690                             	        movq	440(%rsi), %r14
 691                             	        cmoveq	%r14, %r13
 692                             	        cmpb	$6, %r15b
 693                             	        movq	480(%rsi), %r14
 694                             	        cmoveq	%r14, %rax
 695                             	        movq	488(%rsi), %r14
 696                             	        cmoveq	%r14, %rdx
 697                             	        movq	496(%rsi), %r14
 698                             	        cmoveq	%r14, %r8
 699                             	        movq	504(%rsi), %r14
 700                             	        cmoveq	%r14, %r9
 701                             	        movq	512(%rsi), %r14
 702                             	        cmoveq	%r14, %r10
 703                             	        movq	520(%rsi), %r14
 704                             	        cmoveq	%r14, %r11
 705                             	        movq	528(%rsi), %r14
 706                             	        cmoveq	%r14, %r12
 707                             	        movq	536(%rsi), %r14
 708                             	        cmoveq	%r14, %r13
 709                             	        cmpb	$7, %r15b
 710                             	        movq	576(%rsi), %r14
 711                             	        cmoveq	%r14, %rax
 712                             	        movq	584(%rsi), %r14
 713                             	        cmoveq	%r14, %rdx
 714                             	        movq	592(%rsi), %r14
 715                             	        cmoveq	%r14, %r8
 716                             	        movq	600(%rsi), %r14
 717                             	        cmoveq	%r14, %r9
 718                             	        movq	608(%rsi), %r14
 719                             	        cmoveq	%r14, %r10
 720                             	        movq	616(%rsi), %r14
 721                             	        cmoveq	%r14, %r11
 722                             	        movq	624(%rsi), %r14
 723                             	        cmoveq	%r14, %r12
 724                             	        movq	632(%rsi), %r14
 725                             	        cmoveq	%r14, %r13
 726                             	        cmpb	$8, %r15b
 727                             	        movq	672(%rsi), %r14
 728                             	        cmoveq	%r14, %rax
 729                             	        movq	680(%rsi), %r14
 730                             	        cmoveq	%r14, %rdx
 731                             	        movq	688(%rsi), %r14
 732                             	        cmoveq	%r14, %r8
 733                             	        movq	696(%rsi), %r14
 734                             	        cmoveq	%r14, %r9
 735                             	        movq	704(%rsi), %r14
 736                             	        cmoveq	%r14, %r10
 737                             	        movq	712(%rsi), %r14
 738                             	        cmoveq	%r14, %r11
 739                             	        movq	720(%rsi), %r14
 740                             	        cmoveq	%r14, %r12
 741                             	        movq	728(%rsi), %r14
 742                             	        cmoveq	%r14, %r13
 743                             	        cmpb	$0x00, %cl
 744                             	        movq	%rax, %r14
 745                             	        cmovlq	%r10, %rax
 746                             	        cmovlq	%r14, %r10
 747                             	        movq	%rdx, %r14
 748                             	        cmovlq	%r11, %rdx
 749                             	        cmovlq	%r14, %r11
 750                             	        movq	%r8, %r14
 751                             	        cmovlq	%r12, %r8
 752                             	        cmovlq	%r14, %r12
 753                             	        movq	%r9, %r14
 754                             	        cmovlq	%r13, %r9
 755                             	        cmovlq	%r14, %r13
 756                             	        movq	%rax, (%rdi)
 757                             	        movq	%rdx, 8(%rdi)
 758                             	        movq	%r8, 16(%rdi)
 759                             	        movq	%r9, 24(%rdi)
 760                             	        movq	%r10, 32(%rdi)
 761                             	        movq	%r11, 40(%rdi)
 762                             	        movq	%r12, 48(%rdi)
 763                             	        movq	%r13, 56(%rdi)
 764                             	        xorq	%rax, %rax
 765                             	        xorq	%rdx, %rdx
 766                             	        xorq	%r8, %r8
 767                             	        xorq	%r9, %r9
 768                             	        cmpb	$0x01, %r15b
 769                             	        movq	64(%rsi), %r14
 770                             	        cmoveq	%r14, %rax
 771                             	        movq	72(%rsi), %r14
 772                             	        cmoveq	%r14, %rdx
 773                             	        movq	80(%rsi), %r14
 774                             	        cmoveq	%r14, %r8
 775                             	        movq	88(%rsi), %r14
 776                             	        cmoveq	%r14, %r9
 777                             	        cmpb	$2, %r15b
 778                             	        movq	160(%rsi), %r14
 779                             	        cmoveq	%r14, %rax
 780                             	        movq	168(%rsi), %r14
 781                             	        cmoveq	%r14, %rdx
 782                             	        movq	176(%rsi), %r14
 783                             	        cmoveq	%r14, %r8
 784                             	        movq	184(%rsi), %r14
 785                             	        cmoveq	%r14, %r9
 786                             	        cmpb	$3, %r15b
 787                             	        movq	256(%rsi), %r14
 788                             	        cmoveq	%r14, %rax
 789                             	        movq	264(%rsi), %r14
 790                             	        cmoveq	%r14, %rdx
 791                             	        movq	272(%rsi), %r14
 792                             	        cmoveq	%r14, %r8
 793                             	        movq	280(%rsi), %r14
 794                             	        cmoveq	%r14, %r9
 795                             	        cmpb	$4, %r15b
 796                             	        movq	352(%rsi), %r14
 797                             	        cmoveq	%r14, %rax
 798                             	        movq	360(%rsi), %r14
 799                             	        cmoveq	%r14, %rdx
 800                             	        movq	368(%rsi), %r14
 801                             	        cmoveq	%r14, %r8
 802                             	        movq	376(%rsi), %r14
 803                             	        cmoveq	%r14, %r9
 804                             	        cmpb	$5, %r15b
 805                             	        movq	448(%rsi), %r14
 806                             	        cmoveq	%r14, %rax
 807                             	        movq	456(%rsi), %r14
 808                             	        cmoveq	%r14, %rdx
 809                             	        movq	464(%rsi), %r14
 810                             	        cmoveq	%r14, %r8
 811                             	        movq	472(%rsi), %r14
 812                             	        cmoveq	%r14, %r9
 813                             	        cmpb	$6, %r15b
 814                             	        movq	544(%rsi), %r14
 815                             	        cmoveq	%r14, %rax
 816                             	        movq	552(%rsi), %r14
 817                             	        cmoveq	%r14, %rdx
 818                             	        movq	560(%rsi), %r14
 819                             	        cmoveq	%r14, %r8
 820                             	        movq	568(%rsi), %r14
 821                             	        cmoveq	%r14, %r9
 822                             	        cmpb	$7, %r15b
 823                             	        movq	640(%rsi), %r14
 824                             	        cmoveq	%r14, %rax
 825                             	        movq	648(%rsi), %r14
 826                             	        cmoveq	%r14, %rdx
 827                             	        movq	656(%rsi), %r14
 828                             	        cmoveq	%r14, %r8
 829                             	        movq	664(%rsi), %r14
 830                             	        cmoveq	%r14, %r9
 831                             	        cmpb	$8, %r15b
 832                             	        movq	736(%rsi), %r14
 833                             	        cmoveq	%r14, %rax
 834                             	        movq	744(%rsi), %r14
 835                             	        cmoveq	%r14, %rdx
 836                             	        movq	752(%rsi), %r14
 837                             	        cmoveq	%r14, %r8
 838                             	        movq	760(%rsi), %r14
 839                             	        cmoveq	%r14, %r9
 840                             	        movq	$-19, %r10
 841                             	        movq	$-1, %r11
 842                             	        movq	$-1, %r12
 843                             	        movq	$0x7fffffffffffffff, %r13
 844                             	        subq	%rax, %r10
 845                             	        sbbq	%rdx, %r11
 846                             	        sbbq	%r8, %r12
 847                             	        sbbq	%r9, %r13
 848                             	        cmpb	$0x00, %cl
 849                             	        cmovlq	%r10, %rax
 850                             	        cmovlq	%r11, %rdx
 851                             	        cmovlq	%r12, %r8
 852                             	        cmovlq	%r13, %r9
 853                             	        movq	%rax, 64(%rdi)
 854                             	        movq	%rdx, 72(%rdi)
 855                             	        movq	%r8, 80(%rdi)
 856                             	        movq	%r9, 88(%rdi)
 857                             	        popq	%r15
 858                             	        popq	%r14
 859                             	        popq	%r13
 860                             	        popq	%r12
 861                             	        repz retq
 862                             	#ifndef __APPLE__
 864                             	#endif /* __APPLE__ */
 865                             	#ifndef __APPLE__
 866                             	.text
 867                             	.globl	fe_mul
 869                             	.align	16
 870                             	fe_mul:
 871                             	#else
 872                             	.section	__TEXT,__text
 873                             	.globl	_fe_mul
 874                             	.p2align	4
 875                             	_fe_mul:
 876                             	#endif /* __APPLE__ */
 877                             	#ifndef __APPLE__
 878                             	        jmpq	*fe_mul_p(%rip)
 879                             	#else
 880                             	        jmpq	*_fe_mul_p(%rip)
 881                             	#endif /* __APPLE__ */
 882                             	#ifndef __APPLE__
 884                             	#endif /* __APPLE__ */
 885                             	#ifndef __APPLE__
 886                             	.text
 887                             	.globl	fe_sq
 889                             	.align	16
 890                             	fe_sq:
 891                             	#else
 892                             	.section	__TEXT,__text
 893                             	.globl	_fe_sq
 894                             	.p2align	4
 895                             	_fe_sq:
 896                             	#endif /* __APPLE__ */
 897                             	#ifndef __APPLE__
 898                             	        jmpq	*fe_sq_p(%rip)
 899                             	#else
 900                             	        jmpq	*_fe_sq_p(%rip)
 901                             	#endif /* __APPLE__ */
 902                             	#ifndef __APPLE__
 904                             	#endif /* __APPLE__ */
 905                             	#ifndef __APPLE__
 906                             	.text
 907                             	.globl	fe_mul121666
 909                             	.align	16
 910                             	fe_mul121666:
 911                             	#else
 912                             	.section	__TEXT,__text
 913                             	.globl	_fe_mul121666
 914                             	.p2align	4
 915                             	_fe_mul121666:
 916                             	#endif /* __APPLE__ */
 917                             	#ifndef __APPLE__
 918                             	        jmpq	*fe_mul121666_p(%rip)
 919                             	#else
 920                             	        jmpq	*_fe_mul121666_p(%rip)
 921                             	#endif /* __APPLE__ */
 922                             	#ifndef __APPLE__
 924                             	#endif /* __APPLE__ */
 925                             	#ifndef __APPLE__
 926                             	.text
 927                             	.globl	fe_sq2
 929                             	.align	16
 930                             	fe_sq2:
 931                             	#else
 932                             	.section	__TEXT,__text
 933                             	.globl	_fe_sq2
 934                             	.p2align	4
 935                             	_fe_sq2:
 936                             	#endif /* __APPLE__ */
 937                             	#ifndef __APPLE__
 938                             	        jmpq	*fe_sq2_p(%rip)
 939                             	#else
 940                             	        jmpq	*_fe_sq2_p(%rip)
 941                             	#endif /* __APPLE__ */
 942                             	#ifndef __APPLE__
 944                             	#endif /* __APPLE__ */
 945                             	#ifndef __APPLE__
 946                             	.text
 947                             	.globl	fe_invert
 949                             	.align	16
 950                             	fe_invert:
 951                             	#else
 952                             	.section	__TEXT,__text
 953                             	.globl	_fe_invert
 954                             	.p2align	4
 955                             	_fe_invert:
 956                             	#endif /* __APPLE__ */
 957                             	#ifndef __APPLE__
 958                             	        jmpq	*fe_invert_p(%rip)
 959                             	#else
 960                             	        jmpq	*_fe_invert_p(%rip)
 961                             	#endif /* __APPLE__ */
 962                             	#ifndef __APPLE__
 964                             	#endif /* __APPLE__ */
 965                             	#ifndef __APPLE__
 966                             	.text
 967                             	.globl	curve25519
 969                             	.align	16
 970                             	curve25519:
 971                             	#else
 972                             	.section	__TEXT,__text
 973                             	.globl	_curve25519
 974                             	.p2align	4
 975                             	_curve25519:
 976                             	#endif /* __APPLE__ */
 977                             	#ifndef __APPLE__
 978                             	        jmpq	*curve25519_p(%rip)
 979                             	#else
 980                             	        jmpq	*_curve25519_p(%rip)
 981                             	#endif /* __APPLE__ */
 982                             	#ifndef __APPLE__
 984                             	#endif /* __APPLE__ */
 985                             	#ifndef __APPLE__
 986                             	.text
 987                             	.globl	fe_pow22523
 989                             	.align	16
 990                             	fe_pow22523:
 991                             	#else
 992                             	.section	__TEXT,__text
 993                             	.globl	_fe_pow22523
 994                             	.p2align	4
 995                             	_fe_pow22523:
 996                             	#endif /* __APPLE__ */
 997                             	#ifndef __APPLE__
 998                             	        jmpq	*fe_pow22523_p(%rip)
 999                             	#else
 1000                             	        jmpq	*_fe_pow22523_p(%rip)
 1001                             	#endif /* __APPLE__ */
 1002                             	#ifndef __APPLE__
 1004                             	#endif /* __APPLE__ */
 1005                             	#ifndef __APPLE__
 1006                             	.text
 1007                             	.globl	fe_ge_to_p2
 1009                             	.align	16
 1010                             	fe_ge_to_p2:
 1011                             	#else
 1012                             	.section	__TEXT,__text
 1013                             	.globl	_fe_ge_to_p2
 1014                             	.p2align	4
 1015                             	_fe_ge_to_p2:
 1016                             	#endif /* __APPLE__ */
 1017                             	#ifndef __APPLE__
 1018                             	        jmpq	*fe_ge_to_p2_p(%rip)
 1019                             	#else
 1020                             	        jmpq	*_fe_ge_to_p2_p(%rip)
 1021                             	#endif /* __APPLE__ */
 1022                             	#ifndef __APPLE__
 1024                             	#endif /* __APPLE__ */
 1025                             	#ifndef __APPLE__
 1026                             	.text
 1027                             	.globl	fe_ge_to_p3
 1029                             	.align	16
 1030                             	fe_ge_to_p3:
 1031                             	#else
 1032                             	.section	__TEXT,__text
 1033                             	.globl	_fe_ge_to_p3
 1034                             	.p2align	4
 1035                             	_fe_ge_to_p3:
 1036                             	#endif /* __APPLE__ */
 1037                             	#ifndef __APPLE__
 1038                             	        jmpq	*fe_ge_to_p3_p(%rip)
 1039                             	#else
 1040                             	        jmpq	*_fe_ge_to_p3_p(%rip)
 1041                             	#endif /* __APPLE__ */
 1042                             	#ifndef __APPLE__
 1044                             	#endif /* __APPLE__ */
 1045                             	#ifndef __APPLE__
 1046                             	.text
 1047                             	.globl	fe_ge_dbl
 1049                             	.align	16
 1050                             	fe_ge_dbl:
 1051                             	#else
 1052                             	.section	__TEXT,__text
 1053                             	.globl	_fe_ge_dbl
 1054                             	.p2align	4
 1055                             	_fe_ge_dbl:
 1056                             	#endif /* __APPLE__ */
 1057                             	#ifndef __APPLE__
 1058                             	        jmpq	*fe_ge_dbl_p(%rip)
 1059                             	#else
 1060                             	        jmpq	*_fe_ge_dbl_p(%rip)
 1061                             	#endif /* __APPLE__ */
 1062                             	#ifndef __APPLE__
 1064                             	#endif /* __APPLE__ */
 1065                             	#ifndef __APPLE__
 1066                             	.text
 1067                             	.globl	fe_ge_madd
 1069                             	.align	16
 1070                             	fe_ge_madd:
 1071                             	#else
 1072                             	.section	__TEXT,__text
 1073                             	.globl	_fe_ge_madd
 1074                             	.p2align	4
 1075                             	_fe_ge_madd:
 1076                             	#endif /* __APPLE__ */
 1077                             	#ifndef __APPLE__
 1078                             	        jmpq	*fe_ge_madd_p(%rip)
 1079                             	#else
 1080                             	        jmpq	*_fe_ge_madd_p(%rip)
 1081                             	#endif /* __APPLE__ */
 1082                             	#ifndef __APPLE__
 1084                             	#endif /* __APPLE__ */
 1085                             	#ifndef __APPLE__
 1086                             	.text
 1087                             	.globl	fe_ge_msub
 1089                             	.align	16
 1090                             	fe_ge_msub:
 1091                             	#else
 1092                             	.section	__TEXT,__text
 1093                             	.globl	_fe_ge_msub
 1094                             	.p2align	4
 1095                             	_fe_ge_msub:
 1096                             	#endif /* __APPLE__ */
 1097                             	#ifndef __APPLE__
 1098                             	        jmpq	*fe_ge_msub_p(%rip)
 1099                             	#else
 1100                             	        jmpq	*_fe_ge_msub_p(%rip)
 1101                             	#endif /* __APPLE__ */
 1102                             	#ifndef __APPLE__
 1104                             	#endif /* __APPLE__ */
 1105                             	#ifndef __APPLE__
 1106                             	.text
 1107                             	.globl	fe_ge_add
 1109                             	.align	16
 1110                             	fe_ge_add:
 1111                             	#else
 1112                             	.section	__TEXT,__text
 1113                             	.globl	_fe_ge_add
 1114                             	.p2align	4
 1115                             	_fe_ge_add:
 1116                             	#endif /* __APPLE__ */
 1117                             	#ifndef __APPLE__
 1118                             	        jmpq	*fe_ge_add_p(%rip)
 1119                             	#else
 1120                             	        jmpq	*_fe_ge_add_p(%rip)
 1121                             	#endif /* __APPLE__ */
 1122                             	#ifndef __APPLE__
 1124                             	#endif /* __APPLE__ */
 1125                             	#ifndef __APPLE__
 1126                             	.text
 1127                             	.globl	fe_ge_sub
 1129                             	.align	16
 1130                             	fe_ge_sub:
 1131                             	#else
 1132                             	.section	__TEXT,__text
 1133                             	.globl	_fe_ge_sub
 1134                             	.p2align	4
 1135                             	_fe_ge_sub:
 1136                             	#endif /* __APPLE__ */
 1137                             	#ifndef __APPLE__
 1138                             	        jmpq	*fe_ge_sub_p(%rip)
 1139                             	#else
 1140                             	        jmpq	*_fe_ge_sub_p(%rip)
 1141                             	#endif /* __APPLE__ */
 1142                             	#ifndef __APPLE__
 1144                             	#endif /* __APPLE__ */
 1145                             	#ifndef __APPLE__
 1146                             	.data
 1149                             	cpuFlagsSet:
 1150 ???? 00 00 00 00             		.long	0
 1151                             	#else
 1152                             	.section	__DATA,__data
 1153                             	.p2align	2
 1154                             	_cpuFlagsSet:
 1155                             		.long	0
 1156                             	#endif /* __APPLE__ */
 1157                             	#ifndef __APPLE__
 1158                             	.data
 1161                             	intelFlags:
 1162 ???? 00 00 00 00             		.long	0
 1163                             	#else
 1164                             	.section	__DATA,__data
 1165                             	.p2align	2
 1166                             	_intelFlags:
 1167                             		.long	0
 1168                             	#endif /* __APPLE__ */
 1169                             	#ifndef __APPLE__
 1170                             	.data
 1173                             	fe_mul_p:
 1174 ???? 00 00 00 00 00 00 00 00 		.quad	fe_mul_x64
****  Error: unsupported constant size 8

 1175                             	#else
 1176                             	.section	__DATA,__data
 1177                             	.p2align	2
 1178                             	_fe_mul_p:
 1179                             		.quad	_fe_mul_x64
 1180                             	#endif /* __APPLE__ */
 1181                             	#ifndef __APPLE__
 1182                             	.data
 1185                             	fe_sq_p:
 1186 ???? 00 00 00 00 00 00 00 00 		.quad	fe_sq_x64
****  Error: unsupported constant size 8

 1187                             	#else
 1188                             	.section	__DATA,__data
 1189                             	.p2align	2
 1190                             	_fe_sq_p:
 1191                             		.quad	_fe_sq_x64
 1192                             	#endif /* __APPLE__ */
 1193                             	#ifndef __APPLE__
 1194                             	.data
 1197                             	fe_mul121666_p:
 1198 ???? 00 00 00 00 00 00 00 00 		.quad	fe_mul121666_x64
****  Error: unsupported constant size 8

 1199                             	#else
 1200                             	.section	__DATA,__data
 1201                             	.p2align	2
 1202                             	_fe_mul121666_p:
 1203                             		.quad	_fe_mul121666_x64
 1204                             	#endif /* __APPLE__ */
 1205                             	#ifndef __APPLE__
 1206                             	.data
 1209                             	fe_sq2_p:
 1210 ???? 00 00 00 00 00 00 00 00 		.quad	fe_sq2_x64
****  Error: unsupported constant size 8

 1211                             	#else
 1212                             	.section	__DATA,__data
 1213                             	.p2align	2
 1214                             	_fe_sq2_p:
 1215                             		.quad	_fe_sq2_x64
 1216                             	#endif /* __APPLE__ */
 1217                             	#ifndef __APPLE__
 1218                             	.data
 1221                             	fe_invert_p:
 1222 ???? 00 00 00 00 00 00 00 00 		.quad	fe_invert_x64
****  Error: unsupported constant size 8

 1223                             	#else
 1224                             	.section	__DATA,__data
 1225                             	.p2align	2
 1226                             	_fe_invert_p:
 1227                             		.quad	_fe_invert_x64
 1228                             	#endif /* __APPLE__ */
 1229                             	#ifndef __APPLE__
 1230                             	.data
 1233                             	curve25519_p:
 1234 ???? 00 00 00 00 00 00 00 00 		.quad	curve25519_x64
****  Error: unsupported constant size 8

 1235                             	#else
 1236                             	.section	__DATA,__data
 1237                             	.p2align	2
 1238                             	_curve25519_p:
 1239                             		.quad	_curve25519_x64
 1240                             	#endif /* __APPLE__ */
 1241                             	#ifndef __APPLE__
 1242                             	.data
 1245                             	fe_pow22523_p:
 1246 ???? 00 00 00 00 00 00 00 00 		.quad	fe_pow22523_x64
****  Error: unsupported constant size 8

 1247                             	#else
 1248                             	.section	__DATA,__data
 1249                             	.p2align	2
 1250                             	_fe_pow22523_p:
 1251                             		.quad	_fe_pow22523_x64
 1252                             	#endif /* __APPLE__ */
 1253                             	#ifndef __APPLE__
 1254                             	.data
 1257                             	fe_ge_to_p2_p:
 1258 ???? 00 00 00 00 00 00 00 00 		.quad	fe_ge_to_p2_x64
****  Error: unsupported constant size 8

 1259                             	#else
 1260                             	.section	__DATA,__data
 1261                             	.p2align	2
 1262                             	_fe_ge_to_p2_p:
 1263                             		.quad	_fe_ge_to_p2_x64
 1264                             	#endif /* __APPLE__ */
 1265                             	#ifndef __APPLE__
 1266                             	.data
 1269                             	fe_ge_to_p3_p:
 1270 ???? 00 00 00 00 00 00 00 00 		.quad	fe_ge_to_p3_x64
****  Error: unsupported constant size 8

 1271                             	#else
 1272                             	.section	__DATA,__data
 1273                             	.p2align	2
 1274                             	_fe_ge_to_p3_p:
 1275                             		.quad	_fe_ge_to_p3_x64
 1276                             	#endif /* __APPLE__ */
 1277                             	#ifndef __APPLE__
 1278                             	.data
 1281                             	fe_ge_dbl_p:
 1282 ???? 00 00 00 00 00 00 00 00 		.quad	fe_ge_dbl_x64
****  Error: unsupported constant size 8

 1283                             	#else
 1284                             	.section	__DATA,__data
 1285                             	.p2align	2
 1286                             	_fe_ge_dbl_p:
 1287                             		.quad	_fe_ge_dbl_x64
 1288                             	#endif /* __APPLE__ */
 1289                             	#ifndef __APPLE__
 1290                             	.data
 1293                             	fe_ge_madd_p:
 1294 ???? 00 00 00 00 00 00 00 00 		.quad	fe_ge_madd_x64
****  Error: unsupported constant size 8

 1295                             	#else
 1296                             	.section	__DATA,__data
 1297                             	.p2align	2
 1298                             	_fe_ge_madd_p:
 1299                             		.quad	_fe_ge_madd_x64
 1300                             	#endif /* __APPLE__ */
 1301                             	#ifndef __APPLE__
 1302                             	.data
 1305                             	fe_ge_msub_p:
 1306 ???? 00 00 00 00 00 00 00 00 		.quad	fe_ge_msub_x64
****  Error: unsupported constant size 8

 1307                             	#else
 1308                             	.section	__DATA,__data
 1309                             	.p2align	2
 1310                             	_fe_ge_msub_p:
 1311                             		.quad	_fe_ge_msub_x64
 1312                             	#endif /* __APPLE__ */
 1313                             	#ifndef __APPLE__
 1314                             	.data
 1317                             	fe_ge_add_p:
 1318 ???? 00 00 00 00 00 00 00 00 		.quad	fe_ge_add_x64
****  Error: unsupported constant size 8

 1319                             	#else
 1320                             	.section	__DATA,__data
 1321                             	.p2align	2
 1322                             	_fe_ge_add_p:
 1323                             		.quad	_fe_ge_add_x64
 1324                             	#endif /* __APPLE__ */
 1325                             	#ifndef __APPLE__
 1326                             	.data
 1329                             	fe_ge_sub_p:
 1330 ???? 00 00 00 00 00 00 00 00 		.quad	fe_ge_sub_x64
****  Error: unsupported constant size 8

 1331                             	#else
 1332                             	.section	__DATA,__data
 1333                             	.p2align	2
 1334                             	_fe_ge_sub_p:
 1335                             		.quad	_fe_ge_sub_x64
 1336                             	#endif /* __APPLE__ */
 1337                             	#ifndef __APPLE__
 1338                             	.text
 1339                             	.globl	fe_mul_x64
 1341                             	.align	16
 1342                             	fe_mul_x64:
 1343                             	#else
 1344                             	.section	__TEXT,__text
 1345                             	.globl	_fe_mul_x64
 1346                             	.p2align	4
 1347                             	_fe_mul_x64:
 1348                             	#endif /* __APPLE__ */
 1349                             	        pushq	%r12
 1350                             	        pushq	%r13
 1351                             	        pushq	%r14
 1352                             	        pushq	%r15
 1353                             	        pushq	%rbx
 1354                             	        movq	%rdx, %rcx
 1355                             	        # Multiply
 1356                             	        #  A[0] * B[0]
 1357                             	        movq	(%rcx), %rax
 1358                             	        mulq	(%rsi)
 1359                             	        movq	%rax, %r8
 1360                             	        movq	%rdx, %r9
 1361                             	        #  A[0] * B[1]
 1362                             	        movq	8(%rcx), %rax
 1363                             	        mulq	(%rsi)
 1364                             	        xorq	%r10, %r10
 1365                             	        addq	%rax, %r9
 1366                             	        adcq	%rdx, %r10
 1367                             	        #  A[1] * B[0]
 1368                             	        movq	(%rcx), %rax
 1369                             	        mulq	8(%rsi)
 1370                             	        xorq	%r11, %r11
 1371                             	        addq	%rax, %r9
 1372                             	        adcq	%rdx, %r10
 1373                             	        adcq	$0x00, %r11
 1374                             	        #  A[0] * B[2]
 1375                             	        movq	16(%rcx), %rax
 1376                             	        mulq	(%rsi)
 1377                             	        addq	%rax, %r10
 1378                             	        adcq	%rdx, %r11
 1379                             	        #  A[1] * B[1]
 1380                             	        movq	8(%rcx), %rax
 1381                             	        mulq	8(%rsi)
 1382                             	        xorq	%r12, %r12
 1383                             	        addq	%rax, %r10
 1384                             	        adcq	%rdx, %r11
 1385                             	        adcq	$0x00, %r12
 1386                             	        #  A[2] * B[0]
 1387                             	        movq	(%rcx), %rax
 1388                             	        mulq	16(%rsi)
 1389                             	        addq	%rax, %r10
 1390                             	        adcq	%rdx, %r11
 1391                             	        adcq	$0x00, %r12
 1392                             	        #  A[0] * B[3]
 1393                             	        movq	24(%rcx), %rax
 1394                             	        mulq	(%rsi)
 1395                             	        xorq	%r13, %r13
 1396                             	        addq	%rax, %r11
 1397                             	        adcq	%rdx, %r12
 1398                             	        adcq	$0x00, %r13
 1399                             	        #  A[1] * B[2]
 1400                             	        movq	16(%rcx), %rax
 1401                             	        mulq	8(%rsi)
 1402                             	        addq	%rax, %r11
 1403                             	        adcq	%rdx, %r12
 1404                             	        adcq	$0x00, %r13
 1405                             	        #  A[2] * B[1]
 1406                             	        movq	8(%rcx), %rax
 1407                             	        mulq	16(%rsi)
 1408                             	        addq	%rax, %r11
 1409                             	        adcq	%rdx, %r12
 1410                             	        adcq	$0x00, %r13
 1411                             	        #  A[3] * B[0]
 1412                             	        movq	(%rcx), %rax
 1413                             	        mulq	24(%rsi)
 1414                             	        addq	%rax, %r11
 1415                             	        adcq	%rdx, %r12
 1416                             	        adcq	$0x00, %r13
 1417                             	        #  A[1] * B[3]
 1418                             	        movq	24(%rcx), %rax
 1419                             	        mulq	8(%rsi)
 1420                             	        xorq	%r14, %r14
 1421                             	        addq	%rax, %r12
 1422                             	        adcq	%rdx, %r13
 1423                             	        adcq	$0x00, %r14
 1424                             	        #  A[2] * B[2]
 1425                             	        movq	16(%rcx), %rax
 1426                             	        mulq	16(%rsi)
 1427                             	        addq	%rax, %r12
 1428                             	        adcq	%rdx, %r13
 1429                             	        adcq	$0x00, %r14
 1430                             	        #  A[3] * B[1]
 1431                             	        movq	8(%rcx), %rax
 1432                             	        mulq	24(%rsi)
 1433                             	        addq	%rax, %r12
 1434                             	        adcq	%rdx, %r13
 1435                             	        adcq	$0x00, %r14
 1436                             	        #  A[2] * B[3]
 1437                             	        movq	24(%rcx), %rax
 1438                             	        mulq	16(%rsi)
 1439                             	        xorq	%r15, %r15
 1440                             	        addq	%rax, %r13
 1441                             	        adcq	%rdx, %r14
 1442                             	        adcq	$0x00, %r15
 1443                             	        #  A[3] * B[2]
 1444                             	        movq	16(%rcx), %rax
 1445                             	        mulq	24(%rsi)
 1446                             	        addq	%rax, %r13
 1447                             	        adcq	%rdx, %r14
 1448                             	        adcq	$0x00, %r15
 1449                             	        #  A[3] * B[3]
 1450                             	        movq	24(%rcx), %rax
 1451                             	        mulq	24(%rsi)
 1452                             	        addq	%rax, %r14
 1453                             	        adcq	%rdx, %r15
 1454                             	        # Reduce
 1455                             	        movq	$0x7fffffffffffffff, %rbx
 1456                             	        #  Move top half into t4-t7 and remove top bit from t3
 1457                             	        shldq	$0x01, %r14, %r15
 1458                             	        shldq	$0x01, %r13, %r14
 1459                             	        shldq	$0x01, %r12, %r13
 1460                             	        shldq	$0x01, %r11, %r12
 1461                             	        andq	%rbx, %r11
 1462                             	        #  Multiply top half by 19
 1463                             	        movq	$19, %rax
 1464                             	        mulq	%r12
 1465                             	        xorq	%r12, %r12
 1466                             	        addq	%rax, %r8
 1467                             	        movq	$19, %rax
 1468                             	        adcq	%rdx, %r12
 1469                             	        mulq	%r13
 1470                             	        xorq	%r13, %r13
 1471                             	        addq	%rax, %r9
 1472                             	        movq	$19, %rax
 1473                             	        adcq	%rdx, %r13
 1474                             	        mulq	%r14
 1475                             	        xorq	%r14, %r14
 1476                             	        addq	%rax, %r10
 1477                             	        movq	$19, %rax
 1478                             	        adcq	%rdx, %r14
 1479                             	        mulq	%r15
 1480                             	        #  Add remaining product results in
 1481                             	        addq	%r12, %r9
 1482                             	        adcq	%r13, %r10
 1483                             	        adcq	%r14, %r11
 1484                             	        adcq	%rax, %r11
 1485                             	        adcq	$0x00, %rdx
 1486                             	        #  Overflow
 1487                             	        shldq	$0x01, %r11, %rdx
 1488                             	        imulq	$19, %rdx, %rax
 1489                             	        andq	%rbx, %r11
 1490                             	        addq	%rax, %r8
 1491                             	        adcq	$0x00, %r9
 1492                             	        adcq	$0x00, %r10
 1493                             	        adcq	$0x00, %r11
 1494                             	        # Reduce if top bit set
 1495                             	        movq	%r11, %rdx
 1496                             	        sarq	$63, %rdx
 1497                             	        andq	$19, %rdx
 1498                             	        andq	%rbx, %r11
 1499                             	        addq	%rdx, %r8
 1500                             	        adcq	$0x00, %r9
 1501                             	        adcq	$0x00, %r10
 1502                             	        adcq	$0x00, %r11
 1503                             	        # Store
 1504                             	        movq	%r8, (%rdi)
 1505                             	        movq	%r9, 8(%rdi)
 1506                             	        movq	%r10, 16(%rdi)
 1507                             	        movq	%r11, 24(%rdi)
 1508                             	        popq	%rbx
 1509                             	        popq	%r15
 1510                             	        popq	%r14
 1511                             	        popq	%r13
 1512                             	        popq	%r12
 1513                             	        repz retq
 1514                             	#ifndef __APPLE__
 1516                             	#endif /* __APPLE__ */
 1517                             	#ifndef __APPLE__
 1518                             	.text
 1519                             	.globl	fe_sq_x64
 1521                             	.align	16
 1522                             	fe_sq_x64:
 1523                             	#else
 1524                             	.section	__TEXT,__text
 1525                             	.globl	_fe_sq_x64
 1526                             	.p2align	4
 1527                             	_fe_sq_x64:
 1528                             	#endif /* __APPLE__ */
 1529                             	        pushq	%r12
 1530                             	        pushq	%r13
 1531                             	        pushq	%r14
 1532                             	        pushq	%r15
 1533                             	        # Square
 1534                             	        #  A[0] * A[1]
 1535                             	        movq	(%rsi), %rax
 1536                             	        mulq	8(%rsi)
 1537                             	        movq	%rax, %r8
 1538                             	        movq	%rdx, %r9
 1539                             	        #  A[0] * A[2]
 1540                             	        movq	(%rsi), %rax
 1541                             	        mulq	16(%rsi)
 1542                             	        xorq	%r10, %r10
 1543                             	        addq	%rax, %r9
 1544                             	        adcq	%rdx, %r10
 1545                             	        #  A[0] * A[3]
 1546                             	        movq	(%rsi), %rax
 1547                             	        mulq	24(%rsi)
 1548                             	        xorq	%r11, %r11
 1549                             	        addq	%rax, %r10
 1550                             	        adcq	%rdx, %r11
 1551                             	        #  A[1] * A[2]
 1552                             	        movq	8(%rsi), %rax
 1553                             	        mulq	16(%rsi)
 1554                             	        xorq	%r12, %r12
 1555                             	        addq	%rax, %r10
 1556                             	        adcq	%rdx, %r11
 1557                             	        adcq	$0x00, %r12
 1558                             	        #  A[1] * A[3]
 1559                             	        movq	8(%rsi), %rax
 1560                             	        mulq	24(%rsi)
 1561                             	        addq	%rax, %r11
 1562                             	        adcq	%rdx, %r12
 1563                             	        #  A[2] * A[3]
 1564                             	        movq	16(%rsi), %rax
 1565                             	        mulq	24(%rsi)
 1566                             	        xorq	%r13, %r13
 1567                             	        addq	%rax, %r12
 1568                             	        adcq	%rdx, %r13
 1569                             	        # Double
 1570                             	        xorq	%r14, %r14
 1571                             	        addq	%r8, %r8
 1572                             	        adcq	%r9, %r9
 1573                             	        adcq	%r10, %r10
 1574                             	        adcq	%r11, %r11
 1575                             	        adcq	%r12, %r12
 1576                             	        adcq	%r13, %r13
 1577                             	        adcq	$0x00, %r14
 1578                             	        #  A[0] * A[0]
 1579                             	        movq	(%rsi), %rax
 1580                             	        mulq	%rax
 1581                             	        movq	%rax, %rcx
 1582                             	        movq	%rdx, %r15
 1583                             	        #  A[1] * A[1]
 1584                             	        movq	8(%rsi), %rax
 1585                             	        mulq	%rax
 1586                             	        addq	%r15, %r8
 1587                             	        adcq	%rax, %r9
 1588                             	        adcq	$0x00, %rdx
 1589                             	        movq	%rdx, %r15
 1590                             	        #  A[2] * A[2]
 1591                             	        movq	16(%rsi), %rax
 1592                             	        mulq	%rax
 1593                             	        addq	%r15, %r10
 1594                             	        adcq	%rax, %r11
 1595                             	        adcq	$0x00, %rdx
 1596                             	        movq	%rdx, %r15
 1597                             	        #  A[3] * A[3]
 1598                             	        movq	24(%rsi), %rax
 1599                             	        mulq	%rax
 1600                             	        addq	%rax, %r13
 1601                             	        adcq	%rdx, %r14
 1602                             	        addq	%r15, %r12
 1603                             	        adcq	$0x00, %r13
 1604                             	        adcq	$0x00, %r14
 1605                             	        # Reduce
 1606                             	        movq	$0x7fffffffffffffff, %r15
 1607                             	        #  Move top half into t4-t7 and remove top bit from t3
 1608                             	        shldq	$0x01, %r13, %r14
 1609                             	        shldq	$0x01, %r12, %r13
 1610                             	        shldq	$0x01, %r11, %r12
 1611                             	        shldq	$0x01, %r10, %r11
 1612                             	        andq	%r15, %r10
 1613                             	        #  Multiply top half by 19
 1614                             	        movq	$19, %rax
 1615                             	        mulq	%r11
 1616                             	        xorq	%r11, %r11
 1617                             	        addq	%rax, %rcx
 1618                             	        movq	$19, %rax
 1619                             	        adcq	%rdx, %r11
 1620                             	        mulq	%r12
 1621                             	        xorq	%r12, %r12
 1622                             	        addq	%rax, %r8
 1623                             	        movq	$19, %rax
 1624                             	        adcq	%rdx, %r12
 1625                             	        mulq	%r13
 1626                             	        xorq	%r13, %r13
 1627                             	        addq	%rax, %r9
 1628                             	        movq	$19, %rax
 1629                             	        adcq	%rdx, %r13
 1630                             	        mulq	%r14
 1631                             	        #  Add remaining product results in
 1632                             	        addq	%r11, %r8
 1633                             	        adcq	%r12, %r9
 1634                             	        adcq	%r13, %r10
 1635                             	        adcq	%rax, %r10
 1636                             	        adcq	$0x00, %rdx
 1637                             	        #  Overflow
 1638                             	        shldq	$0x01, %r10, %rdx
 1639                             	        imulq	$19, %rdx, %rax
 1640                             	        andq	%r15, %r10
 1641                             	        addq	%rax, %rcx
 1642                             	        adcq	$0x00, %r8
 1643                             	        adcq	$0x00, %r9
 1644                             	        adcq	$0x00, %r10
 1645                             	        # Reduce if top bit set
 1646                             	        movq	%r10, %rdx
 1647                             	        sarq	$63, %rdx
 1648                             	        andq	$19, %rdx
 1649                             	        andq	%r15, %r10
 1650                             	        addq	%rdx, %rcx
 1651                             	        adcq	$0x00, %r8
 1652                             	        adcq	$0x00, %r9
 1653                             	        adcq	$0x00, %r10
 1654                             	        # Store
 1655                             	        movq	%rcx, (%rdi)
 1656                             	        movq	%r8, 8(%rdi)
 1657                             	        movq	%r9, 16(%rdi)
 1658                             	        movq	%r10, 24(%rdi)
 1659                             	        popq	%r15
 1660                             	        popq	%r14
 1661                             	        popq	%r13
 1662                             	        popq	%r12
 1663                             	        repz retq
 1664                             	#ifndef __APPLE__
 1666                             	#endif /* __APPLE__ */
 1667                             	#ifndef __APPLE__
 1668                             	.text
 1669                             	.globl	fe_sq_n_x64
 1671                             	.align	16
 1672                             	fe_sq_n_x64:
 1673                             	#else
 1674                             	.section	__TEXT,__text
 1675                             	.globl	_fe_sq_n_x64
 1676                             	.p2align	4
 1677                             	_fe_sq_n_x64:
 1678                             	#endif /* __APPLE__ */
 1679                             	        pushq	%r12
 1680                             	        pushq	%r13
 1681                             	        pushq	%r14
 1682                             	        pushq	%r15
 1683                             	        pushq	%rbx
 1684                             	        movq	%rdx, %rcx
 1685                             	L_fe_sq_n_x64:
 1686                             	        # Square
 1687                             	        #  A[0] * A[1]
 1688                             	        movq	(%rsi), %rax
 1689                             	        mulq	8(%rsi)
 1690                             	        movq	%rax, %r9
 1691                             	        movq	%rdx, %r10
 1692                             	        #  A[0] * A[2]
 1693                             	        movq	(%rsi), %rax
 1694                             	        mulq	16(%rsi)
 1695                             	        xorq	%r11, %r11
 1696                             	        addq	%rax, %r10
 1697                             	        adcq	%rdx, %r11
 1698                             	        #  A[0] * A[3]
 1699                             	        movq	(%rsi), %rax
 1700                             	        mulq	24(%rsi)
 1701                             	        xorq	%r12, %r12
 1702                             	        addq	%rax, %r11
 1703                             	        adcq	%rdx, %r12
 1704                             	        #  A[1] * A[2]
 1705                             	        movq	8(%rsi), %rax
 1706                             	        mulq	16(%rsi)
 1707                             	        xorq	%r13, %r13
 1708                             	        addq	%rax, %r11
 1709                             	        adcq	%rdx, %r12
 1710                             	        adcq	$0x00, %r13
 1711                             	        #  A[1] * A[3]
 1712                             	        movq	8(%rsi), %rax
 1713                             	        mulq	24(%rsi)
 1714                             	        addq	%rax, %r12
 1715                             	        adcq	%rdx, %r13
 1716                             	        #  A[2] * A[3]
 1717                             	        movq	16(%rsi), %rax
 1718                             	        mulq	24(%rsi)
 1719                             	        xorq	%r14, %r14
 1720                             	        addq	%rax, %r13
 1721                             	        adcq	%rdx, %r14
 1722                             	        # Double
 1723                             	        xorq	%r15, %r15
 1724                             	        addq	%r9, %r9
 1725                             	        adcq	%r10, %r10
 1726                             	        adcq	%r11, %r11
 1727                             	        adcq	%r12, %r12
 1728                             	        adcq	%r13, %r13
 1729                             	        adcq	%r14, %r14
 1730                             	        adcq	$0x00, %r15
 1731                             	        #  A[0] * A[0]
 1732                             	        movq	(%rsi), %rax
 1733                             	        mulq	%rax
 1734                             	        movq	%rax, %r8
 1735                             	        movq	%rdx, %rbx
 1736                             	        #  A[1] * A[1]
 1737                             	        movq	8(%rsi), %rax
 1738                             	        mulq	%rax
 1739                             	        addq	%rbx, %r9
 1740                             	        adcq	%rax, %r10
 1741                             	        adcq	$0x00, %rdx
 1742                             	        movq	%rdx, %rbx
 1743                             	        #  A[2] * A[2]
 1744                             	        movq	16(%rsi), %rax
 1745                             	        mulq	%rax
 1746                             	        addq	%rbx, %r11
 1747                             	        adcq	%rax, %r12
 1748                             	        adcq	$0x00, %rdx
 1749                             	        movq	%rdx, %rbx
 1750                             	        #  A[3] * A[3]
 1751                             	        movq	24(%rsi), %rax
 1752                             	        mulq	%rax
 1753                             	        addq	%rax, %r14
 1754                             	        adcq	%rdx, %r15
 1755                             	        addq	%rbx, %r13
 1756                             	        adcq	$0x00, %r14
 1757                             	        adcq	$0x00, %r15
 1758                             	        # Reduce
 1759                             	        movq	$0x7fffffffffffffff, %rbx
 1760                             	        #  Move top half into t4-t7 and remove top bit from t3
 1761                             	        shldq	$0x01, %r14, %r15
 1762                             	        shldq	$0x01, %r13, %r14
 1763                             	        shldq	$0x01, %r12, %r13
 1764                             	        shldq	$0x01, %r11, %r12
 1765                             	        andq	%rbx, %r11
 1766                             	        #  Multiply top half by 19
 1767                             	        movq	$19, %rax
 1768                             	        mulq	%r12
 1769                             	        xorq	%r12, %r12
 1770                             	        addq	%rax, %r8
 1771                             	        movq	$19, %rax
 1772                             	        adcq	%rdx, %r12
 1773                             	        mulq	%r13
 1774                             	        xorq	%r13, %r13
 1775                             	        addq	%rax, %r9
 1776                             	        movq	$19, %rax
 1777                             	        adcq	%rdx, %r13
 1778                             	        mulq	%r14
 1779                             	        xorq	%r14, %r14
 1780                             	        addq	%rax, %r10
 1781                             	        movq	$19, %rax
 1782                             	        adcq	%rdx, %r14
 1783                             	        mulq	%r15
 1784                             	        #  Add remaining product results in
 1785                             	        addq	%r12, %r9
 1786                             	        adcq	%r13, %r10
 1787                             	        adcq	%r14, %r11
 1788                             	        adcq	%rax, %r11
 1789                             	        adcq	$0x00, %rdx
 1790                             	        #  Overflow
 1791                             	        shldq	$0x01, %r11, %rdx
 1792                             	        imulq	$19, %rdx, %rax
 1793                             	        andq	%rbx, %r11
 1794                             	        addq	%rax, %r8
 1795                             	        adcq	$0x00, %r9
 1796                             	        adcq	$0x00, %r10
 1797                             	        adcq	$0x00, %r11
 1798                             	        # Reduce if top bit set
 1799                             	        movq	%r11, %rdx
 1800                             	        sarq	$63, %rdx
 1801                             	        andq	$19, %rdx
 1802                             	        andq	%rbx, %r11
 1803                             	        addq	%rdx, %r8
 1804                             	        adcq	$0x00, %r9
 1805                             	        adcq	$0x00, %r10
 1806                             	        adcq	$0x00, %r11
 1807                             	        # Store
 1808                             	        movq	%r8, (%rdi)
 1809                             	        movq	%r9, 8(%rdi)
 1810                             	        movq	%r10, 16(%rdi)
 1811                             	        movq	%r11, 24(%rdi)
 1812                             	        decb	%cl
 1813                             	        jnz	L_fe_sq_n_x64
 1814                             	        popq	%rbx
 1815                             	        popq	%r15
 1816                             	        popq	%r14
 1817                             	        popq	%r13
 1818                             	        popq	%r12
 1819                             	        repz retq
 1820                             	#ifndef __APPLE__
 1822                             	#endif /* __APPLE__ */
 1823                             	#ifndef __APPLE__
 1824                             	.text
 1825                             	.globl	fe_mul121666_x64
 1827                             	.align	16
 1828                             	fe_mul121666_x64:
 1829                             	#else
 1830                             	.section	__TEXT,__text
 1831                             	.globl	_fe_mul121666_x64
 1832                             	.p2align	4
 1833                             	_fe_mul121666_x64:
 1834                             	#endif /* __APPLE__ */
 1835                             	        pushq	%r12
 1836                             	        # Multiply by 121666
 1837                             	        movq	$0x1db42, %rax
 1838                             	        mulq	(%rsi)
 1839                             	        xorq	%r10, %r10
 1840                             	        movq	%rax, %r8
 1841                             	        movq	%rdx, %r9
 1842                             	        movq	$0x1db42, %rax
 1843                             	        mulq	8(%rsi)
 1844                             	        xorq	%r11, %r11
 1845                             	        addq	%rax, %r9
 1846                             	        adcq	%rdx, %r10
 1847                             	        movq	$0x1db42, %rax
 1848                             	        mulq	16(%rsi)
 1849                             	        xorq	%r12, %r12
 1850                             	        addq	%rax, %r10
 1851                             	        adcq	%rdx, %r11
 1852                             	        movq	$0x1db42, %rax
 1853                             	        mulq	24(%rsi)
 1854                             	        movq	$0x7fffffffffffffff, %rcx
 1855                             	        addq	%rax, %r11
 1856                             	        adcq	%rdx, %r12
 1857                             	        shldq	$0x01, %r11, %r12
 1858                             	        andq	%rcx, %r11
 1859                             	        movq	$19, %rax
 1860                             	        mulq	%r12
 1861                             	        addq	%rax, %r8
 1862                             	        adcq	$0x00, %r9
 1863                             	        adcq	$0x00, %r10
 1864                             	        adcq	$0x00, %r11
 1865                             	        movq	%r8, (%rdi)
 1866                             	        movq	%r9, 8(%rdi)
 1867                             	        movq	%r10, 16(%rdi)
 1868                             	        movq	%r11, 24(%rdi)
 1869                             	        popq	%r12
 1870                             	        repz retq
 1871                             	#ifndef __APPLE__
 1873                             	#endif /* __APPLE__ */
 1874                             	#ifndef __APPLE__
 1875                             	.text
 1876                             	.globl	fe_sq2_x64
 1878                             	.align	16
 1879                             	fe_sq2_x64:
 1880                             	#else
 1881                             	.section	__TEXT,__text
 1882                             	.globl	_fe_sq2_x64
 1883                             	.p2align	4
 1884                             	_fe_sq2_x64:
 1885                             	#endif /* __APPLE__ */
 1886                             	        pushq	%r12
 1887                             	        pushq	%r13
 1888                             	        pushq	%r14
 1889                             	        pushq	%r15
 1890                             	        pushq	%rbx
 1891                             	        # Square * 2
 1892                             	        #  A[0] * A[1]
 1893                             	        movq	(%rsi), %rax
 1894                             	        mulq	8(%rsi)
 1895                             	        movq	%rax, %r8
 1896                             	        movq	%rdx, %r9
 1897                             	        #  A[0] * A[2]
 1898                             	        movq	(%rsi), %rax
 1899                             	        mulq	16(%rsi)
 1900                             	        xorq	%r10, %r10
 1901                             	        addq	%rax, %r9
 1902                             	        adcq	%rdx, %r10
 1903                             	        #  A[0] * A[3]
 1904                             	        movq	(%rsi), %rax
 1905                             	        mulq	24(%rsi)
 1906                             	        xorq	%r11, %r11
 1907                             	        addq	%rax, %r10
 1908                             	        adcq	%rdx, %r11
 1909                             	        #  A[1] * A[2]
 1910                             	        movq	8(%rsi), %rax
 1911                             	        mulq	16(%rsi)
 1912                             	        xorq	%r12, %r12
 1913                             	        addq	%rax, %r10
 1914                             	        adcq	%rdx, %r11
 1915                             	        adcq	$0x00, %r12
 1916                             	        #  A[1] * A[3]
 1917                             	        movq	8(%rsi), %rax
 1918                             	        mulq	24(%rsi)
 1919                             	        addq	%rax, %r11
 1920                             	        adcq	%rdx, %r12
 1921                             	        #  A[2] * A[3]
 1922                             	        movq	16(%rsi), %rax
 1923                             	        mulq	24(%rsi)
 1924                             	        xorq	%r13, %r13
 1925                             	        addq	%rax, %r12
 1926                             	        adcq	%rdx, %r13
 1927                             	        # Double
 1928                             	        xorq	%r14, %r14
 1929                             	        addq	%r8, %r8
 1930                             	        adcq	%r9, %r9
 1931                             	        adcq	%r10, %r10
 1932                             	        adcq	%r11, %r11
 1933                             	        adcq	%r12, %r12
 1934                             	        adcq	%r13, %r13
 1935                             	        adcq	$0x00, %r14
 1936                             	        #  A[0] * A[0]
 1937                             	        movq	(%rsi), %rax
 1938                             	        mulq	%rax
 1939                             	        movq	%rax, %rcx
 1940                             	        movq	%rdx, %r15
 1941                             	        #  A[1] * A[1]
 1942                             	        movq	8(%rsi), %rax
 1943                             	        mulq	%rax
 1944                             	        addq	%r15, %r8
 1945                             	        adcq	%rax, %r9
 1946                             	        adcq	$0x00, %rdx
 1947                             	        movq	%rdx, %r15
 1948                             	        #  A[2] * A[2]
 1949                             	        movq	16(%rsi), %rax
 1950                             	        mulq	%rax
 1951                             	        addq	%r15, %r10
 1952                             	        adcq	%rax, %r11
 1953                             	        adcq	$0x00, %rdx
 1954                             	        movq	%rdx, %r15
 1955                             	        #  A[3] * A[3]
 1956                             	        movq	24(%rsi), %rax
 1957                             	        mulq	%rax
 1958                             	        addq	%rax, %r13
 1959                             	        adcq	%rdx, %r14
 1960                             	        addq	%r15, %r12
 1961                             	        adcq	$0x00, %r13
 1962                             	        adcq	$0x00, %r14
 1963                             	        # Reduce
 1964                             	        movq	$0x7fffffffffffffff, %rbx
 1965                             	        xorq	%rax, %rax
 1966                             	        #  Move top half into t4-t7 and remove top bit from t3
 1967                             	        shldq	$3, %r14, %rax
 1968                             	        shldq	$2, %r13, %r14
 1969                             	        shldq	$2, %r12, %r13
 1970                             	        shldq	$2, %r11, %r12
 1971                             	        shldq	$2, %r10, %r11
 1972                             	        shldq	$0x01, %r9, %r10
 1973                             	        shldq	$0x01, %r8, %r9
 1974                             	        shldq	$0x01, %rcx, %r8
 1975                             	        shlq	$0x01, %rcx
 1976                             	        andq	%rbx, %r10
 1977                             	        #  Two out left, one in right
 1978                             	        andq	%rbx, %r14
 1979                             	        #  Multiply top bits by 19*19
 1980                             	        imulq	$0x169, %rax, %r15
 1981                             	        #  Multiply top half by 19
 1982                             	        movq	$19, %rax
 1983                             	        mulq	%r11
 1984                             	        xorq	%r11, %r11
 1985                             	        addq	%rax, %rcx
 1986                             	        movq	$19, %rax
 1987                             	        adcq	%rdx, %r11
 1988                             	        mulq	%r12
 1989                             	        xorq	%r12, %r12
 1990                             	        addq	%rax, %r8
 1991                             	        movq	$19, %rax
 1992                             	        adcq	%rdx, %r12
 1993                             	        mulq	%r13
 1994                             	        xorq	%r13, %r13
 1995                             	        addq	%rax, %r9
 1996                             	        movq	$19, %rax
 1997                             	        adcq	%rdx, %r13
 1998                             	        mulq	%r14
 1999                             	        #  Add remaining produce results in
 2000                             	        addq	%r15, %rcx
 2001                             	        adcq	%r11, %r8
 2002                             	        adcq	%r12, %r9
 2003                             	        adcq	%r13, %r10
 2004                             	        adcq	%rax, %r10
 2005                             	        adcq	$0x00, %rdx
 2006                             	        #  Overflow
 2007                             	        shldq	$0x01, %r10, %rdx
 2008                             	        imulq	$19, %rdx, %rax
 2009                             	        andq	%rbx, %r10
 2010                             	        addq	%rax, %rcx
 2011                             	        adcq	$0x00, %r8
 2012                             	        adcq	$0x00, %r9
 2013                             	        adcq	$0x00, %r10
 2014                             	        # Reduce if top bit set
 2015                             	        movq	%r10, %rdx
 2016                             	        sarq	$63, %rdx
 2017                             	        andq	$19, %rdx
 2018                             	        andq	%rbx, %r10
 2019                             	        addq	%rdx, %rcx
 2020                             	        adcq	$0x00, %r8
 2021                             	        adcq	$0x00, %r9
 2022                             	        adcq	$0x00, %r10
 2023                             	        # Store
 2024                             	        movq	%rcx, (%rdi)
 2025                             	        movq	%r8, 8(%rdi)
 2026                             	        movq	%r9, 16(%rdi)
 2027                             	        movq	%r10, 24(%rdi)
 2028                             	        popq	%rbx
 2029                             	        popq	%r15
 2030                             	        popq	%r14
 2031                             	        popq	%r13
 2032                             	        popq	%r12
 2033                             	        repz retq
 2034                             	#ifndef __APPLE__
 2036                             	#endif /* __APPLE__ */
 2037                             	#ifndef __APPLE__
 2038                             	.text
 2039                             	.globl	fe_invert_x64
 2041                             	.align	16
 2042                             	fe_invert_x64:
 2043                             	#else
 2044                             	.section	__TEXT,__text
 2045                             	.globl	_fe_invert_x64
 2046                             	.p2align	4
 2047                             	_fe_invert_x64:
 2048                             	#endif /* __APPLE__ */
 2049                             	        subq	$0x90, %rsp
 2050                             	        # Invert
 2051                             	        movq	%rdi, 128(%rsp)
 2052                             	        movq	%rsi, 136(%rsp)
 2053                             	        movq	%rsp, %rdi
 2054                             	        movq	136(%rsp), %rsi
 2055                             	#ifndef __APPLE__
 2056                             	        callq	fe_sq_x64@plt
 2057                             	#else
 2058                             	        callq	_fe_sq_x64
 2059                             	#endif /* __APPLE__ */
 2060                             	        leaq	32(%rsp), %rdi
 2061                             	        movq	%rsp, %rsi
 2062                             	#ifndef __APPLE__
 2063                             	        callq	fe_sq_x64@plt
 2064                             	#else
 2065                             	        callq	_fe_sq_x64
 2066                             	#endif /* __APPLE__ */
 2067                             	        leaq	32(%rsp), %rdi
 2068                             	        leaq	32(%rsp), %rsi
 2069                             	#ifndef __APPLE__
 2070                             	        callq	fe_sq_x64@plt
 2071                             	#else
 2072                             	        callq	_fe_sq_x64
 2073                             	#endif /* __APPLE__ */
 2074                             	        leaq	32(%rsp), %rdi
 2075                             	        movq	136(%rsp), %rsi
 2076                             	        leaq	32(%rsp), %rdx
 2077                             	#ifndef __APPLE__
 2078                             	        callq	fe_mul_x64@plt
 2079                             	#else
 2080                             	        callq	_fe_mul_x64
 2081                             	#endif /* __APPLE__ */
 2082                             	        movq	%rsp, %rdi
 2083                             	        movq	%rsp, %rsi
 2084                             	        leaq	32(%rsp), %rdx
 2085                             	#ifndef __APPLE__
 2086                             	        callq	fe_mul_x64@plt
 2087                             	#else
 2088                             	        callq	_fe_mul_x64
 2089                             	#endif /* __APPLE__ */
 2090                             	        leaq	64(%rsp), %rdi
 2091                             	        movq	%rsp, %rsi
 2092                             	#ifndef __APPLE__
 2093                             	        callq	fe_sq_x64@plt
 2094                             	#else
 2095                             	        callq	_fe_sq_x64
 2096                             	#endif /* __APPLE__ */
 2097                             	        leaq	32(%rsp), %rdi
 2098                             	        leaq	32(%rsp), %rsi
 2099                             	        leaq	64(%rsp), %rdx
 2100                             	#ifndef __APPLE__
 2101                             	        callq	fe_mul_x64@plt
 2102                             	#else
 2103                             	        callq	_fe_mul_x64
 2104                             	#endif /* __APPLE__ */
 2105                             	        leaq	64(%rsp), %rdi
 2106                             	        leaq	32(%rsp), %rsi
 2107                             	#ifndef __APPLE__
 2108                             	        callq	fe_sq_x64@plt
 2109                             	#else
 2110                             	        callq	_fe_sq_x64
 2111                             	#endif /* __APPLE__ */
 2112                             	        leaq	64(%rsp), %rdi
 2113                             	        leaq	64(%rsp), %rsi
 2114                             	        movq	$4, %rdx
 2115                             	#ifndef __APPLE__
 2116                             	        callq	fe_sq_n_x64@plt
 2117                             	#else
 2118                             	        callq	_fe_sq_n_x64
 2119                             	#endif /* __APPLE__ */
 2120                             	        leaq	32(%rsp), %rdi
 2121                             	        leaq	64(%rsp), %rsi
 2122                             	        leaq	32(%rsp), %rdx
 2123                             	#ifndef __APPLE__
 2124                             	        callq	fe_mul_x64@plt
 2125                             	#else
 2126                             	        callq	_fe_mul_x64
 2127                             	#endif /* __APPLE__ */
 2128                             	        leaq	64(%rsp), %rdi
 2129                             	        leaq	32(%rsp), %rsi
 2130                             	#ifndef __APPLE__
 2131                             	        callq	fe_sq_x64@plt
 2132                             	#else
 2133                             	        callq	_fe_sq_x64
 2134                             	#endif /* __APPLE__ */
 2135                             	        leaq	64(%rsp), %rdi
 2136                             	        leaq	64(%rsp), %rsi
 2137                             	        movq	$9, %rdx
 2138                             	#ifndef __APPLE__
 2139                             	        callq	fe_sq_n_x64@plt
 2140                             	#else
 2141                             	        callq	_fe_sq_n_x64
 2142                             	#endif /* __APPLE__ */
 2143                             	        leaq	64(%rsp), %rdi
 2144                             	        leaq	64(%rsp), %rsi
 2145                             	        leaq	32(%rsp), %rdx
 2146                             	#ifndef __APPLE__
 2147                             	        callq	fe_mul_x64@plt
 2148                             	#else
 2149                             	        callq	_fe_mul_x64
 2150                             	#endif /* __APPLE__ */
 2151                             	        leaq	96(%rsp), %rdi
 2152                             	        leaq	64(%rsp), %rsi
 2153                             	#ifndef __APPLE__
 2154                             	        callq	fe_sq_x64@plt
 2155                             	#else
 2156                             	        callq	_fe_sq_x64
 2157                             	#endif /* __APPLE__ */
 2158                             	        leaq	96(%rsp), %rdi
 2159                             	        leaq	96(%rsp), %rsi
 2160                             	        movq	$19, %rdx
 2161                             	#ifndef __APPLE__
 2162                             	        callq	fe_sq_n_x64@plt
 2163                             	#else
 2164                             	        callq	_fe_sq_n_x64
 2165                             	#endif /* __APPLE__ */
 2166                             	        leaq	64(%rsp), %rdi
 2167                             	        leaq	96(%rsp), %rsi
 2168                             	        leaq	64(%rsp), %rdx
 2169                             	#ifndef __APPLE__
 2170                             	        callq	fe_mul_x64@plt
 2171                             	#else
 2172                             	        callq	_fe_mul_x64
 2173                             	#endif /* __APPLE__ */
 2174                             	        leaq	64(%rsp), %rdi
 2175                             	        leaq	64(%rsp), %rsi
 2176                             	#ifndef __APPLE__
 2177                             	        callq	fe_sq_x64@plt
 2178                             	#else
 2179                             	        callq	_fe_sq_x64
 2180                             	#endif /* __APPLE__ */
 2181                             	        leaq	64(%rsp), %rdi
 2182                             	        leaq	64(%rsp), %rsi
 2183                             	        movq	$9, %rdx
 2184                             	#ifndef __APPLE__
 2185                             	        callq	fe_sq_n_x64@plt
 2186                             	#else
 2187                             	        callq	_fe_sq_n_x64
 2188                             	#endif /* __APPLE__ */
 2189                             	        leaq	32(%rsp), %rdi
 2190                             	        leaq	64(%rsp), %rsi
 2191                             	        leaq	32(%rsp), %rdx
 2192                             	#ifndef __APPLE__
 2193                             	        callq	fe_mul_x64@plt
 2194                             	#else
 2195                             	        callq	_fe_mul_x64
 2196                             	#endif /* __APPLE__ */
 2197                             	        leaq	64(%rsp), %rdi
 2198                             	        leaq	32(%rsp), %rsi
 2199                             	#ifndef __APPLE__
 2200                             	        callq	fe_sq_x64@plt
 2201                             	#else
 2202                             	        callq	_fe_sq_x64
 2203                             	#endif /* __APPLE__ */
 2204                             	        leaq	64(%rsp), %rdi
 2205                             	        leaq	64(%rsp), %rsi
 2206                             	        movq	$49, %rdx
 2207                             	#ifndef __APPLE__
 2208                             	        callq	fe_sq_n_x64@plt
 2209                             	#else
 2210                             	        callq	_fe_sq_n_x64
 2211                             	#endif /* __APPLE__ */
 2212                             	        leaq	64(%rsp), %rdi
 2213                             	        leaq	64(%rsp), %rsi
 2214                             	        leaq	32(%rsp), %rdx
 2215                             	#ifndef __APPLE__
 2216                             	        callq	fe_mul_x64@plt
 2217                             	#else
 2218                             	        callq	_fe_mul_x64
 2219                             	#endif /* __APPLE__ */
 2220                             	        leaq	96(%rsp), %rdi
 2221                             	        leaq	64(%rsp), %rsi
 2222                             	#ifndef __APPLE__
 2223                             	        callq	fe_sq_x64@plt
 2224                             	#else
 2225                             	        callq	_fe_sq_x64
 2226                             	#endif /* __APPLE__ */
 2227                             	        leaq	96(%rsp), %rdi
 2228                             	        leaq	96(%rsp), %rsi
 2229                             	        movq	$0x63, %rdx
 2230                             	#ifndef __APPLE__
 2231                             	        callq	fe_sq_n_x64@plt
 2232                             	#else
 2233                             	        callq	_fe_sq_n_x64
 2234                             	#endif /* __APPLE__ */
 2235                             	        leaq	64(%rsp), %rdi
 2236                             	        leaq	96(%rsp), %rsi
 2237                             	        leaq	64(%rsp), %rdx
 2238                             	#ifndef __APPLE__
 2239                             	        callq	fe_mul_x64@plt
 2240                             	#else
 2241                             	        callq	_fe_mul_x64
 2242                             	#endif /* __APPLE__ */
 2243                             	        leaq	64(%rsp), %rdi
 2244                             	        leaq	64(%rsp), %rsi
 2245                             	#ifndef __APPLE__
 2246                             	        callq	fe_sq_x64@plt
 2247                             	#else
 2248                             	        callq	_fe_sq_x64
 2249                             	#endif /* __APPLE__ */
 2250                             	        leaq	64(%rsp), %rdi
 2251                             	        leaq	64(%rsp), %rsi
 2252                             	        movq	$49, %rdx
 2253                             	#ifndef __APPLE__
 2254                             	        callq	fe_sq_n_x64@plt
 2255                             	#else
 2256                             	        callq	_fe_sq_n_x64
 2257                             	#endif /* __APPLE__ */
 2258                             	        leaq	32(%rsp), %rdi
 2259                             	        leaq	64(%rsp), %rsi
 2260                             	        leaq	32(%rsp), %rdx
 2261                             	#ifndef __APPLE__
 2262                             	        callq	fe_mul_x64@plt
 2263                             	#else
 2264                             	        callq	_fe_mul_x64
 2265                             	#endif /* __APPLE__ */
 2266                             	        leaq	32(%rsp), %rdi
 2267                             	        leaq	32(%rsp), %rsi
 2268                             	#ifndef __APPLE__
 2269                             	        callq	fe_sq_x64@plt
 2270                             	#else
 2271                             	        callq	_fe_sq_x64
 2272                             	#endif /* __APPLE__ */
 2273                             	        leaq	32(%rsp), %rdi
 2274                             	        leaq	32(%rsp), %rsi
 2275                             	        movq	$4, %rdx
 2276                             	#ifndef __APPLE__
 2277                             	        callq	fe_sq_n_x64@plt
 2278                             	#else
 2279                             	        callq	_fe_sq_n_x64
 2280                             	#endif /* __APPLE__ */
 2281                             	        movq	128(%rsp), %rdi
 2282                             	        leaq	32(%rsp), %rsi
 2283                             	        movq	%rsp, %rdx
 2284                             	#ifndef __APPLE__
 2285                             	        callq	fe_mul_x64@plt
 2286                             	#else
 2287                             	        callq	_fe_mul_x64
 2288                             	#endif /* __APPLE__ */
 2289                             	        movq	136(%rsp), %rsi
 2290                             	        movq	128(%rsp), %rdi
 2291                             	        addq	$0x90, %rsp
 2292                             	        repz retq
 2293                             	#ifndef __APPLE__
 2294                             	.text
 2295                             	.globl	curve25519_x64
 2297                             	.align	16
 2298                             	curve25519_x64:
 2299                             	#else
 2300                             	.section	__TEXT,__text
 2301                             	.globl	_curve25519_x64
 2302                             	.p2align	4
 2303                             	_curve25519_x64:
 2304                             	#endif /* __APPLE__ */
 2305                             	        pushq	%r12
 2306                             	        pushq	%r13
 2307                             	        pushq	%r14
 2308                             	        pushq	%r15
 2309                             	        pushq	%rbx
 2310                             	        pushq	%rbp
 2311                             	        movq	%rdx, %r8
 2312                             	        subq	$0xb8, %rsp
 2313                             	        xorq	%rbx, %rbx
 2314                             	        movq	%rdi, 176(%rsp)
 2315                             	        # Set one
 2316                             	        movq	$0x01, (%rdi)
 2317                             	        movq	$0x00, 8(%rdi)
 2318                             	        movq	$0x00, 16(%rdi)
 2319                             	        movq	$0x00, 24(%rdi)
 2320                             	        # Set zero
 2321                             	        movq	$0x00, (%rsp)
 2322                             	        movq	$0x00, 8(%rsp)
 2323                             	        movq	$0x00, 16(%rsp)
 2324                             	        movq	$0x00, 24(%rsp)
 2325                             	        # Set one
 2326                             	        movq	$0x01, 32(%rsp)
 2327                             	        movq	$0x00, 40(%rsp)
 2328                             	        movq	$0x00, 48(%rsp)
 2329                             	        movq	$0x00, 56(%rsp)
 2330                             	        # Copy
 2331                             	        movq	(%r8), %rcx
 2332                             	        movq	8(%r8), %r9
 2333                             	        movq	16(%r8), %r10
 2334                             	        movq	24(%r8), %r11
 2335                             	        movq	%rcx, 64(%rsp)
 2336                             	        movq	%r9, 72(%rsp)
 2337                             	        movq	%r10, 80(%rsp)
 2338                             	        movq	%r11, 88(%rsp)
 2339                             	        movb	$62, 168(%rsp)
 2340                             	        movq	$3, 160(%rsp)
 2341                             	L_curve25519_x64_words:
 2342                             	L_curve25519_x64_bits:
 2343                             	        movq	160(%rsp), %r9
 2344                             	        movb	168(%rsp), %cl
 2345                             	        movq	(%rsi,%r9,8), %rbp
 2346                             	        shrq	%cl, %rbp
 2347                             	        andq	$0x01, %rbp
 2348                             	        xorq	%rbp, %rbx
 2349                             	        negq	%rbx
 2350                             	        # Conditional Swap
 2351                             	        movq	(%rdi), %rcx
 2352                             	        movq	8(%rdi), %r9
 2353                             	        movq	16(%rdi), %r10
 2354                             	        movq	24(%rdi), %r11
 2355                             	        xorq	64(%rsp), %rcx
 2356                             	        xorq	72(%rsp), %r9
 2357                             	        xorq	80(%rsp), %r10
 2358                             	        xorq	88(%rsp), %r11
 2359                             	        andq	%rbx, %rcx
 2360                             	        andq	%rbx, %r9
 2361                             	        andq	%rbx, %r10
 2362                             	        andq	%rbx, %r11
 2363                             	        xorq	%rcx, (%rdi)
 2364                             	        xorq	%r9, 8(%rdi)
 2365                             	        xorq	%r10, 16(%rdi)
 2366                             	        xorq	%r11, 24(%rdi)
 2367                             	        xorq	%rcx, 64(%rsp)
 2368                             	        xorq	%r9, 72(%rsp)
 2369                             	        xorq	%r10, 80(%rsp)
 2370                             	        xorq	%r11, 88(%rsp)
 2371                             	        # Conditional Swap
 2372                             	        movq	(%rsp), %rcx
 2373                             	        movq	8(%rsp), %r9
 2374                             	        movq	16(%rsp), %r10
 2375                             	        movq	24(%rsp), %r11
 2376                             	        xorq	32(%rsp), %rcx
 2377                             	        xorq	40(%rsp), %r9
 2378                             	        xorq	48(%rsp), %r10
 2379                             	        xorq	56(%rsp), %r11
 2380                             	        andq	%rbx, %rcx
 2381                             	        andq	%rbx, %r9
 2382                             	        andq	%rbx, %r10
 2383                             	        andq	%rbx, %r11
 2384                             	        xorq	%rcx, (%rsp)
 2385                             	        xorq	%r9, 8(%rsp)
 2386                             	        xorq	%r10, 16(%rsp)
 2387                             	        xorq	%r11, 24(%rsp)
 2388                             	        xorq	%rcx, 32(%rsp)
 2389                             	        xorq	%r9, 40(%rsp)
 2390                             	        xorq	%r10, 48(%rsp)
 2391                             	        xorq	%r11, 56(%rsp)
 2392                             	        movq	%rbp, %rbx
 2393                             	        # Add
 2394                             	        movq	(%rdi), %rcx
 2395                             	        movq	8(%rdi), %r9
 2396                             	        movq	16(%rdi), %r10
 2397                             	        movq	24(%rdi), %rbp
 2398                             	        movq	%rcx, %r12
 2399                             	        addq	(%rsp), %rcx
 2400                             	        movq	%r9, %r13
 2401                             	        adcq	8(%rsp), %r9
 2402                             	        movq	%r10, %r14
 2403                             	        adcq	16(%rsp), %r10
 2404                             	        movq	%rbp, %r15
 2405                             	        adcq	24(%rsp), %rbp
 2406                             	        movq	$-19, %rax
 2407                             	        movq	%rbp, %r11
 2408                             	        movq	$0x7fffffffffffffff, %rdx
 2409                             	        sarq	$63, %rbp
 2410                             	        #   Mask the modulus
 2411                             	        andq	%rbp, %rax
 2412                             	        andq	%rbp, %rdx
 2413                             	        #   Sub modulus (if overflow)
 2414                             	        subq	%rax, %rcx
 2415                             	        sbbq	%rbp, %r9
 2416                             	        sbbq	%rbp, %r10
 2417                             	        sbbq	%rdx, %r11
 2418                             	        # Sub
 2419                             	        subq	(%rsp), %r12
 2420                             	        movq	$0x00, %rbp
 2421                             	        sbbq	8(%rsp), %r13
 2422                             	        movq	$-19, %rax
 2423                             	        sbbq	16(%rsp), %r14
 2424                             	        movq	$0x7fffffffffffffff, %rdx
 2425                             	        sbbq	24(%rsp), %r15
 2426                             	        sbbq	$0x00, %rbp
 2427                             	        #   Mask the modulus
 2428                             	        andq	%rbp, %rax
 2429                             	        andq	%rbp, %rdx
 2430                             	        #   Add modulus (if underflow)
 2431                             	        addq	%rax, %r12
 2432                             	        adcq	%rbp, %r13
 2433                             	        adcq	%rbp, %r14
 2434                             	        adcq	%rdx, %r15
 2435                             	        movq	%rcx, (%rdi)
 2436                             	        movq	%r9, 8(%rdi)
 2437                             	        movq	%r10, 16(%rdi)
 2438                             	        movq	%r11, 24(%rdi)
 2439                             	        movq	%r12, 128(%rsp)
 2440                             	        movq	%r13, 136(%rsp)
 2441                             	        movq	%r14, 144(%rsp)
 2442                             	        movq	%r15, 152(%rsp)
 2443                             	        # Add
 2444                             	        movq	64(%rsp), %rcx
 2445                             	        movq	72(%rsp), %r9
 2446                             	        movq	80(%rsp), %r10
 2447                             	        movq	88(%rsp), %rbp
 2448                             	        movq	%rcx, %r12
 2449                             	        addq	32(%rsp), %rcx
 2450                             	        movq	%r9, %r13
 2451                             	        adcq	40(%rsp), %r9
 2452                             	        movq	%r10, %r14
 2453                             	        adcq	48(%rsp), %r10
 2454                             	        movq	%rbp, %r15
 2455                             	        adcq	56(%rsp), %rbp
 2456                             	        movq	$-19, %rax
 2457                             	        movq	%rbp, %r11
 2458                             	        movq	$0x7fffffffffffffff, %rdx
 2459                             	        sarq	$63, %rbp
 2460                             	        #   Mask the modulus
 2461                             	        andq	%rbp, %rax
 2462                             	        andq	%rbp, %rdx
 2463                             	        #   Sub modulus (if overflow)
 2464                             	        subq	%rax, %rcx
 2465                             	        sbbq	%rbp, %r9
 2466                             	        sbbq	%rbp, %r10
 2467                             	        sbbq	%rdx, %r11
 2468                             	        # Sub
 2469                             	        subq	32(%rsp), %r12
 2470                             	        movq	$0x00, %rbp
 2471                             	        sbbq	40(%rsp), %r13
 2472                             	        movq	$-19, %rax
 2473                             	        sbbq	48(%rsp), %r14
 2474                             	        movq	$0x7fffffffffffffff, %rdx
 2475                             	        sbbq	56(%rsp), %r15
 2476                             	        sbbq	$0x00, %rbp
 2477                             	        #   Mask the modulus
 2478                             	        andq	%rbp, %rax
 2479                             	        andq	%rbp, %rdx
 2480                             	        #   Add modulus (if underflow)
 2481                             	        addq	%rax, %r12
 2482                             	        adcq	%rbp, %r13
 2483                             	        adcq	%rbp, %r14
 2484                             	        adcq	%rdx, %r15
 2485                             	        movq	%rcx, (%rsp)
 2486                             	        movq	%r9, 8(%rsp)
 2487                             	        movq	%r10, 16(%rsp)
 2488                             	        movq	%r11, 24(%rsp)
 2489                             	        movq	%r12, 96(%rsp)
 2490                             	        movq	%r13, 104(%rsp)
 2491                             	        movq	%r14, 112(%rsp)
 2492                             	        movq	%r15, 120(%rsp)
 2493                             	        # Multiply
 2494                             	        #  A[0] * B[0]
 2495                             	        movq	(%rdi), %rax
 2496                             	        mulq	96(%rsp)
 2497                             	        movq	%rax, %rcx
 2498                             	        movq	%rdx, %r9
 2499                             	        #  A[0] * B[1]
 2500                             	        movq	8(%rdi), %rax
 2501                             	        mulq	96(%rsp)
 2502                             	        xorq	%r10, %r10
 2503                             	        addq	%rax, %r9
 2504                             	        adcq	%rdx, %r10
 2505                             	        #  A[1] * B[0]
 2506                             	        movq	(%rdi), %rax
 2507                             	        mulq	104(%rsp)
 2508                             	        xorq	%r11, %r11
 2509                             	        addq	%rax, %r9
 2510                             	        adcq	%rdx, %r10
 2511                             	        adcq	$0x00, %r11
 2512                             	        #  A[0] * B[2]
 2513                             	        movq	16(%rdi), %rax
 2514                             	        mulq	96(%rsp)
 2515                             	        addq	%rax, %r10
 2516                             	        adcq	%rdx, %r11
 2517                             	        #  A[1] * B[1]
 2518                             	        movq	8(%rdi), %rax
 2519                             	        mulq	104(%rsp)
 2520                             	        xorq	%r12, %r12
 2521                             	        addq	%rax, %r10
 2522                             	        adcq	%rdx, %r11
 2523                             	        adcq	$0x00, %r12
 2524                             	        #  A[2] * B[0]
 2525                             	        movq	(%rdi), %rax
 2526                             	        mulq	112(%rsp)
 2527                             	        addq	%rax, %r10
 2528                             	        adcq	%rdx, %r11
 2529                             	        adcq	$0x00, %r12
 2530                             	        #  A[0] * B[3]
 2531                             	        movq	24(%rdi), %rax
 2532                             	        mulq	96(%rsp)
 2533                             	        xorq	%r13, %r13
 2534                             	        addq	%rax, %r11
 2535                             	        adcq	%rdx, %r12
 2536                             	        adcq	$0x00, %r13
 2537                             	        #  A[1] * B[2]
 2538                             	        movq	16(%rdi), %rax
 2539                             	        mulq	104(%rsp)
 2540                             	        addq	%rax, %r11
 2541                             	        adcq	%rdx, %r12
 2542                             	        adcq	$0x00, %r13
 2543                             	        #  A[2] * B[1]
 2544                             	        movq	8(%rdi), %rax
 2545                             	        mulq	112(%rsp)
 2546                             	        addq	%rax, %r11
 2547                             	        adcq	%rdx, %r12
 2548                             	        adcq	$0x00, %r13
 2549                             	        #  A[3] * B[0]
 2550                             	        movq	(%rdi), %rax
 2551                             	        mulq	120(%rsp)
 2552                             	        addq	%rax, %r11
 2553                             	        adcq	%rdx, %r12
 2554                             	        adcq	$0x00, %r13
 2555                             	        #  A[1] * B[3]
 2556                             	        movq	24(%rdi), %rax
 2557                             	        mulq	104(%rsp)
 2558                             	        xorq	%r14, %r14
 2559                             	        addq	%rax, %r12
 2560                             	        adcq	%rdx, %r13
 2561                             	        adcq	$0x00, %r14
 2562                             	        #  A[2] * B[2]
 2563                             	        movq	16(%rdi), %rax
 2564                             	        mulq	112(%rsp)
 2565                             	        addq	%rax, %r12
 2566                             	        adcq	%rdx, %r13
 2567                             	        adcq	$0x00, %r14
 2568                             	        #  A[3] * B[1]
 2569                             	        movq	8(%rdi), %rax
 2570                             	        mulq	120(%rsp)
 2571                             	        addq	%rax, %r12
 2572                             	        adcq	%rdx, %r13
 2573                             	        adcq	$0x00, %r14
 2574                             	        #  A[2] * B[3]
 2575                             	        movq	24(%rdi), %rax
 2576                             	        mulq	112(%rsp)
 2577                             	        xorq	%r15, %r15
 2578                             	        addq	%rax, %r13
 2579                             	        adcq	%rdx, %r14
 2580                             	        adcq	$0x00, %r15
 2581                             	        #  A[3] * B[2]
 2582                             	        movq	16(%rdi), %rax
 2583                             	        mulq	120(%rsp)
 2584                             	        addq	%rax, %r13
 2585                             	        adcq	%rdx, %r14
 2586                             	        adcq	$0x00, %r15
 2587                             	        #  A[3] * B[3]
 2588                             	        movq	24(%rdi), %rax
 2589                             	        mulq	120(%rsp)
 2590                             	        addq	%rax, %r14
 2591                             	        adcq	%rdx, %r15
 2592                             	        # Reduce
 2593                             	        movq	$0x7fffffffffffffff, %rbp
 2594                             	        #  Move top half into t4-t7 and remove top bit from t3
 2595                             	        shldq	$0x01, %r14, %r15
 2596                             	        shldq	$0x01, %r13, %r14
 2597                             	        shldq	$0x01, %r12, %r13
 2598                             	        shldq	$0x01, %r11, %r12
 2599                             	        andq	%rbp, %r11
 2600                             	        #  Multiply top half by 19
 2601                             	        movq	$19, %rax
 2602                             	        mulq	%r12
 2603                             	        xorq	%r12, %r12
 2604                             	        addq	%rax, %rcx
 2605                             	        movq	$19, %rax
 2606                             	        adcq	%rdx, %r12
 2607                             	        mulq	%r13
 2608                             	        xorq	%r13, %r13
 2609                             	        addq	%rax, %r9
 2610                             	        movq	$19, %rax
 2611                             	        adcq	%rdx, %r13
 2612                             	        mulq	%r14
 2613                             	        xorq	%r14, %r14
 2614                             	        addq	%rax, %r10
 2615                             	        movq	$19, %rax
 2616                             	        adcq	%rdx, %r14
 2617                             	        mulq	%r15
 2618                             	        #  Add remaining product results in
 2619                             	        addq	%r12, %r9
 2620                             	        adcq	%r13, %r10
 2621                             	        adcq	%r14, %r11
 2622                             	        adcq	%rax, %r11
 2623                             	        adcq	$0x00, %rdx
 2624                             	        #  Overflow
 2625                             	        shldq	$0x01, %r11, %rdx
 2626                             	        imulq	$19, %rdx, %rax
 2627                             	        andq	%rbp, %r11
 2628                             	        addq	%rax, %rcx
 2629                             	        adcq	$0x00, %r9
 2630                             	        adcq	$0x00, %r10
 2631                             	        adcq	$0x00, %r11
 2632                             	        # Reduce if top bit set
 2633                             	        movq	%r11, %rdx
 2634                             	        sarq	$63, %rdx
 2635                             	        andq	$19, %rdx
 2636                             	        andq	%rbp, %r11
 2637                             	        addq	%rdx, %rcx
 2638                             	        adcq	$0x00, %r9
 2639                             	        adcq	$0x00, %r10
 2640                             	        adcq	$0x00, %r11
 2641                             	        # Store
 2642                             	        movq	%rcx, 32(%rsp)
 2643                             	        movq	%r9, 40(%rsp)
 2644                             	        movq	%r10, 48(%rsp)
 2645                             	        movq	%r11, 56(%rsp)
 2646                             	        # Multiply
 2647                             	        #  A[0] * B[0]
 2648                             	        movq	128(%rsp), %rax
 2649                             	        mulq	(%rsp)
 2650                             	        movq	%rax, %rcx
 2651                             	        movq	%rdx, %r9
 2652                             	        #  A[0] * B[1]
 2653                             	        movq	136(%rsp), %rax
 2654                             	        mulq	(%rsp)
 2655                             	        xorq	%r10, %r10
 2656                             	        addq	%rax, %r9
 2657                             	        adcq	%rdx, %r10
 2658                             	        #  A[1] * B[0]
 2659                             	        movq	128(%rsp), %rax
 2660                             	        mulq	8(%rsp)
 2661                             	        xorq	%r11, %r11
 2662                             	        addq	%rax, %r9
 2663                             	        adcq	%rdx, %r10
 2664                             	        adcq	$0x00, %r11
 2665                             	        #  A[0] * B[2]
 2666                             	        movq	144(%rsp), %rax
 2667                             	        mulq	(%rsp)
 2668                             	        addq	%rax, %r10
 2669                             	        adcq	%rdx, %r11
 2670                             	        #  A[1] * B[1]
 2671                             	        movq	136(%rsp), %rax
 2672                             	        mulq	8(%rsp)
 2673                             	        xorq	%r12, %r12
 2674                             	        addq	%rax, %r10
 2675                             	        adcq	%rdx, %r11
 2676                             	        adcq	$0x00, %r12
 2677                             	        #  A[2] * B[0]
 2678                             	        movq	128(%rsp), %rax
 2679                             	        mulq	16(%rsp)
 2680                             	        addq	%rax, %r10
 2681                             	        adcq	%rdx, %r11
 2682                             	        adcq	$0x00, %r12
 2683                             	        #  A[0] * B[3]
 2684                             	        movq	152(%rsp), %rax
 2685                             	        mulq	(%rsp)
 2686                             	        xorq	%r13, %r13
 2687                             	        addq	%rax, %r11
 2688                             	        adcq	%rdx, %r12
 2689                             	        adcq	$0x00, %r13
 2690                             	        #  A[1] * B[2]
 2691                             	        movq	144(%rsp), %rax
 2692                             	        mulq	8(%rsp)
 2693                             	        addq	%rax, %r11
 2694                             	        adcq	%rdx, %r12
 2695                             	        adcq	$0x00, %r13
 2696                             	        #  A[2] * B[1]
 2697                             	        movq	136(%rsp), %rax
 2698                             	        mulq	16(%rsp)
 2699                             	        addq	%rax, %r11
 2700                             	        adcq	%rdx, %r12
 2701                             	        adcq	$0x00, %r13
 2702                             	        #  A[3] * B[0]
 2703                             	        movq	128(%rsp), %rax
 2704                             	        mulq	24(%rsp)
 2705                             	        addq	%rax, %r11
 2706                             	        adcq	%rdx, %r12
 2707                             	        adcq	$0x00, %r13
 2708                             	        #  A[1] * B[3]
 2709                             	        movq	152(%rsp), %rax
 2710                             	        mulq	8(%rsp)
 2711                             	        xorq	%r14, %r14
 2712                             	        addq	%rax, %r12
 2713                             	        adcq	%rdx, %r13
 2714                             	        adcq	$0x00, %r14
 2715                             	        #  A[2] * B[2]
 2716                             	        movq	144(%rsp), %rax
 2717                             	        mulq	16(%rsp)
 2718                             	        addq	%rax, %r12
 2719                             	        adcq	%rdx, %r13
 2720                             	        adcq	$0x00, %r14
 2721                             	        #  A[3] * B[1]
 2722                             	        movq	136(%rsp), %rax
 2723                             	        mulq	24(%rsp)
 2724                             	        addq	%rax, %r12
 2725                             	        adcq	%rdx, %r13
 2726                             	        adcq	$0x00, %r14
 2727                             	        #  A[2] * B[3]
 2728                             	        movq	152(%rsp), %rax
 2729                             	        mulq	16(%rsp)
 2730                             	        xorq	%r15, %r15
 2731                             	        addq	%rax, %r13
 2732                             	        adcq	%rdx, %r14
 2733                             	        adcq	$0x00, %r15
 2734                             	        #  A[3] * B[2]
 2735                             	        movq	144(%rsp), %rax
 2736                             	        mulq	24(%rsp)
 2737                             	        addq	%rax, %r13
 2738                             	        adcq	%rdx, %r14
 2739                             	        adcq	$0x00, %r15
 2740                             	        #  A[3] * B[3]
 2741                             	        movq	152(%rsp), %rax
 2742                             	        mulq	24(%rsp)
 2743                             	        addq	%rax, %r14
 2744                             	        adcq	%rdx, %r15
 2745                             	        # Reduce
 2746                             	        movq	$0x7fffffffffffffff, %rbp
 2747                             	        #  Move top half into t4-t7 and remove top bit from t3
 2748                             	        shldq	$0x01, %r14, %r15
 2749                             	        shldq	$0x01, %r13, %r14
 2750                             	        shldq	$0x01, %r12, %r13
 2751                             	        shldq	$0x01, %r11, %r12
 2752                             	        andq	%rbp, %r11
 2753                             	        #  Multiply top half by 19
 2754                             	        movq	$19, %rax
 2755                             	        mulq	%r12
 2756                             	        xorq	%r12, %r12
 2757                             	        addq	%rax, %rcx
 2758                             	        movq	$19, %rax
 2759                             	        adcq	%rdx, %r12
 2760                             	        mulq	%r13
 2761                             	        xorq	%r13, %r13
 2762                             	        addq	%rax, %r9
 2763                             	        movq	$19, %rax
 2764                             	        adcq	%rdx, %r13
 2765                             	        mulq	%r14
 2766                             	        xorq	%r14, %r14
 2767                             	        addq	%rax, %r10
 2768                             	        movq	$19, %rax
 2769                             	        adcq	%rdx, %r14
 2770                             	        mulq	%r15
 2771                             	        #  Add remaining product results in
 2772                             	        addq	%r12, %r9
 2773                             	        adcq	%r13, %r10
 2774                             	        adcq	%r14, %r11
 2775                             	        adcq	%rax, %r11
 2776                             	        adcq	$0x00, %rdx
 2777                             	        #  Overflow
 2778                             	        shldq	$0x01, %r11, %rdx
 2779                             	        imulq	$19, %rdx, %rax
 2780                             	        andq	%rbp, %r11
 2781                             	        addq	%rax, %rcx
 2782                             	        adcq	$0x00, %r9
 2783                             	        adcq	$0x00, %r10
 2784                             	        adcq	$0x00, %r11
 2785                             	        # Reduce if top bit set
 2786                             	        movq	%r11, %rdx
 2787                             	        sarq	$63, %rdx
 2788                             	        andq	$19, %rdx
 2789                             	        andq	%rbp, %r11
 2790                             	        addq	%rdx, %rcx
 2791                             	        adcq	$0x00, %r9
 2792                             	        adcq	$0x00, %r10
 2793                             	        adcq	$0x00, %r11
 2794                             	        # Store
 2795                             	        movq	%rcx, (%rsp)
 2796                             	        movq	%r9, 8(%rsp)
 2797                             	        movq	%r10, 16(%rsp)
 2798                             	        movq	%r11, 24(%rsp)
 2799                             	        # Square
 2800                             	        #  A[0] * A[1]
 2801                             	        movq	128(%rsp), %rax
 2802                             	        mulq	136(%rsp)
 2803                             	        movq	%rax, %r9
 2804                             	        movq	%rdx, %r10
 2805                             	        #  A[0] * A[2]
 2806                             	        movq	128(%rsp), %rax
 2807                             	        mulq	144(%rsp)
 2808                             	        xorq	%r11, %r11
 2809                             	        addq	%rax, %r10
 2810                             	        adcq	%rdx, %r11
 2811                             	        #  A[0] * A[3]
 2812                             	        movq	128(%rsp), %rax
 2813                             	        mulq	152(%rsp)
 2814                             	        xorq	%r12, %r12
 2815                             	        addq	%rax, %r11
 2816                             	        adcq	%rdx, %r12
 2817                             	        #  A[1] * A[2]
 2818                             	        movq	136(%rsp), %rax
 2819                             	        mulq	144(%rsp)
 2820                             	        xorq	%r13, %r13
 2821                             	        addq	%rax, %r11
 2822                             	        adcq	%rdx, %r12
 2823                             	        adcq	$0x00, %r13
 2824                             	        #  A[1] * A[3]
 2825                             	        movq	136(%rsp), %rax
 2826                             	        mulq	152(%rsp)
 2827                             	        addq	%rax, %r12
 2828                             	        adcq	%rdx, %r13
 2829                             	        #  A[2] * A[3]
 2830                             	        movq	144(%rsp), %rax
 2831                             	        mulq	152(%rsp)
 2832                             	        xorq	%r14, %r14
 2833                             	        addq	%rax, %r13
 2834                             	        adcq	%rdx, %r14
 2835                             	        # Double
 2836                             	        xorq	%r15, %r15
 2837                             	        addq	%r9, %r9
 2838                             	        adcq	%r10, %r10
 2839                             	        adcq	%r11, %r11
 2840                             	        adcq	%r12, %r12
 2841                             	        adcq	%r13, %r13
 2842                             	        adcq	%r14, %r14
 2843                             	        adcq	$0x00, %r15
 2844                             	        #  A[0] * A[0]
 2845                             	        movq	128(%rsp), %rax
 2846                             	        mulq	%rax
 2847                             	        movq	%rax, %rcx
 2848                             	        movq	%rdx, %rbp
 2849                             	        #  A[1] * A[1]
 2850                             	        movq	136(%rsp), %rax
 2851                             	        mulq	%rax
 2852                             	        addq	%rbp, %r9
 2853                             	        adcq	%rax, %r10
 2854                             	        adcq	$0x00, %rdx
 2855                             	        movq	%rdx, %rbp
 2856                             	        #  A[2] * A[2]
 2857                             	        movq	144(%rsp), %rax
 2858                             	        mulq	%rax
 2859                             	        addq	%rbp, %r11
 2860                             	        adcq	%rax, %r12
 2861                             	        adcq	$0x00, %rdx
 2862                             	        movq	%rdx, %rbp
 2863                             	        #  A[3] * A[3]
 2864                             	        movq	152(%rsp), %rax
 2865                             	        mulq	%rax
 2866                             	        addq	%rax, %r14
 2867                             	        adcq	%rdx, %r15
 2868                             	        addq	%rbp, %r13
 2869                             	        adcq	$0x00, %r14
 2870                             	        adcq	$0x00, %r15
 2871                             	        # Reduce
 2872                             	        movq	$0x7fffffffffffffff, %rbp
 2873                             	        #  Move top half into t4-t7 and remove top bit from t3
 2874                             	        shldq	$0x01, %r14, %r15
 2875                             	        shldq	$0x01, %r13, %r14
 2876                             	        shldq	$0x01, %r12, %r13
 2877                             	        shldq	$0x01, %r11, %r12
 2878                             	        andq	%rbp, %r11
 2879                             	        #  Multiply top half by 19
 2880                             	        movq	$19, %rax
 2881                             	        mulq	%r12
 2882                             	        xorq	%r12, %r12
 2883                             	        addq	%rax, %rcx
 2884                             	        movq	$19, %rax
 2885                             	        adcq	%rdx, %r12
 2886                             	        mulq	%r13
 2887                             	        xorq	%r13, %r13
 2888                             	        addq	%rax, %r9
 2889                             	        movq	$19, %rax
 2890                             	        adcq	%rdx, %r13
 2891                             	        mulq	%r14
 2892                             	        xorq	%r14, %r14
 2893                             	        addq	%rax, %r10
 2894                             	        movq	$19, %rax
 2895                             	        adcq	%rdx, %r14
 2896                             	        mulq	%r15
 2897                             	        #  Add remaining product results in
 2898                             	        addq	%r12, %r9
 2899                             	        adcq	%r13, %r10
 2900                             	        adcq	%r14, %r11
 2901                             	        adcq	%rax, %r11
 2902                             	        adcq	$0x00, %rdx
 2903                             	        #  Overflow
 2904                             	        shldq	$0x01, %r11, %rdx
 2905                             	        imulq	$19, %rdx, %rax
 2906                             	        andq	%rbp, %r11
 2907                             	        addq	%rax, %rcx
 2908                             	        adcq	$0x00, %r9
 2909                             	        adcq	$0x00, %r10
 2910                             	        adcq	$0x00, %r11
 2911                             	        # Reduce if top bit set
 2912                             	        movq	%r11, %rdx
 2913                             	        sarq	$63, %rdx
 2914                             	        andq	$19, %rdx
 2915                             	        andq	%rbp, %r11
 2916                             	        addq	%rdx, %rcx
 2917                             	        adcq	$0x00, %r9
 2918                             	        adcq	$0x00, %r10
 2919                             	        adcq	$0x00, %r11
 2920                             	        # Store
 2921                             	        movq	%rcx, 96(%rsp)
 2922                             	        movq	%r9, 104(%rsp)
 2923                             	        movq	%r10, 112(%rsp)
 2924                             	        movq	%r11, 120(%rsp)
 2925                             	        # Square
 2926                             	        #  A[0] * A[1]
 2927                             	        movq	(%rdi), %rax
 2928                             	        mulq	8(%rdi)
 2929                             	        movq	%rax, %r9
 2930                             	        movq	%rdx, %r10
 2931                             	        #  A[0] * A[2]
 2932                             	        movq	(%rdi), %rax
 2933                             	        mulq	16(%rdi)
 2934                             	        xorq	%r11, %r11
 2935                             	        addq	%rax, %r10
 2936                             	        adcq	%rdx, %r11
 2937                             	        #  A[0] * A[3]
 2938                             	        movq	(%rdi), %rax
 2939                             	        mulq	24(%rdi)
 2940                             	        xorq	%r12, %r12
 2941                             	        addq	%rax, %r11
 2942                             	        adcq	%rdx, %r12
 2943                             	        #  A[1] * A[2]
 2944                             	        movq	8(%rdi), %rax
 2945                             	        mulq	16(%rdi)
 2946                             	        xorq	%r13, %r13
 2947                             	        addq	%rax, %r11
 2948                             	        adcq	%rdx, %r12
 2949                             	        adcq	$0x00, %r13
 2950                             	        #  A[1] * A[3]
 2951                             	        movq	8(%rdi), %rax
 2952                             	        mulq	24(%rdi)
 2953                             	        addq	%rax, %r12
 2954                             	        adcq	%rdx, %r13
 2955                             	        #  A[2] * A[3]
 2956                             	        movq	16(%rdi), %rax
 2957                             	        mulq	24(%rdi)
 2958                             	        xorq	%r14, %r14
 2959                             	        addq	%rax, %r13
 2960                             	        adcq	%rdx, %r14
 2961                             	        # Double
 2962                             	        xorq	%r15, %r15
 2963                             	        addq	%r9, %r9
 2964                             	        adcq	%r10, %r10
 2965                             	        adcq	%r11, %r11
 2966                             	        adcq	%r12, %r12
 2967                             	        adcq	%r13, %r13
 2968                             	        adcq	%r14, %r14
 2969                             	        adcq	$0x00, %r15
 2970                             	        #  A[0] * A[0]
 2971                             	        movq	(%rdi), %rax
 2972                             	        mulq	%rax
 2973                             	        movq	%rax, %rcx
 2974                             	        movq	%rdx, %rbp
 2975                             	        #  A[1] * A[1]
 2976                             	        movq	8(%rdi), %rax
 2977                             	        mulq	%rax
 2978                             	        addq	%rbp, %r9
 2979                             	        adcq	%rax, %r10
 2980                             	        adcq	$0x00, %rdx
 2981                             	        movq	%rdx, %rbp
 2982                             	        #  A[2] * A[2]
 2983                             	        movq	16(%rdi), %rax
 2984                             	        mulq	%rax
 2985                             	        addq	%rbp, %r11
 2986                             	        adcq	%rax, %r12
 2987                             	        adcq	$0x00, %rdx
 2988                             	        movq	%rdx, %rbp
 2989                             	        #  A[3] * A[3]
 2990                             	        movq	24(%rdi), %rax
 2991                             	        mulq	%rax
 2992                             	        addq	%rax, %r14
 2993                             	        adcq	%rdx, %r15
 2994                             	        addq	%rbp, %r13
 2995                             	        adcq	$0x00, %r14
 2996                             	        adcq	$0x00, %r15
 2997                             	        # Reduce
 2998                             	        movq	$0x7fffffffffffffff, %rbp
 2999                             	        #  Move top half into t4-t7 and remove top bit from t3
 3000                             	        shldq	$0x01, %r14, %r15
 3001                             	        shldq	$0x01, %r13, %r14
 3002                             	        shldq	$0x01, %r12, %r13
 3003                             	        shldq	$0x01, %r11, %r12
 3004                             	        andq	%rbp, %r11
 3005                             	        #  Multiply top half by 19
 3006                             	        movq	$19, %rax
 3007                             	        mulq	%r12
 3008                             	        xorq	%r12, %r12
 3009                             	        addq	%rax, %rcx
 3010                             	        movq	$19, %rax
 3011                             	        adcq	%rdx, %r12
 3012                             	        mulq	%r13
 3013                             	        xorq	%r13, %r13
 3014                             	        addq	%rax, %r9
 3015                             	        movq	$19, %rax
 3016                             	        adcq	%rdx, %r13
 3017                             	        mulq	%r14
 3018                             	        xorq	%r14, %r14
 3019                             	        addq	%rax, %r10
 3020                             	        movq	$19, %rax
 3021                             	        adcq	%rdx, %r14
 3022                             	        mulq	%r15
 3023                             	        #  Add remaining product results in
 3024                             	        addq	%r12, %r9
 3025                             	        adcq	%r13, %r10
 3026                             	        adcq	%r14, %r11
 3027                             	        adcq	%rax, %r11
 3028                             	        adcq	$0x00, %rdx
 3029                             	        #  Overflow
 3030                             	        shldq	$0x01, %r11, %rdx
 3031                             	        imulq	$19, %rdx, %rax
 3032                             	        andq	%rbp, %r11
 3033                             	        addq	%rax, %rcx
 3034                             	        adcq	$0x00, %r9
 3035                             	        adcq	$0x00, %r10
 3036                             	        adcq	$0x00, %r11
 3037                             	        # Reduce if top bit set
 3038                             	        movq	%r11, %rdx
 3039                             	        sarq	$63, %rdx
 3040                             	        andq	$19, %rdx
 3041                             	        andq	%rbp, %r11
 3042                             	        addq	%rdx, %rcx
 3043                             	        adcq	$0x00, %r9
 3044                             	        adcq	$0x00, %r10
 3045                             	        adcq	$0x00, %r11
 3046                             	        # Store
 3047                             	        movq	%rcx, 128(%rsp)
 3048                             	        movq	%r9, 136(%rsp)
 3049                             	        movq	%r10, 144(%rsp)
 3050                             	        movq	%r11, 152(%rsp)
 3051                             	        # Add
 3052                             	        movq	32(%rsp), %rcx
 3053                             	        movq	40(%rsp), %r9
 3054                             	        movq	48(%rsp), %r10
 3055                             	        movq	56(%rsp), %rbp
 3056                             	        movq	%rcx, %r12
 3057                             	        addq	(%rsp), %rcx
 3058                             	        movq	%r9, %r13
 3059                             	        adcq	8(%rsp), %r9
 3060                             	        movq	%r10, %r14
 3061                             	        adcq	16(%rsp), %r10
 3062                             	        movq	%rbp, %r15
 3063                             	        adcq	24(%rsp), %rbp
 3064                             	        movq	$-19, %rax
 3065                             	        movq	%rbp, %r11
 3066                             	        movq	$0x7fffffffffffffff, %rdx
 3067                             	        sarq	$63, %rbp
 3068                             	        #   Mask the modulus
 3069                             	        andq	%rbp, %rax
 3070                             	        andq	%rbp, %rdx
 3071                             	        #   Sub modulus (if overflow)
 3072                             	        subq	%rax, %rcx
 3073                             	        sbbq	%rbp, %r9
 3074                             	        sbbq	%rbp, %r10
 3075                             	        sbbq	%rdx, %r11
 3076                             	        # Sub
 3077                             	        subq	(%rsp), %r12
 3078                             	        movq	$0x00, %rbp
 3079                             	        sbbq	8(%rsp), %r13
 3080                             	        movq	$-19, %rax
 3081                             	        sbbq	16(%rsp), %r14
 3082                             	        movq	$0x7fffffffffffffff, %rdx
 3083                             	        sbbq	24(%rsp), %r15
 3084                             	        sbbq	$0x00, %rbp
 3085                             	        #   Mask the modulus
 3086                             	        andq	%rbp, %rax
 3087                             	        andq	%rbp, %rdx
 3088                             	        #   Add modulus (if underflow)
 3089                             	        addq	%rax, %r12
 3090                             	        adcq	%rbp, %r13
 3091                             	        adcq	%rbp, %r14
 3092                             	        adcq	%rdx, %r15
 3093                             	        movq	%rcx, 64(%rsp)
 3094                             	        movq	%r9, 72(%rsp)
 3095                             	        movq	%r10, 80(%rsp)
 3096                             	        movq	%r11, 88(%rsp)
 3097                             	        movq	%r12, (%rsp)
 3098                             	        movq	%r13, 8(%rsp)
 3099                             	        movq	%r14, 16(%rsp)
 3100                             	        movq	%r15, 24(%rsp)
 3101                             	        # Multiply
 3102                             	        #  A[0] * B[0]
 3103                             	        movq	96(%rsp), %rax
 3104                             	        mulq	128(%rsp)
 3105                             	        movq	%rax, %rcx
 3106                             	        movq	%rdx, %r9
 3107                             	        #  A[0] * B[1]
 3108                             	        movq	104(%rsp), %rax
 3109                             	        mulq	128(%rsp)
 3110                             	        xorq	%r10, %r10
 3111                             	        addq	%rax, %r9
 3112                             	        adcq	%rdx, %r10
 3113                             	        #  A[1] * B[0]
 3114                             	        movq	96(%rsp), %rax
 3115                             	        mulq	136(%rsp)
 3116                             	        xorq	%r11, %r11
 3117                             	        addq	%rax, %r9
 3118                             	        adcq	%rdx, %r10
 3119                             	        adcq	$0x00, %r11
 3120                             	        #  A[0] * B[2]
 3121                             	        movq	112(%rsp), %rax
 3122                             	        mulq	128(%rsp)
 3123                             	        addq	%rax, %r10
 3124                             	        adcq	%rdx, %r11
 3125                             	        #  A[1] * B[1]
 3126                             	        movq	104(%rsp), %rax
 3127                             	        mulq	136(%rsp)
 3128                             	        xorq	%r12, %r12
 3129                             	        addq	%rax, %r10
 3130                             	        adcq	%rdx, %r11
 3131                             	        adcq	$0x00, %r12
 3132                             	        #  A[2] * B[0]
 3133                             	        movq	96(%rsp), %rax
 3134                             	        mulq	144(%rsp)
 3135                             	        addq	%rax, %r10
 3136                             	        adcq	%rdx, %r11
 3137                             	        adcq	$0x00, %r12
 3138                             	        #  A[0] * B[3]
 3139                             	        movq	120(%rsp), %rax
 3140                             	        mulq	128(%rsp)
 3141                             	        xorq	%r13, %r13
 3142                             	        addq	%rax, %r11
 3143                             	        adcq	%rdx, %r12
 3144                             	        adcq	$0x00, %r13
 3145                             	        #  A[1] * B[2]
 3146                             	        movq	112(%rsp), %rax
 3147                             	        mulq	136(%rsp)
 3148                             	        addq	%rax, %r11
 3149                             	        adcq	%rdx, %r12
 3150                             	        adcq	$0x00, %r13
 3151                             	        #  A[2] * B[1]
 3152                             	        movq	104(%rsp), %rax
 3153                             	        mulq	144(%rsp)
 3154                             	        addq	%rax, %r11
 3155                             	        adcq	%rdx, %r12
 3156                             	        adcq	$0x00, %r13
 3157                             	        #  A[3] * B[0]
 3158                             	        movq	96(%rsp), %rax
 3159                             	        mulq	152(%rsp)
 3160                             	        addq	%rax, %r11
 3161                             	        adcq	%rdx, %r12
 3162                             	        adcq	$0x00, %r13
 3163                             	        #  A[1] * B[3]
 3164                             	        movq	120(%rsp), %rax
 3165                             	        mulq	136(%rsp)
 3166                             	        xorq	%r14, %r14
 3167                             	        addq	%rax, %r12
 3168                             	        adcq	%rdx, %r13
 3169                             	        adcq	$0x00, %r14
 3170                             	        #  A[2] * B[2]
 3171                             	        movq	112(%rsp), %rax
 3172                             	        mulq	144(%rsp)
 3173                             	        addq	%rax, %r12
 3174                             	        adcq	%rdx, %r13
 3175                             	        adcq	$0x00, %r14
 3176                             	        #  A[3] * B[1]
 3177                             	        movq	104(%rsp), %rax
 3178                             	        mulq	152(%rsp)
 3179                             	        addq	%rax, %r12
 3180                             	        adcq	%rdx, %r13
 3181                             	        adcq	$0x00, %r14
 3182                             	        #  A[2] * B[3]
 3183                             	        movq	120(%rsp), %rax
 3184                             	        mulq	144(%rsp)
 3185                             	        xorq	%r15, %r15
 3186                             	        addq	%rax, %r13
 3187                             	        adcq	%rdx, %r14
 3188                             	        adcq	$0x00, %r15
 3189                             	        #  A[3] * B[2]
 3190                             	        movq	112(%rsp), %rax
 3191                             	        mulq	152(%rsp)
 3192                             	        addq	%rax, %r13
 3193                             	        adcq	%rdx, %r14
 3194                             	        adcq	$0x00, %r15
 3195                             	        #  A[3] * B[3]
 3196                             	        movq	120(%rsp), %rax
 3197                             	        mulq	152(%rsp)
 3198                             	        addq	%rax, %r14
 3199                             	        adcq	%rdx, %r15
 3200                             	        # Reduce
 3201                             	        movq	$0x7fffffffffffffff, %rbp
 3202                             	        #  Move top half into t4-t7 and remove top bit from t3
 3203                             	        shldq	$0x01, %r14, %r15
 3204                             	        shldq	$0x01, %r13, %r14
 3205                             	        shldq	$0x01, %r12, %r13
 3206                             	        shldq	$0x01, %r11, %r12
 3207                             	        andq	%rbp, %r11
 3208                             	        #  Multiply top half by 19
 3209                             	        movq	$19, %rax
 3210                             	        mulq	%r12
 3211                             	        xorq	%r12, %r12
 3212                             	        addq	%rax, %rcx
 3213                             	        movq	$19, %rax
 3214                             	        adcq	%rdx, %r12
 3215                             	        mulq	%r13
 3216                             	        xorq	%r13, %r13
 3217                             	        addq	%rax, %r9
 3218                             	        movq	$19, %rax
 3219                             	        adcq	%rdx, %r13
 3220                             	        mulq	%r14
 3221                             	        xorq	%r14, %r14
 3222                             	        addq	%rax, %r10
 3223                             	        movq	$19, %rax
 3224                             	        adcq	%rdx, %r14
 3225                             	        mulq	%r15
 3226                             	        #  Add remaining product results in
 3227                             	        addq	%r12, %r9
 3228                             	        adcq	%r13, %r10
 3229                             	        adcq	%r14, %r11
 3230                             	        adcq	%rax, %r11
 3231                             	        adcq	$0x00, %rdx
 3232                             	        #  Overflow
 3233                             	        shldq	$0x01, %r11, %rdx
 3234                             	        imulq	$19, %rdx, %rax
 3235                             	        andq	%rbp, %r11
 3236                             	        addq	%rax, %rcx
 3237                             	        adcq	$0x00, %r9
 3238                             	        adcq	$0x00, %r10
 3239                             	        adcq	$0x00, %r11
 3240                             	        # Reduce if top bit set
 3241                             	        movq	%r11, %rdx
 3242                             	        sarq	$63, %rdx
 3243                             	        andq	$19, %rdx
 3244                             	        andq	%rbp, %r11
 3245                             	        addq	%rdx, %rcx
 3246                             	        adcq	$0x00, %r9
 3247                             	        adcq	$0x00, %r10
 3248                             	        adcq	$0x00, %r11
 3249                             	        # Store
 3250                             	        movq	%rcx, (%rdi)
 3251                             	        movq	%r9, 8(%rdi)
 3252                             	        movq	%r10, 16(%rdi)
 3253                             	        movq	%r11, 24(%rdi)
 3254                             	        # Sub
 3255                             	        movq	128(%rsp), %rcx
 3256                             	        movq	136(%rsp), %r9
 3257                             	        movq	144(%rsp), %r10
 3258                             	        movq	152(%rsp), %r11
 3259                             	        subq	96(%rsp), %rcx
 3260                             	        movq	$0x00, %rbp
 3261                             	        sbbq	104(%rsp), %r9
 3262                             	        movq	$-19, %rax
 3263                             	        sbbq	112(%rsp), %r10
 3264                             	        movq	$0x7fffffffffffffff, %rdx
 3265                             	        sbbq	120(%rsp), %r11
 3266                             	        sbbq	$0x00, %rbp
 3267                             	        #   Mask the modulus
 3268                             	        andq	%rbp, %rax
 3269                             	        andq	%rbp, %rdx
 3270                             	        #   Add modulus (if underflow)
 3271                             	        addq	%rax, %rcx
 3272                             	        adcq	%rbp, %r9
 3273                             	        adcq	%rbp, %r10
 3274                             	        adcq	%rdx, %r11
 3275                             	        movq	%rcx, 128(%rsp)
 3276                             	        movq	%r9, 136(%rsp)
 3277                             	        movq	%r10, 144(%rsp)
 3278                             	        movq	%r11, 152(%rsp)
 3279                             	        # Square
 3280                             	        #  A[0] * A[1]
 3281                             	        movq	(%rsp), %rax
 3282                             	        mulq	8(%rsp)
 3283                             	        movq	%rax, %r9
 3284                             	        movq	%rdx, %r10
 3285                             	        #  A[0] * A[2]
 3286                             	        movq	(%rsp), %rax
 3287                             	        mulq	16(%rsp)
 3288                             	        xorq	%r11, %r11
 3289                             	        addq	%rax, %r10
 3290                             	        adcq	%rdx, %r11
 3291                             	        #  A[0] * A[3]
 3292                             	        movq	(%rsp), %rax
 3293                             	        mulq	24(%rsp)
 3294                             	        xorq	%r12, %r12
 3295                             	        addq	%rax, %r11
 3296                             	        adcq	%rdx, %r12
 3297                             	        #  A[1] * A[2]
 3298                             	        movq	8(%rsp), %rax
 3299                             	        mulq	16(%rsp)
 3300                             	        xorq	%r13, %r13
 3301                             	        addq	%rax, %r11
 3302                             	        adcq	%rdx, %r12
 3303                             	        adcq	$0x00, %r13
 3304                             	        #  A[1] * A[3]
 3305                             	        movq	8(%rsp), %rax
 3306                             	        mulq	24(%rsp)
 3307                             	        addq	%rax, %r12
 3308                             	        adcq	%rdx, %r13
 3309                             	        #  A[2] * A[3]
 3310                             	        movq	16(%rsp), %rax
 3311                             	        mulq	24(%rsp)
 3312                             	        xorq	%r14, %r14
 3313                             	        addq	%rax, %r13
 3314                             	        adcq	%rdx, %r14
 3315                             	        # Double
 3316                             	        xorq	%r15, %r15
 3317                             	        addq	%r9, %r9
 3318                             	        adcq	%r10, %r10
 3319                             	        adcq	%r11, %r11
 3320                             	        adcq	%r12, %r12
 3321                             	        adcq	%r13, %r13
 3322                             	        adcq	%r14, %r14
 3323                             	        adcq	$0x00, %r15
 3324                             	        #  A[0] * A[0]
 3325                             	        movq	(%rsp), %rax
 3326                             	        mulq	%rax
 3327                             	        movq	%rax, %rcx
 3328                             	        movq	%rdx, %rbp
 3329                             	        #  A[1] * A[1]
 3330                             	        movq	8(%rsp), %rax
 3331                             	        mulq	%rax
 3332                             	        addq	%rbp, %r9
 3333                             	        adcq	%rax, %r10
 3334                             	        adcq	$0x00, %rdx
 3335                             	        movq	%rdx, %rbp
 3336                             	        #  A[2] * A[2]
 3337                             	        movq	16(%rsp), %rax
 3338                             	        mulq	%rax
 3339                             	        addq	%rbp, %r11
 3340                             	        adcq	%rax, %r12
 3341                             	        adcq	$0x00, %rdx
 3342                             	        movq	%rdx, %rbp
 3343                             	        #  A[3] * A[3]
 3344                             	        movq	24(%rsp), %rax
 3345                             	        mulq	%rax
 3346                             	        addq	%rax, %r14
 3347                             	        adcq	%rdx, %r15
 3348                             	        addq	%rbp, %r13
 3349                             	        adcq	$0x00, %r14
 3350                             	        adcq	$0x00, %r15
 3351                             	        # Reduce
 3352                             	        movq	$0x7fffffffffffffff, %rbp
 3353                             	        #  Move top half into t4-t7 and remove top bit from t3
 3354                             	        shldq	$0x01, %r14, %r15
 3355                             	        shldq	$0x01, %r13, %r14
 3356                             	        shldq	$0x01, %r12, %r13
 3357                             	        shldq	$0x01, %r11, %r12
 3358                             	        andq	%rbp, %r11
 3359                             	        #  Multiply top half by 19
 3360                             	        movq	$19, %rax
 3361                             	        mulq	%r12
 3362                             	        xorq	%r12, %r12
 3363                             	        addq	%rax, %rcx
 3364                             	        movq	$19, %rax
 3365                             	        adcq	%rdx, %r12
 3366                             	        mulq	%r13
 3367                             	        xorq	%r13, %r13
 3368                             	        addq	%rax, %r9
 3369                             	        movq	$19, %rax
 3370                             	        adcq	%rdx, %r13
 3371                             	        mulq	%r14
 3372                             	        xorq	%r14, %r14
 3373                             	        addq	%rax, %r10
 3374                             	        movq	$19, %rax
 3375                             	        adcq	%rdx, %r14
 3376                             	        mulq	%r15
 3377                             	        #  Add remaining product results in
 3378                             	        addq	%r12, %r9
 3379                             	        adcq	%r13, %r10
 3380                             	        adcq	%r14, %r11
 3381                             	        adcq	%rax, %r11
 3382                             	        adcq	$0x00, %rdx
 3383                             	        #  Overflow
 3384                             	        shldq	$0x01, %r11, %rdx
 3385                             	        imulq	$19, %rdx, %rax
 3386                             	        andq	%rbp, %r11
 3387                             	        addq	%rax, %rcx
 3388                             	        adcq	$0x00, %r9
 3389                             	        adcq	$0x00, %r10
 3390                             	        adcq	$0x00, %r11
 3391                             	        # Reduce if top bit set
 3392                             	        movq	%r11, %rdx
 3393                             	        sarq	$63, %rdx
 3394                             	        andq	$19, %rdx
 3395                             	        andq	%rbp, %r11
 3396                             	        addq	%rdx, %rcx
 3397                             	        adcq	$0x00, %r9
 3398                             	        adcq	$0x00, %r10
 3399                             	        adcq	$0x00, %r11
 3400                             	        # Store
 3401                             	        movq	%rcx, (%rsp)
 3402                             	        movq	%r9, 8(%rsp)
 3403                             	        movq	%r10, 16(%rsp)
 3404                             	        movq	%r11, 24(%rsp)
 3405                             	        # Multiply by 121666
 3406                             	        movq	$0x1db42, %rax
 3407                             	        mulq	128(%rsp)
 3408                             	        xorq	%r10, %r10
 3409                             	        movq	%rax, %rcx
 3410                             	        movq	%rdx, %r9
 3411                             	        movq	$0x1db42, %rax
 3412                             	        mulq	136(%rsp)
 3413                             	        xorq	%r11, %r11
 3414                             	        addq	%rax, %r9
 3415                             	        adcq	%rdx, %r10
 3416                             	        movq	$0x1db42, %rax
 3417                             	        mulq	144(%rsp)
 3418                             	        xorq	%r13, %r13
 3419                             	        addq	%rax, %r10
 3420                             	        adcq	%rdx, %r11
 3421                             	        movq	$0x1db42, %rax
 3422                             	        mulq	152(%rsp)
 3423                             	        movq	$0x7fffffffffffffff, %r12
 3424                             	        addq	%rax, %r11
 3425                             	        adcq	%rdx, %r13
 3426                             	        shldq	$0x01, %r11, %r13
 3427                             	        andq	%r12, %r11
 3428                             	        movq	$19, %rax
 3429                             	        mulq	%r13
 3430                             	        addq	%rax, %rcx
 3431                             	        adcq	$0x00, %r9
 3432                             	        adcq	$0x00, %r10
 3433                             	        adcq	$0x00, %r11
 3434                             	        movq	%rcx, 32(%rsp)
 3435                             	        movq	%r9, 40(%rsp)
 3436                             	        movq	%r10, 48(%rsp)
 3437                             	        movq	%r11, 56(%rsp)
 3438                             	        # Square
 3439                             	        #  A[0] * A[1]
 3440                             	        movq	64(%rsp), %rax
 3441                             	        mulq	72(%rsp)
 3442                             	        movq	%rax, %r9
 3443                             	        movq	%rdx, %r10
 3444                             	        #  A[0] * A[2]
 3445                             	        movq	64(%rsp), %rax
 3446                             	        mulq	80(%rsp)
 3447                             	        xorq	%r11, %r11
 3448                             	        addq	%rax, %r10
 3449                             	        adcq	%rdx, %r11
 3450                             	        #  A[0] * A[3]
 3451                             	        movq	64(%rsp), %rax
 3452                             	        mulq	88(%rsp)
 3453                             	        xorq	%r12, %r12
 3454                             	        addq	%rax, %r11
 3455                             	        adcq	%rdx, %r12
 3456                             	        #  A[1] * A[2]
 3457                             	        movq	72(%rsp), %rax
 3458                             	        mulq	80(%rsp)
 3459                             	        xorq	%r13, %r13
 3460                             	        addq	%rax, %r11
 3461                             	        adcq	%rdx, %r12
 3462                             	        adcq	$0x00, %r13
 3463                             	        #  A[1] * A[3]
 3464                             	        movq	72(%rsp), %rax
 3465                             	        mulq	88(%rsp)
 3466                             	        addq	%rax, %r12
 3467                             	        adcq	%rdx, %r13
 3468                             	        #  A[2] * A[3]
 3469                             	        movq	80(%rsp), %rax
 3470                             	        mulq	88(%rsp)
 3471                             	        xorq	%r14, %r14
 3472                             	        addq	%rax, %r13
 3473                             	        adcq	%rdx, %r14
 3474                             	        # Double
 3475                             	        xorq	%r15, %r15
 3476                             	        addq	%r9, %r9
 3477                             	        adcq	%r10, %r10
 3478                             	        adcq	%r11, %r11
 3479                             	        adcq	%r12, %r12
 3480                             	        adcq	%r13, %r13
 3481                             	        adcq	%r14, %r14
 3482                             	        adcq	$0x00, %r15
 3483                             	        #  A[0] * A[0]
 3484                             	        movq	64(%rsp), %rax
 3485                             	        mulq	%rax
 3486                             	        movq	%rax, %rcx
 3487                             	        movq	%rdx, %rbp
 3488                             	        #  A[1] * A[1]
 3489                             	        movq	72(%rsp), %rax
 3490                             	        mulq	%rax
 3491                             	        addq	%rbp, %r9
 3492                             	        adcq	%rax, %r10
 3493                             	        adcq	$0x00, %rdx
 3494                             	        movq	%rdx, %rbp
 3495                             	        #  A[2] * A[2]
 3496                             	        movq	80(%rsp), %rax
 3497                             	        mulq	%rax
 3498                             	        addq	%rbp, %r11
 3499                             	        adcq	%rax, %r12
 3500                             	        adcq	$0x00, %rdx
 3501                             	        movq	%rdx, %rbp
 3502                             	        #  A[3] * A[3]
 3503                             	        movq	88(%rsp), %rax
 3504                             	        mulq	%rax
 3505                             	        addq	%rax, %r14
 3506                             	        adcq	%rdx, %r15
 3507                             	        addq	%rbp, %r13
 3508                             	        adcq	$0x00, %r14
 3509                             	        adcq	$0x00, %r15
 3510                             	        # Reduce
 3511                             	        movq	$0x7fffffffffffffff, %rbp
 3512                             	        #  Move top half into t4-t7 and remove top bit from t3
 3513                             	        shldq	$0x01, %r14, %r15
 3514                             	        shldq	$0x01, %r13, %r14
 3515                             	        shldq	$0x01, %r12, %r13
 3516                             	        shldq	$0x01, %r11, %r12
 3517                             	        andq	%rbp, %r11
 3518                             	        #  Multiply top half by 19
 3519                             	        movq	$19, %rax
 3520                             	        mulq	%r12
 3521                             	        xorq	%r12, %r12
 3522                             	        addq	%rax, %rcx
 3523                             	        movq	$19, %rax
 3524                             	        adcq	%rdx, %r12
 3525                             	        mulq	%r13
 3526                             	        xorq	%r13, %r13
 3527                             	        addq	%rax, %r9
 3528                             	        movq	$19, %rax
 3529                             	        adcq	%rdx, %r13
 3530                             	        mulq	%r14
 3531                             	        xorq	%r14, %r14
 3532                             	        addq	%rax, %r10
 3533                             	        movq	$19, %rax
 3534                             	        adcq	%rdx, %r14
 3535                             	        mulq	%r15
 3536                             	        #  Add remaining product results in
 3537                             	        addq	%r12, %r9
 3538                             	        adcq	%r13, %r10
 3539                             	        adcq	%r14, %r11
 3540                             	        adcq	%rax, %r11
 3541                             	        adcq	$0x00, %rdx
 3542                             	        #  Overflow
 3543                             	        shldq	$0x01, %r11, %rdx
 3544                             	        imulq	$19, %rdx, %rax
 3545                             	        andq	%rbp, %r11
 3546                             	        addq	%rax, %rcx
 3547                             	        adcq	$0x00, %r9
 3548                             	        adcq	$0x00, %r10
 3549                             	        adcq	$0x00, %r11
 3550                             	        # Reduce if top bit set
 3551                             	        movq	%r11, %rdx
 3552                             	        sarq	$63, %rdx
 3553                             	        andq	$19, %rdx
 3554                             	        andq	%rbp, %r11
 3555                             	        addq	%rdx, %rcx
 3556                             	        adcq	$0x00, %r9
 3557                             	        adcq	$0x00, %r10
 3558                             	        adcq	$0x00, %r11
 3559                             	        # Store
 3560                             	        movq	%rcx, 64(%rsp)
 3561                             	        movq	%r9, 72(%rsp)
 3562                             	        movq	%r10, 80(%rsp)
 3563                             	        movq	%r11, 88(%rsp)
 3564                             	        # Add
 3565                             	        movq	96(%rsp), %rcx
 3566                             	        movq	104(%rsp), %r9
 3567                             	        addq	32(%rsp), %rcx
 3568                             	        movq	112(%rsp), %r10
 3569                             	        adcq	40(%rsp), %r9
 3570                             	        movq	120(%rsp), %rbp
 3571                             	        adcq	48(%rsp), %r10
 3572                             	        movq	$-19, %rax
 3573                             	        adcq	56(%rsp), %rbp
 3574                             	        movq	$0x7fffffffffffffff, %rdx
 3575                             	        movq	%rbp, %r11
 3576                             	        sarq	$63, %rbp
 3577                             	        #   Mask the modulus
 3578                             	        andq	%rbp, %rax
 3579                             	        andq	%rbp, %rdx
 3580                             	        #   Sub modulus (if overflow)
 3581                             	        subq	%rax, %rcx
 3582                             	        sbbq	%rbp, %r9
 3583                             	        sbbq	%rbp, %r10
 3584                             	        sbbq	%rdx, %r11
 3585                             	        movq	%rcx, 96(%rsp)
 3586                             	        movq	%r9, 104(%rsp)
 3587                             	        movq	%r10, 112(%rsp)
 3588                             	        movq	%r11, 120(%rsp)
 3589                             	        # Multiply
 3590                             	        #  A[0] * B[0]
 3591                             	        movq	(%rsp), %rax
 3592                             	        mulq	(%r8)
 3593                             	        movq	%rax, %rcx
 3594                             	        movq	%rdx, %r9
 3595                             	        #  A[0] * B[1]
 3596                             	        movq	8(%rsp), %rax
 3597                             	        mulq	(%r8)
 3598                             	        xorq	%r10, %r10
 3599                             	        addq	%rax, %r9
 3600                             	        adcq	%rdx, %r10
 3601                             	        #  A[1] * B[0]
 3602                             	        movq	(%rsp), %rax
 3603                             	        mulq	8(%r8)
 3604                             	        xorq	%r11, %r11
 3605                             	        addq	%rax, %r9
 3606                             	        adcq	%rdx, %r10
 3607                             	        adcq	$0x00, %r11
 3608                             	        #  A[0] * B[2]
 3609                             	        movq	16(%rsp), %rax
 3610                             	        mulq	(%r8)
 3611                             	        addq	%rax, %r10
 3612                             	        adcq	%rdx, %r11
 3613                             	        #  A[1] * B[1]
 3614                             	        movq	8(%rsp), %rax
 3615                             	        mulq	8(%r8)
 3616                             	        xorq	%r12, %r12
 3617                             	        addq	%rax, %r10
 3618                             	        adcq	%rdx, %r11
 3619                             	        adcq	$0x00, %r12
 3620                             	        #  A[2] * B[0]
 3621                             	        movq	(%rsp), %rax
 3622                             	        mulq	16(%r8)
 3623                             	        addq	%rax, %r10
 3624                             	        adcq	%rdx, %r11
 3625                             	        adcq	$0x00, %r12
 3626                             	        #  A[0] * B[3]
 3627                             	        movq	24(%rsp), %rax
 3628                             	        mulq	(%r8)
 3629                             	        xorq	%r13, %r13
 3630                             	        addq	%rax, %r11
 3631                             	        adcq	%rdx, %r12
 3632                             	        adcq	$0x00, %r13
 3633                             	        #  A[1] * B[2]
 3634                             	        movq	16(%rsp), %rax
 3635                             	        mulq	8(%r8)
 3636                             	        addq	%rax, %r11
 3637                             	        adcq	%rdx, %r12
 3638                             	        adcq	$0x00, %r13
 3639                             	        #  A[2] * B[1]
 3640                             	        movq	8(%rsp), %rax
 3641                             	        mulq	16(%r8)
 3642                             	        addq	%rax, %r11
 3643                             	        adcq	%rdx, %r12
 3644                             	        adcq	$0x00, %r13
 3645                             	        #  A[3] * B[0]
 3646                             	        movq	(%rsp), %rax
 3647                             	        mulq	24(%r8)
 3648                             	        addq	%rax, %r11
 3649                             	        adcq	%rdx, %r12
 3650                             	        adcq	$0x00, %r13
 3651                             	        #  A[1] * B[3]
 3652                             	        movq	24(%rsp), %rax
 3653                             	        mulq	8(%r8)
 3654                             	        xorq	%r14, %r14
 3655                             	        addq	%rax, %r12
 3656                             	        adcq	%rdx, %r13
 3657                             	        adcq	$0x00, %r14
 3658                             	        #  A[2] * B[2]
 3659                             	        movq	16(%rsp), %rax
 3660                             	        mulq	16(%r8)
 3661                             	        addq	%rax, %r12
 3662                             	        adcq	%rdx, %r13
 3663                             	        adcq	$0x00, %r14
 3664                             	        #  A[3] * B[1]
 3665                             	        movq	8(%rsp), %rax
 3666                             	        mulq	24(%r8)
 3667                             	        addq	%rax, %r12
 3668                             	        adcq	%rdx, %r13
 3669                             	        adcq	$0x00, %r14
 3670                             	        #  A[2] * B[3]
 3671                             	        movq	24(%rsp), %rax
 3672                             	        mulq	16(%r8)
 3673                             	        xorq	%r15, %r15
 3674                             	        addq	%rax, %r13
 3675                             	        adcq	%rdx, %r14
 3676                             	        adcq	$0x00, %r15
 3677                             	        #  A[3] * B[2]
 3678                             	        movq	16(%rsp), %rax
 3679                             	        mulq	24(%r8)
 3680                             	        addq	%rax, %r13
 3681                             	        adcq	%rdx, %r14
 3682                             	        adcq	$0x00, %r15
 3683                             	        #  A[3] * B[3]
 3684                             	        movq	24(%rsp), %rax
 3685                             	        mulq	24(%r8)
 3686                             	        addq	%rax, %r14
 3687                             	        adcq	%rdx, %r15
 3688                             	        # Reduce
 3689                             	        movq	$0x7fffffffffffffff, %rbp
 3690                             	        #  Move top half into t4-t7 and remove top bit from t3
 3691                             	        shldq	$0x01, %r14, %r15
 3692                             	        shldq	$0x01, %r13, %r14
 3693                             	        shldq	$0x01, %r12, %r13
 3694                             	        shldq	$0x01, %r11, %r12
 3695                             	        andq	%rbp, %r11
 3696                             	        #  Multiply top half by 19
 3697                             	        movq	$19, %rax
 3698                             	        mulq	%r12
 3699                             	        xorq	%r12, %r12
 3700                             	        addq	%rax, %rcx
 3701                             	        movq	$19, %rax
 3702                             	        adcq	%rdx, %r12
 3703                             	        mulq	%r13
 3704                             	        xorq	%r13, %r13
 3705                             	        addq	%rax, %r9
 3706                             	        movq	$19, %rax
 3707                             	        adcq	%rdx, %r13
 3708                             	        mulq	%r14
 3709                             	        xorq	%r14, %r14
 3710                             	        addq	%rax, %r10
 3711                             	        movq	$19, %rax
 3712                             	        adcq	%rdx, %r14
 3713                             	        mulq	%r15
 3714                             	        #  Add remaining product results in
 3715                             	        addq	%r12, %r9
 3716                             	        adcq	%r13, %r10
 3717                             	        adcq	%r14, %r11
 3718                             	        adcq	%rax, %r11
 3719                             	        adcq	$0x00, %rdx
 3720                             	        #  Overflow
 3721                             	        shldq	$0x01, %r11, %rdx
 3722                             	        imulq	$19, %rdx, %rax
 3723                             	        andq	%rbp, %r11
 3724                             	        addq	%rax, %rcx
 3725                             	        adcq	$0x00, %r9
 3726                             	        adcq	$0x00, %r10
 3727                             	        adcq	$0x00, %r11
 3728                             	        # Reduce if top bit set
 3729                             	        movq	%r11, %rdx
 3730                             	        sarq	$63, %rdx
 3731                             	        andq	$19, %rdx
 3732                             	        andq	%rbp, %r11
 3733                             	        addq	%rdx, %rcx
 3734                             	        adcq	$0x00, %r9
 3735                             	        adcq	$0x00, %r10
 3736                             	        adcq	$0x00, %r11
 3737                             	        # Store
 3738                             	        movq	%rcx, 32(%rsp)
 3739                             	        movq	%r9, 40(%rsp)
 3740                             	        movq	%r10, 48(%rsp)
 3741                             	        movq	%r11, 56(%rsp)
 3742                             	        # Multiply
 3743                             	        #  A[0] * B[0]
 3744                             	        movq	96(%rsp), %rax
 3745                             	        mulq	128(%rsp)
 3746                             	        movq	%rax, %rcx
 3747                             	        movq	%rdx, %r9
 3748                             	        #  A[0] * B[1]
 3749                             	        movq	104(%rsp), %rax
 3750                             	        mulq	128(%rsp)
 3751                             	        xorq	%r10, %r10
 3752                             	        addq	%rax, %r9
 3753                             	        adcq	%rdx, %r10
 3754                             	        #  A[1] * B[0]
 3755                             	        movq	96(%rsp), %rax
 3756                             	        mulq	136(%rsp)
 3757                             	        xorq	%r11, %r11
 3758                             	        addq	%rax, %r9
 3759                             	        adcq	%rdx, %r10
 3760                             	        adcq	$0x00, %r11
 3761                             	        #  A[0] * B[2]
 3762                             	        movq	112(%rsp), %rax
 3763                             	        mulq	128(%rsp)
 3764                             	        addq	%rax, %r10
 3765                             	        adcq	%rdx, %r11
 3766                             	        #  A[1] * B[1]
 3767                             	        movq	104(%rsp), %rax
 3768                             	        mulq	136(%rsp)
 3769                             	        xorq	%r12, %r12
 3770                             	        addq	%rax, %r10
 3771                             	        adcq	%rdx, %r11
 3772                             	        adcq	$0x00, %r12
 3773                             	        #  A[2] * B[0]
 3774                             	        movq	96(%rsp), %rax
 3775                             	        mulq	144(%rsp)
 3776                             	        addq	%rax, %r10
 3777                             	        adcq	%rdx, %r11
 3778                             	        adcq	$0x00, %r12
 3779                             	        #  A[0] * B[3]
 3780                             	        movq	120(%rsp), %rax
 3781                             	        mulq	128(%rsp)
 3782                             	        xorq	%r13, %r13
 3783                             	        addq	%rax, %r11
 3784                             	        adcq	%rdx, %r12
 3785                             	        adcq	$0x00, %r13
 3786                             	        #  A[1] * B[2]
 3787                             	        movq	112(%rsp), %rax
 3788                             	        mulq	136(%rsp)
 3789                             	        addq	%rax, %r11
 3790                             	        adcq	%rdx, %r12
 3791                             	        adcq	$0x00, %r13
 3792                             	        #  A[2] * B[1]
 3793                             	        movq	104(%rsp), %rax
 3794                             	        mulq	144(%rsp)
 3795                             	        addq	%rax, %r11
 3796                             	        adcq	%rdx, %r12
 3797                             	        adcq	$0x00, %r13
 3798                             	        #  A[3] * B[0]
 3799                             	        movq	96(%rsp), %rax
 3800                             	        mulq	152(%rsp)
 3801                             	        addq	%rax, %r11
 3802                             	        adcq	%rdx, %r12
 3803                             	        adcq	$0x00, %r13
 3804                             	        #  A[1] * B[3]
 3805                             	        movq	120(%rsp), %rax
 3806                             	        mulq	136(%rsp)
 3807                             	        xorq	%r14, %r14
 3808                             	        addq	%rax, %r12
 3809                             	        adcq	%rdx, %r13
 3810                             	        adcq	$0x00, %r14
 3811                             	        #  A[2] * B[2]
 3812                             	        movq	112(%rsp), %rax
 3813                             	        mulq	144(%rsp)
 3814                             	        addq	%rax, %r12
 3815                             	        adcq	%rdx, %r13
 3816                             	        adcq	$0x00, %r14
 3817                             	        #  A[3] * B[1]
 3818                             	        movq	104(%rsp), %rax
 3819                             	        mulq	152(%rsp)
 3820                             	        addq	%rax, %r12
 3821                             	        adcq	%rdx, %r13
 3822                             	        adcq	$0x00, %r14
 3823                             	        #  A[2] * B[3]
 3824                             	        movq	120(%rsp), %rax
 3825                             	        mulq	144(%rsp)
 3826                             	        xorq	%r15, %r15
 3827                             	        addq	%rax, %r13
 3828                             	        adcq	%rdx, %r14
 3829                             	        adcq	$0x00, %r15
 3830                             	        #  A[3] * B[2]
 3831                             	        movq	112(%rsp), %rax
 3832                             	        mulq	152(%rsp)
 3833                             	        addq	%rax, %r13
 3834                             	        adcq	%rdx, %r14
 3835                             	        adcq	$0x00, %r15
 3836                             	        #  A[3] * B[3]
 3837                             	        movq	120(%rsp), %rax
 3838                             	        mulq	152(%rsp)
 3839                             	        addq	%rax, %r14
 3840                             	        adcq	%rdx, %r15
 3841                             	        # Reduce
 3842                             	        movq	$0x7fffffffffffffff, %rbp
 3843                             	        #  Move top half into t4-t7 and remove top bit from t3
 3844                             	        shldq	$0x01, %r14, %r15
 3845                             	        shldq	$0x01, %r13, %r14
 3846                             	        shldq	$0x01, %r12, %r13
 3847                             	        shldq	$0x01, %r11, %r12
 3848                             	        andq	%rbp, %r11
 3849                             	        #  Multiply top half by 19
 3850                             	        movq	$19, %rax
 3851                             	        mulq	%r12
 3852                             	        xorq	%r12, %r12
 3853                             	        addq	%rax, %rcx
 3854                             	        movq	$19, %rax
 3855                             	        adcq	%rdx, %r12
 3856                             	        mulq	%r13
 3857                             	        xorq	%r13, %r13
 3858                             	        addq	%rax, %r9
 3859                             	        movq	$19, %rax
 3860                             	        adcq	%rdx, %r13
 3861                             	        mulq	%r14
 3862                             	        xorq	%r14, %r14
 3863                             	        addq	%rax, %r10
 3864                             	        movq	$19, %rax
 3865                             	        adcq	%rdx, %r14
 3866                             	        mulq	%r15
 3867                             	        #  Add remaining product results in
 3868                             	        addq	%r12, %r9
 3869                             	        adcq	%r13, %r10
 3870                             	        adcq	%r14, %r11
 3871                             	        adcq	%rax, %r11
 3872                             	        adcq	$0x00, %rdx
 3873                             	        #  Overflow
 3874                             	        shldq	$0x01, %r11, %rdx
 3875                             	        imulq	$19, %rdx, %rax
 3876                             	        andq	%rbp, %r11
 3877                             	        addq	%rax, %rcx
 3878                             	        adcq	$0x00, %r9
 3879                             	        adcq	$0x00, %r10
 3880                             	        adcq	$0x00, %r11
 3881                             	        # Reduce if top bit set
 3882                             	        movq	%r11, %rdx
 3883                             	        sarq	$63, %rdx
 3884                             	        andq	$19, %rdx
 3885                             	        andq	%rbp, %r11
 3886                             	        addq	%rdx, %rcx
 3887                             	        adcq	$0x00, %r9
 3888                             	        adcq	$0x00, %r10
 3889                             	        adcq	$0x00, %r11
 3890                             	        # Store
 3891                             	        movq	%rcx, (%rsp)
 3892                             	        movq	%r9, 8(%rsp)
 3893                             	        movq	%r10, 16(%rsp)
 3894                             	        movq	%r11, 24(%rsp)
 3895                             	        decb	168(%rsp)
 3896                             	        jge	L_curve25519_x64_bits
 3897                             	        movq	$63, 168(%rsp)
 3898                             	        decb	160(%rsp)
 3899                             	        jge	L_curve25519_x64_words
 3900                             	        # Invert
 3901                             	        leaq	32(%rsp), %rdi
 3902                             	        movq	%rsp, %rsi
 3903                             	#ifndef __APPLE__
 3904                             	        callq	fe_sq_x64@plt
 3905                             	#else
 3906                             	        callq	_fe_sq_x64
 3907                             	#endif /* __APPLE__ */
 3908                             	        leaq	64(%rsp), %rdi
 3909                             	        leaq	32(%rsp), %rsi
 3910                             	#ifndef __APPLE__
 3911                             	        callq	fe_sq_x64@plt
 3912                             	#else
 3913                             	        callq	_fe_sq_x64
 3914                             	#endif /* __APPLE__ */
 3915                             	        leaq	64(%rsp), %rdi
 3916                             	        leaq	64(%rsp), %rsi
 3917                             	#ifndef __APPLE__
 3918                             	        callq	fe_sq_x64@plt
 3919                             	#else
 3920                             	        callq	_fe_sq_x64
 3921                             	#endif /* __APPLE__ */
 3922                             	        leaq	64(%rsp), %rdi
 3923                             	        movq	%rsp, %rsi
 3924                             	        leaq	64(%rsp), %rdx
 3925                             	#ifndef __APPLE__
 3926                             	        callq	fe_mul_x64@plt
 3927                             	#else
 3928                             	        callq	_fe_mul_x64
 3929                             	#endif /* __APPLE__ */
 3930                             	        leaq	32(%rsp), %rdi
 3931                             	        leaq	32(%rsp), %rsi
 3932                             	        leaq	64(%rsp), %rdx
 3933                             	#ifndef __APPLE__
 3934                             	        callq	fe_mul_x64@plt
 3935                             	#else
 3936                             	        callq	_fe_mul_x64
 3937                             	#endif /* __APPLE__ */
 3938                             	        leaq	96(%rsp), %rdi
 3939                             	        leaq	32(%rsp), %rsi
 3940                             	#ifndef __APPLE__
 3941                             	        callq	fe_sq_x64@plt
 3942                             	#else
 3943                             	        callq	_fe_sq_x64
 3944                             	#endif /* __APPLE__ */
 3945                             	        leaq	64(%rsp), %rdi
 3946                             	        leaq	64(%rsp), %rsi
 3947                             	        leaq	96(%rsp), %rdx
 3948                             	#ifndef __APPLE__
 3949                             	        callq	fe_mul_x64@plt
 3950                             	#else
 3951                             	        callq	_fe_mul_x64
 3952                             	#endif /* __APPLE__ */
 3953                             	        leaq	96(%rsp), %rdi
 3954                             	        leaq	64(%rsp), %rsi
 3955                             	#ifndef __APPLE__
 3956                             	        callq	fe_sq_x64@plt
 3957                             	#else
 3958                             	        callq	_fe_sq_x64
 3959                             	#endif /* __APPLE__ */
 3960                             	        leaq	96(%rsp), %rdi
 3961                             	        leaq	96(%rsp), %rsi
 3962                             	        movq	$4, %rdx
 3963                             	#ifndef __APPLE__
 3964                             	        callq	fe_sq_n_x64@plt
 3965                             	#else
 3966                             	        callq	_fe_sq_n_x64
 3967                             	#endif /* __APPLE__ */
 3968                             	        leaq	64(%rsp), %rdi
 3969                             	        leaq	96(%rsp), %rsi
 3970                             	        leaq	64(%rsp), %rdx
 3971                             	#ifndef __APPLE__
 3972                             	        callq	fe_mul_x64@plt
 3973                             	#else
 3974                             	        callq	_fe_mul_x64
 3975                             	#endif /* __APPLE__ */
 3976                             	        leaq	96(%rsp), %rdi
 3977                             	        leaq	64(%rsp), %rsi
 3978                             	#ifndef __APPLE__
 3979                             	        callq	fe_sq_x64@plt
 3980                             	#else
 3981                             	        callq	_fe_sq_x64
 3982                             	#endif /* __APPLE__ */
 3983                             	        leaq	96(%rsp), %rdi
 3984                             	        leaq	96(%rsp), %rsi
 3985                             	        movq	$9, %rdx
 3986                             	#ifndef __APPLE__
 3987                             	        callq	fe_sq_n_x64@plt
 3988                             	#else
 3989                             	        callq	_fe_sq_n_x64
 3990                             	#endif /* __APPLE__ */
 3991                             	        leaq	96(%rsp), %rdi
 3992                             	        leaq	96(%rsp), %rsi
 3993                             	        leaq	64(%rsp), %rdx
 3994                             	#ifndef __APPLE__
 3995                             	        callq	fe_mul_x64@plt
 3996                             	#else
 3997                             	        callq	_fe_mul_x64
 3998                             	#endif /* __APPLE__ */
 3999                             	        leaq	128(%rsp), %rdi
 4000                             	        leaq	96(%rsp), %rsi
 4001                             	#ifndef __APPLE__
 4002                             	        callq	fe_sq_x64@plt
 4003                             	#else
 4004                             	        callq	_fe_sq_x64
 4005                             	#endif /* __APPLE__ */
 4006                             	        leaq	128(%rsp), %rdi
 4007                             	        leaq	128(%rsp), %rsi
 4008                             	        movq	$19, %rdx
 4009                             	#ifndef __APPLE__
 4010                             	        callq	fe_sq_n_x64@plt
 4011                             	#else
 4012                             	        callq	_fe_sq_n_x64
 4013                             	#endif /* __APPLE__ */
 4014                             	        leaq	96(%rsp), %rdi
 4015                             	        leaq	128(%rsp), %rsi
 4016                             	        leaq	96(%rsp), %rdx
 4017                             	#ifndef __APPLE__
 4018                             	        callq	fe_mul_x64@plt
 4019                             	#else
 4020                             	        callq	_fe_mul_x64
 4021                             	#endif /* __APPLE__ */
 4022                             	        leaq	96(%rsp), %rdi
 4023                             	        leaq	96(%rsp), %rsi
 4024                             	#ifndef __APPLE__
 4025                             	        callq	fe_sq_x64@plt
 4026                             	#else
 4027                             	        callq	_fe_sq_x64
 4028                             	#endif /* __APPLE__ */
 4029                             	        leaq	96(%rsp), %rdi
 4030                             	        leaq	96(%rsp), %rsi
 4031                             	        movq	$9, %rdx
 4032                             	#ifndef __APPLE__
 4033                             	        callq	fe_sq_n_x64@plt
 4034                             	#else
 4035                             	        callq	_fe_sq_n_x64
 4036                             	#endif /* __APPLE__ */
 4037                             	        leaq	64(%rsp), %rdi
 4038                             	        leaq	96(%rsp), %rsi
 4039                             	        leaq	64(%rsp), %rdx
 4040                             	#ifndef __APPLE__
 4041                             	        callq	fe_mul_x64@plt
 4042                             	#else
 4043                             	        callq	_fe_mul_x64
 4044                             	#endif /* __APPLE__ */
 4045                             	        leaq	96(%rsp), %rdi
 4046                             	        leaq	64(%rsp), %rsi
 4047                             	#ifndef __APPLE__
 4048                             	        callq	fe_sq_x64@plt
 4049                             	#else
 4050                             	        callq	_fe_sq_x64
 4051                             	#endif /* __APPLE__ */
 4052                             	        leaq	96(%rsp), %rdi
 4053                             	        leaq	96(%rsp), %rsi
 4054                             	        movq	$49, %rdx
 4055                             	#ifndef __APPLE__
 4056                             	        callq	fe_sq_n_x64@plt
 4057                             	#else
 4058                             	        callq	_fe_sq_n_x64
 4059                             	#endif /* __APPLE__ */
 4060                             	        leaq	96(%rsp), %rdi
 4061                             	        leaq	96(%rsp), %rsi
 4062                             	        leaq	64(%rsp), %rdx
 4063                             	#ifndef __APPLE__
 4064                             	        callq	fe_mul_x64@plt
 4065                             	#else
 4066                             	        callq	_fe_mul_x64
 4067                             	#endif /* __APPLE__ */
 4068                             	        leaq	128(%rsp), %rdi
 4069                             	        leaq	96(%rsp), %rsi
 4070                             	#ifndef __APPLE__
 4071                             	        callq	fe_sq_x64@plt
 4072                             	#else
 4073                             	        callq	_fe_sq_x64
 4074                             	#endif /* __APPLE__ */
 4075                             	        leaq	128(%rsp), %rdi
 4076                             	        leaq	128(%rsp), %rsi
 4077                             	        movq	$0x63, %rdx
 4078                             	#ifndef __APPLE__
 4079                             	        callq	fe_sq_n_x64@plt
 4080                             	#else
 4081                             	        callq	_fe_sq_n_x64
 4082                             	#endif /* __APPLE__ */
 4083                             	        leaq	96(%rsp), %rdi
 4084                             	        leaq	128(%rsp), %rsi
 4085                             	        leaq	96(%rsp), %rdx
 4086                             	#ifndef __APPLE__
 4087                             	        callq	fe_mul_x64@plt
 4088                             	#else
 4089                             	        callq	_fe_mul_x64
 4090                             	#endif /* __APPLE__ */
 4091                             	        leaq	96(%rsp), %rdi
 4092                             	        leaq	96(%rsp), %rsi
 4093                             	#ifndef __APPLE__
 4094                             	        callq	fe_sq_x64@plt
 4095                             	#else
 4096                             	        callq	_fe_sq_x64
 4097                             	#endif /* __APPLE__ */
 4098                             	        leaq	96(%rsp), %rdi
 4099                             	        leaq	96(%rsp), %rsi
 4100                             	        movq	$49, %rdx
 4101                             	#ifndef __APPLE__
 4102                             	        callq	fe_sq_n_x64@plt
 4103                             	#else
 4104                             	        callq	_fe_sq_n_x64
 4105                             	#endif /* __APPLE__ */
 4106                             	        leaq	64(%rsp), %rdi
 4107                             	        leaq	96(%rsp), %rsi
 4108                             	        leaq	64(%rsp), %rdx
 4109                             	#ifndef __APPLE__
 4110                             	        callq	fe_mul_x64@plt
 4111                             	#else
 4112                             	        callq	_fe_mul_x64
 4113                             	#endif /* __APPLE__ */
 4114                             	        leaq	64(%rsp), %rdi
 4115                             	        leaq	64(%rsp), %rsi
 4116                             	#ifndef __APPLE__
 4117                             	        callq	fe_sq_x64@plt
 4118                             	#else
 4119                             	        callq	_fe_sq_x64
 4120                             	#endif /* __APPLE__ */
 4121                             	        leaq	64(%rsp), %rdi
 4122                             	        leaq	64(%rsp), %rsi
 4123                             	        movq	$4, %rdx
 4124                             	#ifndef __APPLE__
 4125                             	        callq	fe_sq_n_x64@plt
 4126                             	#else
 4127                             	        callq	_fe_sq_n_x64
 4128                             	#endif /* __APPLE__ */
 4129                             	        movq	%rsp, %rdi
 4130                             	        leaq	64(%rsp), %rsi
 4131                             	        leaq	32(%rsp), %rdx
 4132                             	#ifndef __APPLE__
 4133                             	        callq	fe_mul_x64@plt
 4134                             	#else
 4135                             	        callq	_fe_mul_x64
 4136                             	#endif /* __APPLE__ */
 4137                             	        movq	176(%rsp), %rdi
 4138                             	        # Multiply
 4139                             	        #  A[0] * B[0]
 4140                             	        movq	(%rsp), %rax
 4141                             	        mulq	(%rdi)
 4142                             	        movq	%rax, %rcx
 4143                             	        movq	%rdx, %r9
 4144                             	        #  A[0] * B[1]
 4145                             	        movq	8(%rsp), %rax
 4146                             	        mulq	(%rdi)
 4147                             	        xorq	%r10, %r10
 4148                             	        addq	%rax, %r9
 4149                             	        adcq	%rdx, %r10
 4150                             	        #  A[1] * B[0]
 4151                             	        movq	(%rsp), %rax
 4152                             	        mulq	8(%rdi)
 4153                             	        xorq	%r11, %r11
 4154                             	        addq	%rax, %r9
 4155                             	        adcq	%rdx, %r10
 4156                             	        adcq	$0x00, %r11
 4157                             	        #  A[0] * B[2]
 4158                             	        movq	16(%rsp), %rax
 4159                             	        mulq	(%rdi)
 4160                             	        addq	%rax, %r10
 4161                             	        adcq	%rdx, %r11
 4162                             	        #  A[1] * B[1]
 4163                             	        movq	8(%rsp), %rax
 4164                             	        mulq	8(%rdi)
 4165                             	        xorq	%r12, %r12
 4166                             	        addq	%rax, %r10
 4167                             	        adcq	%rdx, %r11
 4168                             	        adcq	$0x00, %r12
 4169                             	        #  A[2] * B[0]
 4170                             	        movq	(%rsp), %rax
 4171                             	        mulq	16(%rdi)
 4172                             	        addq	%rax, %r10
 4173                             	        adcq	%rdx, %r11
 4174                             	        adcq	$0x00, %r12
 4175                             	        #  A[0] * B[3]
 4176                             	        movq	24(%rsp), %rax
 4177                             	        mulq	(%rdi)
 4178                             	        xorq	%r13, %r13
 4179                             	        addq	%rax, %r11
 4180                             	        adcq	%rdx, %r12
 4181                             	        adcq	$0x00, %r13
 4182                             	        #  A[1] * B[2]
 4183                             	        movq	16(%rsp), %rax
 4184                             	        mulq	8(%rdi)
 4185                             	        addq	%rax, %r11
 4186                             	        adcq	%rdx, %r12
 4187                             	        adcq	$0x00, %r13
 4188                             	        #  A[2] * B[1]
 4189                             	        movq	8(%rsp), %rax
 4190                             	        mulq	16(%rdi)
 4191                             	        addq	%rax, %r11
 4192                             	        adcq	%rdx, %r12
 4193                             	        adcq	$0x00, %r13
 4194                             	        #  A[3] * B[0]
 4195                             	        movq	(%rsp), %rax
 4196                             	        mulq	24(%rdi)
 4197                             	        addq	%rax, %r11
 4198                             	        adcq	%rdx, %r12
 4199                             	        adcq	$0x00, %r13
 4200                             	        #  A[1] * B[3]
 4201                             	        movq	24(%rsp), %rax
 4202                             	        mulq	8(%rdi)
 4203                             	        xorq	%r14, %r14
 4204                             	        addq	%rax, %r12
 4205                             	        adcq	%rdx, %r13
 4206                             	        adcq	$0x00, %r14
 4207                             	        #  A[2] * B[2]
 4208                             	        movq	16(%rsp), %rax
 4209                             	        mulq	16(%rdi)
 4210                             	        addq	%rax, %r12
 4211                             	        adcq	%rdx, %r13
 4212                             	        adcq	$0x00, %r14
 4213                             	        #  A[3] * B[1]
 4214                             	        movq	8(%rsp), %rax
 4215                             	        mulq	24(%rdi)
 4216                             	        addq	%rax, %r12
 4217                             	        adcq	%rdx, %r13
 4218                             	        adcq	$0x00, %r14
 4219                             	        #  A[2] * B[3]
 4220                             	        movq	24(%rsp), %rax
 4221                             	        mulq	16(%rdi)
 4222                             	        xorq	%r15, %r15
 4223                             	        addq	%rax, %r13
 4224                             	        adcq	%rdx, %r14
 4225                             	        adcq	$0x00, %r15
 4226                             	        #  A[3] * B[2]
 4227                             	        movq	16(%rsp), %rax
 4228                             	        mulq	24(%rdi)
 4229                             	        addq	%rax, %r13
 4230                             	        adcq	%rdx, %r14
 4231                             	        adcq	$0x00, %r15
 4232                             	        #  A[3] * B[3]
 4233                             	        movq	24(%rsp), %rax
 4234                             	        mulq	24(%rdi)
 4235                             	        addq	%rax, %r14
 4236                             	        adcq	%rdx, %r15
 4237                             	        # Reduce
 4238                             	        movq	$0x7fffffffffffffff, %rbp
 4239                             	        #  Move top half into t4-t7 and remove top bit from t3
 4240                             	        shldq	$0x01, %r14, %r15
 4241                             	        shldq	$0x01, %r13, %r14
 4242                             	        shldq	$0x01, %r12, %r13
 4243                             	        shldq	$0x01, %r11, %r12
 4244                             	        andq	%rbp, %r11
 4245                             	        #  Multiply top half by 19
 4246                             	        movq	$19, %rax
 4247                             	        mulq	%r12
 4248                             	        xorq	%r12, %r12
 4249                             	        addq	%rax, %rcx
 4250                             	        movq	$19, %rax
 4251                             	        adcq	%rdx, %r12
 4252                             	        mulq	%r13
 4253                             	        xorq	%r13, %r13
 4254                             	        addq	%rax, %r9
 4255                             	        movq	$19, %rax
 4256                             	        adcq	%rdx, %r13
 4257                             	        mulq	%r14
 4258                             	        xorq	%r14, %r14
 4259                             	        addq	%rax, %r10
 4260                             	        movq	$19, %rax
 4261                             	        adcq	%rdx, %r14
 4262                             	        mulq	%r15
 4263                             	        #  Add remaining product results in
 4264                             	        addq	%r12, %r9
 4265                             	        adcq	%r13, %r10
 4266                             	        adcq	%r14, %r11
 4267                             	        adcq	%rax, %r11
 4268                             	        adcq	$0x00, %rdx
 4269                             	        #  Overflow
 4270                             	        shldq	$0x01, %r11, %rdx
 4271                             	        imulq	$19, %rdx, %rax
 4272                             	        andq	%rbp, %r11
 4273                             	        addq	%rax, %rcx
 4274                             	        adcq	$0x00, %r9
 4275                             	        adcq	$0x00, %r10
 4276                             	        adcq	$0x00, %r11
 4277                             	        # Reduce if top bit set
 4278                             	        movq	%r11, %rdx
 4279                             	        sarq	$63, %rdx
 4280                             	        andq	$19, %rdx
 4281                             	        andq	%rbp, %r11
 4282                             	        addq	%rdx, %rcx
 4283                             	        adcq	$0x00, %r9
 4284                             	        adcq	$0x00, %r10
 4285                             	        adcq	$0x00, %r11
 4286                             	        movq	%rcx, %rax
 4287                             	        addq	$19, %rax
 4288                             	        movq	%r9, %rax
 4289                             	        adcq	$0x00, %rax
 4290                             	        movq	%r10, %rax
 4291                             	        adcq	$0x00, %rax
 4292                             	        movq	%r11, %rax
 4293                             	        adcq	$0x00, %rax
 4294                             	        sarq	$63, %rax
 4295                             	        andq	$19, %rax
 4296                             	        addq	%rax, %rcx
 4297                             	        adcq	$0x00, %r9
 4298                             	        adcq	$0x00, %r10
 4299                             	        adcq	$0x00, %r11
 4300                             	        andq	%rbp, %r11
 4301                             	        # Store
 4302                             	        movq	%rcx, (%rdi)
 4303                             	        movq	%r9, 8(%rdi)
 4304                             	        movq	%r10, 16(%rdi)
 4305                             	        movq	%r11, 24(%rdi)
 4306                             	        xorq	%rax, %rax
 4307                             	        addq	$0xb8, %rsp
 4308                             	        popq	%rbp
 4309                             	        popq	%rbx
 4310                             	        popq	%r15
 4311                             	        popq	%r14
 4312                             	        popq	%r13
 4313                             	        popq	%r12
 4314                             	        repz retq
 4315                             	#ifndef __APPLE__
 4317                             	#endif /* __APPLE__ */
 4318                             	#ifndef __APPLE__
 4319                             	.text
 4320                             	.globl	fe_pow22523_x64
 4322                             	.align	16
 4323                             	fe_pow22523_x64:
 4324                             	#else
 4325                             	.section	__TEXT,__text
 4326                             	.globl	_fe_pow22523_x64
 4327                             	.p2align	4
 4328                             	_fe_pow22523_x64:
 4329                             	#endif /* __APPLE__ */
 4330                             	        subq	$0x70, %rsp
 4331                             	        # pow22523
 4332                             	        movq	%rdi, 96(%rsp)
 4333                             	        movq	%rsi, 104(%rsp)
 4334                             	        movq	%rsp, %rdi
 4335                             	        movq	104(%rsp), %rsi
 4336                             	#ifndef __APPLE__
 4337                             	        callq	fe_sq_x64@plt
 4338                             	#else
 4339                             	        callq	_fe_sq_x64
 4340                             	#endif /* __APPLE__ */
 4341                             	        leaq	32(%rsp), %rdi
 4342                             	        movq	%rsp, %rsi
 4343                             	#ifndef __APPLE__
 4344                             	        callq	fe_sq_x64@plt
 4345                             	#else
 4346                             	        callq	_fe_sq_x64
 4347                             	#endif /* __APPLE__ */
 4348                             	        leaq	32(%rsp), %rdi
 4349                             	        leaq	32(%rsp), %rsi
 4350                             	#ifndef __APPLE__
 4351                             	        callq	fe_sq_x64@plt
 4352                             	#else
 4353                             	        callq	_fe_sq_x64
 4354                             	#endif /* __APPLE__ */
 4355                             	        leaq	32(%rsp), %rdi
 4356                             	        movq	104(%rsp), %rsi
 4357                             	        leaq	32(%rsp), %rdx
 4358                             	#ifndef __APPLE__
 4359                             	        callq	fe_mul_x64@plt
 4360                             	#else
 4361                             	        callq	_fe_mul_x64
 4362                             	#endif /* __APPLE__ */
 4363                             	        movq	%rsp, %rdi
 4364                             	        movq	%rsp, %rsi
 4365                             	        leaq	32(%rsp), %rdx
 4366                             	#ifndef __APPLE__
 4367                             	        callq	fe_mul_x64@plt
 4368                             	#else
 4369                             	        callq	_fe_mul_x64
 4370                             	#endif /* __APPLE__ */
 4371                             	        movq	%rsp, %rdi
 4372                             	        movq	%rsp, %rsi
 4373                             	#ifndef __APPLE__
 4374                             	        callq	fe_sq_x64@plt
 4375                             	#else
 4376                             	        callq	_fe_sq_x64
 4377                             	#endif /* __APPLE__ */
 4378                             	        movq	%rsp, %rdi
 4379                             	        leaq	32(%rsp), %rsi
 4380                             	        movq	%rsp, %rdx
 4381                             	#ifndef __APPLE__
 4382                             	        callq	fe_mul_x64@plt
 4383                             	#else
 4384                             	        callq	_fe_mul_x64
 4385                             	#endif /* __APPLE__ */
 4386                             	        leaq	32(%rsp), %rdi
 4387                             	        movq	%rsp, %rsi
 4388                             	#ifndef __APPLE__
 4389                             	        callq	fe_sq_x64@plt
 4390                             	#else
 4391                             	        callq	_fe_sq_x64
 4392                             	#endif /* __APPLE__ */
 4393                             	        leaq	32(%rsp), %rdi
 4394                             	        leaq	32(%rsp), %rsi
 4395                             	        movq	$4, %rdx
 4396                             	#ifndef __APPLE__
 4397                             	        callq	fe_sq_n_x64@plt
 4398                             	#else
 4399                             	        callq	_fe_sq_n_x64
 4400                             	#endif /* __APPLE__ */
 4401                             	        movq	%rsp, %rdi
 4402                             	        leaq	32(%rsp), %rsi
 4403                             	        movq	%rsp, %rdx
 4404                             	#ifndef __APPLE__
 4405                             	        callq	fe_mul_x64@plt
 4406                             	#else
 4407                             	        callq	_fe_mul_x64
 4408                             	#endif /* __APPLE__ */
 4409                             	        leaq	32(%rsp), %rdi
 4410                             	        movq	%rsp, %rsi
 4411                             	#ifndef __APPLE__
 4412                             	        callq	fe_sq_x64@plt
 4413                             	#else
 4414                             	        callq	_fe_sq_x64
 4415                             	#endif /* __APPLE__ */
 4416                             	        leaq	32(%rsp), %rdi
 4417                             	        leaq	32(%rsp), %rsi
 4418                             	        movq	$9, %rdx
 4419                             	#ifndef __APPLE__
 4420                             	        callq	fe_sq_n_x64@plt
 4421                             	#else
 4422                             	        callq	_fe_sq_n_x64
 4423                             	#endif /* __APPLE__ */
 4424                             	        leaq	32(%rsp), %rdi
 4425                             	        leaq	32(%rsp), %rsi
 4426                             	        movq	%rsp, %rdx
 4427                             	#ifndef __APPLE__
 4428                             	        callq	fe_mul_x64@plt
 4429                             	#else
 4430                             	        callq	_fe_mul_x64
 4431                             	#endif /* __APPLE__ */
 4432                             	        leaq	64(%rsp), %rdi
 4433                             	        leaq	32(%rsp), %rsi
 4434                             	#ifndef __APPLE__
 4435                             	        callq	fe_sq_x64@plt
 4436                             	#else
 4437                             	        callq	_fe_sq_x64
 4438                             	#endif /* __APPLE__ */
 4439                             	        leaq	64(%rsp), %rdi
 4440                             	        leaq	64(%rsp), %rsi
 4441                             	        movq	$19, %rdx
 4442                             	#ifndef __APPLE__
 4443                             	        callq	fe_sq_n_x64@plt
 4444                             	#else
 4445                             	        callq	_fe_sq_n_x64
 4446                             	#endif /* __APPLE__ */
 4447                             	        leaq	32(%rsp), %rdi
 4448                             	        leaq	64(%rsp), %rsi
 4449                             	        leaq	32(%rsp), %rdx
 4450                             	#ifndef __APPLE__
 4451                             	        callq	fe_mul_x64@plt
 4452                             	#else
 4453                             	        callq	_fe_mul_x64
 4454                             	#endif /* __APPLE__ */
 4455                             	        leaq	32(%rsp), %rdi
 4456                             	        leaq	32(%rsp), %rsi
 4457                             	#ifndef __APPLE__
 4458                             	        callq	fe_sq_x64@plt
 4459                             	#else
 4460                             	        callq	_fe_sq_x64
 4461                             	#endif /* __APPLE__ */
 4462                             	        leaq	32(%rsp), %rdi
 4463                             	        leaq	32(%rsp), %rsi
 4464                             	        movq	$9, %rdx
 4465                             	#ifndef __APPLE__
 4466                             	        callq	fe_sq_n_x64@plt
 4467                             	#else
 4468                             	        callq	_fe_sq_n_x64
 4469                             	#endif /* __APPLE__ */
 4470                             	        movq	%rsp, %rdi
 4471                             	        leaq	32(%rsp), %rsi
 4472                             	        movq	%rsp, %rdx
 4473                             	#ifndef __APPLE__
 4474                             	        callq	fe_mul_x64@plt
 4475                             	#else
 4476                             	        callq	_fe_mul_x64
 4477                             	#endif /* __APPLE__ */
 4478                             	        leaq	32(%rsp), %rdi
 4479                             	        movq	%rsp, %rsi
 4480                             	#ifndef __APPLE__
 4481                             	        callq	fe_sq_x64@plt
 4482                             	#else
 4483                             	        callq	_fe_sq_x64
 4484                             	#endif /* __APPLE__ */
 4485                             	        leaq	32(%rsp), %rdi
 4486                             	        leaq	32(%rsp), %rsi
 4487                             	        movq	$49, %rdx
 4488                             	#ifndef __APPLE__
 4489                             	        callq	fe_sq_n_x64@plt
 4490                             	#else
 4491                             	        callq	_fe_sq_n_x64
 4492                             	#endif /* __APPLE__ */
 4493                             	        leaq	32(%rsp), %rdi
 4494                             	        leaq	32(%rsp), %rsi
 4495                             	        movq	%rsp, %rdx
 4496                             	#ifndef __APPLE__
 4497                             	        callq	fe_mul_x64@plt
 4498                             	#else
 4499                             	        callq	_fe_mul_x64
 4500                             	#endif /* __APPLE__ */
 4501                             	        leaq	64(%rsp), %rdi
 4502                             	        leaq	32(%rsp), %rsi
 4503                             	#ifndef __APPLE__
 4504                             	        callq	fe_sq_x64@plt
 4505                             	#else
 4506                             	        callq	_fe_sq_x64
 4507                             	#endif /* __APPLE__ */
 4508                             	        leaq	64(%rsp), %rdi
 4509                             	        leaq	64(%rsp), %rsi
 4510                             	        movq	$0x63, %rdx
 4511                             	#ifndef __APPLE__
 4512                             	        callq	fe_sq_n_x64@plt
 4513                             	#else
 4514                             	        callq	_fe_sq_n_x64
 4515                             	#endif /* __APPLE__ */
 4516                             	        leaq	32(%rsp), %rdi
 4517                             	        leaq	64(%rsp), %rsi
 4518                             	        leaq	32(%rsp), %rdx
 4519                             	#ifndef __APPLE__
 4520                             	        callq	fe_mul_x64@plt
 4521                             	#else
 4522                             	        callq	_fe_mul_x64
 4523                             	#endif /* __APPLE__ */
 4524                             	        leaq	32(%rsp), %rdi
 4525                             	        leaq	32(%rsp), %rsi
 4526                             	#ifndef __APPLE__
 4527                             	        callq	fe_sq_x64@plt
 4528                             	#else
 4529                             	        callq	_fe_sq_x64
 4530                             	#endif /* __APPLE__ */
 4531                             	        leaq	32(%rsp), %rdi
 4532                             	        leaq	32(%rsp), %rsi
 4533                             	        movq	$49, %rdx
 4534                             	#ifndef __APPLE__
 4535                             	        callq	fe_sq_n_x64@plt
 4536                             	#else
 4537                             	        callq	_fe_sq_n_x64
 4538                             	#endif /* __APPLE__ */
 4539                             	        movq	%rsp, %rdi
 4540                             	        leaq	32(%rsp), %rsi
 4541                             	        movq	%rsp, %rdx
 4542                             	#ifndef __APPLE__
 4543                             	        callq	fe_mul_x64@plt
 4544                             	#else
 4545                             	        callq	_fe_mul_x64
 4546                             	#endif /* __APPLE__ */
 4547                             	        movq	%rsp, %rdi
 4548                             	        movq	%rsp, %rsi
 4549                             	#ifndef __APPLE__
 4550                             	        callq	fe_sq_x64@plt
 4551                             	#else
 4552                             	        callq	_fe_sq_x64
 4553                             	#endif /* __APPLE__ */
 4554                             	        movq	%rsp, %rdi
 4555                             	        movq	%rsp, %rsi
 4556                             	#ifndef __APPLE__
 4557                             	        callq	fe_sq_x64@plt
 4558                             	#else
 4559                             	        callq	_fe_sq_x64
 4560                             	#endif /* __APPLE__ */
 4561                             	        movq	96(%rsp), %rdi
 4562                             	        movq	%rsp, %rsi
 4563                             	        movq	104(%rsp), %rdx
 4564                             	#ifndef __APPLE__
 4565                             	        callq	fe_mul_x64@plt
 4566                             	#else
 4567                             	        callq	_fe_mul_x64
 4568                             	#endif /* __APPLE__ */
 4569                             	        movq	104(%rsp), %rsi
 4570                             	        movq	96(%rsp), %rdi
 4571                             	        addq	$0x70, %rsp
 4572                             	        repz retq
 4573                             	#ifndef __APPLE__
 4574                             	.text
 4575                             	.globl	fe_ge_to_p2_x64
 4577                             	.align	16
 4578                             	fe_ge_to_p2_x64:
 4579                             	#else
 4580                             	.section	__TEXT,__text
 4581                             	.globl	_fe_ge_to_p2_x64
 4582                             	.p2align	4
 4583                             	_fe_ge_to_p2_x64:
 4584                             	#endif /* __APPLE__ */
 4585                             	        pushq	%rbx
 4586                             	        pushq	%r12
 4587                             	        pushq	%r13
 4588                             	        pushq	%r14
 4589                             	        pushq	%r15
 4590                             	        subq	$40, %rsp
 4591                             	        movq	%rsi, (%rsp)
 4592                             	        movq	%rdx, 8(%rsp)
 4593                             	        movq	%rcx, 16(%rsp)
 4594                             	        movq	%r8, 24(%rsp)
 4595                             	        movq	%r9, 32(%rsp)
 4596                             	        movq	16(%rsp), %rsi
 4597                             	        movq	88(%rsp), %rbx
 4598                             	        # Multiply
 4599                             	        #  A[0] * B[0]
 4600                             	        movq	(%rbx), %rax
 4601                             	        mulq	(%rsi)
 4602                             	        movq	%rax, %r8
 4603                             	        movq	%rdx, %r9
 4604                             	        #  A[0] * B[1]
 4605                             	        movq	8(%rbx), %rax
 4606                             	        mulq	(%rsi)
 4607                             	        xorq	%r10, %r10
 4608                             	        addq	%rax, %r9
 4609                             	        adcq	%rdx, %r10
 4610                             	        #  A[1] * B[0]
 4611                             	        movq	(%rbx), %rax
 4612                             	        mulq	8(%rsi)
 4613                             	        xorq	%r11, %r11
 4614                             	        addq	%rax, %r9
 4615                             	        adcq	%rdx, %r10
 4616                             	        adcq	$0x00, %r11
 4617                             	        #  A[0] * B[2]
 4618                             	        movq	16(%rbx), %rax
 4619                             	        mulq	(%rsi)
 4620                             	        addq	%rax, %r10
 4621                             	        adcq	%rdx, %r11
 4622                             	        #  A[1] * B[1]
 4623                             	        movq	8(%rbx), %rax
 4624                             	        mulq	8(%rsi)
 4625                             	        xorq	%r12, %r12
 4626                             	        addq	%rax, %r10
 4627                             	        adcq	%rdx, %r11
 4628                             	        adcq	$0x00, %r12
 4629                             	        #  A[2] * B[0]
 4630                             	        movq	(%rbx), %rax
 4631                             	        mulq	16(%rsi)
 4632                             	        addq	%rax, %r10
 4633                             	        adcq	%rdx, %r11
 4634                             	        adcq	$0x00, %r12
 4635                             	        #  A[0] * B[3]
 4636                             	        movq	24(%rbx), %rax
 4637                             	        mulq	(%rsi)
 4638                             	        xorq	%r13, %r13
 4639                             	        addq	%rax, %r11
 4640                             	        adcq	%rdx, %r12
 4641                             	        adcq	$0x00, %r13
 4642                             	        #  A[1] * B[2]
 4643                             	        movq	16(%rbx), %rax
 4644                             	        mulq	8(%rsi)
 4645                             	        addq	%rax, %r11
 4646                             	        adcq	%rdx, %r12
 4647                             	        adcq	$0x00, %r13
 4648                             	        #  A[2] * B[1]
 4649                             	        movq	8(%rbx), %rax
 4650                             	        mulq	16(%rsi)
 4651                             	        addq	%rax, %r11
 4652                             	        adcq	%rdx, %r12
 4653                             	        adcq	$0x00, %r13
 4654                             	        #  A[3] * B[0]
 4655                             	        movq	(%rbx), %rax
 4656                             	        mulq	24(%rsi)
 4657                             	        addq	%rax, %r11
 4658                             	        adcq	%rdx, %r12
 4659                             	        adcq	$0x00, %r13
 4660                             	        #  A[1] * B[3]
 4661                             	        movq	24(%rbx), %rax
 4662                             	        mulq	8(%rsi)
 4663                             	        xorq	%r14, %r14
 4664                             	        addq	%rax, %r12
 4665                             	        adcq	%rdx, %r13
 4666                             	        adcq	$0x00, %r14
 4667                             	        #  A[2] * B[2]
 4668                             	        movq	16(%rbx), %rax
 4669                             	        mulq	16(%rsi)
 4670                             	        addq	%rax, %r12
 4671                             	        adcq	%rdx, %r13
 4672                             	        adcq	$0x00, %r14
 4673                             	        #  A[3] * B[1]
 4674                             	        movq	8(%rbx), %rax
 4675                             	        mulq	24(%rsi)
 4676                             	        addq	%rax, %r12
 4677                             	        adcq	%rdx, %r13
 4678                             	        adcq	$0x00, %r14
 4679                             	        #  A[2] * B[3]
 4680                             	        movq	24(%rbx), %rax
 4681                             	        mulq	16(%rsi)
 4682                             	        xorq	%r15, %r15
 4683                             	        addq	%rax, %r13
 4684                             	        adcq	%rdx, %r14
 4685                             	        adcq	$0x00, %r15
 4686                             	        #  A[3] * B[2]
 4687                             	        movq	16(%rbx), %rax
 4688                             	        mulq	24(%rsi)
 4689                             	        addq	%rax, %r13
 4690                             	        adcq	%rdx, %r14
 4691                             	        adcq	$0x00, %r15
 4692                             	        #  A[3] * B[3]
 4693                             	        movq	24(%rbx), %rax
 4694                             	        mulq	24(%rsi)
 4695                             	        addq	%rax, %r14
 4696                             	        adcq	%rdx, %r15
 4697                             	        # Reduce
 4698                             	        movq	$0x7fffffffffffffff, %rcx
 4699                             	        #  Move top half into t4-t7 and remove top bit from t3
 4700                             	        shldq	$0x01, %r14, %r15
 4701                             	        shldq	$0x01, %r13, %r14
 4702                             	        shldq	$0x01, %r12, %r13
 4703                             	        shldq	$0x01, %r11, %r12
 4704                             	        andq	%rcx, %r11
 4705                             	        #  Multiply top half by 19
 4706                             	        movq	$19, %rax
 4707                             	        mulq	%r12
 4708                             	        xorq	%r12, %r12
 4709                             	        addq	%rax, %r8
 4710                             	        movq	$19, %rax
 4711                             	        adcq	%rdx, %r12
 4712                             	        mulq	%r13
 4713                             	        xorq	%r13, %r13
 4714                             	        addq	%rax, %r9
 4715                             	        movq	$19, %rax
 4716                             	        adcq	%rdx, %r13
 4717                             	        mulq	%r14
 4718                             	        xorq	%r14, %r14
 4719                             	        addq	%rax, %r10
 4720                             	        movq	$19, %rax
 4721                             	        adcq	%rdx, %r14
 4722                             	        mulq	%r15
 4723                             	        #  Add remaining product results in
 4724                             	        addq	%r12, %r9
 4725                             	        adcq	%r13, %r10
 4726                             	        adcq	%r14, %r11
 4727                             	        adcq	%rax, %r11
 4728                             	        adcq	$0x00, %rdx
 4729                             	        #  Overflow
 4730                             	        shldq	$0x01, %r11, %rdx
 4731                             	        imulq	$19, %rdx, %rax
 4732                             	        andq	%rcx, %r11
 4733                             	        addq	%rax, %r8
 4734                             	        adcq	$0x00, %r9
 4735                             	        adcq	$0x00, %r10
 4736                             	        adcq	$0x00, %r11
 4737                             	        # Reduce if top bit set
 4738                             	        movq	%r11, %rdx
 4739                             	        sarq	$63, %rdx
 4740                             	        andq	$19, %rdx
 4741                             	        andq	%rcx, %r11
 4742                             	        addq	%rdx, %r8
 4743                             	        adcq	$0x00, %r9
 4744                             	        adcq	$0x00, %r10
 4745                             	        adcq	$0x00, %r11
 4746                             	        # Store
 4747                             	        movq	%r8, (%rdi)
 4748                             	        movq	%r9, 8(%rdi)
 4749                             	        movq	%r10, 16(%rdi)
 4750                             	        movq	%r11, 24(%rdi)
 4751                             	        movq	(%rsp), %rdi
 4752                             	        movq	24(%rsp), %rsi
 4753                             	        movq	32(%rsp), %rbx
 4754                             	        # Multiply
 4755                             	        #  A[0] * B[0]
 4756                             	        movq	(%rbx), %rax
 4757                             	        mulq	(%rsi)
 4758                             	        movq	%rax, %r8
 4759                             	        movq	%rdx, %r9
 4760                             	        #  A[0] * B[1]
 4761                             	        movq	8(%rbx), %rax
 4762                             	        mulq	(%rsi)
 4763                             	        xorq	%r10, %r10
 4764                             	        addq	%rax, %r9
 4765                             	        adcq	%rdx, %r10
 4766                             	        #  A[1] * B[0]
 4767                             	        movq	(%rbx), %rax
 4768                             	        mulq	8(%rsi)
 4769                             	        xorq	%r11, %r11
 4770                             	        addq	%rax, %r9
 4771                             	        adcq	%rdx, %r10
 4772                             	        adcq	$0x00, %r11
 4773                             	        #  A[0] * B[2]
 4774                             	        movq	16(%rbx), %rax
 4775                             	        mulq	(%rsi)
 4776                             	        addq	%rax, %r10
 4777                             	        adcq	%rdx, %r11
 4778                             	        #  A[1] * B[1]
 4779                             	        movq	8(%rbx), %rax
 4780                             	        mulq	8(%rsi)
 4781                             	        xorq	%r12, %r12
 4782                             	        addq	%rax, %r10
 4783                             	        adcq	%rdx, %r11
 4784                             	        adcq	$0x00, %r12
 4785                             	        #  A[2] * B[0]
 4786                             	        movq	(%rbx), %rax
 4787                             	        mulq	16(%rsi)
 4788                             	        addq	%rax, %r10
 4789                             	        adcq	%rdx, %r11
 4790                             	        adcq	$0x00, %r12
 4791                             	        #  A[0] * B[3]
 4792                             	        movq	24(%rbx), %rax
 4793                             	        mulq	(%rsi)
 4794                             	        xorq	%r13, %r13
 4795                             	        addq	%rax, %r11
 4796                             	        adcq	%rdx, %r12
 4797                             	        adcq	$0x00, %r13
 4798                             	        #  A[1] * B[2]
 4799                             	        movq	16(%rbx), %rax
 4800                             	        mulq	8(%rsi)
 4801                             	        addq	%rax, %r11
 4802                             	        adcq	%rdx, %r12
 4803                             	        adcq	$0x00, %r13
 4804                             	        #  A[2] * B[1]
 4805                             	        movq	8(%rbx), %rax
 4806                             	        mulq	16(%rsi)
 4807                             	        addq	%rax, %r11
 4808                             	        adcq	%rdx, %r12
 4809                             	        adcq	$0x00, %r13
 4810                             	        #  A[3] * B[0]
 4811                             	        movq	(%rbx), %rax
 4812                             	        mulq	24(%rsi)
 4813                             	        addq	%rax, %r11
 4814                             	        adcq	%rdx, %r12
 4815                             	        adcq	$0x00, %r13
 4816                             	        #  A[1] * B[3]
 4817                             	        movq	24(%rbx), %rax
 4818                             	        mulq	8(%rsi)
 4819                             	        xorq	%r14, %r14
 4820                             	        addq	%rax, %r12
 4821                             	        adcq	%rdx, %r13
 4822                             	        adcq	$0x00, %r14
 4823                             	        #  A[2] * B[2]
 4824                             	        movq	16(%rbx), %rax
 4825                             	        mulq	16(%rsi)
 4826                             	        addq	%rax, %r12
 4827                             	        adcq	%rdx, %r13
 4828                             	        adcq	$0x00, %r14
 4829                             	        #  A[3] * B[1]
 4830                             	        movq	8(%rbx), %rax
 4831                             	        mulq	24(%rsi)
 4832                             	        addq	%rax, %r12
 4833                             	        adcq	%rdx, %r13
 4834                             	        adcq	$0x00, %r14
 4835                             	        #  A[2] * B[3]
 4836                             	        movq	24(%rbx), %rax
 4837                             	        mulq	16(%rsi)
 4838                             	        xorq	%r15, %r15
 4839                             	        addq	%rax, %r13
 4840                             	        adcq	%rdx, %r14
 4841                             	        adcq	$0x00, %r15
 4842                             	        #  A[3] * B[2]
 4843                             	        movq	16(%rbx), %rax
 4844                             	        mulq	24(%rsi)
 4845                             	        addq	%rax, %r13
 4846                             	        adcq	%rdx, %r14
 4847                             	        adcq	$0x00, %r15
 4848                             	        #  A[3] * B[3]
 4849                             	        movq	24(%rbx), %rax
 4850                             	        mulq	24(%rsi)
 4851                             	        addq	%rax, %r14
 4852                             	        adcq	%rdx, %r15
 4853                             	        # Reduce
 4854                             	        movq	$0x7fffffffffffffff, %rcx
 4855                             	        #  Move top half into t4-t7 and remove top bit from t3
 4856                             	        shldq	$0x01, %r14, %r15
 4857                             	        shldq	$0x01, %r13, %r14
 4858                             	        shldq	$0x01, %r12, %r13
 4859                             	        shldq	$0x01, %r11, %r12
 4860                             	        andq	%rcx, %r11
 4861                             	        #  Multiply top half by 19
 4862                             	        movq	$19, %rax
 4863                             	        mulq	%r12
 4864                             	        xorq	%r12, %r12
 4865                             	        addq	%rax, %r8
 4866                             	        movq	$19, %rax
 4867                             	        adcq	%rdx, %r12
 4868                             	        mulq	%r13
 4869                             	        xorq	%r13, %r13
 4870                             	        addq	%rax, %r9
 4871                             	        movq	$19, %rax
 4872                             	        adcq	%rdx, %r13
 4873                             	        mulq	%r14
 4874                             	        xorq	%r14, %r14
 4875                             	        addq	%rax, %r10
 4876                             	        movq	$19, %rax
 4877                             	        adcq	%rdx, %r14
 4878                             	        mulq	%r15
 4879                             	        #  Add remaining product results in
 4880                             	        addq	%r12, %r9
 4881                             	        adcq	%r13, %r10
 4882                             	        adcq	%r14, %r11
 4883                             	        adcq	%rax, %r11
 4884                             	        adcq	$0x00, %rdx
 4885                             	        #  Overflow
 4886                             	        shldq	$0x01, %r11, %rdx
 4887                             	        imulq	$19, %rdx, %rax
 4888                             	        andq	%rcx, %r11
 4889                             	        addq	%rax, %r8
 4890                             	        adcq	$0x00, %r9
 4891                             	        adcq	$0x00, %r10
 4892                             	        adcq	$0x00, %r11
 4893                             	        # Reduce if top bit set
 4894                             	        movq	%r11, %rdx
 4895                             	        sarq	$63, %rdx
 4896                             	        andq	$19, %rdx
 4897                             	        andq	%rcx, %r11
 4898                             	        addq	%rdx, %r8
 4899                             	        adcq	$0x00, %r9
 4900                             	        adcq	$0x00, %r10
 4901                             	        adcq	$0x00, %r11
 4902                             	        # Store
 4903                             	        movq	%r8, (%rdi)
 4904                             	        movq	%r9, 8(%rdi)
 4905                             	        movq	%r10, 16(%rdi)
 4906                             	        movq	%r11, 24(%rdi)
 4907                             	        movq	8(%rsp), %rdi
 4908                             	        movq	32(%rsp), %rsi
 4909                             	        movq	88(%rsp), %rbx
 4910                             	        # Multiply
 4911                             	        #  A[0] * B[0]
 4912                             	        movq	(%rbx), %rax
 4913                             	        mulq	(%rsi)
 4914                             	        movq	%rax, %r8
 4915                             	        movq	%rdx, %r9
 4916                             	        #  A[0] * B[1]
 4917                             	        movq	8(%rbx), %rax
 4918                             	        mulq	(%rsi)
 4919                             	        xorq	%r10, %r10
 4920                             	        addq	%rax, %r9
 4921                             	        adcq	%rdx, %r10
 4922                             	        #  A[1] * B[0]
 4923                             	        movq	(%rbx), %rax
 4924                             	        mulq	8(%rsi)
 4925                             	        xorq	%r11, %r11
 4926                             	        addq	%rax, %r9
 4927                             	        adcq	%rdx, %r10
 4928                             	        adcq	$0x00, %r11
 4929                             	        #  A[0] * B[2]
 4930                             	        movq	16(%rbx), %rax
 4931                             	        mulq	(%rsi)
 4932                             	        addq	%rax, %r10
 4933                             	        adcq	%rdx, %r11
 4934                             	        #  A[1] * B[1]
 4935                             	        movq	8(%rbx), %rax
 4936                             	        mulq	8(%rsi)
 4937                             	        xorq	%r12, %r12
 4938                             	        addq	%rax, %r10
 4939                             	        adcq	%rdx, %r11
 4940                             	        adcq	$0x00, %r12
 4941                             	        #  A[2] * B[0]
 4942                             	        movq	(%rbx), %rax
 4943                             	        mulq	16(%rsi)
 4944                             	        addq	%rax, %r10
 4945                             	        adcq	%rdx, %r11
 4946                             	        adcq	$0x00, %r12
 4947                             	        #  A[0] * B[3]
 4948                             	        movq	24(%rbx), %rax
 4949                             	        mulq	(%rsi)
 4950                             	        xorq	%r13, %r13
 4951                             	        addq	%rax, %r11
 4952                             	        adcq	%rdx, %r12
 4953                             	        adcq	$0x00, %r13
 4954                             	        #  A[1] * B[2]
 4955                             	        movq	16(%rbx), %rax
 4956                             	        mulq	8(%rsi)
 4957                             	        addq	%rax, %r11
 4958                             	        adcq	%rdx, %r12
 4959                             	        adcq	$0x00, %r13
 4960                             	        #  A[2] * B[1]
 4961                             	        movq	8(%rbx), %rax
 4962                             	        mulq	16(%rsi)
 4963                             	        addq	%rax, %r11
 4964                             	        adcq	%rdx, %r12
 4965                             	        adcq	$0x00, %r13
 4966                             	        #  A[3] * B[0]
 4967                             	        movq	(%rbx), %rax
 4968                             	        mulq	24(%rsi)
 4969                             	        addq	%rax, %r11
 4970                             	        adcq	%rdx, %r12
 4971                             	        adcq	$0x00, %r13
 4972                             	        #  A[1] * B[3]
 4973                             	        movq	24(%rbx), %rax
 4974                             	        mulq	8(%rsi)
 4975                             	        xorq	%r14, %r14
 4976                             	        addq	%rax, %r12
 4977                             	        adcq	%rdx, %r13
 4978                             	        adcq	$0x00, %r14
 4979                             	        #  A[2] * B[2]
 4980                             	        movq	16(%rbx), %rax
 4981                             	        mulq	16(%rsi)
 4982                             	        addq	%rax, %r12
 4983                             	        adcq	%rdx, %r13
 4984                             	        adcq	$0x00, %r14
 4985                             	        #  A[3] * B[1]
 4986                             	        movq	8(%rbx), %rax
 4987                             	        mulq	24(%rsi)
 4988                             	        addq	%rax, %r12
 4989                             	        adcq	%rdx, %r13
 4990                             	        adcq	$0x00, %r14
 4991                             	        #  A[2] * B[3]
 4992                             	        movq	24(%rbx), %rax
 4993                             	        mulq	16(%rsi)
 4994                             	        xorq	%r15, %r15
 4995                             	        addq	%rax, %r13
 4996                             	        adcq	%rdx, %r14
 4997                             	        adcq	$0x00, %r15
 4998                             	        #  A[3] * B[2]
 4999                             	        movq	16(%rbx), %rax
 5000                             	        mulq	24(%rsi)
 5001                             	        addq	%rax, %r13
 5002                             	        adcq	%rdx, %r14
 5003                             	        adcq	$0x00, %r15
 5004                             	        #  A[3] * B[3]
 5005                             	        movq	24(%rbx), %rax
 5006                             	        mulq	24(%rsi)
 5007                             	        addq	%rax, %r14
 5008                             	        adcq	%rdx, %r15
 5009                             	        # Reduce
 5010                             	        movq	$0x7fffffffffffffff, %rcx
 5011                             	        #  Move top half into t4-t7 and remove top bit from t3
 5012                             	        shldq	$0x01, %r14, %r15
 5013                             	        shldq	$0x01, %r13, %r14
 5014                             	        shldq	$0x01, %r12, %r13
 5015                             	        shldq	$0x01, %r11, %r12
 5016                             	        andq	%rcx, %r11
 5017                             	        #  Multiply top half by 19
 5018                             	        movq	$19, %rax
 5019                             	        mulq	%r12
 5020                             	        xorq	%r12, %r12
 5021                             	        addq	%rax, %r8
 5022                             	        movq	$19, %rax
 5023                             	        adcq	%rdx, %r12
 5024                             	        mulq	%r13
 5025                             	        xorq	%r13, %r13
 5026                             	        addq	%rax, %r9
 5027                             	        movq	$19, %rax
 5028                             	        adcq	%rdx, %r13
 5029                             	        mulq	%r14
 5030                             	        xorq	%r14, %r14
 5031                             	        addq	%rax, %r10
 5032                             	        movq	$19, %rax
 5033                             	        adcq	%rdx, %r14
 5034                             	        mulq	%r15
 5035                             	        #  Add remaining product results in
 5036                             	        addq	%r12, %r9
 5037                             	        adcq	%r13, %r10
 5038                             	        adcq	%r14, %r11
 5039                             	        adcq	%rax, %r11
 5040                             	        adcq	$0x00, %rdx
 5041                             	        #  Overflow
 5042                             	        shldq	$0x01, %r11, %rdx
 5043                             	        imulq	$19, %rdx, %rax
 5044                             	        andq	%rcx, %r11
 5045                             	        addq	%rax, %r8
 5046                             	        adcq	$0x00, %r9
 5047                             	        adcq	$0x00, %r10
 5048                             	        adcq	$0x00, %r11
 5049                             	        # Reduce if top bit set
 5050                             	        movq	%r11, %rdx
 5051                             	        sarq	$63, %rdx
 5052                             	        andq	$19, %rdx
 5053                             	        andq	%rcx, %r11
 5054                             	        addq	%rdx, %r8
 5055                             	        adcq	$0x00, %r9
 5056                             	        adcq	$0x00, %r10
 5057                             	        adcq	$0x00, %r11
 5058                             	        # Store
 5059                             	        movq	%r8, (%rdi)
 5060                             	        movq	%r9, 8(%rdi)
 5061                             	        movq	%r10, 16(%rdi)
 5062                             	        movq	%r11, 24(%rdi)
 5063                             	        addq	$40, %rsp
 5064                             	        popq	%r15
 5065                             	        popq	%r14
 5066                             	        popq	%r13
 5067                             	        popq	%r12
 5068                             	        popq	%rbx
 5069                             	        repz retq
 5070                             	#ifndef __APPLE__
 5072                             	#endif /* __APPLE__ */
 5073                             	#ifndef __APPLE__
 5074                             	.text
 5075                             	.globl	fe_ge_to_p3_x64
 5077                             	.align	16
 5078                             	fe_ge_to_p3_x64:
 5079                             	#else
 5080                             	.section	__TEXT,__text
 5081                             	.globl	_fe_ge_to_p3_x64
 5082                             	.p2align	4
 5083                             	_fe_ge_to_p3_x64:
 5084                             	#endif /* __APPLE__ */
 5085                             	        pushq	%rbx
 5086                             	        pushq	%r12
 5087                             	        pushq	%r13
 5088                             	        pushq	%r14
 5089                             	        pushq	%r15
 5090                             	        subq	$40, %rsp
 5091                             	        movq	%rsi, (%rsp)
 5092                             	        movq	%rdx, 8(%rsp)
 5093                             	        movq	%rcx, 16(%rsp)
 5094                             	        movq	%r8, 24(%rsp)
 5095                             	        movq	%r9, 32(%rsp)
 5096                             	        movq	24(%rsp), %rsi
 5097                             	        movq	96(%rsp), %rbx
 5098                             	        # Multiply
 5099                             	        #  A[0] * B[0]
 5100                             	        movq	(%rbx), %rax
 5101                             	        mulq	(%rsi)
 5102                             	        movq	%rax, %r8
 5103                             	        movq	%rdx, %r9
 5104                             	        #  A[0] * B[1]
 5105                             	        movq	8(%rbx), %rax
 5106                             	        mulq	(%rsi)
 5107                             	        xorq	%r10, %r10
 5108                             	        addq	%rax, %r9
 5109                             	        adcq	%rdx, %r10
 5110                             	        #  A[1] * B[0]
 5111                             	        movq	(%rbx), %rax
 5112                             	        mulq	8(%rsi)
 5113                             	        xorq	%r11, %r11
 5114                             	        addq	%rax, %r9
 5115                             	        adcq	%rdx, %r10
 5116                             	        adcq	$0x00, %r11
 5117                             	        #  A[0] * B[2]
 5118                             	        movq	16(%rbx), %rax
 5119                             	        mulq	(%rsi)
 5120                             	        addq	%rax, %r10
 5121                             	        adcq	%rdx, %r11
 5122                             	        #  A[1] * B[1]
 5123                             	        movq	8(%rbx), %rax
 5124                             	        mulq	8(%rsi)
 5125                             	        xorq	%r12, %r12
 5126                             	        addq	%rax, %r10
 5127                             	        adcq	%rdx, %r11
 5128                             	        adcq	$0x00, %r12
 5129                             	        #  A[2] * B[0]
 5130                             	        movq	(%rbx), %rax
 5131                             	        mulq	16(%rsi)
 5132                             	        addq	%rax, %r10
 5133                             	        adcq	%rdx, %r11
 5134                             	        adcq	$0x00, %r12
 5135                             	        #  A[0] * B[3]
 5136                             	        movq	24(%rbx), %rax
 5137                             	        mulq	(%rsi)
 5138                             	        xorq	%r13, %r13
 5139                             	        addq	%rax, %r11
 5140                             	        adcq	%rdx, %r12
 5141                             	        adcq	$0x00, %r13
 5142                             	        #  A[1] * B[2]
 5143                             	        movq	16(%rbx), %rax
 5144                             	        mulq	8(%rsi)
 5145                             	        addq	%rax, %r11
 5146                             	        adcq	%rdx, %r12
 5147                             	        adcq	$0x00, %r13
 5148                             	        #  A[2] * B[1]
 5149                             	        movq	8(%rbx), %rax
 5150                             	        mulq	16(%rsi)
 5151                             	        addq	%rax, %r11
 5152                             	        adcq	%rdx, %r12
 5153                             	        adcq	$0x00, %r13
 5154                             	        #  A[3] * B[0]
 5155                             	        movq	(%rbx), %rax
 5156                             	        mulq	24(%rsi)
 5157                             	        addq	%rax, %r11
 5158                             	        adcq	%rdx, %r12
 5159                             	        adcq	$0x00, %r13
 5160                             	        #  A[1] * B[3]
 5161                             	        movq	24(%rbx), %rax
 5162                             	        mulq	8(%rsi)
 5163                             	        xorq	%r14, %r14
 5164                             	        addq	%rax, %r12
 5165                             	        adcq	%rdx, %r13
 5166                             	        adcq	$0x00, %r14
 5167                             	        #  A[2] * B[2]
 5168                             	        movq	16(%rbx), %rax
 5169                             	        mulq	16(%rsi)
 5170                             	        addq	%rax, %r12
 5171                             	        adcq	%rdx, %r13
 5172                             	        adcq	$0x00, %r14
 5173                             	        #  A[3] * B[1]
 5174                             	        movq	8(%rbx), %rax
 5175                             	        mulq	24(%rsi)
 5176                             	        addq	%rax, %r12
 5177                             	        adcq	%rdx, %r13
 5178                             	        adcq	$0x00, %r14
 5179                             	        #  A[2] * B[3]
 5180                             	        movq	24(%rbx), %rax
 5181                             	        mulq	16(%rsi)
 5182                             	        xorq	%r15, %r15
 5183                             	        addq	%rax, %r13
 5184                             	        adcq	%rdx, %r14
 5185                             	        adcq	$0x00, %r15
 5186                             	        #  A[3] * B[2]
 5187                             	        movq	16(%rbx), %rax
 5188                             	        mulq	24(%rsi)
 5189                             	        addq	%rax, %r13
 5190                             	        adcq	%rdx, %r14
 5191                             	        adcq	$0x00, %r15
 5192                             	        #  A[3] * B[3]
 5193                             	        movq	24(%rbx), %rax
 5194                             	        mulq	24(%rsi)
 5195                             	        addq	%rax, %r14
 5196                             	        adcq	%rdx, %r15
 5197                             	        # Reduce
 5198                             	        movq	$0x7fffffffffffffff, %rcx
 5199                             	        #  Move top half into t4-t7 and remove top bit from t3
 5200                             	        shldq	$0x01, %r14, %r15
 5201                             	        shldq	$0x01, %r13, %r14
 5202                             	        shldq	$0x01, %r12, %r13
 5203                             	        shldq	$0x01, %r11, %r12
 5204                             	        andq	%rcx, %r11
 5205                             	        #  Multiply top half by 19
 5206                             	        movq	$19, %rax
 5207                             	        mulq	%r12
 5208                             	        xorq	%r12, %r12
 5209                             	        addq	%rax, %r8
 5210                             	        movq	$19, %rax
 5211                             	        adcq	%rdx, %r12
 5212                             	        mulq	%r13
 5213                             	        xorq	%r13, %r13
 5214                             	        addq	%rax, %r9
 5215                             	        movq	$19, %rax
 5216                             	        adcq	%rdx, %r13
 5217                             	        mulq	%r14
 5218                             	        xorq	%r14, %r14
 5219                             	        addq	%rax, %r10
 5220                             	        movq	$19, %rax
 5221                             	        adcq	%rdx, %r14
 5222                             	        mulq	%r15
 5223                             	        #  Add remaining product results in
 5224                             	        addq	%r12, %r9
 5225                             	        adcq	%r13, %r10
 5226                             	        adcq	%r14, %r11
 5227                             	        adcq	%rax, %r11
 5228                             	        adcq	$0x00, %rdx
 5229                             	        #  Overflow
 5230                             	        shldq	$0x01, %r11, %rdx
 5231                             	        imulq	$19, %rdx, %rax
 5232                             	        andq	%rcx, %r11
 5233                             	        addq	%rax, %r8
 5234                             	        adcq	$0x00, %r9
 5235                             	        adcq	$0x00, %r10
 5236                             	        adcq	$0x00, %r11
 5237                             	        # Reduce if top bit set
 5238                             	        movq	%r11, %rdx
 5239                             	        sarq	$63, %rdx
 5240                             	        andq	$19, %rdx
 5241                             	        andq	%rcx, %r11
 5242                             	        addq	%rdx, %r8
 5243                             	        adcq	$0x00, %r9
 5244                             	        adcq	$0x00, %r10
 5245                             	        adcq	$0x00, %r11
 5246                             	        # Store
 5247                             	        movq	%r8, (%rdi)
 5248                             	        movq	%r9, 8(%rdi)
 5249                             	        movq	%r10, 16(%rdi)
 5250                             	        movq	%r11, 24(%rdi)
 5251                             	        movq	(%rsp), %rdi
 5252                             	        movq	32(%rsp), %rsi
 5253                             	        movq	88(%rsp), %rbx
 5254                             	        # Multiply
 5255                             	        #  A[0] * B[0]
 5256                             	        movq	(%rbx), %rax
 5257                             	        mulq	(%rsi)
 5258                             	        movq	%rax, %r8
 5259                             	        movq	%rdx, %r9
 5260                             	        #  A[0] * B[1]
 5261                             	        movq	8(%rbx), %rax
 5262                             	        mulq	(%rsi)
 5263                             	        xorq	%r10, %r10
 5264                             	        addq	%rax, %r9
 5265                             	        adcq	%rdx, %r10
 5266                             	        #  A[1] * B[0]
 5267                             	        movq	(%rbx), %rax
 5268                             	        mulq	8(%rsi)
 5269                             	        xorq	%r11, %r11
 5270                             	        addq	%rax, %r9
 5271                             	        adcq	%rdx, %r10
 5272                             	        adcq	$0x00, %r11
 5273                             	        #  A[0] * B[2]
 5274                             	        movq	16(%rbx), %rax
 5275                             	        mulq	(%rsi)
 5276                             	        addq	%rax, %r10
 5277                             	        adcq	%rdx, %r11
 5278                             	        #  A[1] * B[1]
 5279                             	        movq	8(%rbx), %rax
 5280                             	        mulq	8(%rsi)
 5281                             	        xorq	%r12, %r12
 5282                             	        addq	%rax, %r10
 5283                             	        adcq	%rdx, %r11
 5284                             	        adcq	$0x00, %r12
 5285                             	        #  A[2] * B[0]
 5286                             	        movq	(%rbx), %rax
 5287                             	        mulq	16(%rsi)
 5288                             	        addq	%rax, %r10
 5289                             	        adcq	%rdx, %r11
 5290                             	        adcq	$0x00, %r12
 5291                             	        #  A[0] * B[3]
 5292                             	        movq	24(%rbx), %rax
 5293                             	        mulq	(%rsi)
 5294                             	        xorq	%r13, %r13
 5295                             	        addq	%rax, %r11
 5296                             	        adcq	%rdx, %r12
 5297                             	        adcq	$0x00, %r13
 5298                             	        #  A[1] * B[2]
 5299                             	        movq	16(%rbx), %rax
 5300                             	        mulq	8(%rsi)
 5301                             	        addq	%rax, %r11
 5302                             	        adcq	%rdx, %r12
 5303                             	        adcq	$0x00, %r13
 5304                             	        #  A[2] * B[1]
 5305                             	        movq	8(%rbx), %rax
 5306                             	        mulq	16(%rsi)
 5307                             	        addq	%rax, %r11
 5308                             	        adcq	%rdx, %r12
 5309                             	        adcq	$0x00, %r13
 5310                             	        #  A[3] * B[0]
 5311                             	        movq	(%rbx), %rax
 5312                             	        mulq	24(%rsi)
 5313                             	        addq	%rax, %r11
 5314                             	        adcq	%rdx, %r12
 5315                             	        adcq	$0x00, %r13
 5316                             	        #  A[1] * B[3]
 5317                             	        movq	24(%rbx), %rax
 5318                             	        mulq	8(%rsi)
 5319                             	        xorq	%r14, %r14
 5320                             	        addq	%rax, %r12
 5321                             	        adcq	%rdx, %r13
 5322                             	        adcq	$0x00, %r14
 5323                             	        #  A[2] * B[2]
 5324                             	        movq	16(%rbx), %rax
 5325                             	        mulq	16(%rsi)
 5326                             	        addq	%rax, %r12
 5327                             	        adcq	%rdx, %r13
 5328                             	        adcq	$0x00, %r14
 5329                             	        #  A[3] * B[1]
 5330                             	        movq	8(%rbx), %rax
 5331                             	        mulq	24(%rsi)
 5332                             	        addq	%rax, %r12
 5333                             	        adcq	%rdx, %r13
 5334                             	        adcq	$0x00, %r14
 5335                             	        #  A[2] * B[3]
 5336                             	        movq	24(%rbx), %rax
 5337                             	        mulq	16(%rsi)
 5338                             	        xorq	%r15, %r15
 5339                             	        addq	%rax, %r13
 5340                             	        adcq	%rdx, %r14
 5341                             	        adcq	$0x00, %r15
 5342                             	        #  A[3] * B[2]
 5343                             	        movq	16(%rbx), %rax
 5344                             	        mulq	24(%rsi)
 5345                             	        addq	%rax, %r13
 5346                             	        adcq	%rdx, %r14
 5347                             	        adcq	$0x00, %r15
 5348                             	        #  A[3] * B[3]
 5349                             	        movq	24(%rbx), %rax
 5350                             	        mulq	24(%rsi)
 5351                             	        addq	%rax, %r14
 5352                             	        adcq	%rdx, %r15
 5353                             	        # Reduce
 5354                             	        movq	$0x7fffffffffffffff, %rcx
 5355                             	        #  Move top half into t4-t7 and remove top bit from t3
 5356                             	        shldq	$0x01, %r14, %r15
 5357                             	        shldq	$0x01, %r13, %r14
 5358                             	        shldq	$0x01, %r12, %r13
 5359                             	        shldq	$0x01, %r11, %r12
 5360                             	        andq	%rcx, %r11
 5361                             	        #  Multiply top half by 19
 5362                             	        movq	$19, %rax
 5363                             	        mulq	%r12
 5364                             	        xorq	%r12, %r12
 5365                             	        addq	%rax, %r8
 5366                             	        movq	$19, %rax
 5367                             	        adcq	%rdx, %r12
 5368                             	        mulq	%r13
 5369                             	        xorq	%r13, %r13
 5370                             	        addq	%rax, %r9
 5371                             	        movq	$19, %rax
 5372                             	        adcq	%rdx, %r13
 5373                             	        mulq	%r14
 5374                             	        xorq	%r14, %r14
 5375                             	        addq	%rax, %r10
 5376                             	        movq	$19, %rax
 5377                             	        adcq	%rdx, %r14
 5378                             	        mulq	%r15
 5379                             	        #  Add remaining product results in
 5380                             	        addq	%r12, %r9
 5381                             	        adcq	%r13, %r10
 5382                             	        adcq	%r14, %r11
 5383                             	        adcq	%rax, %r11
 5384                             	        adcq	$0x00, %rdx
 5385                             	        #  Overflow
 5386                             	        shldq	$0x01, %r11, %rdx
 5387                             	        imulq	$19, %rdx, %rax
 5388                             	        andq	%rcx, %r11
 5389                             	        addq	%rax, %r8
 5390                             	        adcq	$0x00, %r9
 5391                             	        adcq	$0x00, %r10
 5392                             	        adcq	$0x00, %r11
 5393                             	        # Reduce if top bit set
 5394                             	        movq	%r11, %rdx
 5395                             	        sarq	$63, %rdx
 5396                             	        andq	$19, %rdx
 5397                             	        andq	%rcx, %r11
 5398                             	        addq	%rdx, %r8
 5399                             	        adcq	$0x00, %r9
 5400                             	        adcq	$0x00, %r10
 5401                             	        adcq	$0x00, %r11
 5402                             	        # Store
 5403                             	        movq	%r8, (%rdi)
 5404                             	        movq	%r9, 8(%rdi)
 5405                             	        movq	%r10, 16(%rdi)
 5406                             	        movq	%r11, 24(%rdi)
 5407                             	        movq	8(%rsp), %rdi
 5408                             	        movq	88(%rsp), %rsi
 5409                             	        movq	96(%rsp), %rbx
 5410                             	        # Multiply
 5411                             	        #  A[0] * B[0]
 5412                             	        movq	(%rbx), %rax
 5413                             	        mulq	(%rsi)
 5414                             	        movq	%rax, %r8
 5415                             	        movq	%rdx, %r9
 5416                             	        #  A[0] * B[1]
 5417                             	        movq	8(%rbx), %rax
 5418                             	        mulq	(%rsi)
 5419                             	        xorq	%r10, %r10
 5420                             	        addq	%rax, %r9
 5421                             	        adcq	%rdx, %r10
 5422                             	        #  A[1] * B[0]
 5423                             	        movq	(%rbx), %rax
 5424                             	        mulq	8(%rsi)
 5425                             	        xorq	%r11, %r11
 5426                             	        addq	%rax, %r9
 5427                             	        adcq	%rdx, %r10
 5428                             	        adcq	$0x00, %r11
 5429                             	        #  A[0] * B[2]
 5430                             	        movq	16(%rbx), %rax
 5431                             	        mulq	(%rsi)
 5432                             	        addq	%rax, %r10
 5433                             	        adcq	%rdx, %r11
 5434                             	        #  A[1] * B[1]
 5435                             	        movq	8(%rbx), %rax
 5436                             	        mulq	8(%rsi)
 5437                             	        xorq	%r12, %r12
 5438                             	        addq	%rax, %r10
 5439                             	        adcq	%rdx, %r11
 5440                             	        adcq	$0x00, %r12
 5441                             	        #  A[2] * B[0]
 5442                             	        movq	(%rbx), %rax
 5443                             	        mulq	16(%rsi)
 5444                             	        addq	%rax, %r10
 5445                             	        adcq	%rdx, %r11
 5446                             	        adcq	$0x00, %r12
 5447                             	        #  A[0] * B[3]
 5448                             	        movq	24(%rbx), %rax
 5449                             	        mulq	(%rsi)
 5450                             	        xorq	%r13, %r13
 5451                             	        addq	%rax, %r11
 5452                             	        adcq	%rdx, %r12
 5453                             	        adcq	$0x00, %r13
 5454                             	        #  A[1] * B[2]
 5455                             	        movq	16(%rbx), %rax
 5456                             	        mulq	8(%rsi)
 5457                             	        addq	%rax, %r11
 5458                             	        adcq	%rdx, %r12
 5459                             	        adcq	$0x00, %r13
 5460                             	        #  A[2] * B[1]
 5461                             	        movq	8(%rbx), %rax
 5462                             	        mulq	16(%rsi)
 5463                             	        addq	%rax, %r11
 5464                             	        adcq	%rdx, %r12
 5465                             	        adcq	$0x00, %r13
 5466                             	        #  A[3] * B[0]
 5467                             	        movq	(%rbx), %rax
 5468                             	        mulq	24(%rsi)
 5469                             	        addq	%rax, %r11
 5470                             	        adcq	%rdx, %r12
 5471                             	        adcq	$0x00, %r13
 5472                             	        #  A[1] * B[3]
 5473                             	        movq	24(%rbx), %rax
 5474                             	        mulq	8(%rsi)
 5475                             	        xorq	%r14, %r14
 5476                             	        addq	%rax, %r12
 5477                             	        adcq	%rdx, %r13
 5478                             	        adcq	$0x00, %r14
 5479                             	        #  A[2] * B[2]
 5480                             	        movq	16(%rbx), %rax
 5481                             	        mulq	16(%rsi)
 5482                             	        addq	%rax, %r12
 5483                             	        adcq	%rdx, %r13
 5484                             	        adcq	$0x00, %r14
 5485                             	        #  A[3] * B[1]
 5486                             	        movq	8(%rbx), %rax
 5487                             	        mulq	24(%rsi)
 5488                             	        addq	%rax, %r12
 5489                             	        adcq	%rdx, %r13
 5490                             	        adcq	$0x00, %r14
 5491                             	        #  A[2] * B[3]
 5492                             	        movq	24(%rbx), %rax
 5493                             	        mulq	16(%rsi)
 5494                             	        xorq	%r15, %r15
 5495                             	        addq	%rax, %r13
 5496                             	        adcq	%rdx, %r14
 5497                             	        adcq	$0x00, %r15
 5498                             	        #  A[3] * B[2]
 5499                             	        movq	16(%rbx), %rax
 5500                             	        mulq	24(%rsi)
 5501                             	        addq	%rax, %r13
 5502                             	        adcq	%rdx, %r14
 5503                             	        adcq	$0x00, %r15
 5504                             	        #  A[3] * B[3]
 5505                             	        movq	24(%rbx), %rax
 5506                             	        mulq	24(%rsi)
 5507                             	        addq	%rax, %r14
 5508                             	        adcq	%rdx, %r15
 5509                             	        # Reduce
 5510                             	        movq	$0x7fffffffffffffff, %rcx
 5511                             	        #  Move top half into t4-t7 and remove top bit from t3
 5512                             	        shldq	$0x01, %r14, %r15
 5513                             	        shldq	$0x01, %r13, %r14
 5514                             	        shldq	$0x01, %r12, %r13
 5515                             	        shldq	$0x01, %r11, %r12
 5516                             	        andq	%rcx, %r11
 5517                             	        #  Multiply top half by 19
 5518                             	        movq	$19, %rax
 5519                             	        mulq	%r12
 5520                             	        xorq	%r12, %r12
 5521                             	        addq	%rax, %r8
 5522                             	        movq	$19, %rax
 5523                             	        adcq	%rdx, %r12
 5524                             	        mulq	%r13
 5525                             	        xorq	%r13, %r13
 5526                             	        addq	%rax, %r9
 5527                             	        movq	$19, %rax
 5528                             	        adcq	%rdx, %r13
 5529                             	        mulq	%r14
 5530                             	        xorq	%r14, %r14
 5531                             	        addq	%rax, %r10
 5532                             	        movq	$19, %rax
 5533                             	        adcq	%rdx, %r14
 5534                             	        mulq	%r15
 5535                             	        #  Add remaining product results in
 5536                             	        addq	%r12, %r9
 5537                             	        adcq	%r13, %r10
 5538                             	        adcq	%r14, %r11
 5539                             	        adcq	%rax, %r11
 5540                             	        adcq	$0x00, %rdx
 5541                             	        #  Overflow
 5542                             	        shldq	$0x01, %r11, %rdx
 5543                             	        imulq	$19, %rdx, %rax
 5544                             	        andq	%rcx, %r11
 5545                             	        addq	%rax, %r8
 5546                             	        adcq	$0x00, %r9
 5547                             	        adcq	$0x00, %r10
 5548                             	        adcq	$0x00, %r11
 5549                             	        # Reduce if top bit set
 5550                             	        movq	%r11, %rdx
 5551                             	        sarq	$63, %rdx
 5552                             	        andq	$19, %rdx
 5553                             	        andq	%rcx, %r11
 5554                             	        addq	%rdx, %r8
 5555                             	        adcq	$0x00, %r9
 5556                             	        adcq	$0x00, %r10
 5557                             	        adcq	$0x00, %r11
 5558                             	        # Store
 5559                             	        movq	%r8, (%rdi)
 5560                             	        movq	%r9, 8(%rdi)
 5561                             	        movq	%r10, 16(%rdi)
 5562                             	        movq	%r11, 24(%rdi)
 5563                             	        movq	16(%rsp), %rdi
 5564                             	        movq	24(%rsp), %rsi
 5565                             	        movq	32(%rsp), %rbx
 5566                             	        # Multiply
 5567                             	        #  A[0] * B[0]
 5568                             	        movq	(%rbx), %rax
 5569                             	        mulq	(%rsi)
 5570                             	        movq	%rax, %r8
 5571                             	        movq	%rdx, %r9
 5572                             	        #  A[0] * B[1]
 5573                             	        movq	8(%rbx), %rax
 5574                             	        mulq	(%rsi)
 5575                             	        xorq	%r10, %r10
 5576                             	        addq	%rax, %r9
 5577                             	        adcq	%rdx, %r10
 5578                             	        #  A[1] * B[0]
 5579                             	        movq	(%rbx), %rax
 5580                             	        mulq	8(%rsi)
 5581                             	        xorq	%r11, %r11
 5582                             	        addq	%rax, %r9
 5583                             	        adcq	%rdx, %r10
 5584                             	        adcq	$0x00, %r11
 5585                             	        #  A[0] * B[2]
 5586                             	        movq	16(%rbx), %rax
 5587                             	        mulq	(%rsi)
 5588                             	        addq	%rax, %r10
 5589                             	        adcq	%rdx, %r11
 5590                             	        #  A[1] * B[1]
 5591                             	        movq	8(%rbx), %rax
 5592                             	        mulq	8(%rsi)
 5593                             	        xorq	%r12, %r12
 5594                             	        addq	%rax, %r10
 5595                             	        adcq	%rdx, %r11
 5596                             	        adcq	$0x00, %r12
 5597                             	        #  A[2] * B[0]
 5598                             	        movq	(%rbx), %rax
 5599                             	        mulq	16(%rsi)
 5600                             	        addq	%rax, %r10
 5601                             	        adcq	%rdx, %r11
 5602                             	        adcq	$0x00, %r12
 5603                             	        #  A[0] * B[3]
 5604                             	        movq	24(%rbx), %rax
 5605                             	        mulq	(%rsi)
 5606                             	        xorq	%r13, %r13
 5607                             	        addq	%rax, %r11
 5608                             	        adcq	%rdx, %r12
 5609                             	        adcq	$0x00, %r13
 5610                             	        #  A[1] * B[2]
 5611                             	        movq	16(%rbx), %rax
 5612                             	        mulq	8(%rsi)
 5613                             	        addq	%rax, %r11
 5614                             	        adcq	%rdx, %r12
 5615                             	        adcq	$0x00, %r13
 5616                             	        #  A[2] * B[1]
 5617                             	        movq	8(%rbx), %rax
 5618                             	        mulq	16(%rsi)
 5619                             	        addq	%rax, %r11
 5620                             	        adcq	%rdx, %r12
 5621                             	        adcq	$0x00, %r13
 5622                             	        #  A[3] * B[0]
 5623                             	        movq	(%rbx), %rax
 5624                             	        mulq	24(%rsi)
 5625                             	        addq	%rax, %r11
 5626                             	        adcq	%rdx, %r12
 5627                             	        adcq	$0x00, %r13
 5628                             	        #  A[1] * B[3]
 5629                             	        movq	24(%rbx), %rax
 5630                             	        mulq	8(%rsi)
 5631                             	        xorq	%r14, %r14
 5632                             	        addq	%rax, %r12
 5633                             	        adcq	%rdx, %r13
 5634                             	        adcq	$0x00, %r14
 5635                             	        #  A[2] * B[2]
 5636                             	        movq	16(%rbx), %rax
 5637                             	        mulq	16(%rsi)
 5638                             	        addq	%rax, %r12
 5639                             	        adcq	%rdx, %r13
 5640                             	        adcq	$0x00, %r14
 5641                             	        #  A[3] * B[1]
 5642                             	        movq	8(%rbx), %rax
 5643                             	        mulq	24(%rsi)
 5644                             	        addq	%rax, %r12
 5645                             	        adcq	%rdx, %r13
 5646                             	        adcq	$0x00, %r14
 5647                             	        #  A[2] * B[3]
 5648                             	        movq	24(%rbx), %rax
 5649                             	        mulq	16(%rsi)
 5650                             	        xorq	%r15, %r15
 5651                             	        addq	%rax, %r13
 5652                             	        adcq	%rdx, %r14
 5653                             	        adcq	$0x00, %r15
 5654                             	        #  A[3] * B[2]
 5655                             	        movq	16(%rbx), %rax
 5656                             	        mulq	24(%rsi)
 5657                             	        addq	%rax, %r13
 5658                             	        adcq	%rdx, %r14
 5659                             	        adcq	$0x00, %r15
 5660                             	        #  A[3] * B[3]
 5661                             	        movq	24(%rbx), %rax
 5662                             	        mulq	24(%rsi)
 5663                             	        addq	%rax, %r14
 5664                             	        adcq	%rdx, %r15
 5665                             	        # Reduce
 5666                             	        movq	$0x7fffffffffffffff, %rcx
 5667                             	        #  Move top half into t4-t7 and remove top bit from t3
 5668                             	        shldq	$0x01, %r14, %r15
 5669                             	        shldq	$0x01, %r13, %r14
 5670                             	        shldq	$0x01, %r12, %r13
 5671                             	        shldq	$0x01, %r11, %r12
 5672                             	        andq	%rcx, %r11
 5673                             	        #  Multiply top half by 19
 5674                             	        movq	$19, %rax
 5675                             	        mulq	%r12
 5676                             	        xorq	%r12, %r12
 5677                             	        addq	%rax, %r8
 5678                             	        movq	$19, %rax
 5679                             	        adcq	%rdx, %r12
 5680                             	        mulq	%r13
 5681                             	        xorq	%r13, %r13
 5682                             	        addq	%rax, %r9
 5683                             	        movq	$19, %rax
 5684                             	        adcq	%rdx, %r13
 5685                             	        mulq	%r14
 5686                             	        xorq	%r14, %r14
 5687                             	        addq	%rax, %r10
 5688                             	        movq	$19, %rax
 5689                             	        adcq	%rdx, %r14
 5690                             	        mulq	%r15
 5691                             	        #  Add remaining product results in
 5692                             	        addq	%r12, %r9
 5693                             	        adcq	%r13, %r10
 5694                             	        adcq	%r14, %r11
 5695                             	        adcq	%rax, %r11
 5696                             	        adcq	$0x00, %rdx
 5697                             	        #  Overflow
 5698                             	        shldq	$0x01, %r11, %rdx
 5699                             	        imulq	$19, %rdx, %rax
 5700                             	        andq	%rcx, %r11
 5701                             	        addq	%rax, %r8
 5702                             	        adcq	$0x00, %r9
 5703                             	        adcq	$0x00, %r10
 5704                             	        adcq	$0x00, %r11
 5705                             	        # Reduce if top bit set
 5706                             	        movq	%r11, %rdx
 5707                             	        sarq	$63, %rdx
 5708                             	        andq	$19, %rdx
 5709                             	        andq	%rcx, %r11
 5710                             	        addq	%rdx, %r8
 5711                             	        adcq	$0x00, %r9
 5712                             	        adcq	$0x00, %r10
 5713                             	        adcq	$0x00, %r11
 5714                             	        # Store
 5715                             	        movq	%r8, (%rdi)
 5716                             	        movq	%r9, 8(%rdi)
 5717                             	        movq	%r10, 16(%rdi)
 5718                             	        movq	%r11, 24(%rdi)
 5719                             	        addq	$40, %rsp
 5720                             	        popq	%r15
 5721                             	        popq	%r14
 5722                             	        popq	%r13
 5723                             	        popq	%r12
 5724                             	        popq	%rbx
 5725                             	        repz retq
 5726                             	#ifndef __APPLE__
 5728                             	#endif /* __APPLE__ */
 5729                             	#ifndef __APPLE__
 5730                             	.text
 5731                             	.globl	fe_ge_dbl_x64
 5733                             	.align	16
 5734                             	fe_ge_dbl_x64:
 5735                             	#else
 5736                             	.section	__TEXT,__text
 5737                             	.globl	_fe_ge_dbl_x64
 5738                             	.p2align	4
 5739                             	_fe_ge_dbl_x64:
 5740                             	#endif /* __APPLE__ */
 5741                             	        pushq	%rbx
 5742                             	        pushq	%r12
 5743                             	        pushq	%r13
 5744                             	        pushq	%r14
 5745                             	        pushq	%r15
 5746                             	        subq	$0x50, %rsp
 5747                             	        movq	%rdi, (%rsp)
 5748                             	        movq	%rsi, 8(%rsp)
 5749                             	        movq	%rdx, 16(%rsp)
 5750                             	        movq	%rcx, 24(%rsp)
 5751                             	        movq	%r8, 32(%rsp)
 5752                             	        movq	%r9, 40(%rsp)
 5753                             	        movq	(%rsp), %rdi
 5754                             	        movq	32(%rsp), %rsi
 5755                             	        # Square
 5756                             	        #  A[0] * A[1]
 5757                             	        movq	(%rsi), %rax
 5758                             	        mulq	8(%rsi)
 5759                             	        movq	%rax, %r9
 5760                             	        movq	%rdx, %r10
 5761                             	        #  A[0] * A[2]
 5762                             	        movq	(%rsi), %rax
 5763                             	        mulq	16(%rsi)
 5764                             	        xorq	%r11, %r11
 5765                             	        addq	%rax, %r10
 5766                             	        adcq	%rdx, %r11
 5767                             	        #  A[0] * A[3]
 5768                             	        movq	(%rsi), %rax
 5769                             	        mulq	24(%rsi)
 5770                             	        xorq	%r12, %r12
 5771                             	        addq	%rax, %r11
 5772                             	        adcq	%rdx, %r12
 5773                             	        #  A[1] * A[2]
 5774                             	        movq	8(%rsi), %rax
 5775                             	        mulq	16(%rsi)
 5776                             	        xorq	%r13, %r13
 5777                             	        addq	%rax, %r11
 5778                             	        adcq	%rdx, %r12
 5779                             	        adcq	$0x00, %r13
 5780                             	        #  A[1] * A[3]
 5781                             	        movq	8(%rsi), %rax
 5782                             	        mulq	24(%rsi)
 5783                             	        addq	%rax, %r12
 5784                             	        adcq	%rdx, %r13
 5785                             	        #  A[2] * A[3]
 5786                             	        movq	16(%rsi), %rax
 5787                             	        mulq	24(%rsi)
 5788                             	        xorq	%r14, %r14
 5789                             	        addq	%rax, %r13
 5790                             	        adcq	%rdx, %r14
 5791                             	        # Double
 5792                             	        xorq	%r15, %r15
 5793                             	        addq	%r9, %r9
 5794                             	        adcq	%r10, %r10
 5795                             	        adcq	%r11, %r11
 5796                             	        adcq	%r12, %r12
 5797                             	        adcq	%r13, %r13
 5798                             	        adcq	%r14, %r14
 5799                             	        adcq	$0x00, %r15
 5800                             	        #  A[0] * A[0]
 5801                             	        movq	(%rsi), %rax
 5802                             	        mulq	%rax
 5803                             	        movq	%rax, %r8
 5804                             	        movq	%rdx, %rcx
 5805                             	        #  A[1] * A[1]
 5806                             	        movq	8(%rsi), %rax
 5807                             	        mulq	%rax
 5808                             	        addq	%rcx, %r9
 5809                             	        adcq	%rax, %r10
 5810                             	        adcq	$0x00, %rdx
 5811                             	        movq	%rdx, %rcx
 5812                             	        #  A[2] * A[2]
 5813                             	        movq	16(%rsi), %rax
 5814                             	        mulq	%rax
 5815                             	        addq	%rcx, %r11
 5816                             	        adcq	%rax, %r12
 5817                             	        adcq	$0x00, %rdx
 5818                             	        movq	%rdx, %rcx
 5819                             	        #  A[3] * A[3]
 5820                             	        movq	24(%rsi), %rax
 5821                             	        mulq	%rax
 5822                             	        addq	%rax, %r14
 5823                             	        adcq	%rdx, %r15
 5824                             	        addq	%rcx, %r13
 5825                             	        adcq	$0x00, %r14
 5826                             	        adcq	$0x00, %r15
 5827                             	        # Reduce
 5828                             	        movq	$0x7fffffffffffffff, %rcx
 5829                             	        #  Move top half into t4-t7 and remove top bit from t3
 5830                             	        shldq	$0x01, %r14, %r15
 5831                             	        shldq	$0x01, %r13, %r14
 5832                             	        shldq	$0x01, %r12, %r13
 5833                             	        shldq	$0x01, %r11, %r12
 5834                             	        andq	%rcx, %r11
 5835                             	        #  Multiply top half by 19
 5836                             	        movq	$19, %rax
 5837                             	        mulq	%r12
 5838                             	        xorq	%r12, %r12
 5839                             	        addq	%rax, %r8
 5840                             	        movq	$19, %rax
 5841                             	        adcq	%rdx, %r12
 5842                             	        mulq	%r13
 5843                             	        xorq	%r13, %r13
 5844                             	        addq	%rax, %r9
 5845                             	        movq	$19, %rax
 5846                             	        adcq	%rdx, %r13
 5847                             	        mulq	%r14
 5848                             	        xorq	%r14, %r14
 5849                             	        addq	%rax, %r10
 5850                             	        movq	$19, %rax
 5851                             	        adcq	%rdx, %r14
 5852                             	        mulq	%r15
 5853                             	        #  Add remaining product results in
 5854                             	        addq	%r12, %r9
 5855                             	        adcq	%r13, %r10
 5856                             	        adcq	%r14, %r11
 5857                             	        adcq	%rax, %r11
 5858                             	        adcq	$0x00, %rdx
 5859                             	        #  Overflow
 5860                             	        shldq	$0x01, %r11, %rdx
 5861                             	        imulq	$19, %rdx, %rax
 5862                             	        andq	%rcx, %r11
 5863                             	        addq	%rax, %r8
 5864                             	        adcq	$0x00, %r9
 5865                             	        adcq	$0x00, %r10
 5866                             	        adcq	$0x00, %r11
 5867                             	        # Reduce if top bit set
 5868                             	        movq	%r11, %rdx
 5869                             	        sarq	$63, %rdx
 5870                             	        andq	$19, %rdx
 5871                             	        andq	%rcx, %r11
 5872                             	        addq	%rdx, %r8
 5873                             	        adcq	$0x00, %r9
 5874                             	        adcq	$0x00, %r10
 5875                             	        adcq	$0x00, %r11
 5876                             	        # Store
 5877                             	        movq	%r8, (%rdi)
 5878                             	        movq	%r9, 8(%rdi)
 5879                             	        movq	%r10, 16(%rdi)
 5880                             	        movq	%r11, 24(%rdi)
 5881                             	        movq	16(%rsp), %rdi
 5882                             	        movq	40(%rsp), %rsi
 5883                             	        # Square
 5884                             	        #  A[0] * A[1]
 5885                             	        movq	(%rsi), %rax
 5886                             	        mulq	8(%rsi)
 5887                             	        movq	%rax, %r9
 5888                             	        movq	%rdx, %r10
 5889                             	        #  A[0] * A[2]
 5890                             	        movq	(%rsi), %rax
 5891                             	        mulq	16(%rsi)
 5892                             	        xorq	%r11, %r11
 5893                             	        addq	%rax, %r10
 5894                             	        adcq	%rdx, %r11
 5895                             	        #  A[0] * A[3]
 5896                             	        movq	(%rsi), %rax
 5897                             	        mulq	24(%rsi)
 5898                             	        xorq	%r12, %r12
 5899                             	        addq	%rax, %r11
 5900                             	        adcq	%rdx, %r12
 5901                             	        #  A[1] * A[2]
 5902                             	        movq	8(%rsi), %rax
 5903                             	        mulq	16(%rsi)
 5904                             	        xorq	%r13, %r13
 5905                             	        addq	%rax, %r11
 5906                             	        adcq	%rdx, %r12
 5907                             	        adcq	$0x00, %r13
 5908                             	        #  A[1] * A[3]
 5909                             	        movq	8(%rsi), %rax
 5910                             	        mulq	24(%rsi)
 5911                             	        addq	%rax, %r12
 5912                             	        adcq	%rdx, %r13
 5913                             	        #  A[2] * A[3]
 5914                             	        movq	16(%rsi), %rax
 5915                             	        mulq	24(%rsi)
 5916                             	        xorq	%r14, %r14
 5917                             	        addq	%rax, %r13
 5918                             	        adcq	%rdx, %r14
 5919                             	        # Double
 5920                             	        xorq	%r15, %r15
 5921                             	        addq	%r9, %r9
 5922                             	        adcq	%r10, %r10
 5923                             	        adcq	%r11, %r11
 5924                             	        adcq	%r12, %r12
 5925                             	        adcq	%r13, %r13
 5926                             	        adcq	%r14, %r14
 5927                             	        adcq	$0x00, %r15
 5928                             	        #  A[0] * A[0]
 5929                             	        movq	(%rsi), %rax
 5930                             	        mulq	%rax
 5931                             	        movq	%rax, %r8
 5932                             	        movq	%rdx, %rcx
 5933                             	        #  A[1] * A[1]
 5934                             	        movq	8(%rsi), %rax
 5935                             	        mulq	%rax
 5936                             	        addq	%rcx, %r9
 5937                             	        adcq	%rax, %r10
 5938                             	        adcq	$0x00, %rdx
 5939                             	        movq	%rdx, %rcx
 5940                             	        #  A[2] * A[2]
 5941                             	        movq	16(%rsi), %rax
 5942                             	        mulq	%rax
 5943                             	        addq	%rcx, %r11
 5944                             	        adcq	%rax, %r12
 5945                             	        adcq	$0x00, %rdx
 5946                             	        movq	%rdx, %rcx
 5947                             	        #  A[3] * A[3]
 5948                             	        movq	24(%rsi), %rax
 5949                             	        mulq	%rax
 5950                             	        addq	%rax, %r14
 5951                             	        adcq	%rdx, %r15
 5952                             	        addq	%rcx, %r13
 5953                             	        adcq	$0x00, %r14
 5954                             	        adcq	$0x00, %r15
 5955                             	        # Reduce
 5956                             	        movq	$0x7fffffffffffffff, %rcx
 5957                             	        #  Move top half into t4-t7 and remove top bit from t3
 5958                             	        shldq	$0x01, %r14, %r15
 5959                             	        shldq	$0x01, %r13, %r14
 5960                             	        shldq	$0x01, %r12, %r13
 5961                             	        shldq	$0x01, %r11, %r12
 5962                             	        andq	%rcx, %r11
 5963                             	        #  Multiply top half by 19
 5964                             	        movq	$19, %rax
 5965                             	        mulq	%r12
 5966                             	        xorq	%r12, %r12
 5967                             	        addq	%rax, %r8
 5968                             	        movq	$19, %rax
 5969                             	        adcq	%rdx, %r12
 5970                             	        mulq	%r13
 5971                             	        xorq	%r13, %r13
 5972                             	        addq	%rax, %r9
 5973                             	        movq	$19, %rax
 5974                             	        adcq	%rdx, %r13
 5975                             	        mulq	%r14
 5976                             	        xorq	%r14, %r14
 5977                             	        addq	%rax, %r10
 5978                             	        movq	$19, %rax
 5979                             	        adcq	%rdx, %r14
 5980                             	        mulq	%r15
 5981                             	        #  Add remaining product results in
 5982                             	        addq	%r12, %r9
 5983                             	        adcq	%r13, %r10
 5984                             	        adcq	%r14, %r11
 5985                             	        adcq	%rax, %r11
 5986                             	        adcq	$0x00, %rdx
 5987                             	        #  Overflow
 5988                             	        shldq	$0x01, %r11, %rdx
 5989                             	        imulq	$19, %rdx, %rax
 5990                             	        andq	%rcx, %r11
 5991                             	        addq	%rax, %r8
 5992                             	        adcq	$0x00, %r9
 5993                             	        adcq	$0x00, %r10
 5994                             	        adcq	$0x00, %r11
 5995                             	        # Reduce if top bit set
 5996                             	        movq	%r11, %rdx
 5997                             	        sarq	$63, %rdx
 5998                             	        andq	$19, %rdx
 5999                             	        andq	%rcx, %r11
 6000                             	        addq	%rdx, %r8
 6001                             	        adcq	$0x00, %r9
 6002                             	        adcq	$0x00, %r10
 6003                             	        adcq	$0x00, %r11
 6004                             	        # Store
 6005                             	        movq	%r8, (%rdi)
 6006                             	        movq	%r9, 8(%rdi)
 6007                             	        movq	%r10, 16(%rdi)
 6008                             	        movq	%r11, 24(%rdi)
 6009                             	        movq	24(%rsp), %rdi
 6010                             	        movq	128(%rsp), %rsi
 6011                             	        # Square * 2
 6012                             	        #  A[0] * A[1]
 6013                             	        movq	(%rsi), %rax
 6014                             	        mulq	8(%rsi)
 6015                             	        movq	%rax, %r9
 6016                             	        movq	%rdx, %r10
 6017                             	        #  A[0] * A[2]
 6018                             	        movq	(%rsi), %rax
 6019                             	        mulq	16(%rsi)
 6020                             	        xorq	%r11, %r11
 6021                             	        addq	%rax, %r10
 6022                             	        adcq	%rdx, %r11
 6023                             	        #  A[0] * A[3]
 6024                             	        movq	(%rsi), %rax
 6025                             	        mulq	24(%rsi)
 6026                             	        xorq	%r12, %r12
 6027                             	        addq	%rax, %r11
 6028                             	        adcq	%rdx, %r12
 6029                             	        #  A[1] * A[2]
 6030                             	        movq	8(%rsi), %rax
 6031                             	        mulq	16(%rsi)
 6032                             	        xorq	%r13, %r13
 6033                             	        addq	%rax, %r11
 6034                             	        adcq	%rdx, %r12
 6035                             	        adcq	$0x00, %r13
 6036                             	        #  A[1] * A[3]
 6037                             	        movq	8(%rsi), %rax
 6038                             	        mulq	24(%rsi)
 6039                             	        addq	%rax, %r12
 6040                             	        adcq	%rdx, %r13
 6041                             	        #  A[2] * A[3]
 6042                             	        movq	16(%rsi), %rax
 6043                             	        mulq	24(%rsi)
 6044                             	        xorq	%r14, %r14
 6045                             	        addq	%rax, %r13
 6046                             	        adcq	%rdx, %r14
 6047                             	        # Double
 6048                             	        xorq	%r15, %r15
 6049                             	        addq	%r9, %r9
 6050                             	        adcq	%r10, %r10
 6051                             	        adcq	%r11, %r11
 6052                             	        adcq	%r12, %r12
 6053                             	        adcq	%r13, %r13
 6054                             	        adcq	%r14, %r14
 6055                             	        adcq	$0x00, %r15
 6056                             	        #  A[0] * A[0]
 6057                             	        movq	(%rsi), %rax
 6058                             	        mulq	%rax
 6059                             	        movq	%rax, %r8
 6060                             	        movq	%rdx, %rcx
 6061                             	        #  A[1] * A[1]
 6062                             	        movq	8(%rsi), %rax
 6063                             	        mulq	%rax
 6064                             	        addq	%rcx, %r9
 6065                             	        adcq	%rax, %r10
 6066                             	        adcq	$0x00, %rdx
 6067                             	        movq	%rdx, %rcx
 6068                             	        #  A[2] * A[2]
 6069                             	        movq	16(%rsi), %rax
 6070                             	        mulq	%rax
 6071                             	        addq	%rcx, %r11
 6072                             	        adcq	%rax, %r12
 6073                             	        adcq	$0x00, %rdx
 6074                             	        movq	%rdx, %rcx
 6075                             	        #  A[3] * A[3]
 6076                             	        movq	24(%rsi), %rax
 6077                             	        mulq	%rax
 6078                             	        addq	%rax, %r14
 6079                             	        adcq	%rdx, %r15
 6080                             	        addq	%rcx, %r13
 6081                             	        adcq	$0x00, %r14
 6082                             	        adcq	$0x00, %r15
 6083                             	        # Reduce
 6084                             	        movq	$0x7fffffffffffffff, %rbx
 6085                             	        xorq	%rax, %rax
 6086                             	        #  Move top half into t4-t7 and remove top bit from t3
 6087                             	        shldq	$3, %r15, %rax
 6088                             	        shldq	$2, %r14, %r15
 6089                             	        shldq	$2, %r13, %r14
 6090                             	        shldq	$2, %r12, %r13
 6091                             	        shldq	$2, %r11, %r12
 6092                             	        shldq	$0x01, %r10, %r11
 6093                             	        shldq	$0x01, %r9, %r10
 6094                             	        shldq	$0x01, %r8, %r9
 6095                             	        shlq	$0x01, %r8
 6096                             	        andq	%rbx, %r11
 6097                             	        #  Two out left, one in right
 6098                             	        andq	%rbx, %r15
 6099                             	        #  Multiply top bits by 19*19
 6100                             	        imulq	$0x169, %rax, %rcx
 6101                             	        #  Multiply top half by 19
 6102                             	        movq	$19, %rax
 6103                             	        mulq	%r12
 6104                             	        xorq	%r12, %r12
 6105                             	        addq	%rax, %r8
 6106                             	        movq	$19, %rax
 6107                             	        adcq	%rdx, %r12
 6108                             	        mulq	%r13
 6109                             	        xorq	%r13, %r13
 6110                             	        addq	%rax, %r9
 6111                             	        movq	$19, %rax
 6112                             	        adcq	%rdx, %r13
 6113                             	        mulq	%r14
 6114                             	        xorq	%r14, %r14
 6115                             	        addq	%rax, %r10
 6116                             	        movq	$19, %rax
 6117                             	        adcq	%rdx, %r14
 6118                             	        mulq	%r15
 6119                             	        #  Add remaining produce results in
 6120                             	        addq	%rcx, %r8
 6121                             	        adcq	%r12, %r9
 6122                             	        adcq	%r13, %r10
 6123                             	        adcq	%r14, %r11
 6124                             	        adcq	%rax, %r11
 6125                             	        adcq	$0x00, %rdx
 6126                             	        #  Overflow
 6127                             	        shldq	$0x01, %r11, %rdx
 6128                             	        imulq	$19, %rdx, %rax
 6129                             	        andq	%rbx, %r11
 6130                             	        addq	%rax, %r8
 6131                             	        adcq	$0x00, %r9
 6132                             	        adcq	$0x00, %r10
 6133                             	        adcq	$0x00, %r11
 6134                             	        # Reduce if top bit set
 6135                             	        movq	%r11, %rdx
 6136                             	        sarq	$63, %rdx
 6137                             	        andq	$19, %rdx
 6138                             	        andq	%rbx, %r11
 6139                             	        addq	%rdx, %r8
 6140                             	        adcq	$0x00, %r9
 6141                             	        adcq	$0x00, %r10
 6142                             	        adcq	$0x00, %r11
 6143                             	        # Store
 6144                             	        movq	%r8, (%rdi)
 6145                             	        movq	%r9, 8(%rdi)
 6146                             	        movq	%r10, 16(%rdi)
 6147                             	        movq	%r11, 24(%rdi)
 6148                             	        movq	8(%rsp), %rdi
 6149                             	        movq	32(%rsp), %rsi
 6150                             	        movq	40(%rsp), %rbx
 6151                             	        # Add
 6152                             	        movq	(%rsi), %r8
 6153                             	        movq	8(%rsi), %r9
 6154                             	        addq	(%rbx), %r8
 6155                             	        movq	16(%rsi), %r10
 6156                             	        adcq	8(%rbx), %r9
 6157                             	        movq	24(%rsi), %rcx
 6158                             	        adcq	16(%rbx), %r10
 6159                             	        movq	$-19, %rax
 6160                             	        adcq	24(%rbx), %rcx
 6161                             	        movq	$0x7fffffffffffffff, %rdx
 6162                             	        movq	%rcx, %r11
 6163                             	        sarq	$63, %rcx
 6164                             	        #   Mask the modulus
 6165                             	        andq	%rcx, %rax
 6166                             	        andq	%rcx, %rdx
 6167                             	        #   Sub modulus (if overflow)
 6168                             	        subq	%rax, %r8
 6169                             	        sbbq	%rcx, %r9
 6170                             	        sbbq	%rcx, %r10
 6171                             	        sbbq	%rdx, %r11
 6172                             	        movq	%r8, (%rdi)
 6173                             	        movq	%r9, 8(%rdi)
 6174                             	        movq	%r10, 16(%rdi)
 6175                             	        movq	%r11, 24(%rdi)
 6176                             	        leaq	48(%rsp), %rdi
 6177                             	        movq	8(%rsp), %rsi
 6178                             	        # Square
 6179                             	        #  A[0] * A[1]
 6180                             	        movq	(%rsi), %rax
 6181                             	        mulq	8(%rsi)
 6182                             	        movq	%rax, %r9
 6183                             	        movq	%rdx, %r10
 6184                             	        #  A[0] * A[2]
 6185                             	        movq	(%rsi), %rax
 6186                             	        mulq	16(%rsi)
 6187                             	        xorq	%r11, %r11
 6188                             	        addq	%rax, %r10
 6189                             	        adcq	%rdx, %r11
 6190                             	        #  A[0] * A[3]
 6191                             	        movq	(%rsi), %rax
 6192                             	        mulq	24(%rsi)
 6193                             	        xorq	%r12, %r12
 6194                             	        addq	%rax, %r11
 6195                             	        adcq	%rdx, %r12
 6196                             	        #  A[1] * A[2]
 6197                             	        movq	8(%rsi), %rax
 6198                             	        mulq	16(%rsi)
 6199                             	        xorq	%r13, %r13
 6200                             	        addq	%rax, %r11
 6201                             	        adcq	%rdx, %r12
 6202                             	        adcq	$0x00, %r13
 6203                             	        #  A[1] * A[3]
 6204                             	        movq	8(%rsi), %rax
 6205                             	        mulq	24(%rsi)
 6206                             	        addq	%rax, %r12
 6207                             	        adcq	%rdx, %r13
 6208                             	        #  A[2] * A[3]
 6209                             	        movq	16(%rsi), %rax
 6210                             	        mulq	24(%rsi)
 6211                             	        xorq	%r14, %r14
 6212                             	        addq	%rax, %r13
 6213                             	        adcq	%rdx, %r14
 6214                             	        # Double
 6215                             	        xorq	%r15, %r15
 6216                             	        addq	%r9, %r9
 6217                             	        adcq	%r10, %r10
 6218                             	        adcq	%r11, %r11
 6219                             	        adcq	%r12, %r12
 6220                             	        adcq	%r13, %r13
 6221                             	        adcq	%r14, %r14
 6222                             	        adcq	$0x00, %r15
 6223                             	        #  A[0] * A[0]
 6224                             	        movq	(%rsi), %rax
 6225                             	        mulq	%rax
 6226                             	        movq	%rax, %r8
 6227                             	        movq	%rdx, %rcx
 6228                             	        #  A[1] * A[1]
 6229                             	        movq	8(%rsi), %rax
 6230                             	        mulq	%rax
 6231                             	        addq	%rcx, %r9
 6232                             	        adcq	%rax, %r10
 6233                             	        adcq	$0x00, %rdx
 6234                             	        movq	%rdx, %rcx
 6235                             	        #  A[2] * A[2]
 6236                             	        movq	16(%rsi), %rax
 6237                             	        mulq	%rax
 6238                             	        addq	%rcx, %r11
 6239                             	        adcq	%rax, %r12
 6240                             	        adcq	$0x00, %rdx
 6241                             	        movq	%rdx, %rcx
 6242                             	        #  A[3] * A[3]
 6243                             	        movq	24(%rsi), %rax
 6244                             	        mulq	%rax
 6245                             	        addq	%rax, %r14
 6246                             	        adcq	%rdx, %r15
 6247                             	        addq	%rcx, %r13
 6248                             	        adcq	$0x00, %r14
 6249                             	        adcq	$0x00, %r15
 6250                             	        # Reduce
 6251                             	        movq	$0x7fffffffffffffff, %rcx
 6252                             	        #  Move top half into t4-t7 and remove top bit from t3
 6253                             	        shldq	$0x01, %r14, %r15
 6254                             	        shldq	$0x01, %r13, %r14
 6255                             	        shldq	$0x01, %r12, %r13
 6256                             	        shldq	$0x01, %r11, %r12
 6257                             	        andq	%rcx, %r11
 6258                             	        #  Multiply top half by 19
 6259                             	        movq	$19, %rax
 6260                             	        mulq	%r12
 6261                             	        xorq	%r12, %r12
 6262                             	        addq	%rax, %r8
 6263                             	        movq	$19, %rax
 6264                             	        adcq	%rdx, %r12
 6265                             	        mulq	%r13
 6266                             	        xorq	%r13, %r13
 6267                             	        addq	%rax, %r9
 6268                             	        movq	$19, %rax
 6269                             	        adcq	%rdx, %r13
 6270                             	        mulq	%r14
 6271                             	        xorq	%r14, %r14
 6272                             	        addq	%rax, %r10
 6273                             	        movq	$19, %rax
 6274                             	        adcq	%rdx, %r14
 6275                             	        mulq	%r15
 6276                             	        #  Add remaining product results in
 6277                             	        addq	%r12, %r9
 6278                             	        adcq	%r13, %r10
 6279                             	        adcq	%r14, %r11
 6280                             	        adcq	%rax, %r11
 6281                             	        adcq	$0x00, %rdx
 6282                             	        #  Overflow
 6283                             	        shldq	$0x01, %r11, %rdx
 6284                             	        imulq	$19, %rdx, %rax
 6285                             	        andq	%rcx, %r11
 6286                             	        addq	%rax, %r8
 6287                             	        adcq	$0x00, %r9
 6288                             	        adcq	$0x00, %r10
 6289                             	        adcq	$0x00, %r11
 6290                             	        # Reduce if top bit set
 6291                             	        movq	%r11, %rdx
 6292                             	        sarq	$63, %rdx
 6293                             	        andq	$19, %rdx
 6294                             	        andq	%rcx, %r11
 6295                             	        addq	%rdx, %r8
 6296                             	        adcq	$0x00, %r9
 6297                             	        adcq	$0x00, %r10
 6298                             	        adcq	$0x00, %r11
 6299                             	        # Store
 6300                             	        movq	%r8, (%rdi)
 6301                             	        movq	%r9, 8(%rdi)
 6302                             	        movq	%r10, 16(%rdi)
 6303                             	        movq	%r11, 24(%rdi)
 6304                             	        movq	8(%rsp), %rdi
 6305                             	        movq	16(%rsp), %rsi
 6306                             	        movq	(%rsp), %rbx
 6307                             	        # Add
 6308                             	        movq	(%rsi), %r8
 6309                             	        movq	8(%rsi), %r9
 6310                             	        addq	(%rbx), %r8
 6311                             	        movq	16(%rsi), %r10
 6312                             	        adcq	8(%rbx), %r9
 6313                             	        movq	24(%rsi), %rcx
 6314                             	        adcq	16(%rbx), %r10
 6315                             	        movq	$-19, %rax
 6316                             	        adcq	24(%rbx), %rcx
 6317                             	        movq	$0x7fffffffffffffff, %rdx
 6318                             	        movq	%rcx, %r11
 6319                             	        sarq	$63, %rcx
 6320                             	        #   Mask the modulus
 6321                             	        andq	%rcx, %rax
 6322                             	        andq	%rcx, %rdx
 6323                             	        #   Sub modulus (if overflow)
 6324                             	        subq	%rax, %r8
 6325                             	        sbbq	%rcx, %r9
 6326                             	        sbbq	%rcx, %r10
 6327                             	        sbbq	%rdx, %r11
 6328                             	        movq	%r8, (%rdi)
 6329                             	        movq	%r9, 8(%rdi)
 6330                             	        movq	%r10, 16(%rdi)
 6331                             	        movq	%r11, 24(%rdi)
 6332                             	        movq	16(%rsp), %rdi
 6333                             	        movq	16(%rsp), %rsi
 6334                             	        movq	(%rsp), %rbx
 6335                             	        # Sub
 6336                             	        movq	(%rsi), %r8
 6337                             	        movq	8(%rsi), %r9
 6338                             	        movq	16(%rsi), %r10
 6339                             	        movq	24(%rsi), %r11
 6340                             	        subq	(%rbx), %r8
 6341                             	        movq	$0x00, %rcx
 6342                             	        sbbq	8(%rbx), %r9
 6343                             	        movq	$-19, %rax
 6344                             	        sbbq	16(%rbx), %r10
 6345                             	        movq	$0x7fffffffffffffff, %rdx
 6346                             	        sbbq	24(%rbx), %r11
 6347                             	        sbbq	$0x00, %rcx
 6348                             	        #   Mask the modulus
 6349                             	        andq	%rcx, %rax
 6350                             	        andq	%rcx, %rdx
 6351                             	        #   Add modulus (if underflow)
 6352                             	        addq	%rax, %r8
 6353                             	        adcq	%rcx, %r9
 6354                             	        adcq	%rcx, %r10
 6355                             	        adcq	%rdx, %r11
 6356                             	        movq	%r8, (%rdi)
 6357                             	        movq	%r9, 8(%rdi)
 6358                             	        movq	%r10, 16(%rdi)
 6359                             	        movq	%r11, 24(%rdi)
 6360                             	        movq	(%rsp), %rdi
 6361                             	        leaq	48(%rsp), %rsi
 6362                             	        movq	8(%rsp), %rbx
 6363                             	        # Sub
 6364                             	        movq	(%rsi), %r8
 6365                             	        movq	8(%rsi), %r9
 6366                             	        movq	16(%rsi), %r10
 6367                             	        movq	24(%rsi), %r11
 6368                             	        subq	(%rbx), %r8
 6369                             	        movq	$0x00, %rcx
 6370                             	        sbbq	8(%rbx), %r9
 6371                             	        movq	$-19, %rax
 6372                             	        sbbq	16(%rbx), %r10
 6373                             	        movq	$0x7fffffffffffffff, %rdx
 6374                             	        sbbq	24(%rbx), %r11
 6375                             	        sbbq	$0x00, %rcx
 6376                             	        #   Mask the modulus
 6377                             	        andq	%rcx, %rax
 6378                             	        andq	%rcx, %rdx
 6379                             	        #   Add modulus (if underflow)
 6380                             	        addq	%rax, %r8
 6381                             	        adcq	%rcx, %r9
 6382                             	        adcq	%rcx, %r10
 6383                             	        adcq	%rdx, %r11
 6384                             	        movq	%r8, (%rdi)
 6385                             	        movq	%r9, 8(%rdi)
 6386                             	        movq	%r10, 16(%rdi)
 6387                             	        movq	%r11, 24(%rdi)
 6388                             	        movq	24(%rsp), %rdi
 6389                             	        movq	24(%rsp), %rsi
 6390                             	        movq	16(%rsp), %rbx
 6391                             	        # Sub
 6392                             	        movq	(%rsi), %r8
 6393                             	        movq	8(%rsi), %r9
 6394                             	        movq	16(%rsi), %r10
 6395                             	        movq	24(%rsi), %r11
 6396                             	        subq	(%rbx), %r8
 6397                             	        movq	$0x00, %rcx
 6398                             	        sbbq	8(%rbx), %r9
 6399                             	        movq	$-19, %rax
 6400                             	        sbbq	16(%rbx), %r10
 6401                             	        movq	$0x7fffffffffffffff, %rdx
 6402                             	        sbbq	24(%rbx), %r11
 6403                             	        sbbq	$0x00, %rcx
 6404                             	        #   Mask the modulus
 6405                             	        andq	%rcx, %rax
 6406                             	        andq	%rcx, %rdx
 6407                             	        #   Add modulus (if underflow)
 6408                             	        addq	%rax, %r8
 6409                             	        adcq	%rcx, %r9
 6410                             	        adcq	%rcx, %r10
 6411                             	        adcq	%rdx, %r11
 6412                             	        movq	%r8, (%rdi)
 6413                             	        movq	%r9, 8(%rdi)
 6414                             	        movq	%r10, 16(%rdi)
 6415                             	        movq	%r11, 24(%rdi)
 6416                             	        addq	$0x50, %rsp
 6417                             	        popq	%r15
 6418                             	        popq	%r14
 6419                             	        popq	%r13
 6420                             	        popq	%r12
 6421                             	        popq	%rbx
 6422                             	        repz retq
 6423                             	#ifndef __APPLE__
 6425                             	#endif /* __APPLE__ */
 6426                             	#ifndef __APPLE__
 6427                             	.text
 6428                             	.globl	fe_ge_madd_x64
 6430                             	.align	16
 6431                             	fe_ge_madd_x64:
 6432                             	#else
 6433                             	.section	__TEXT,__text
 6434                             	.globl	_fe_ge_madd_x64
 6435                             	.p2align	4
 6436                             	_fe_ge_madd_x64:
 6437                             	#endif /* __APPLE__ */
 6438                             	        pushq	%rbx
 6439                             	        pushq	%r12
 6440                             	        pushq	%r13
 6441                             	        pushq	%r14
 6442                             	        pushq	%r15
 6443                             	        subq	$0x50, %rsp
 6444                             	        movq	%rdi, (%rsp)
 6445                             	        movq	%rsi, 8(%rsp)
 6446                             	        movq	%rdx, 16(%rsp)
 6447                             	        movq	%rcx, 24(%rsp)
 6448                             	        movq	%r8, 32(%rsp)
 6449                             	        movq	%r9, 40(%rsp)
 6450                             	        movq	(%rsp), %rdi
 6451                             	        movq	40(%rsp), %rsi
 6452                             	        movq	32(%rsp), %rbx
 6453                             	        # Add
 6454                             	        movq	(%rsi), %r8
 6455                             	        movq	8(%rsi), %r9
 6456                             	        addq	(%rbx), %r8
 6457                             	        movq	16(%rsi), %r10
 6458                             	        adcq	8(%rbx), %r9
 6459                             	        movq	24(%rsi), %rcx
 6460                             	        adcq	16(%rbx), %r10
 6461                             	        movq	$-19, %rax
 6462                             	        adcq	24(%rbx), %rcx
 6463                             	        movq	$0x7fffffffffffffff, %rdx
 6464                             	        movq	%rcx, %r11
 6465                             	        sarq	$63, %rcx
 6466                             	        #   Mask the modulus
 6467                             	        andq	%rcx, %rax
 6468                             	        andq	%rcx, %rdx
 6469                             	        #   Sub modulus (if overflow)
 6470                             	        subq	%rax, %r8
 6471                             	        sbbq	%rcx, %r9
 6472                             	        sbbq	%rcx, %r10
 6473                             	        sbbq	%rdx, %r11
 6474                             	        movq	%r8, (%rdi)
 6475                             	        movq	%r9, 8(%rdi)
 6476                             	        movq	%r10, 16(%rdi)
 6477                             	        movq	%r11, 24(%rdi)
 6478                             	        movq	8(%rsp), %rdi
 6479                             	        movq	40(%rsp), %rsi
 6480                             	        movq	32(%rsp), %rbx
 6481                             	        # Sub
 6482                             	        movq	(%rsi), %r8
 6483                             	        movq	8(%rsi), %r9
 6484                             	        movq	16(%rsi), %r10
 6485                             	        movq	24(%rsi), %r11
 6486                             	        subq	(%rbx), %r8
 6487                             	        movq	$0x00, %rcx
 6488                             	        sbbq	8(%rbx), %r9
 6489                             	        movq	$-19, %rax
 6490                             	        sbbq	16(%rbx), %r10
 6491                             	        movq	$0x7fffffffffffffff, %rdx
 6492                             	        sbbq	24(%rbx), %r11
 6493                             	        sbbq	$0x00, %rcx
 6494                             	        #   Mask the modulus
 6495                             	        andq	%rcx, %rax
 6496                             	        andq	%rcx, %rdx
 6497                             	        #   Add modulus (if underflow)
 6498                             	        addq	%rax, %r8
 6499                             	        adcq	%rcx, %r9
 6500                             	        adcq	%rcx, %r10
 6501                             	        adcq	%rdx, %r11
 6502                             	        movq	%r8, (%rdi)
 6503                             	        movq	%r9, 8(%rdi)
 6504                             	        movq	%r10, 16(%rdi)
 6505                             	        movq	%r11, 24(%rdi)
 6506                             	        movq	16(%rsp), %rdi
 6507                             	        movq	(%rsp), %rsi
 6508                             	        movq	152(%rsp), %rbx
 6509                             	        # Multiply
 6510                             	        #  A[0] * B[0]
 6511                             	        movq	(%rbx), %rax
 6512                             	        mulq	(%rsi)
 6513                             	        movq	%rax, %r8
 6514                             	        movq	%rdx, %r9
 6515                             	        #  A[0] * B[1]
 6516                             	        movq	8(%rbx), %rax
 6517                             	        mulq	(%rsi)
 6518                             	        xorq	%r10, %r10
 6519                             	        addq	%rax, %r9
 6520                             	        adcq	%rdx, %r10
 6521                             	        #  A[1] * B[0]
 6522                             	        movq	(%rbx), %rax
 6523                             	        mulq	8(%rsi)
 6524                             	        xorq	%r11, %r11
 6525                             	        addq	%rax, %r9
 6526                             	        adcq	%rdx, %r10
 6527                             	        adcq	$0x00, %r11
 6528                             	        #  A[0] * B[2]
 6529                             	        movq	16(%rbx), %rax
 6530                             	        mulq	(%rsi)
 6531                             	        addq	%rax, %r10
 6532                             	        adcq	%rdx, %r11
 6533                             	        #  A[1] * B[1]
 6534                             	        movq	8(%rbx), %rax
 6535                             	        mulq	8(%rsi)
 6536                             	        xorq	%r12, %r12
 6537                             	        addq	%rax, %r10
 6538                             	        adcq	%rdx, %r11
 6539                             	        adcq	$0x00, %r12
 6540                             	        #  A[2] * B[0]
 6541                             	        movq	(%rbx), %rax
 6542                             	        mulq	16(%rsi)
 6543                             	        addq	%rax, %r10
 6544                             	        adcq	%rdx, %r11
 6545                             	        adcq	$0x00, %r12
 6546                             	        #  A[0] * B[3]
 6547                             	        movq	24(%rbx), %rax
 6548                             	        mulq	(%rsi)
 6549                             	        xorq	%r13, %r13
 6550                             	        addq	%rax, %r11
 6551                             	        adcq	%rdx, %r12
 6552                             	        adcq	$0x00, %r13
 6553                             	        #  A[1] * B[2]
 6554                             	        movq	16(%rbx), %rax
 6555                             	        mulq	8(%rsi)
 6556                             	        addq	%rax, %r11
 6557                             	        adcq	%rdx, %r12
 6558                             	        adcq	$0x00, %r13
 6559                             	        #  A[2] * B[1]
 6560                             	        movq	8(%rbx), %rax
 6561                             	        mulq	16(%rsi)
 6562                             	        addq	%rax, %r11
 6563                             	        adcq	%rdx, %r12
 6564                             	        adcq	$0x00, %r13
 6565                             	        #  A[3] * B[0]
 6566                             	        movq	(%rbx), %rax
 6567                             	        mulq	24(%rsi)
 6568                             	        addq	%rax, %r11
 6569                             	        adcq	%rdx, %r12
 6570                             	        adcq	$0x00, %r13
 6571                             	        #  A[1] * B[3]
 6572                             	        movq	24(%rbx), %rax
 6573                             	        mulq	8(%rsi)
 6574                             	        xorq	%r14, %r14
 6575                             	        addq	%rax, %r12
 6576                             	        adcq	%rdx, %r13
 6577                             	        adcq	$0x00, %r14
 6578                             	        #  A[2] * B[2]
 6579                             	        movq	16(%rbx), %rax
 6580                             	        mulq	16(%rsi)
 6581                             	        addq	%rax, %r12
 6582                             	        adcq	%rdx, %r13
 6583                             	        adcq	$0x00, %r14
 6584                             	        #  A[3] * B[1]
 6585                             	        movq	8(%rbx), %rax
 6586                             	        mulq	24(%rsi)
 6587                             	        addq	%rax, %r12
 6588                             	        adcq	%rdx, %r13
 6589                             	        adcq	$0x00, %r14
 6590                             	        #  A[2] * B[3]
 6591                             	        movq	24(%rbx), %rax
 6592                             	        mulq	16(%rsi)
 6593                             	        xorq	%r15, %r15
 6594                             	        addq	%rax, %r13
 6595                             	        adcq	%rdx, %r14
 6596                             	        adcq	$0x00, %r15
 6597                             	        #  A[3] * B[2]
 6598                             	        movq	16(%rbx), %rax
 6599                             	        mulq	24(%rsi)
 6600                             	        addq	%rax, %r13
 6601                             	        adcq	%rdx, %r14
 6602                             	        adcq	$0x00, %r15
 6603                             	        #  A[3] * B[3]
 6604                             	        movq	24(%rbx), %rax
 6605                             	        mulq	24(%rsi)
 6606                             	        addq	%rax, %r14
 6607                             	        adcq	%rdx, %r15
 6608                             	        # Reduce
 6609                             	        movq	$0x7fffffffffffffff, %rcx
 6610                             	        #  Move top half into t4-t7 and remove top bit from t3
 6611                             	        shldq	$0x01, %r14, %r15
 6612                             	        shldq	$0x01, %r13, %r14
 6613                             	        shldq	$0x01, %r12, %r13
 6614                             	        shldq	$0x01, %r11, %r12
 6615                             	        andq	%rcx, %r11
 6616                             	        #  Multiply top half by 19
 6617                             	        movq	$19, %rax
 6618                             	        mulq	%r12
 6619                             	        xorq	%r12, %r12
 6620                             	        addq	%rax, %r8
 6621                             	        movq	$19, %rax
 6622                             	        adcq	%rdx, %r12
 6623                             	        mulq	%r13
 6624                             	        xorq	%r13, %r13
 6625                             	        addq	%rax, %r9
 6626                             	        movq	$19, %rax
 6627                             	        adcq	%rdx, %r13
 6628                             	        mulq	%r14
 6629                             	        xorq	%r14, %r14
 6630                             	        addq	%rax, %r10
 6631                             	        movq	$19, %rax
 6632                             	        adcq	%rdx, %r14
 6633                             	        mulq	%r15
 6634                             	        #  Add remaining product results in
 6635                             	        addq	%r12, %r9
 6636                             	        adcq	%r13, %r10
 6637                             	        adcq	%r14, %r11
 6638                             	        adcq	%rax, %r11
 6639                             	        adcq	$0x00, %rdx
 6640                             	        #  Overflow
 6641                             	        shldq	$0x01, %r11, %rdx
 6642                             	        imulq	$19, %rdx, %rax
 6643                             	        andq	%rcx, %r11
 6644                             	        addq	%rax, %r8
 6645                             	        adcq	$0x00, %r9
 6646                             	        adcq	$0x00, %r10
 6647                             	        adcq	$0x00, %r11
 6648                             	        # Reduce if top bit set
 6649                             	        movq	%r11, %rdx
 6650                             	        sarq	$63, %rdx
 6651                             	        andq	$19, %rdx
 6652                             	        andq	%rcx, %r11
 6653                             	        addq	%rdx, %r8
 6654                             	        adcq	$0x00, %r9
 6655                             	        adcq	$0x00, %r10
 6656                             	        adcq	$0x00, %r11
 6657                             	        # Store
 6658                             	        movq	%r8, (%rdi)
 6659                             	        movq	%r9, 8(%rdi)
 6660                             	        movq	%r10, 16(%rdi)
 6661                             	        movq	%r11, 24(%rdi)
 6662                             	        movq	8(%rsp), %rdi
 6663                             	        movq	8(%rsp), %rsi
 6664                             	        movq	160(%rsp), %rbx
 6665                             	        # Multiply
 6666                             	        #  A[0] * B[0]
 6667                             	        movq	(%rbx), %rax
 6668                             	        mulq	(%rsi)
 6669                             	        movq	%rax, %r8
 6670                             	        movq	%rdx, %r9
 6671                             	        #  A[0] * B[1]
 6672                             	        movq	8(%rbx), %rax
 6673                             	        mulq	(%rsi)
 6674                             	        xorq	%r10, %r10
 6675                             	        addq	%rax, %r9
 6676                             	        adcq	%rdx, %r10
 6677                             	        #  A[1] * B[0]
 6678                             	        movq	(%rbx), %rax
 6679                             	        mulq	8(%rsi)
 6680                             	        xorq	%r11, %r11
 6681                             	        addq	%rax, %r9
 6682                             	        adcq	%rdx, %r10
 6683                             	        adcq	$0x00, %r11
 6684                             	        #  A[0] * B[2]
 6685                             	        movq	16(%rbx), %rax
 6686                             	        mulq	(%rsi)
 6687                             	        addq	%rax, %r10
 6688                             	        adcq	%rdx, %r11
 6689                             	        #  A[1] * B[1]
 6690                             	        movq	8(%rbx), %rax
 6691                             	        mulq	8(%rsi)
 6692                             	        xorq	%r12, %r12
 6693                             	        addq	%rax, %r10
 6694                             	        adcq	%rdx, %r11
 6695                             	        adcq	$0x00, %r12
 6696                             	        #  A[2] * B[0]
 6697                             	        movq	(%rbx), %rax
 6698                             	        mulq	16(%rsi)
 6699                             	        addq	%rax, %r10
 6700                             	        adcq	%rdx, %r11
 6701                             	        adcq	$0x00, %r12
 6702                             	        #  A[0] * B[3]
 6703                             	        movq	24(%rbx), %rax
 6704                             	        mulq	(%rsi)
 6705                             	        xorq	%r13, %r13
 6706                             	        addq	%rax, %r11
 6707                             	        adcq	%rdx, %r12
 6708                             	        adcq	$0x00, %r13
 6709                             	        #  A[1] * B[2]
 6710                             	        movq	16(%rbx), %rax
 6711                             	        mulq	8(%rsi)
 6712                             	        addq	%rax, %r11
 6713                             	        adcq	%rdx, %r12
 6714                             	        adcq	$0x00, %r13
 6715                             	        #  A[2] * B[1]
 6716                             	        movq	8(%rbx), %rax
 6717                             	        mulq	16(%rsi)
 6718                             	        addq	%rax, %r11
 6719                             	        adcq	%rdx, %r12
 6720                             	        adcq	$0x00, %r13
 6721                             	        #  A[3] * B[0]
 6722                             	        movq	(%rbx), %rax
 6723                             	        mulq	24(%rsi)
 6724                             	        addq	%rax, %r11
 6725                             	        adcq	%rdx, %r12
 6726                             	        adcq	$0x00, %r13
 6727                             	        #  A[1] * B[3]
 6728                             	        movq	24(%rbx), %rax
 6729                             	        mulq	8(%rsi)
 6730                             	        xorq	%r14, %r14
 6731                             	        addq	%rax, %r12
 6732                             	        adcq	%rdx, %r13
 6733                             	        adcq	$0x00, %r14
 6734                             	        #  A[2] * B[2]
 6735                             	        movq	16(%rbx), %rax
 6736                             	        mulq	16(%rsi)
 6737                             	        addq	%rax, %r12
 6738                             	        adcq	%rdx, %r13
 6739                             	        adcq	$0x00, %r14
 6740                             	        #  A[3] * B[1]
 6741                             	        movq	8(%rbx), %rax
 6742                             	        mulq	24(%rsi)
 6743                             	        addq	%rax, %r12
 6744                             	        adcq	%rdx, %r13
 6745                             	        adcq	$0x00, %r14
 6746                             	        #  A[2] * B[3]
 6747                             	        movq	24(%rbx), %rax
 6748                             	        mulq	16(%rsi)
 6749                             	        xorq	%r15, %r15
 6750                             	        addq	%rax, %r13
 6751                             	        adcq	%rdx, %r14
 6752                             	        adcq	$0x00, %r15
 6753                             	        #  A[3] * B[2]
 6754                             	        movq	16(%rbx), %rax
 6755                             	        mulq	24(%rsi)
 6756                             	        addq	%rax, %r13
 6757                             	        adcq	%rdx, %r14
 6758                             	        adcq	$0x00, %r15
 6759                             	        #  A[3] * B[3]
 6760                             	        movq	24(%rbx), %rax
 6761                             	        mulq	24(%rsi)
 6762                             	        addq	%rax, %r14
 6763                             	        adcq	%rdx, %r15
 6764                             	        # Reduce
 6765                             	        movq	$0x7fffffffffffffff, %rcx
 6766                             	        #  Move top half into t4-t7 and remove top bit from t3
 6767                             	        shldq	$0x01, %r14, %r15
 6768                             	        shldq	$0x01, %r13, %r14
 6769                             	        shldq	$0x01, %r12, %r13
 6770                             	        shldq	$0x01, %r11, %r12
 6771                             	        andq	%rcx, %r11
 6772                             	        #  Multiply top half by 19
 6773                             	        movq	$19, %rax
 6774                             	        mulq	%r12
 6775                             	        xorq	%r12, %r12
 6776                             	        addq	%rax, %r8
 6777                             	        movq	$19, %rax
 6778                             	        adcq	%rdx, %r12
 6779                             	        mulq	%r13
 6780                             	        xorq	%r13, %r13
 6781                             	        addq	%rax, %r9
 6782                             	        movq	$19, %rax
 6783                             	        adcq	%rdx, %r13
 6784                             	        mulq	%r14
 6785                             	        xorq	%r14, %r14
 6786                             	        addq	%rax, %r10
 6787                             	        movq	$19, %rax
 6788                             	        adcq	%rdx, %r14
 6789                             	        mulq	%r15
 6790                             	        #  Add remaining product results in
 6791                             	        addq	%r12, %r9
 6792                             	        adcq	%r13, %r10
 6793                             	        adcq	%r14, %r11
 6794                             	        adcq	%rax, %r11
 6795                             	        adcq	$0x00, %rdx
 6796                             	        #  Overflow
 6797                             	        shldq	$0x01, %r11, %rdx
 6798                             	        imulq	$19, %rdx, %rax
 6799                             	        andq	%rcx, %r11
 6800                             	        addq	%rax, %r8
 6801                             	        adcq	$0x00, %r9
 6802                             	        adcq	$0x00, %r10
 6803                             	        adcq	$0x00, %r11
 6804                             	        # Reduce if top bit set
 6805                             	        movq	%r11, %rdx
 6806                             	        sarq	$63, %rdx
 6807                             	        andq	$19, %rdx
 6808                             	        andq	%rcx, %r11
 6809                             	        addq	%rdx, %r8
 6810                             	        adcq	$0x00, %r9
 6811                             	        adcq	$0x00, %r10
 6812                             	        adcq	$0x00, %r11
 6813                             	        # Store
 6814                             	        movq	%r8, (%rdi)
 6815                             	        movq	%r9, 8(%rdi)
 6816                             	        movq	%r10, 16(%rdi)
 6817                             	        movq	%r11, 24(%rdi)
 6818                             	        movq	24(%rsp), %rdi
 6819                             	        movq	144(%rsp), %rsi
 6820                             	        movq	136(%rsp), %rbx
 6821                             	        # Multiply
 6822                             	        #  A[0] * B[0]
 6823                             	        movq	(%rbx), %rax
 6824                             	        mulq	(%rsi)
 6825                             	        movq	%rax, %r8
 6826                             	        movq	%rdx, %r9
 6827                             	        #  A[0] * B[1]
 6828                             	        movq	8(%rbx), %rax
 6829                             	        mulq	(%rsi)
 6830                             	        xorq	%r10, %r10
 6831                             	        addq	%rax, %r9
 6832                             	        adcq	%rdx, %r10
 6833                             	        #  A[1] * B[0]
 6834                             	        movq	(%rbx), %rax
 6835                             	        mulq	8(%rsi)
 6836                             	        xorq	%r11, %r11
 6837                             	        addq	%rax, %r9
 6838                             	        adcq	%rdx, %r10
 6839                             	        adcq	$0x00, %r11
 6840                             	        #  A[0] * B[2]
 6841                             	        movq	16(%rbx), %rax
 6842                             	        mulq	(%rsi)
 6843                             	        addq	%rax, %r10
 6844                             	        adcq	%rdx, %r11
 6845                             	        #  A[1] * B[1]
 6846                             	        movq	8(%rbx), %rax
 6847                             	        mulq	8(%rsi)
 6848                             	        xorq	%r12, %r12
 6849                             	        addq	%rax, %r10
 6850                             	        adcq	%rdx, %r11
 6851                             	        adcq	$0x00, %r12
 6852                             	        #  A[2] * B[0]
 6853                             	        movq	(%rbx), %rax
 6854                             	        mulq	16(%rsi)
 6855                             	        addq	%rax, %r10
 6856                             	        adcq	%rdx, %r11
 6857                             	        adcq	$0x00, %r12
 6858                             	        #  A[0] * B[3]
 6859                             	        movq	24(%rbx), %rax
 6860                             	        mulq	(%rsi)
 6861                             	        xorq	%r13, %r13
 6862                             	        addq	%rax, %r11
 6863                             	        adcq	%rdx, %r12
 6864                             	        adcq	$0x00, %r13
 6865                             	        #  A[1] * B[2]
 6866                             	        movq	16(%rbx), %rax
 6867                             	        mulq	8(%rsi)
 6868                             	        addq	%rax, %r11
 6869                             	        adcq	%rdx, %r12
 6870                             	        adcq	$0x00, %r13
 6871                             	        #  A[2] * B[1]
 6872                             	        movq	8(%rbx), %rax
 6873                             	        mulq	16(%rsi)
 6874                             	        addq	%rax, %r11
 6875                             	        adcq	%rdx, %r12
 6876                             	        adcq	$0x00, %r13
 6877                             	        #  A[3] * B[0]
 6878                             	        movq	(%rbx), %rax
 6879                             	        mulq	24(%rsi)
 6880                             	        addq	%rax, %r11
 6881                             	        adcq	%rdx, %r12
 6882                             	        adcq	$0x00, %r13
 6883                             	        #  A[1] * B[3]
 6884                             	        movq	24(%rbx), %rax
 6885                             	        mulq	8(%rsi)
 6886                             	        xorq	%r14, %r14
 6887                             	        addq	%rax, %r12
 6888                             	        adcq	%rdx, %r13
 6889                             	        adcq	$0x00, %r14
 6890                             	        #  A[2] * B[2]
 6891                             	        movq	16(%rbx), %rax
 6892                             	        mulq	16(%rsi)
 6893                             	        addq	%rax, %r12
 6894                             	        adcq	%rdx, %r13
 6895                             	        adcq	$0x00, %r14
 6896                             	        #  A[3] * B[1]
 6897                             	        movq	8(%rbx), %rax
 6898                             	        mulq	24(%rsi)
 6899                             	        addq	%rax, %r12
 6900                             	        adcq	%rdx, %r13
 6901                             	        adcq	$0x00, %r14
 6902                             	        #  A[2] * B[3]
 6903                             	        movq	24(%rbx), %rax
 6904                             	        mulq	16(%rsi)
 6905                             	        xorq	%r15, %r15
 6906                             	        addq	%rax, %r13
 6907                             	        adcq	%rdx, %r14
 6908                             	        adcq	$0x00, %r15
 6909                             	        #  A[3] * B[2]
 6910                             	        movq	16(%rbx), %rax
 6911                             	        mulq	24(%rsi)
 6912                             	        addq	%rax, %r13
 6913                             	        adcq	%rdx, %r14
 6914                             	        adcq	$0x00, %r15
 6915                             	        #  A[3] * B[3]
 6916                             	        movq	24(%rbx), %rax
 6917                             	        mulq	24(%rsi)
 6918                             	        addq	%rax, %r14
 6919                             	        adcq	%rdx, %r15
 6920                             	        # Reduce
 6921                             	        movq	$0x7fffffffffffffff, %rcx
 6922                             	        #  Move top half into t4-t7 and remove top bit from t3
 6923                             	        shldq	$0x01, %r14, %r15
 6924                             	        shldq	$0x01, %r13, %r14
 6925                             	        shldq	$0x01, %r12, %r13
 6926                             	        shldq	$0x01, %r11, %r12
 6927                             	        andq	%rcx, %r11
 6928                             	        #  Multiply top half by 19
 6929                             	        movq	$19, %rax
 6930                             	        mulq	%r12
 6931                             	        xorq	%r12, %r12
 6932                             	        addq	%rax, %r8
 6933                             	        movq	$19, %rax
 6934                             	        adcq	%rdx, %r12
 6935                             	        mulq	%r13
 6936                             	        xorq	%r13, %r13
 6937                             	        addq	%rax, %r9
 6938                             	        movq	$19, %rax
 6939                             	        adcq	%rdx, %r13
 6940                             	        mulq	%r14
 6941                             	        xorq	%r14, %r14
 6942                             	        addq	%rax, %r10
 6943                             	        movq	$19, %rax
 6944                             	        adcq	%rdx, %r14
 6945                             	        mulq	%r15
 6946                             	        #  Add remaining product results in
 6947                             	        addq	%r12, %r9
 6948                             	        adcq	%r13, %r10
 6949                             	        adcq	%r14, %r11
 6950                             	        adcq	%rax, %r11
 6951                             	        adcq	$0x00, %rdx
 6952                             	        #  Overflow
 6953                             	        shldq	$0x01, %r11, %rdx
 6954                             	        imulq	$19, %rdx, %rax
 6955                             	        andq	%rcx, %r11
 6956                             	        addq	%rax, %r8
 6957                             	        adcq	$0x00, %r9
 6958                             	        adcq	$0x00, %r10
 6959                             	        adcq	$0x00, %r11
 6960                             	        # Reduce if top bit set
 6961                             	        movq	%r11, %rdx
 6962                             	        sarq	$63, %rdx
 6963                             	        andq	$19, %rdx
 6964                             	        andq	%rcx, %r11
 6965                             	        addq	%rdx, %r8
 6966                             	        adcq	$0x00, %r9
 6967                             	        adcq	$0x00, %r10
 6968                             	        adcq	$0x00, %r11
 6969                             	        # Store
 6970                             	        movq	%r8, (%rdi)
 6971                             	        movq	%r9, 8(%rdi)
 6972                             	        movq	%r10, 16(%rdi)
 6973                             	        movq	%r11, 24(%rdi)
 6974                             	        leaq	48(%rsp), %rdi
 6975                             	        movq	128(%rsp), %rsi
 6976                             	        movq	128(%rsp), %rbx
 6977                             	        # Add
 6978                             	        movq	(%rsi), %r8
 6979                             	        movq	8(%rsi), %r9
 6980                             	        addq	(%rbx), %r8
 6981                             	        movq	16(%rsi), %r10
 6982                             	        adcq	8(%rbx), %r9
 6983                             	        movq	24(%rsi), %rcx
 6984                             	        adcq	16(%rbx), %r10
 6985                             	        movq	$-19, %rax
 6986                             	        adcq	24(%rbx), %rcx
 6987                             	        movq	$0x7fffffffffffffff, %rdx
 6988                             	        movq	%rcx, %r11
 6989                             	        sarq	$63, %rcx
 6990                             	        #   Mask the modulus
 6991                             	        andq	%rcx, %rax
 6992                             	        andq	%rcx, %rdx
 6993                             	        #   Sub modulus (if overflow)
 6994                             	        subq	%rax, %r8
 6995                             	        sbbq	%rcx, %r9
 6996                             	        sbbq	%rcx, %r10
 6997                             	        sbbq	%rdx, %r11
 6998                             	        movq	%r8, (%rdi)
 6999                             	        movq	%r9, 8(%rdi)
 7000                             	        movq	%r10, 16(%rdi)
 7001                             	        movq	%r11, 24(%rdi)
 7002                             	        movq	(%rsp), %rdi
 7003                             	        movq	16(%rsp), %rsi
 7004                             	        movq	8(%rsp), %rbx
 7005                             	        # Sub
 7006                             	        movq	(%rsi), %r8
 7007                             	        movq	8(%rsi), %r9
 7008                             	        movq	16(%rsi), %r10
 7009                             	        movq	24(%rsi), %r11
 7010                             	        subq	(%rbx), %r8
 7011                             	        movq	$0x00, %rcx
 7012                             	        sbbq	8(%rbx), %r9
 7013                             	        movq	$-19, %rax
 7014                             	        sbbq	16(%rbx), %r10
 7015                             	        movq	$0x7fffffffffffffff, %rdx
 7016                             	        sbbq	24(%rbx), %r11
 7017                             	        sbbq	$0x00, %rcx
 7018                             	        #   Mask the modulus
 7019                             	        andq	%rcx, %rax
 7020                             	        andq	%rcx, %rdx
 7021                             	        #   Add modulus (if underflow)
 7022                             	        addq	%rax, %r8
 7023                             	        adcq	%rcx, %r9
 7024                             	        adcq	%rcx, %r10
 7025                             	        adcq	%rdx, %r11
 7026                             	        movq	%r8, (%rdi)
 7027                             	        movq	%r9, 8(%rdi)
 7028                             	        movq	%r10, 16(%rdi)
 7029                             	        movq	%r11, 24(%rdi)
 7030                             	        movq	8(%rsp), %rdi
 7031                             	        movq	16(%rsp), %rsi
 7032                             	        movq	8(%rsp), %rbx
 7033                             	        # Add
 7034                             	        movq	(%rsi), %r8
 7035                             	        movq	8(%rsi), %r9
 7036                             	        addq	(%rbx), %r8
 7037                             	        movq	16(%rsi), %r10
 7038                             	        adcq	8(%rbx), %r9
 7039                             	        movq	24(%rsi), %rcx
 7040                             	        adcq	16(%rbx), %r10
 7041                             	        movq	$-19, %rax
 7042                             	        adcq	24(%rbx), %rcx
 7043                             	        movq	$0x7fffffffffffffff, %rdx
 7044                             	        movq	%rcx, %r11
 7045                             	        sarq	$63, %rcx
 7046                             	        #   Mask the modulus
 7047                             	        andq	%rcx, %rax
 7048                             	        andq	%rcx, %rdx
 7049                             	        #   Sub modulus (if overflow)
 7050                             	        subq	%rax, %r8
 7051                             	        sbbq	%rcx, %r9
 7052                             	        sbbq	%rcx, %r10
 7053                             	        sbbq	%rdx, %r11
 7054                             	        movq	%r8, (%rdi)
 7055                             	        movq	%r9, 8(%rdi)
 7056                             	        movq	%r10, 16(%rdi)
 7057                             	        movq	%r11, 24(%rdi)
 7058                             	        movq	16(%rsp), %rdi
 7059                             	        leaq	48(%rsp), %rsi
 7060                             	        movq	24(%rsp), %rbx
 7061                             	        # Add
 7062                             	        movq	(%rsi), %r8
 7063                             	        movq	8(%rsi), %r9
 7064                             	        addq	(%rbx), %r8
 7065                             	        movq	16(%rsi), %r10
 7066                             	        adcq	8(%rbx), %r9
 7067                             	        movq	24(%rsi), %rcx
 7068                             	        adcq	16(%rbx), %r10
 7069                             	        movq	$-19, %rax
 7070                             	        adcq	24(%rbx), %rcx
 7071                             	        movq	$0x7fffffffffffffff, %rdx
 7072                             	        movq	%rcx, %r11
 7073                             	        sarq	$63, %rcx
 7074                             	        #   Mask the modulus
 7075                             	        andq	%rcx, %rax
 7076                             	        andq	%rcx, %rdx
 7077                             	        #   Sub modulus (if overflow)
 7078                             	        subq	%rax, %r8
 7079                             	        sbbq	%rcx, %r9
 7080                             	        sbbq	%rcx, %r10
 7081                             	        sbbq	%rdx, %r11
 7082                             	        movq	%r8, (%rdi)
 7083                             	        movq	%r9, 8(%rdi)
 7084                             	        movq	%r10, 16(%rdi)
 7085                             	        movq	%r11, 24(%rdi)
 7086                             	        movq	24(%rsp), %rdi
 7087                             	        leaq	48(%rsp), %rsi
 7088                             	        movq	24(%rsp), %rbx
 7089                             	        # Sub
 7090                             	        movq	(%rsi), %r8
 7091                             	        movq	8(%rsi), %r9
 7092                             	        movq	16(%rsi), %r10
 7093                             	        movq	24(%rsi), %r11
 7094                             	        subq	(%rbx), %r8
 7095                             	        movq	$0x00, %rcx
 7096                             	        sbbq	8(%rbx), %r9
 7097                             	        movq	$-19, %rax
 7098                             	        sbbq	16(%rbx), %r10
 7099                             	        movq	$0x7fffffffffffffff, %rdx
 7100                             	        sbbq	24(%rbx), %r11
 7101                             	        sbbq	$0x00, %rcx
 7102                             	        #   Mask the modulus
 7103                             	        andq	%rcx, %rax
 7104                             	        andq	%rcx, %rdx
 7105                             	        #   Add modulus (if underflow)
 7106                             	        addq	%rax, %r8
 7107                             	        adcq	%rcx, %r9
 7108                             	        adcq	%rcx, %r10
 7109                             	        adcq	%rdx, %r11
 7110                             	        movq	%r8, (%rdi)
 7111                             	        movq	%r9, 8(%rdi)
 7112                             	        movq	%r10, 16(%rdi)
 7113                             	        movq	%r11, 24(%rdi)
 7114                             	        addq	$0x50, %rsp
 7115                             	        popq	%r15
 7116                             	        popq	%r14
 7117                             	        popq	%r13
 7118                             	        popq	%r12
 7119                             	        popq	%rbx
 7120                             	        repz retq
 7121                             	#ifndef __APPLE__
 7123                             	#endif /* __APPLE__ */
 7124                             	#ifndef __APPLE__
 7125                             	.text
 7126                             	.globl	fe_ge_msub_x64
 7128                             	.align	16
 7129                             	fe_ge_msub_x64:
 7130                             	#else
 7131                             	.section	__TEXT,__text
 7132                             	.globl	_fe_ge_msub_x64
 7133                             	.p2align	4
 7134                             	_fe_ge_msub_x64:
 7135                             	#endif /* __APPLE__ */
 7136                             	        pushq	%rbx
 7137                             	        pushq	%r12
 7138                             	        pushq	%r13
 7139                             	        pushq	%r14
 7140                             	        pushq	%r15
 7141                             	        subq	$0x50, %rsp
 7142                             	        movq	%rdi, (%rsp)
 7143                             	        movq	%rsi, 8(%rsp)
 7144                             	        movq	%rdx, 16(%rsp)
 7145                             	        movq	%rcx, 24(%rsp)
 7146                             	        movq	%r8, 32(%rsp)
 7147                             	        movq	%r9, 40(%rsp)
 7148                             	        movq	(%rsp), %rdi
 7149                             	        movq	40(%rsp), %rsi
 7150                             	        movq	32(%rsp), %rbx
 7151                             	        # Add
 7152                             	        movq	(%rsi), %r8
 7153                             	        movq	8(%rsi), %r9
 7154                             	        addq	(%rbx), %r8
 7155                             	        movq	16(%rsi), %r10
 7156                             	        adcq	8(%rbx), %r9
 7157                             	        movq	24(%rsi), %rcx
 7158                             	        adcq	16(%rbx), %r10
 7159                             	        movq	$-19, %rax
 7160                             	        adcq	24(%rbx), %rcx
 7161                             	        movq	$0x7fffffffffffffff, %rdx
 7162                             	        movq	%rcx, %r11
 7163                             	        sarq	$63, %rcx
 7164                             	        #   Mask the modulus
 7165                             	        andq	%rcx, %rax
 7166                             	        andq	%rcx, %rdx
 7167                             	        #   Sub modulus (if overflow)
 7168                             	        subq	%rax, %r8
 7169                             	        sbbq	%rcx, %r9
 7170                             	        sbbq	%rcx, %r10
 7171                             	        sbbq	%rdx, %r11
 7172                             	        movq	%r8, (%rdi)
 7173                             	        movq	%r9, 8(%rdi)
 7174                             	        movq	%r10, 16(%rdi)
 7175                             	        movq	%r11, 24(%rdi)
 7176                             	        movq	8(%rsp), %rdi
 7177                             	        movq	40(%rsp), %rsi
 7178                             	        movq	32(%rsp), %rbx
 7179                             	        # Sub
 7180                             	        movq	(%rsi), %r8
 7181                             	        movq	8(%rsi), %r9
 7182                             	        movq	16(%rsi), %r10
 7183                             	        movq	24(%rsi), %r11
 7184                             	        subq	(%rbx), %r8
 7185                             	        movq	$0x00, %rcx
 7186                             	        sbbq	8(%rbx), %r9
 7187                             	        movq	$-19, %rax
 7188                             	        sbbq	16(%rbx), %r10
 7189                             	        movq	$0x7fffffffffffffff, %rdx
 7190                             	        sbbq	24(%rbx), %r11
 7191                             	        sbbq	$0x00, %rcx
 7192                             	        #   Mask the modulus
 7193                             	        andq	%rcx, %rax
 7194                             	        andq	%rcx, %rdx
 7195                             	        #   Add modulus (if underflow)
 7196                             	        addq	%rax, %r8
 7197                             	        adcq	%rcx, %r9
 7198                             	        adcq	%rcx, %r10
 7199                             	        adcq	%rdx, %r11
 7200                             	        movq	%r8, (%rdi)
 7201                             	        movq	%r9, 8(%rdi)
 7202                             	        movq	%r10, 16(%rdi)
 7203                             	        movq	%r11, 24(%rdi)
 7204                             	        movq	16(%rsp), %rdi
 7205                             	        movq	(%rsp), %rsi
 7206                             	        movq	160(%rsp), %rbx
 7207                             	        # Multiply
 7208                             	        #  A[0] * B[0]
 7209                             	        movq	(%rbx), %rax
 7210                             	        mulq	(%rsi)
 7211                             	        movq	%rax, %r8
 7212                             	        movq	%rdx, %r9
 7213                             	        #  A[0] * B[1]
 7214                             	        movq	8(%rbx), %rax
 7215                             	        mulq	(%rsi)
 7216                             	        xorq	%r10, %r10
 7217                             	        addq	%rax, %r9
 7218                             	        adcq	%rdx, %r10
 7219                             	        #  A[1] * B[0]
 7220                             	        movq	(%rbx), %rax
 7221                             	        mulq	8(%rsi)
 7222                             	        xorq	%r11, %r11
 7223                             	        addq	%rax, %r9
 7224                             	        adcq	%rdx, %r10
 7225                             	        adcq	$0x00, %r11
 7226                             	        #  A[0] * B[2]
 7227                             	        movq	16(%rbx), %rax
 7228                             	        mulq	(%rsi)
 7229                             	        addq	%rax, %r10
 7230                             	        adcq	%rdx, %r11
 7231                             	        #  A[1] * B[1]
 7232                             	        movq	8(%rbx), %rax
 7233                             	        mulq	8(%rsi)
 7234                             	        xorq	%r12, %r12
 7235                             	        addq	%rax, %r10
 7236                             	        adcq	%rdx, %r11
 7237                             	        adcq	$0x00, %r12
 7238                             	        #  A[2] * B[0]
 7239                             	        movq	(%rbx), %rax
 7240                             	        mulq	16(%rsi)
 7241                             	        addq	%rax, %r10
 7242                             	        adcq	%rdx, %r11
 7243                             	        adcq	$0x00, %r12
 7244                             	        #  A[0] * B[3]
 7245                             	        movq	24(%rbx), %rax
 7246                             	        mulq	(%rsi)
 7247                             	        xorq	%r13, %r13
 7248                             	        addq	%rax, %r11
 7249                             	        adcq	%rdx, %r12
 7250                             	        adcq	$0x00, %r13
 7251                             	        #  A[1] * B[2]
 7252                             	        movq	16(%rbx), %rax
 7253                             	        mulq	8(%rsi)
 7254                             	        addq	%rax, %r11
 7255                             	        adcq	%rdx, %r12
 7256                             	        adcq	$0x00, %r13
 7257                             	        #  A[2] * B[1]
 7258                             	        movq	8(%rbx), %rax
 7259                             	        mulq	16(%rsi)
 7260                             	        addq	%rax, %r11
 7261                             	        adcq	%rdx, %r12
 7262                             	        adcq	$0x00, %r13
 7263                             	        #  A[3] * B[0]
 7264                             	        movq	(%rbx), %rax
 7265                             	        mulq	24(%rsi)
 7266                             	        addq	%rax, %r11
 7267                             	        adcq	%rdx, %r12
 7268                             	        adcq	$0x00, %r13
 7269                             	        #  A[1] * B[3]
 7270                             	        movq	24(%rbx), %rax
 7271                             	        mulq	8(%rsi)
 7272                             	        xorq	%r14, %r14
 7273                             	        addq	%rax, %r12
 7274                             	        adcq	%rdx, %r13
 7275                             	        adcq	$0x00, %r14
 7276                             	        #  A[2] * B[2]
 7277                             	        movq	16(%rbx), %rax
 7278                             	        mulq	16(%rsi)
 7279                             	        addq	%rax, %r12
 7280                             	        adcq	%rdx, %r13
 7281                             	        adcq	$0x00, %r14
 7282                             	        #  A[3] * B[1]
 7283                             	        movq	8(%rbx), %rax
 7284                             	        mulq	24(%rsi)
 7285                             	        addq	%rax, %r12
 7286                             	        adcq	%rdx, %r13
 7287                             	        adcq	$0x00, %r14
 7288                             	        #  A[2] * B[3]
 7289                             	        movq	24(%rbx), %rax
 7290                             	        mulq	16(%rsi)
 7291                             	        xorq	%r15, %r15
 7292                             	        addq	%rax, %r13
 7293                             	        adcq	%rdx, %r14
 7294                             	        adcq	$0x00, %r15
 7295                             	        #  A[3] * B[2]
 7296                             	        movq	16(%rbx), %rax
 7297                             	        mulq	24(%rsi)
 7298                             	        addq	%rax, %r13
 7299                             	        adcq	%rdx, %r14
 7300                             	        adcq	$0x00, %r15
 7301                             	        #  A[3] * B[3]
 7302                             	        movq	24(%rbx), %rax
 7303                             	        mulq	24(%rsi)
 7304                             	        addq	%rax, %r14
 7305                             	        adcq	%rdx, %r15
 7306                             	        # Reduce
 7307                             	        movq	$0x7fffffffffffffff, %rcx
 7308                             	        #  Move top half into t4-t7 and remove top bit from t3
 7309                             	        shldq	$0x01, %r14, %r15
 7310                             	        shldq	$0x01, %r13, %r14
 7311                             	        shldq	$0x01, %r12, %r13
 7312                             	        shldq	$0x01, %r11, %r12
 7313                             	        andq	%rcx, %r11
 7314                             	        #  Multiply top half by 19
 7315                             	        movq	$19, %rax
 7316                             	        mulq	%r12
 7317                             	        xorq	%r12, %r12
 7318                             	        addq	%rax, %r8
 7319                             	        movq	$19, %rax
 7320                             	        adcq	%rdx, %r12
 7321                             	        mulq	%r13
 7322                             	        xorq	%r13, %r13
 7323                             	        addq	%rax, %r9
 7324                             	        movq	$19, %rax
 7325                             	        adcq	%rdx, %r13
 7326                             	        mulq	%r14
 7327                             	        xorq	%r14, %r14
 7328                             	        addq	%rax, %r10
 7329                             	        movq	$19, %rax
 7330                             	        adcq	%rdx, %r14
 7331                             	        mulq	%r15
 7332                             	        #  Add remaining product results in
 7333                             	        addq	%r12, %r9
 7334                             	        adcq	%r13, %r10
 7335                             	        adcq	%r14, %r11
 7336                             	        adcq	%rax, %r11
 7337                             	        adcq	$0x00, %rdx
 7338                             	        #  Overflow
 7339                             	        shldq	$0x01, %r11, %rdx
 7340                             	        imulq	$19, %rdx, %rax
 7341                             	        andq	%rcx, %r11
 7342                             	        addq	%rax, %r8
 7343                             	        adcq	$0x00, %r9
 7344                             	        adcq	$0x00, %r10
 7345                             	        adcq	$0x00, %r11
 7346                             	        # Reduce if top bit set
 7347                             	        movq	%r11, %rdx
 7348                             	        sarq	$63, %rdx
 7349                             	        andq	$19, %rdx
 7350                             	        andq	%rcx, %r11
 7351                             	        addq	%rdx, %r8
 7352                             	        adcq	$0x00, %r9
 7353                             	        adcq	$0x00, %r10
 7354                             	        adcq	$0x00, %r11
 7355                             	        # Store
 7356                             	        movq	%r8, (%rdi)
 7357                             	        movq	%r9, 8(%rdi)
 7358                             	        movq	%r10, 16(%rdi)
 7359                             	        movq	%r11, 24(%rdi)
 7360                             	        movq	8(%rsp), %rdi
 7361                             	        movq	8(%rsp), %rsi
 7362                             	        movq	152(%rsp), %rbx
 7363                             	        # Multiply
 7364                             	        #  A[0] * B[0]
 7365                             	        movq	(%rbx), %rax
 7366                             	        mulq	(%rsi)
 7367                             	        movq	%rax, %r8
 7368                             	        movq	%rdx, %r9
 7369                             	        #  A[0] * B[1]
 7370                             	        movq	8(%rbx), %rax
 7371                             	        mulq	(%rsi)
 7372                             	        xorq	%r10, %r10
 7373                             	        addq	%rax, %r9
 7374                             	        adcq	%rdx, %r10
 7375                             	        #  A[1] * B[0]
 7376                             	        movq	(%rbx), %rax
 7377                             	        mulq	8(%rsi)
 7378                             	        xorq	%r11, %r11
 7379                             	        addq	%rax, %r9
 7380                             	        adcq	%rdx, %r10
 7381                             	        adcq	$0x00, %r11
 7382                             	        #  A[0] * B[2]
 7383                             	        movq	16(%rbx), %rax
 7384                             	        mulq	(%rsi)
 7385                             	        addq	%rax, %r10
 7386                             	        adcq	%rdx, %r11
 7387                             	        #  A[1] * B[1]
 7388                             	        movq	8(%rbx), %rax
 7389                             	        mulq	8(%rsi)
 7390                             	        xorq	%r12, %r12
 7391                             	        addq	%rax, %r10
 7392                             	        adcq	%rdx, %r11
 7393                             	        adcq	$0x00, %r12
 7394                             	        #  A[2] * B[0]
 7395                             	        movq	(%rbx), %rax
 7396                             	        mulq	16(%rsi)
 7397                             	        addq	%rax, %r10
 7398                             	        adcq	%rdx, %r11
 7399                             	        adcq	$0x00, %r12
 7400                             	        #  A[0] * B[3]
 7401                             	        movq	24(%rbx), %rax
 7402                             	        mulq	(%rsi)
 7403                             	        xorq	%r13, %r13
 7404                             	        addq	%rax, %r11
 7405                             	        adcq	%rdx, %r12
 7406                             	        adcq	$0x00, %r13
 7407                             	        #  A[1] * B[2]
 7408                             	        movq	16(%rbx), %rax
 7409                             	        mulq	8(%rsi)
 7410                             	        addq	%rax, %r11
 7411                             	        adcq	%rdx, %r12
 7412                             	        adcq	$0x00, %r13
 7413                             	        #  A[2] * B[1]
 7414                             	        movq	8(%rbx), %rax
 7415                             	        mulq	16(%rsi)
 7416                             	        addq	%rax, %r11
 7417                             	        adcq	%rdx, %r12
 7418                             	        adcq	$0x00, %r13
 7419                             	        #  A[3] * B[0]
 7420                             	        movq	(%rbx), %rax
 7421                             	        mulq	24(%rsi)
 7422                             	        addq	%rax, %r11
 7423                             	        adcq	%rdx, %r12
 7424                             	        adcq	$0x00, %r13
 7425                             	        #  A[1] * B[3]
 7426                             	        movq	24(%rbx), %rax
 7427                             	        mulq	8(%rsi)
 7428                             	        xorq	%r14, %r14
 7429                             	        addq	%rax, %r12
 7430                             	        adcq	%rdx, %r13
 7431                             	        adcq	$0x00, %r14
 7432                             	        #  A[2] * B[2]
 7433                             	        movq	16(%rbx), %rax
 7434                             	        mulq	16(%rsi)
 7435                             	        addq	%rax, %r12
 7436                             	        adcq	%rdx, %r13
 7437                             	        adcq	$0x00, %r14
 7438                             	        #  A[3] * B[1]
 7439                             	        movq	8(%rbx), %rax
 7440                             	        mulq	24(%rsi)
 7441                             	        addq	%rax, %r12
 7442                             	        adcq	%rdx, %r13
 7443                             	        adcq	$0x00, %r14
 7444                             	        #  A[2] * B[3]
 7445                             	        movq	24(%rbx), %rax
 7446                             	        mulq	16(%rsi)
 7447                             	        xorq	%r15, %r15
 7448                             	        addq	%rax, %r13
 7449                             	        adcq	%rdx, %r14
 7450                             	        adcq	$0x00, %r15
 7451                             	        #  A[3] * B[2]
 7452                             	        movq	16(%rbx), %rax
 7453                             	        mulq	24(%rsi)
 7454                             	        addq	%rax, %r13
 7455                             	        adcq	%rdx, %r14
 7456                             	        adcq	$0x00, %r15
 7457                             	        #  A[3] * B[3]
 7458                             	        movq	24(%rbx), %rax
 7459                             	        mulq	24(%rsi)
 7460                             	        addq	%rax, %r14
 7461                             	        adcq	%rdx, %r15
 7462                             	        # Reduce
 7463                             	        movq	$0x7fffffffffffffff, %rcx
 7464                             	        #  Move top half into t4-t7 and remove top bit from t3
 7465                             	        shldq	$0x01, %r14, %r15
 7466                             	        shldq	$0x01, %r13, %r14
 7467                             	        shldq	$0x01, %r12, %r13
 7468                             	        shldq	$0x01, %r11, %r12
 7469                             	        andq	%rcx, %r11
 7470                             	        #  Multiply top half by 19
 7471                             	        movq	$19, %rax
 7472                             	        mulq	%r12
 7473                             	        xorq	%r12, %r12
 7474                             	        addq	%rax, %r8
 7475                             	        movq	$19, %rax
 7476                             	        adcq	%rdx, %r12
 7477                             	        mulq	%r13
 7478                             	        xorq	%r13, %r13
 7479                             	        addq	%rax, %r9
 7480                             	        movq	$19, %rax
 7481                             	        adcq	%rdx, %r13
 7482                             	        mulq	%r14
 7483                             	        xorq	%r14, %r14
 7484                             	        addq	%rax, %r10
 7485                             	        movq	$19, %rax
 7486                             	        adcq	%rdx, %r14
 7487                             	        mulq	%r15
 7488                             	        #  Add remaining product results in
 7489                             	        addq	%r12, %r9
 7490                             	        adcq	%r13, %r10
 7491                             	        adcq	%r14, %r11
 7492                             	        adcq	%rax, %r11
 7493                             	        adcq	$0x00, %rdx
 7494                             	        #  Overflow
 7495                             	        shldq	$0x01, %r11, %rdx
 7496                             	        imulq	$19, %rdx, %rax
 7497                             	        andq	%rcx, %r11
 7498                             	        addq	%rax, %r8
 7499                             	        adcq	$0x00, %r9
 7500                             	        adcq	$0x00, %r10
 7501                             	        adcq	$0x00, %r11
 7502                             	        # Reduce if top bit set
 7503                             	        movq	%r11, %rdx
 7504                             	        sarq	$63, %rdx
 7505                             	        andq	$19, %rdx
 7506                             	        andq	%rcx, %r11
 7507                             	        addq	%rdx, %r8
 7508                             	        adcq	$0x00, %r9
 7509                             	        adcq	$0x00, %r10
 7510                             	        adcq	$0x00, %r11
 7511                             	        # Store
 7512                             	        movq	%r8, (%rdi)
 7513                             	        movq	%r9, 8(%rdi)
 7514                             	        movq	%r10, 16(%rdi)
 7515                             	        movq	%r11, 24(%rdi)
 7516                             	        movq	24(%rsp), %rdi
 7517                             	        movq	144(%rsp), %rsi
 7518                             	        movq	136(%rsp), %rbx
 7519                             	        # Multiply
 7520                             	        #  A[0] * B[0]
 7521                             	        movq	(%rbx), %rax
 7522                             	        mulq	(%rsi)
 7523                             	        movq	%rax, %r8
 7524                             	        movq	%rdx, %r9
 7525                             	        #  A[0] * B[1]
 7526                             	        movq	8(%rbx), %rax
 7527                             	        mulq	(%rsi)
 7528                             	        xorq	%r10, %r10
 7529                             	        addq	%rax, %r9
 7530                             	        adcq	%rdx, %r10
 7531                             	        #  A[1] * B[0]
 7532                             	        movq	(%rbx), %rax
 7533                             	        mulq	8(%rsi)
 7534                             	        xorq	%r11, %r11
 7535                             	        addq	%rax, %r9
 7536                             	        adcq	%rdx, %r10
 7537                             	        adcq	$0x00, %r11
 7538                             	        #  A[0] * B[2]
 7539                             	        movq	16(%rbx), %rax
 7540                             	        mulq	(%rsi)
 7541                             	        addq	%rax, %r10
 7542                             	        adcq	%rdx, %r11
 7543                             	        #  A[1] * B[1]
 7544                             	        movq	8(%rbx), %rax
 7545                             	        mulq	8(%rsi)
 7546                             	        xorq	%r12, %r12
 7547                             	        addq	%rax, %r10
 7548                             	        adcq	%rdx, %r11
 7549                             	        adcq	$0x00, %r12
 7550                             	        #  A[2] * B[0]
 7551                             	        movq	(%rbx), %rax
 7552                             	        mulq	16(%rsi)
 7553                             	        addq	%rax, %r10
 7554                             	        adcq	%rdx, %r11
 7555                             	        adcq	$0x00, %r12
 7556                             	        #  A[0] * B[3]
 7557                             	        movq	24(%rbx), %rax
 7558                             	        mulq	(%rsi)
 7559                             	        xorq	%r13, %r13
 7560                             	        addq	%rax, %r11
 7561                             	        adcq	%rdx, %r12
 7562                             	        adcq	$0x00, %r13
 7563                             	        #  A[1] * B[2]
 7564                             	        movq	16(%rbx), %rax
 7565                             	        mulq	8(%rsi)
 7566                             	        addq	%rax, %r11
 7567                             	        adcq	%rdx, %r12
 7568                             	        adcq	$0x00, %r13
 7569                             	        #  A[2] * B[1]
 7570                             	        movq	8(%rbx), %rax
 7571                             	        mulq	16(%rsi)
 7572                             	        addq	%rax, %r11
 7573                             	        adcq	%rdx, %r12
 7574                             	        adcq	$0x00, %r13
 7575                             	        #  A[3] * B[0]
 7576                             	        movq	(%rbx), %rax
 7577                             	        mulq	24(%rsi)
 7578                             	        addq	%rax, %r11
 7579                             	        adcq	%rdx, %r12
 7580                             	        adcq	$0x00, %r13
 7581                             	        #  A[1] * B[3]
 7582                             	        movq	24(%rbx), %rax
 7583                             	        mulq	8(%rsi)
 7584                             	        xorq	%r14, %r14
 7585                             	        addq	%rax, %r12
 7586                             	        adcq	%rdx, %r13
 7587                             	        adcq	$0x00, %r14
 7588                             	        #  A[2] * B[2]
 7589                             	        movq	16(%rbx), %rax
 7590                             	        mulq	16(%rsi)
 7591                             	        addq	%rax, %r12
 7592                             	        adcq	%rdx, %r13
 7593                             	        adcq	$0x00, %r14
 7594                             	        #  A[3] * B[1]
 7595                             	        movq	8(%rbx), %rax
 7596                             	        mulq	24(%rsi)
 7597                             	        addq	%rax, %r12
 7598                             	        adcq	%rdx, %r13
 7599                             	        adcq	$0x00, %r14
 7600                             	        #  A[2] * B[3]
 7601                             	        movq	24(%rbx), %rax
 7602                             	        mulq	16(%rsi)
 7603                             	        xorq	%r15, %r15
 7604                             	        addq	%rax, %r13
 7605                             	        adcq	%rdx, %r14
 7606                             	        adcq	$0x00, %r15
 7607                             	        #  A[3] * B[2]
 7608                             	        movq	16(%rbx), %rax
 7609                             	        mulq	24(%rsi)
 7610                             	        addq	%rax, %r13
 7611                             	        adcq	%rdx, %r14
 7612                             	        adcq	$0x00, %r15
 7613                             	        #  A[3] * B[3]
 7614                             	        movq	24(%rbx), %rax
 7615                             	        mulq	24(%rsi)
 7616                             	        addq	%rax, %r14
 7617                             	        adcq	%rdx, %r15
 7618                             	        # Reduce
 7619                             	        movq	$0x7fffffffffffffff, %rcx
 7620                             	        #  Move top half into t4-t7 and remove top bit from t3
 7621                             	        shldq	$0x01, %r14, %r15
 7622                             	        shldq	$0x01, %r13, %r14
 7623                             	        shldq	$0x01, %r12, %r13
 7624                             	        shldq	$0x01, %r11, %r12
 7625                             	        andq	%rcx, %r11
 7626                             	        #  Multiply top half by 19
 7627                             	        movq	$19, %rax
 7628                             	        mulq	%r12
 7629                             	        xorq	%r12, %r12
 7630                             	        addq	%rax, %r8
 7631                             	        movq	$19, %rax
 7632                             	        adcq	%rdx, %r12
 7633                             	        mulq	%r13
 7634                             	        xorq	%r13, %r13
 7635                             	        addq	%rax, %r9
 7636                             	        movq	$19, %rax
 7637                             	        adcq	%rdx, %r13
 7638                             	        mulq	%r14
 7639                             	        xorq	%r14, %r14
 7640                             	        addq	%rax, %r10
 7641                             	        movq	$19, %rax
 7642                             	        adcq	%rdx, %r14
 7643                             	        mulq	%r15
 7644                             	        #  Add remaining product results in
 7645                             	        addq	%r12, %r9
 7646                             	        adcq	%r13, %r10
 7647                             	        adcq	%r14, %r11
 7648                             	        adcq	%rax, %r11
 7649                             	        adcq	$0x00, %rdx
 7650                             	        #  Overflow
 7651                             	        shldq	$0x01, %r11, %rdx
 7652                             	        imulq	$19, %rdx, %rax
 7653                             	        andq	%rcx, %r11
 7654                             	        addq	%rax, %r8
 7655                             	        adcq	$0x00, %r9
 7656                             	        adcq	$0x00, %r10
 7657                             	        adcq	$0x00, %r11
 7658                             	        # Reduce if top bit set
 7659                             	        movq	%r11, %rdx
 7660                             	        sarq	$63, %rdx
 7661                             	        andq	$19, %rdx
 7662                             	        andq	%rcx, %r11
 7663                             	        addq	%rdx, %r8
 7664                             	        adcq	$0x00, %r9
 7665                             	        adcq	$0x00, %r10
 7666                             	        adcq	$0x00, %r11
 7667                             	        # Store
 7668                             	        movq	%r8, (%rdi)
 7669                             	        movq	%r9, 8(%rdi)
 7670                             	        movq	%r10, 16(%rdi)
 7671                             	        movq	%r11, 24(%rdi)
 7672                             	        leaq	48(%rsp), %rdi
 7673                             	        movq	128(%rsp), %rsi
 7674                             	        movq	128(%rsp), %rbx
 7675                             	        # Add
 7676                             	        movq	(%rsi), %r8
 7677                             	        movq	8(%rsi), %r9
 7678                             	        addq	(%rbx), %r8
 7679                             	        movq	16(%rsi), %r10
 7680                             	        adcq	8(%rbx), %r9
 7681                             	        movq	24(%rsi), %rcx
 7682                             	        adcq	16(%rbx), %r10
 7683                             	        movq	$-19, %rax
 7684                             	        adcq	24(%rbx), %rcx
 7685                             	        movq	$0x7fffffffffffffff, %rdx
 7686                             	        movq	%rcx, %r11
 7687                             	        sarq	$63, %rcx
 7688                             	        #   Mask the modulus
 7689                             	        andq	%rcx, %rax
 7690                             	        andq	%rcx, %rdx
 7691                             	        #   Sub modulus (if overflow)
 7692                             	        subq	%rax, %r8
 7693                             	        sbbq	%rcx, %r9
 7694                             	        sbbq	%rcx, %r10
 7695                             	        sbbq	%rdx, %r11
 7696                             	        movq	%r8, (%rdi)
 7697                             	        movq	%r9, 8(%rdi)
 7698                             	        movq	%r10, 16(%rdi)
 7699                             	        movq	%r11, 24(%rdi)
 7700                             	        movq	(%rsp), %rdi
 7701                             	        movq	16(%rsp), %rsi
 7702                             	        movq	8(%rsp), %rbx
 7703                             	        # Sub
 7704                             	        movq	(%rsi), %r8
 7705                             	        movq	8(%rsi), %r9
 7706                             	        movq	16(%rsi), %r10
 7707                             	        movq	24(%rsi), %r11
 7708                             	        subq	(%rbx), %r8
 7709                             	        movq	$0x00, %rcx
 7710                             	        sbbq	8(%rbx), %r9
 7711                             	        movq	$-19, %rax
 7712                             	        sbbq	16(%rbx), %r10
 7713                             	        movq	$0x7fffffffffffffff, %rdx
 7714                             	        sbbq	24(%rbx), %r11
 7715                             	        sbbq	$0x00, %rcx
 7716                             	        #   Mask the modulus
 7717                             	        andq	%rcx, %rax
 7718                             	        andq	%rcx, %rdx
 7719                             	        #   Add modulus (if underflow)
 7720                             	        addq	%rax, %r8
 7721                             	        adcq	%rcx, %r9
 7722                             	        adcq	%rcx, %r10
 7723                             	        adcq	%rdx, %r11
 7724                             	        movq	%r8, (%rdi)
 7725                             	        movq	%r9, 8(%rdi)
 7726                             	        movq	%r10, 16(%rdi)
 7727                             	        movq	%r11, 24(%rdi)
 7728                             	        movq	8(%rsp), %rdi
 7729                             	        movq	16(%rsp), %rsi
 7730                             	        movq	8(%rsp), %rbx
 7731                             	        # Add
 7732                             	        movq	(%rsi), %r8
 7733                             	        movq	8(%rsi), %r9
 7734                             	        addq	(%rbx), %r8
 7735                             	        movq	16(%rsi), %r10
 7736                             	        adcq	8(%rbx), %r9
 7737                             	        movq	24(%rsi), %rcx
 7738                             	        adcq	16(%rbx), %r10
 7739                             	        movq	$-19, %rax
 7740                             	        adcq	24(%rbx), %rcx
 7741                             	        movq	$0x7fffffffffffffff, %rdx
 7742                             	        movq	%rcx, %r11
 7743                             	        sarq	$63, %rcx
 7744                             	        #   Mask the modulus
 7745                             	        andq	%rcx, %rax
 7746                             	        andq	%rcx, %rdx
 7747                             	        #   Sub modulus (if overflow)
 7748                             	        subq	%rax, %r8
 7749                             	        sbbq	%rcx, %r9
 7750                             	        sbbq	%rcx, %r10
 7751                             	        sbbq	%rdx, %r11
 7752                             	        movq	%r8, (%rdi)
 7753                             	        movq	%r9, 8(%rdi)
 7754                             	        movq	%r10, 16(%rdi)
 7755                             	        movq	%r11, 24(%rdi)
 7756                             	        movq	16(%rsp), %rdi
 7757                             	        leaq	48(%rsp), %rsi
 7758                             	        movq	24(%rsp), %rbx
 7759                             	        # Sub
 7760                             	        movq	(%rsi), %r8
 7761                             	        movq	8(%rsi), %r9
 7762                             	        movq	16(%rsi), %r10
 7763                             	        movq	24(%rsi), %r11
 7764                             	        subq	(%rbx), %r8
 7765                             	        movq	$0x00, %rcx
 7766                             	        sbbq	8(%rbx), %r9
 7767                             	        movq	$-19, %rax
 7768                             	        sbbq	16(%rbx), %r10
 7769                             	        movq	$0x7fffffffffffffff, %rdx
 7770                             	        sbbq	24(%rbx), %r11
 7771                             	        sbbq	$0x00, %rcx
 7772                             	        #   Mask the modulus
 7773                             	        andq	%rcx, %rax
 7774                             	        andq	%rcx, %rdx
 7775                             	        #   Add modulus (if underflow)
 7776                             	        addq	%rax, %r8
 7777                             	        adcq	%rcx, %r9
 7778                             	        adcq	%rcx, %r10
 7779                             	        adcq	%rdx, %r11
 7780                             	        movq	%r8, (%rdi)
 7781                             	        movq	%r9, 8(%rdi)
 7782                             	        movq	%r10, 16(%rdi)
 7783                             	        movq	%r11, 24(%rdi)
 7784                             	        movq	24(%rsp), %rdi
 7785                             	        leaq	48(%rsp), %rsi
 7786                             	        movq	24(%rsp), %rbx
 7787                             	        # Add
 7788                             	        movq	(%rsi), %r8
 7789                             	        movq	8(%rsi), %r9
 7790                             	        addq	(%rbx), %r8
 7791                             	        movq	16(%rsi), %r10
 7792                             	        adcq	8(%rbx), %r9
 7793                             	        movq	24(%rsi), %rcx
 7794                             	        adcq	16(%rbx), %r10
 7795                             	        movq	$-19, %rax
 7796                             	        adcq	24(%rbx), %rcx
 7797                             	        movq	$0x7fffffffffffffff, %rdx
 7798                             	        movq	%rcx, %r11
 7799                             	        sarq	$63, %rcx
 7800                             	        #   Mask the modulus
 7801                             	        andq	%rcx, %rax
 7802                             	        andq	%rcx, %rdx
 7803                             	        #   Sub modulus (if overflow)
 7804                             	        subq	%rax, %r8
 7805                             	        sbbq	%rcx, %r9
 7806                             	        sbbq	%rcx, %r10
 7807                             	        sbbq	%rdx, %r11
 7808                             	        movq	%r8, (%rdi)
 7809                             	        movq	%r9, 8(%rdi)
 7810                             	        movq	%r10, 16(%rdi)
 7811                             	        movq	%r11, 24(%rdi)
 7812                             	        addq	$0x50, %rsp
 7813                             	        popq	%r15
 7814                             	        popq	%r14
 7815                             	        popq	%r13
 7816                             	        popq	%r12
 7817                             	        popq	%rbx
 7818                             	        repz retq
 7819                             	#ifndef __APPLE__
 7821                             	#endif /* __APPLE__ */
 7822                             	#ifndef __APPLE__
 7823                             	.text
 7824                             	.globl	fe_ge_add_x64
 7826                             	.align	16
 7827                             	fe_ge_add_x64:
 7828                             	#else
 7829                             	.section	__TEXT,__text
 7830                             	.globl	_fe_ge_add_x64
 7831                             	.p2align	4
 7832                             	_fe_ge_add_x64:
 7833                             	#endif /* __APPLE__ */
 7834                             	        pushq	%rbx
 7835                             	        pushq	%r12
 7836                             	        pushq	%r13
 7837                             	        pushq	%r14
 7838                             	        pushq	%r15
 7839                             	        subq	$0x50, %rsp
 7840                             	        movq	%rdi, (%rsp)
 7841                             	        movq	%rsi, 8(%rsp)
 7842                             	        movq	%rdx, 16(%rsp)
 7843                             	        movq	%rcx, 24(%rsp)
 7844                             	        movq	%r8, 32(%rsp)
 7845                             	        movq	%r9, 40(%rsp)
 7846                             	        movq	(%rsp), %rdi
 7847                             	        movq	40(%rsp), %rsi
 7848                             	        movq	32(%rsp), %rbx
 7849                             	        # Add
 7850                             	        movq	(%rsi), %r8
 7851                             	        movq	8(%rsi), %r9
 7852                             	        addq	(%rbx), %r8
 7853                             	        movq	16(%rsi), %r10
 7854                             	        adcq	8(%rbx), %r9
 7855                             	        movq	24(%rsi), %rcx
 7856                             	        adcq	16(%rbx), %r10
 7857                             	        movq	$-19, %rax
 7858                             	        adcq	24(%rbx), %rcx
 7859                             	        movq	$0x7fffffffffffffff, %rdx
 7860                             	        movq	%rcx, %r11
 7861                             	        sarq	$63, %rcx
 7862                             	        #   Mask the modulus
 7863                             	        andq	%rcx, %rax
 7864                             	        andq	%rcx, %rdx
 7865                             	        #   Sub modulus (if overflow)
 7866                             	        subq	%rax, %r8
 7867                             	        sbbq	%rcx, %r9
 7868                             	        sbbq	%rcx, %r10
 7869                             	        sbbq	%rdx, %r11
 7870                             	        movq	%r8, (%rdi)
 7871                             	        movq	%r9, 8(%rdi)
 7872                             	        movq	%r10, 16(%rdi)
 7873                             	        movq	%r11, 24(%rdi)
 7874                             	        movq	8(%rsp), %rdi
 7875                             	        movq	40(%rsp), %rsi
 7876                             	        movq	32(%rsp), %rbx
 7877                             	        # Sub
 7878                             	        movq	(%rsi), %r8
 7879                             	        movq	8(%rsi), %r9
 7880                             	        movq	16(%rsi), %r10
 7881                             	        movq	24(%rsi), %r11
 7882                             	        subq	(%rbx), %r8
 7883                             	        movq	$0x00, %rcx
 7884                             	        sbbq	8(%rbx), %r9
 7885                             	        movq	$-19, %rax
 7886                             	        sbbq	16(%rbx), %r10
 7887                             	        movq	$0x7fffffffffffffff, %rdx
 7888                             	        sbbq	24(%rbx), %r11
 7889                             	        sbbq	$0x00, %rcx
 7890                             	        #   Mask the modulus
 7891                             	        andq	%rcx, %rax
 7892                             	        andq	%rcx, %rdx
 7893                             	        #   Add modulus (if underflow)
 7894                             	        addq	%rax, %r8
 7895                             	        adcq	%rcx, %r9
 7896                             	        adcq	%rcx, %r10
 7897                             	        adcq	%rdx, %r11
 7898                             	        movq	%r8, (%rdi)
 7899                             	        movq	%r9, 8(%rdi)
 7900                             	        movq	%r10, 16(%rdi)
 7901                             	        movq	%r11, 24(%rdi)
 7902                             	        movq	16(%rsp), %rdi
 7903                             	        movq	(%rsp), %rsi
 7904                             	        movq	160(%rsp), %rbx
 7905                             	        # Multiply
 7906                             	        #  A[0] * B[0]
 7907                             	        movq	(%rbx), %rax
 7908                             	        mulq	(%rsi)
 7909                             	        movq	%rax, %r8
 7910                             	        movq	%rdx, %r9
 7911                             	        #  A[0] * B[1]
 7912                             	        movq	8(%rbx), %rax
 7913                             	        mulq	(%rsi)
 7914                             	        xorq	%r10, %r10
 7915                             	        addq	%rax, %r9
 7916                             	        adcq	%rdx, %r10
 7917                             	        #  A[1] * B[0]
 7918                             	        movq	(%rbx), %rax
 7919                             	        mulq	8(%rsi)
 7920                             	        xorq	%r11, %r11
 7921                             	        addq	%rax, %r9
 7922                             	        adcq	%rdx, %r10
 7923                             	        adcq	$0x00, %r11
 7924                             	        #  A[0] * B[2]
 7925                             	        movq	16(%rbx), %rax
 7926                             	        mulq	(%rsi)
 7927                             	        addq	%rax, %r10
 7928                             	        adcq	%rdx, %r11
 7929                             	        #  A[1] * B[1]
 7930                             	        movq	8(%rbx), %rax
 7931                             	        mulq	8(%rsi)
 7932                             	        xorq	%r12, %r12
 7933                             	        addq	%rax, %r10
 7934                             	        adcq	%rdx, %r11
 7935                             	        adcq	$0x00, %r12
 7936                             	        #  A[2] * B[0]
 7937                             	        movq	(%rbx), %rax
 7938                             	        mulq	16(%rsi)
 7939                             	        addq	%rax, %r10
 7940                             	        adcq	%rdx, %r11
 7941                             	        adcq	$0x00, %r12
 7942                             	        #  A[0] * B[3]
 7943                             	        movq	24(%rbx), %rax
 7944                             	        mulq	(%rsi)
 7945                             	        xorq	%r13, %r13
 7946                             	        addq	%rax, %r11
 7947                             	        adcq	%rdx, %r12
 7948                             	        adcq	$0x00, %r13
 7949                             	        #  A[1] * B[2]
 7950                             	        movq	16(%rbx), %rax
 7951                             	        mulq	8(%rsi)
 7952                             	        addq	%rax, %r11
 7953                             	        adcq	%rdx, %r12
 7954                             	        adcq	$0x00, %r13
 7955                             	        #  A[2] * B[1]
 7956                             	        movq	8(%rbx), %rax
 7957                             	        mulq	16(%rsi)
 7958                             	        addq	%rax, %r11
 7959                             	        adcq	%rdx, %r12
 7960                             	        adcq	$0x00, %r13
 7961                             	        #  A[3] * B[0]
 7962                             	        movq	(%rbx), %rax
 7963                             	        mulq	24(%rsi)
 7964                             	        addq	%rax, %r11
 7965                             	        adcq	%rdx, %r12
 7966                             	        adcq	$0x00, %r13
 7967                             	        #  A[1] * B[3]
 7968                             	        movq	24(%rbx), %rax
 7969                             	        mulq	8(%rsi)
 7970                             	        xorq	%r14, %r14
 7971                             	        addq	%rax, %r12
 7972                             	        adcq	%rdx, %r13
 7973                             	        adcq	$0x00, %r14
 7974                             	        #  A[2] * B[2]
 7975                             	        movq	16(%rbx), %rax
 7976                             	        mulq	16(%rsi)
 7977                             	        addq	%rax, %r12
 7978                             	        adcq	%rdx, %r13
 7979                             	        adcq	$0x00, %r14
 7980                             	        #  A[3] * B[1]
 7981                             	        movq	8(%rbx), %rax
 7982                             	        mulq	24(%rsi)
 7983                             	        addq	%rax, %r12
 7984                             	        adcq	%rdx, %r13
 7985                             	        adcq	$0x00, %r14
 7986                             	        #  A[2] * B[3]
 7987                             	        movq	24(%rbx), %rax
 7988                             	        mulq	16(%rsi)
 7989                             	        xorq	%r15, %r15
 7990                             	        addq	%rax, %r13
 7991                             	        adcq	%rdx, %r14
 7992                             	        adcq	$0x00, %r15
 7993                             	        #  A[3] * B[2]
 7994                             	        movq	16(%rbx), %rax
 7995                             	        mulq	24(%rsi)
 7996                             	        addq	%rax, %r13
 7997                             	        adcq	%rdx, %r14
 7998                             	        adcq	$0x00, %r15
 7999                             	        #  A[3] * B[3]
 8000                             	        movq	24(%rbx), %rax
 8001                             	        mulq	24(%rsi)
 8002                             	        addq	%rax, %r14
 8003                             	        adcq	%rdx, %r15
 8004                             	        # Reduce
 8005                             	        movq	$0x7fffffffffffffff, %rcx
 8006                             	        #  Move top half into t4-t7 and remove top bit from t3
 8007                             	        shldq	$0x01, %r14, %r15
 8008                             	        shldq	$0x01, %r13, %r14
 8009                             	        shldq	$0x01, %r12, %r13
 8010                             	        shldq	$0x01, %r11, %r12
 8011                             	        andq	%rcx, %r11
 8012                             	        #  Multiply top half by 19
 8013                             	        movq	$19, %rax
 8014                             	        mulq	%r12
 8015                             	        xorq	%r12, %r12
 8016                             	        addq	%rax, %r8
 8017                             	        movq	$19, %rax
 8018                             	        adcq	%rdx, %r12
 8019                             	        mulq	%r13
 8020                             	        xorq	%r13, %r13
 8021                             	        addq	%rax, %r9
 8022                             	        movq	$19, %rax
 8023                             	        adcq	%rdx, %r13
 8024                             	        mulq	%r14
 8025                             	        xorq	%r14, %r14
 8026                             	        addq	%rax, %r10
 8027                             	        movq	$19, %rax
 8028                             	        adcq	%rdx, %r14
 8029                             	        mulq	%r15
 8030                             	        #  Add remaining product results in
 8031                             	        addq	%r12, %r9
 8032                             	        adcq	%r13, %r10
 8033                             	        adcq	%r14, %r11
 8034                             	        adcq	%rax, %r11
 8035                             	        adcq	$0x00, %rdx
 8036                             	        #  Overflow
 8037                             	        shldq	$0x01, %r11, %rdx
 8038                             	        imulq	$19, %rdx, %rax
 8039                             	        andq	%rcx, %r11
 8040                             	        addq	%rax, %r8
 8041                             	        adcq	$0x00, %r9
 8042                             	        adcq	$0x00, %r10
 8043                             	        adcq	$0x00, %r11
 8044                             	        # Reduce if top bit set
 8045                             	        movq	%r11, %rdx
 8046                             	        sarq	$63, %rdx
 8047                             	        andq	$19, %rdx
 8048                             	        andq	%rcx, %r11
 8049                             	        addq	%rdx, %r8
 8050                             	        adcq	$0x00, %r9
 8051                             	        adcq	$0x00, %r10
 8052                             	        adcq	$0x00, %r11
 8053                             	        # Store
 8054                             	        movq	%r8, (%rdi)
 8055                             	        movq	%r9, 8(%rdi)
 8056                             	        movq	%r10, 16(%rdi)
 8057                             	        movq	%r11, 24(%rdi)
 8058                             	        movq	8(%rsp), %rdi
 8059                             	        movq	8(%rsp), %rsi
 8060                             	        movq	168(%rsp), %rbx
 8061                             	        # Multiply
 8062                             	        #  A[0] * B[0]
 8063                             	        movq	(%rbx), %rax
 8064                             	        mulq	(%rsi)
 8065                             	        movq	%rax, %r8
 8066                             	        movq	%rdx, %r9
 8067                             	        #  A[0] * B[1]
 8068                             	        movq	8(%rbx), %rax
 8069                             	        mulq	(%rsi)
 8070                             	        xorq	%r10, %r10
 8071                             	        addq	%rax, %r9
 8072                             	        adcq	%rdx, %r10
 8073                             	        #  A[1] * B[0]
 8074                             	        movq	(%rbx), %rax
 8075                             	        mulq	8(%rsi)
 8076                             	        xorq	%r11, %r11
 8077                             	        addq	%rax, %r9
 8078                             	        adcq	%rdx, %r10
 8079                             	        adcq	$0x00, %r11
 8080                             	        #  A[0] * B[2]
 8081                             	        movq	16(%rbx), %rax
 8082                             	        mulq	(%rsi)
 8083                             	        addq	%rax, %r10
 8084                             	        adcq	%rdx, %r11
 8085                             	        #  A[1] * B[1]
 8086                             	        movq	8(%rbx), %rax
 8087                             	        mulq	8(%rsi)
 8088                             	        xorq	%r12, %r12
 8089                             	        addq	%rax, %r10
 8090                             	        adcq	%rdx, %r11
 8091                             	        adcq	$0x00, %r12
 8092                             	        #  A[2] * B[0]
 8093                             	        movq	(%rbx), %rax
 8094                             	        mulq	16(%rsi)
 8095                             	        addq	%rax, %r10
 8096                             	        adcq	%rdx, %r11
 8097                             	        adcq	$0x00, %r12
 8098                             	        #  A[0] * B[3]
 8099                             	        movq	24(%rbx), %rax
 8100                             	        mulq	(%rsi)
 8101                             	        xorq	%r13, %r13
 8102                             	        addq	%rax, %r11
 8103                             	        adcq	%rdx, %r12
 8104                             	        adcq	$0x00, %r13
 8105                             	        #  A[1] * B[2]
 8106                             	        movq	16(%rbx), %rax
 8107                             	        mulq	8(%rsi)
 8108                             	        addq	%rax, %r11
 8109                             	        adcq	%rdx, %r12
 8110                             	        adcq	$0x00, %r13
 8111                             	        #  A[2] * B[1]
 8112                             	        movq	8(%rbx), %rax
 8113                             	        mulq	16(%rsi)
 8114                             	        addq	%rax, %r11
 8115                             	        adcq	%rdx, %r12
 8116                             	        adcq	$0x00, %r13
 8117                             	        #  A[3] * B[0]
 8118                             	        movq	(%rbx), %rax
 8119                             	        mulq	24(%rsi)
 8120                             	        addq	%rax, %r11
 8121                             	        adcq	%rdx, %r12
 8122                             	        adcq	$0x00, %r13
 8123                             	        #  A[1] * B[3]
 8124                             	        movq	24(%rbx), %rax
 8125                             	        mulq	8(%rsi)
 8126                             	        xorq	%r14, %r14
 8127                             	        addq	%rax, %r12
 8128                             	        adcq	%rdx, %r13
 8129                             	        adcq	$0x00, %r14
 8130                             	        #  A[2] * B[2]
 8131                             	        movq	16(%rbx), %rax
 8132                             	        mulq	16(%rsi)
 8133                             	        addq	%rax, %r12
 8134                             	        adcq	%rdx, %r13
 8135                             	        adcq	$0x00, %r14
 8136                             	        #  A[3] * B[1]
 8137                             	        movq	8(%rbx), %rax
 8138                             	        mulq	24(%rsi)
 8139                             	        addq	%rax, %r12
 8140                             	        adcq	%rdx, %r13
 8141                             	        adcq	$0x00, %r14
 8142                             	        #  A[2] * B[3]
 8143                             	        movq	24(%rbx), %rax
 8144                             	        mulq	16(%rsi)
 8145                             	        xorq	%r15, %r15
 8146                             	        addq	%rax, %r13
 8147                             	        adcq	%rdx, %r14
 8148                             	        adcq	$0x00, %r15
 8149                             	        #  A[3] * B[2]
 8150                             	        movq	16(%rbx), %rax
 8151                             	        mulq	24(%rsi)
 8152                             	        addq	%rax, %r13
 8153                             	        adcq	%rdx, %r14
 8154                             	        adcq	$0x00, %r15
 8155                             	        #  A[3] * B[3]
 8156                             	        movq	24(%rbx), %rax
 8157                             	        mulq	24(%rsi)
 8158                             	        addq	%rax, %r14
 8159                             	        adcq	%rdx, %r15
 8160                             	        # Reduce
 8161                             	        movq	$0x7fffffffffffffff, %rcx
 8162                             	        #  Move top half into t4-t7 and remove top bit from t3
 8163                             	        shldq	$0x01, %r14, %r15
 8164                             	        shldq	$0x01, %r13, %r14
 8165                             	        shldq	$0x01, %r12, %r13
 8166                             	        shldq	$0x01, %r11, %r12
 8167                             	        andq	%rcx, %r11
 8168                             	        #  Multiply top half by 19
 8169                             	        movq	$19, %rax
 8170                             	        mulq	%r12
 8171                             	        xorq	%r12, %r12
 8172                             	        addq	%rax, %r8
 8173                             	        movq	$19, %rax
 8174                             	        adcq	%rdx, %r12
 8175                             	        mulq	%r13
 8176                             	        xorq	%r13, %r13
 8177                             	        addq	%rax, %r9
 8178                             	        movq	$19, %rax
 8179                             	        adcq	%rdx, %r13
 8180                             	        mulq	%r14
 8181                             	        xorq	%r14, %r14
 8182                             	        addq	%rax, %r10
 8183                             	        movq	$19, %rax
 8184                             	        adcq	%rdx, %r14
 8185                             	        mulq	%r15
 8186                             	        #  Add remaining product results in
 8187                             	        addq	%r12, %r9
 8188                             	        adcq	%r13, %r10
 8189                             	        adcq	%r14, %r11
 8190                             	        adcq	%rax, %r11
 8191                             	        adcq	$0x00, %rdx
 8192                             	        #  Overflow
 8193                             	        shldq	$0x01, %r11, %rdx
 8194                             	        imulq	$19, %rdx, %rax
 8195                             	        andq	%rcx, %r11
 8196                             	        addq	%rax, %r8
 8197                             	        adcq	$0x00, %r9
 8198                             	        adcq	$0x00, %r10
 8199                             	        adcq	$0x00, %r11
 8200                             	        # Reduce if top bit set
 8201                             	        movq	%r11, %rdx
 8202                             	        sarq	$63, %rdx
 8203                             	        andq	$19, %rdx
 8204                             	        andq	%rcx, %r11
 8205                             	        addq	%rdx, %r8
 8206                             	        adcq	$0x00, %r9
 8207                             	        adcq	$0x00, %r10
 8208                             	        adcq	$0x00, %r11
 8209                             	        # Store
 8210                             	        movq	%r8, (%rdi)
 8211                             	        movq	%r9, 8(%rdi)
 8212                             	        movq	%r10, 16(%rdi)
 8213                             	        movq	%r11, 24(%rdi)
 8214                             	        movq	24(%rsp), %rdi
 8215                             	        movq	152(%rsp), %rsi
 8216                             	        movq	136(%rsp), %rbx
 8217                             	        # Multiply
 8218                             	        #  A[0] * B[0]
 8219                             	        movq	(%rbx), %rax
 8220                             	        mulq	(%rsi)
 8221                             	        movq	%rax, %r8
 8222                             	        movq	%rdx, %r9
 8223                             	        #  A[0] * B[1]
 8224                             	        movq	8(%rbx), %rax
 8225                             	        mulq	(%rsi)
 8226                             	        xorq	%r10, %r10
 8227                             	        addq	%rax, %r9
 8228                             	        adcq	%rdx, %r10
 8229                             	        #  A[1] * B[0]
 8230                             	        movq	(%rbx), %rax
 8231                             	        mulq	8(%rsi)
 8232                             	        xorq	%r11, %r11
 8233                             	        addq	%rax, %r9
 8234                             	        adcq	%rdx, %r10
 8235                             	        adcq	$0x00, %r11
 8236                             	        #  A[0] * B[2]
 8237                             	        movq	16(%rbx), %rax
 8238                             	        mulq	(%rsi)
 8239                             	        addq	%rax, %r10
 8240                             	        adcq	%rdx, %r11
 8241                             	        #  A[1] * B[1]
 8242                             	        movq	8(%rbx), %rax
 8243                             	        mulq	8(%rsi)
 8244                             	        xorq	%r12, %r12
 8245                             	        addq	%rax, %r10
 8246                             	        adcq	%rdx, %r11
 8247                             	        adcq	$0x00, %r12
 8248                             	        #  A[2] * B[0]
 8249                             	        movq	(%rbx), %rax
 8250                             	        mulq	16(%rsi)
 8251                             	        addq	%rax, %r10
 8252                             	        adcq	%rdx, %r11
 8253                             	        adcq	$0x00, %r12
 8254                             	        #  A[0] * B[3]
 8255                             	        movq	24(%rbx), %rax
 8256                             	        mulq	(%rsi)
 8257                             	        xorq	%r13, %r13
 8258                             	        addq	%rax, %r11
 8259                             	        adcq	%rdx, %r12
 8260                             	        adcq	$0x00, %r13
 8261                             	        #  A[1] * B[2]
 8262                             	        movq	16(%rbx), %rax
 8263                             	        mulq	8(%rsi)
 8264                             	        addq	%rax, %r11
 8265                             	        adcq	%rdx, %r12
 8266                             	        adcq	$0x00, %r13
 8267                             	        #  A[2] * B[1]
 8268                             	        movq	8(%rbx), %rax
 8269                             	        mulq	16(%rsi)
 8270                             	        addq	%rax, %r11
 8271                             	        adcq	%rdx, %r12
 8272                             	        adcq	$0x00, %r13
 8273                             	        #  A[3] * B[0]
 8274                             	        movq	(%rbx), %rax
 8275                             	        mulq	24(%rsi)
 8276                             	        addq	%rax, %r11
 8277                             	        adcq	%rdx, %r12
 8278                             	        adcq	$0x00, %r13
 8279                             	        #  A[1] * B[3]
 8280                             	        movq	24(%rbx), %rax
 8281                             	        mulq	8(%rsi)
 8282                             	        xorq	%r14, %r14
 8283                             	        addq	%rax, %r12
 8284                             	        adcq	%rdx, %r13
 8285                             	        adcq	$0x00, %r14
 8286                             	        #  A[2] * B[2]
 8287                             	        movq	16(%rbx), %rax
 8288                             	        mulq	16(%rsi)
 8289                             	        addq	%rax, %r12
 8290                             	        adcq	%rdx, %r13
 8291                             	        adcq	$0x00, %r14
 8292                             	        #  A[3] * B[1]
 8293                             	        movq	8(%rbx), %rax
 8294                             	        mulq	24(%rsi)
 8295                             	        addq	%rax, %r12
 8296                             	        adcq	%rdx, %r13
 8297                             	        adcq	$0x00, %r14
 8298                             	        #  A[2] * B[3]
 8299                             	        movq	24(%rbx), %rax
 8300                             	        mulq	16(%rsi)
 8301                             	        xorq	%r15, %r15
 8302                             	        addq	%rax, %r13
 8303                             	        adcq	%rdx, %r14
 8304                             	        adcq	$0x00, %r15
 8305                             	        #  A[3] * B[2]
 8306                             	        movq	16(%rbx), %rax
 8307                             	        mulq	24(%rsi)
 8308                             	        addq	%rax, %r13
 8309                             	        adcq	%rdx, %r14
 8310                             	        adcq	$0x00, %r15
 8311                             	        #  A[3] * B[3]
 8312                             	        movq	24(%rbx), %rax
 8313                             	        mulq	24(%rsi)
 8314                             	        addq	%rax, %r14
 8315                             	        adcq	%rdx, %r15
 8316                             	        # Reduce
 8317                             	        movq	$0x7fffffffffffffff, %rcx
 8318                             	        #  Move top half into t4-t7 and remove top bit from t3
 8319                             	        shldq	$0x01, %r14, %r15
 8320                             	        shldq	$0x01, %r13, %r14
 8321                             	        shldq	$0x01, %r12, %r13
 8322                             	        shldq	$0x01, %r11, %r12
 8323                             	        andq	%rcx, %r11
 8324                             	        #  Multiply top half by 19
 8325                             	        movq	$19, %rax
 8326                             	        mulq	%r12
 8327                             	        xorq	%r12, %r12
 8328                             	        addq	%rax, %r8
 8329                             	        movq	$19, %rax
 8330                             	        adcq	%rdx, %r12
 8331                             	        mulq	%r13
 8332                             	        xorq	%r13, %r13
 8333                             	        addq	%rax, %r9
 8334                             	        movq	$19, %rax
 8335                             	        adcq	%rdx, %r13
 8336                             	        mulq	%r14
 8337                             	        xorq	%r14, %r14
 8338                             	        addq	%rax, %r10
 8339                             	        movq	$19, %rax
 8340                             	        adcq	%rdx, %r14
 8341                             	        mulq	%r15
 8342                             	        #  Add remaining product results in
 8343                             	        addq	%r12, %r9
 8344                             	        adcq	%r13, %r10
 8345                             	        adcq	%r14, %r11
 8346                             	        adcq	%rax, %r11
 8347                             	        adcq	$0x00, %rdx
 8348                             	        #  Overflow
 8349                             	        shldq	$0x01, %r11, %rdx
 8350                             	        imulq	$19, %rdx, %rax
 8351                             	        andq	%rcx, %r11
 8352                             	        addq	%rax, %r8
 8353                             	        adcq	$0x00, %r9
 8354                             	        adcq	$0x00, %r10
 8355                             	        adcq	$0x00, %r11
 8356                             	        # Reduce if top bit set
 8357                             	        movq	%r11, %rdx
 8358                             	        sarq	$63, %rdx
 8359                             	        andq	$19, %rdx
 8360                             	        andq	%rcx, %r11
 8361                             	        addq	%rdx, %r8
 8362                             	        adcq	$0x00, %r9
 8363                             	        adcq	$0x00, %r10
 8364                             	        adcq	$0x00, %r11
 8365                             	        # Store
 8366                             	        movq	%r8, (%rdi)
 8367                             	        movq	%r9, 8(%rdi)
 8368                             	        movq	%r10, 16(%rdi)
 8369                             	        movq	%r11, 24(%rdi)
 8370                             	        movq	(%rsp), %rdi
 8371                             	        movq	128(%rsp), %rsi
 8372                             	        movq	144(%rsp), %rbx
 8373                             	        # Multiply
 8374                             	        #  A[0] * B[0]
 8375                             	        movq	(%rbx), %rax
 8376                             	        mulq	(%rsi)
 8377                             	        movq	%rax, %r8
 8378                             	        movq	%rdx, %r9
 8379                             	        #  A[0] * B[1]
 8380                             	        movq	8(%rbx), %rax
 8381                             	        mulq	(%rsi)
 8382                             	        xorq	%r10, %r10
 8383                             	        addq	%rax, %r9
 8384                             	        adcq	%rdx, %r10
 8385                             	        #  A[1] * B[0]
 8386                             	        movq	(%rbx), %rax
 8387                             	        mulq	8(%rsi)
 8388                             	        xorq	%r11, %r11
 8389                             	        addq	%rax, %r9
 8390                             	        adcq	%rdx, %r10
 8391                             	        adcq	$0x00, %r11
 8392                             	        #  A[0] * B[2]
 8393                             	        movq	16(%rbx), %rax
 8394                             	        mulq	(%rsi)
 8395                             	        addq	%rax, %r10
 8396                             	        adcq	%rdx, %r11
 8397                             	        #  A[1] * B[1]
 8398                             	        movq	8(%rbx), %rax
 8399                             	        mulq	8(%rsi)
 8400                             	        xorq	%r12, %r12
 8401                             	        addq	%rax, %r10
 8402                             	        adcq	%rdx, %r11
 8403                             	        adcq	$0x00, %r12
 8404                             	        #  A[2] * B[0]
 8405                             	        movq	(%rbx), %rax
 8406                             	        mulq	16(%rsi)
 8407                             	        addq	%rax, %r10
 8408                             	        adcq	%rdx, %r11
 8409                             	        adcq	$0x00, %r12
 8410                             	        #  A[0] * B[3]
 8411                             	        movq	24(%rbx), %rax
 8412                             	        mulq	(%rsi)
 8413                             	        xorq	%r13, %r13
 8414                             	        addq	%rax, %r11
 8415                             	        adcq	%rdx, %r12
 8416                             	        adcq	$0x00, %r13
 8417                             	        #  A[1] * B[2]
 8418                             	        movq	16(%rbx), %rax
 8419                             	        mulq	8(%rsi)
 8420                             	        addq	%rax, %r11
 8421                             	        adcq	%rdx, %r12
 8422                             	        adcq	$0x00, %r13
 8423                             	        #  A[2] * B[1]
 8424                             	        movq	8(%rbx), %rax
 8425                             	        mulq	16(%rsi)
 8426                             	        addq	%rax, %r11
 8427                             	        adcq	%rdx, %r12
 8428                             	        adcq	$0x00, %r13
 8429                             	        #  A[3] * B[0]
 8430                             	        movq	(%rbx), %rax
 8431                             	        mulq	24(%rsi)
 8432                             	        addq	%rax, %r11
 8433                             	        adcq	%rdx, %r12
 8434                             	        adcq	$0x00, %r13
 8435                             	        #  A[1] * B[3]
 8436                             	        movq	24(%rbx), %rax
 8437                             	        mulq	8(%rsi)
 8438                             	        xorq	%r14, %r14
 8439                             	        addq	%rax, %r12
 8440                             	        adcq	%rdx, %r13
 8441                             	        adcq	$0x00, %r14
 8442                             	        #  A[2] * B[2]
 8443                             	        movq	16(%rbx), %rax
 8444                             	        mulq	16(%rsi)
 8445                             	        addq	%rax, %r12
 8446                             	        adcq	%rdx, %r13
 8447                             	        adcq	$0x00, %r14
 8448                             	        #  A[3] * B[1]
 8449                             	        movq	8(%rbx), %rax
 8450                             	        mulq	24(%rsi)
 8451                             	        addq	%rax, %r12
 8452                             	        adcq	%rdx, %r13
 8453                             	        adcq	$0x00, %r14
 8454                             	        #  A[2] * B[3]
 8455                             	        movq	24(%rbx), %rax
 8456                             	        mulq	16(%rsi)
 8457                             	        xorq	%r15, %r15
 8458                             	        addq	%rax, %r13
 8459                             	        adcq	%rdx, %r14
 8460                             	        adcq	$0x00, %r15
 8461                             	        #  A[3] * B[2]
 8462                             	        movq	16(%rbx), %rax
 8463                             	        mulq	24(%rsi)
 8464                             	        addq	%rax, %r13
 8465                             	        adcq	%rdx, %r14
 8466                             	        adcq	$0x00, %r15
 8467                             	        #  A[3] * B[3]
 8468                             	        movq	24(%rbx), %rax
 8469                             	        mulq	24(%rsi)
 8470                             	        addq	%rax, %r14
 8471                             	        adcq	%rdx, %r15
 8472                             	        # Reduce
 8473                             	        movq	$0x7fffffffffffffff, %rcx
 8474                             	        #  Move top half into t4-t7 and remove top bit from t3
 8475                             	        shldq	$0x01, %r14, %r15
 8476                             	        shldq	$0x01, %r13, %r14
 8477                             	        shldq	$0x01, %r12, %r13
 8478                             	        shldq	$0x01, %r11, %r12
 8479                             	        andq	%rcx, %r11
 8480                             	        #  Multiply top half by 19
 8481                             	        movq	$19, %rax
 8482                             	        mulq	%r12
 8483                             	        xorq	%r12, %r12
 8484                             	        addq	%rax, %r8
 8485                             	        movq	$19, %rax
 8486                             	        adcq	%rdx, %r12
 8487                             	        mulq	%r13
 8488                             	        xorq	%r13, %r13
 8489                             	        addq	%rax, %r9
 8490                             	        movq	$19, %rax
 8491                             	        adcq	%rdx, %r13
 8492                             	        mulq	%r14
 8493                             	        xorq	%r14, %r14
 8494                             	        addq	%rax, %r10
 8495                             	        movq	$19, %rax
 8496                             	        adcq	%rdx, %r14
 8497                             	        mulq	%r15
 8498                             	        #  Add remaining product results in
 8499                             	        addq	%r12, %r9
 8500                             	        adcq	%r13, %r10
 8501                             	        adcq	%r14, %r11
 8502                             	        adcq	%rax, %r11
 8503                             	        adcq	$0x00, %rdx
 8504                             	        #  Overflow
 8505                             	        shldq	$0x01, %r11, %rdx
 8506                             	        imulq	$19, %rdx, %rax
 8507                             	        andq	%rcx, %r11
 8508                             	        addq	%rax, %r8
 8509                             	        adcq	$0x00, %r9
 8510                             	        adcq	$0x00, %r10
 8511                             	        adcq	$0x00, %r11
 8512                             	        # Reduce if top bit set
 8513                             	        movq	%r11, %rdx
 8514                             	        sarq	$63, %rdx
 8515                             	        andq	$19, %rdx
 8516                             	        andq	%rcx, %r11
 8517                             	        addq	%rdx, %r8
 8518                             	        adcq	$0x00, %r9
 8519                             	        adcq	$0x00, %r10
 8520                             	        adcq	$0x00, %r11
 8521                             	        # Store
 8522                             	        movq	%r8, (%rdi)
 8523                             	        movq	%r9, 8(%rdi)
 8524                             	        movq	%r10, 16(%rdi)
 8525                             	        movq	%r11, 24(%rdi)
 8526                             	        leaq	48(%rsp), %rdi
 8527                             	        movq	(%rsp), %rsi
 8528                             	        movq	(%rsp), %rbx
 8529                             	        # Add
 8530                             	        movq	(%rsi), %r8
 8531                             	        movq	8(%rsi), %r9
 8532                             	        addq	(%rbx), %r8
 8533                             	        movq	16(%rsi), %r10
 8534                             	        adcq	8(%rbx), %r9
 8535                             	        movq	24(%rsi), %rcx
 8536                             	        adcq	16(%rbx), %r10
 8537                             	        movq	$-19, %rax
 8538                             	        adcq	24(%rbx), %rcx
 8539                             	        movq	$0x7fffffffffffffff, %rdx
 8540                             	        movq	%rcx, %r11
 8541                             	        sarq	$63, %rcx
 8542                             	        #   Mask the modulus
 8543                             	        andq	%rcx, %rax
 8544                             	        andq	%rcx, %rdx
 8545                             	        #   Sub modulus (if overflow)
 8546                             	        subq	%rax, %r8
 8547                             	        sbbq	%rcx, %r9
 8548                             	        sbbq	%rcx, %r10
 8549                             	        sbbq	%rdx, %r11
 8550                             	        movq	%r8, (%rdi)
 8551                             	        movq	%r9, 8(%rdi)
 8552                             	        movq	%r10, 16(%rdi)
 8553                             	        movq	%r11, 24(%rdi)
 8554                             	        movq	(%rsp), %rdi
 8555                             	        movq	16(%rsp), %rsi
 8556                             	        movq	8(%rsp), %rbx
 8557                             	        # Sub
 8558                             	        movq	(%rsi), %r8
 8559                             	        movq	8(%rsi), %r9
 8560                             	        movq	16(%rsi), %r10
 8561                             	        movq	24(%rsi), %r11
 8562                             	        subq	(%rbx), %r8
 8563                             	        movq	$0x00, %rcx
 8564                             	        sbbq	8(%rbx), %r9
 8565                             	        movq	$-19, %rax
 8566                             	        sbbq	16(%rbx), %r10
 8567                             	        movq	$0x7fffffffffffffff, %rdx
 8568                             	        sbbq	24(%rbx), %r11
 8569                             	        sbbq	$0x00, %rcx
 8570                             	        #   Mask the modulus
 8571                             	        andq	%rcx, %rax
 8572                             	        andq	%rcx, %rdx
 8573                             	        #   Add modulus (if underflow)
 8574                             	        addq	%rax, %r8
 8575                             	        adcq	%rcx, %r9
 8576                             	        adcq	%rcx, %r10
 8577                             	        adcq	%rdx, %r11
 8578                             	        movq	%r8, (%rdi)
 8579                             	        movq	%r9, 8(%rdi)
 8580                             	        movq	%r10, 16(%rdi)
 8581                             	        movq	%r11, 24(%rdi)
 8582                             	        movq	8(%rsp), %rdi
 8583                             	        movq	16(%rsp), %rsi
 8584                             	        movq	8(%rsp), %rbx
 8585                             	        # Add
 8586                             	        movq	(%rsi), %r8
 8587                             	        movq	8(%rsi), %r9
 8588                             	        addq	(%rbx), %r8
 8589                             	        movq	16(%rsi), %r10
 8590                             	        adcq	8(%rbx), %r9
 8591                             	        movq	24(%rsi), %rcx
 8592                             	        adcq	16(%rbx), %r10
 8593                             	        movq	$-19, %rax
 8594                             	        adcq	24(%rbx), %rcx
 8595                             	        movq	$0x7fffffffffffffff, %rdx
 8596                             	        movq	%rcx, %r11
 8597                             	        sarq	$63, %rcx
 8598                             	        #   Mask the modulus
 8599                             	        andq	%rcx, %rax
 8600                             	        andq	%rcx, %rdx
 8601                             	        #   Sub modulus (if overflow)
 8602                             	        subq	%rax, %r8
 8603                             	        sbbq	%rcx, %r9
 8604                             	        sbbq	%rcx, %r10
 8605                             	        sbbq	%rdx, %r11
 8606                             	        movq	%r8, (%rdi)
 8607                             	        movq	%r9, 8(%rdi)
 8608                             	        movq	%r10, 16(%rdi)
 8609                             	        movq	%r11, 24(%rdi)
 8610                             	        movq	16(%rsp), %rdi
 8611                             	        leaq	48(%rsp), %rsi
 8612                             	        movq	24(%rsp), %rbx
 8613                             	        # Add
 8614                             	        movq	(%rsi), %r8
 8615                             	        movq	8(%rsi), %r9
 8616                             	        addq	(%rbx), %r8
 8617                             	        movq	16(%rsi), %r10
 8618                             	        adcq	8(%rbx), %r9
 8619                             	        movq	24(%rsi), %rcx
 8620                             	        adcq	16(%rbx), %r10
 8621                             	        movq	$-19, %rax
 8622                             	        adcq	24(%rbx), %rcx
 8623                             	        movq	$0x7fffffffffffffff, %rdx
 8624                             	        movq	%rcx, %r11
 8625                             	        sarq	$63, %rcx
 8626                             	        #   Mask the modulus
 8627                             	        andq	%rcx, %rax
 8628                             	        andq	%rcx, %rdx
 8629                             	        #   Sub modulus (if overflow)
 8630                             	        subq	%rax, %r8
 8631                             	        sbbq	%rcx, %r9
 8632                             	        sbbq	%rcx, %r10
 8633                             	        sbbq	%rdx, %r11
 8634                             	        movq	%r8, (%rdi)
 8635                             	        movq	%r9, 8(%rdi)
 8636                             	        movq	%r10, 16(%rdi)
 8637                             	        movq	%r11, 24(%rdi)
 8638                             	        movq	24(%rsp), %rdi
 8639                             	        leaq	48(%rsp), %rsi
 8640                             	        movq	24(%rsp), %rbx
 8641                             	        # Sub
 8642                             	        movq	(%rsi), %r8
 8643                             	        movq	8(%rsi), %r9
 8644                             	        movq	16(%rsi), %r10
 8645                             	        movq	24(%rsi), %r11
 8646                             	        subq	(%rbx), %r8
 8647                             	        movq	$0x00, %rcx
 8648                             	        sbbq	8(%rbx), %r9
 8649                             	        movq	$-19, %rax
 8650                             	        sbbq	16(%rbx), %r10
 8651                             	        movq	$0x7fffffffffffffff, %rdx
 8652                             	        sbbq	24(%rbx), %r11
 8653                             	        sbbq	$0x00, %rcx
 8654                             	        #   Mask the modulus
 8655                             	        andq	%rcx, %rax
 8656                             	        andq	%rcx, %rdx
 8657                             	        #   Add modulus (if underflow)
 8658                             	        addq	%rax, %r8
 8659                             	        adcq	%rcx, %r9
 8660                             	        adcq	%rcx, %r10
 8661                             	        adcq	%rdx, %r11
 8662                             	        movq	%r8, (%rdi)
 8663                             	        movq	%r9, 8(%rdi)
 8664                             	        movq	%r10, 16(%rdi)
 8665                             	        movq	%r11, 24(%rdi)
 8666                             	        addq	$0x50, %rsp
 8667                             	        popq	%r15
 8668                             	        popq	%r14
 8669                             	        popq	%r13
 8670                             	        popq	%r12
 8671                             	        popq	%rbx
 8672                             	        repz retq
 8673                             	#ifndef __APPLE__
 8675                             	#endif /* __APPLE__ */
 8676                             	#ifndef __APPLE__
 8677                             	.text
 8678                             	.globl	fe_ge_sub_x64
 8680                             	.align	16
 8681                             	fe_ge_sub_x64:
 8682                             	#else
 8683                             	.section	__TEXT,__text
 8684                             	.globl	_fe_ge_sub_x64
 8685                             	.p2align	4
 8686                             	_fe_ge_sub_x64:
 8687                             	#endif /* __APPLE__ */
 8688                             	        pushq	%rbx
 8689                             	        pushq	%r12
 8690                             	        pushq	%r13
 8691                             	        pushq	%r14
 8692                             	        pushq	%r15
 8693                             	        subq	$0x50, %rsp
 8694                             	        movq	%rdi, (%rsp)
 8695                             	        movq	%rsi, 8(%rsp)
 8696                             	        movq	%rdx, 16(%rsp)
 8697                             	        movq	%rcx, 24(%rsp)
 8698                             	        movq	%r8, 32(%rsp)
 8699                             	        movq	%r9, 40(%rsp)
 8700                             	        movq	(%rsp), %rdi
 8701                             	        movq	40(%rsp), %rsi
 8702                             	        movq	32(%rsp), %rbx
 8703                             	        # Add
 8704                             	        movq	(%rsi), %r8
 8705                             	        movq	8(%rsi), %r9
 8706                             	        addq	(%rbx), %r8
 8707                             	        movq	16(%rsi), %r10
 8708                             	        adcq	8(%rbx), %r9
 8709                             	        movq	24(%rsi), %rcx
 8710                             	        adcq	16(%rbx), %r10
 8711                             	        movq	$-19, %rax
 8712                             	        adcq	24(%rbx), %rcx
 8713                             	        movq	$0x7fffffffffffffff, %rdx
 8714                             	        movq	%rcx, %r11
 8715                             	        sarq	$63, %rcx
 8716                             	        #   Mask the modulus
 8717                             	        andq	%rcx, %rax
 8718                             	        andq	%rcx, %rdx
 8719                             	        #   Sub modulus (if overflow)
 8720                             	        subq	%rax, %r8
 8721                             	        sbbq	%rcx, %r9
 8722                             	        sbbq	%rcx, %r10
 8723                             	        sbbq	%rdx, %r11
 8724                             	        movq	%r8, (%rdi)
 8725                             	        movq	%r9, 8(%rdi)
 8726                             	        movq	%r10, 16(%rdi)
 8727                             	        movq	%r11, 24(%rdi)
 8728                             	        movq	8(%rsp), %rdi
 8729                             	        movq	40(%rsp), %rsi
 8730                             	        movq	32(%rsp), %rbx
 8731                             	        # Sub
 8732                             	        movq	(%rsi), %r8
 8733                             	        movq	8(%rsi), %r9
 8734                             	        movq	16(%rsi), %r10
 8735                             	        movq	24(%rsi), %r11
 8736                             	        subq	(%rbx), %r8
 8737                             	        movq	$0x00, %rcx
 8738                             	        sbbq	8(%rbx), %r9
 8739                             	        movq	$-19, %rax
 8740                             	        sbbq	16(%rbx), %r10
 8741                             	        movq	$0x7fffffffffffffff, %rdx
 8742                             	        sbbq	24(%rbx), %r11
 8743                             	        sbbq	$0x00, %rcx
 8744                             	        #   Mask the modulus
 8745                             	        andq	%rcx, %rax
 8746                             	        andq	%rcx, %rdx
 8747                             	        #   Add modulus (if underflow)
 8748                             	        addq	%rax, %r8
 8749                             	        adcq	%rcx, %r9
 8750                             	        adcq	%rcx, %r10
 8751                             	        adcq	%rdx, %r11
 8752                             	        movq	%r8, (%rdi)
 8753                             	        movq	%r9, 8(%rdi)
 8754                             	        movq	%r10, 16(%rdi)
 8755                             	        movq	%r11, 24(%rdi)
 8756                             	        movq	16(%rsp), %rdi
 8757                             	        movq	(%rsp), %rsi
 8758                             	        movq	168(%rsp), %rbx
 8759                             	        # Multiply
 8760                             	        #  A[0] * B[0]
 8761                             	        movq	(%rbx), %rax
 8762                             	        mulq	(%rsi)
 8763                             	        movq	%rax, %r8
 8764                             	        movq	%rdx, %r9
 8765                             	        #  A[0] * B[1]
 8766                             	        movq	8(%rbx), %rax
 8767                             	        mulq	(%rsi)
 8768                             	        xorq	%r10, %r10
 8769                             	        addq	%rax, %r9
 8770                             	        adcq	%rdx, %r10
 8771                             	        #  A[1] * B[0]
 8772                             	        movq	(%rbx), %rax
 8773                             	        mulq	8(%rsi)
 8774                             	        xorq	%r11, %r11
 8775                             	        addq	%rax, %r9
 8776                             	        adcq	%rdx, %r10
 8777                             	        adcq	$0x00, %r11
 8778                             	        #  A[0] * B[2]
 8779                             	        movq	16(%rbx), %rax
 8780                             	        mulq	(%rsi)
 8781                             	        addq	%rax, %r10
 8782                             	        adcq	%rdx, %r11
 8783                             	        #  A[1] * B[1]
 8784                             	        movq	8(%rbx), %rax
 8785                             	        mulq	8(%rsi)
 8786                             	        xorq	%r12, %r12
 8787                             	        addq	%rax, %r10
 8788                             	        adcq	%rdx, %r11
 8789                             	        adcq	$0x00, %r12
 8790                             	        #  A[2] * B[0]
 8791                             	        movq	(%rbx), %rax
 8792                             	        mulq	16(%rsi)
 8793                             	        addq	%rax, %r10
 8794                             	        adcq	%rdx, %r11
 8795                             	        adcq	$0x00, %r12
 8796                             	        #  A[0] * B[3]
 8797                             	        movq	24(%rbx), %rax
 8798                             	        mulq	(%rsi)
 8799                             	        xorq	%r13, %r13
 8800                             	        addq	%rax, %r11
 8801                             	        adcq	%rdx, %r12
 8802                             	        adcq	$0x00, %r13
 8803                             	        #  A[1] * B[2]
 8804                             	        movq	16(%rbx), %rax
 8805                             	        mulq	8(%rsi)
 8806                             	        addq	%rax, %r11
 8807                             	        adcq	%rdx, %r12
 8808                             	        adcq	$0x00, %r13
 8809                             	        #  A[2] * B[1]
 8810                             	        movq	8(%rbx), %rax
 8811                             	        mulq	16(%rsi)
 8812                             	        addq	%rax, %r11
 8813                             	        adcq	%rdx, %r12
 8814                             	        adcq	$0x00, %r13
 8815                             	        #  A[3] * B[0]
 8816                             	        movq	(%rbx), %rax
 8817                             	        mulq	24(%rsi)
 8818                             	        addq	%rax, %r11
 8819                             	        adcq	%rdx, %r12
 8820                             	        adcq	$0x00, %r13
 8821                             	        #  A[1] * B[3]
 8822                             	        movq	24(%rbx), %rax
 8823                             	        mulq	8(%rsi)
 8824                             	        xorq	%r14, %r14
 8825                             	        addq	%rax, %r12
 8826                             	        adcq	%rdx, %r13
 8827                             	        adcq	$0x00, %r14
 8828                             	        #  A[2] * B[2]
 8829                             	        movq	16(%rbx), %rax
 8830                             	        mulq	16(%rsi)
 8831                             	        addq	%rax, %r12
 8832                             	        adcq	%rdx, %r13
 8833                             	        adcq	$0x00, %r14
 8834                             	        #  A[3] * B[1]
 8835                             	        movq	8(%rbx), %rax
 8836                             	        mulq	24(%rsi)
 8837                             	        addq	%rax, %r12
 8838                             	        adcq	%rdx, %r13
 8839                             	        adcq	$0x00, %r14
 8840                             	        #  A[2] * B[3]
 8841                             	        movq	24(%rbx), %rax
 8842                             	        mulq	16(%rsi)
 8843                             	        xorq	%r15, %r15
 8844                             	        addq	%rax, %r13
 8845                             	        adcq	%rdx, %r14
 8846                             	        adcq	$0x00, %r15
 8847                             	        #  A[3] * B[2]
 8848                             	        movq	16(%rbx), %rax
 8849                             	        mulq	24(%rsi)
 8850                             	        addq	%rax, %r13
 8851                             	        adcq	%rdx, %r14
 8852                             	        adcq	$0x00, %r15
 8853                             	        #  A[3] * B[3]
 8854                             	        movq	24(%rbx), %rax
 8855                             	        mulq	24(%rsi)
 8856                             	        addq	%rax, %r14
 8857                             	        adcq	%rdx, %r15
 8858                             	        # Reduce
 8859                             	        movq	$0x7fffffffffffffff, %rcx
 8860                             	        #  Move top half into t4-t7 and remove top bit from t3
 8861                             	        shldq	$0x01, %r14, %r15
 8862                             	        shldq	$0x01, %r13, %r14
 8863                             	        shldq	$0x01, %r12, %r13
 8864                             	        shldq	$0x01, %r11, %r12
 8865                             	        andq	%rcx, %r11
 8866                             	        #  Multiply top half by 19
 8867                             	        movq	$19, %rax
 8868                             	        mulq	%r12
 8869                             	        xorq	%r12, %r12
 8870                             	        addq	%rax, %r8
 8871                             	        movq	$19, %rax
 8872                             	        adcq	%rdx, %r12
 8873                             	        mulq	%r13
 8874                             	        xorq	%r13, %r13
 8875                             	        addq	%rax, %r9
 8876                             	        movq	$19, %rax
 8877                             	        adcq	%rdx, %r13
 8878                             	        mulq	%r14
 8879                             	        xorq	%r14, %r14
 8880                             	        addq	%rax, %r10
 8881                             	        movq	$19, %rax
 8882                             	        adcq	%rdx, %r14
 8883                             	        mulq	%r15
 8884                             	        #  Add remaining product results in
 8885                             	        addq	%r12, %r9
 8886                             	        adcq	%r13, %r10
 8887                             	        adcq	%r14, %r11
 8888                             	        adcq	%rax, %r11
 8889                             	        adcq	$0x00, %rdx
 8890                             	        #  Overflow
 8891                             	        shldq	$0x01, %r11, %rdx
 8892                             	        imulq	$19, %rdx, %rax
 8893                             	        andq	%rcx, %r11
 8894                             	        addq	%rax, %r8
 8895                             	        adcq	$0x00, %r9
 8896                             	        adcq	$0x00, %r10
 8897                             	        adcq	$0x00, %r11
 8898                             	        # Reduce if top bit set
 8899                             	        movq	%r11, %rdx
 8900                             	        sarq	$63, %rdx
 8901                             	        andq	$19, %rdx
 8902                             	        andq	%rcx, %r11
 8903                             	        addq	%rdx, %r8
 8904                             	        adcq	$0x00, %r9
 8905                             	        adcq	$0x00, %r10
 8906                             	        adcq	$0x00, %r11
 8907                             	        # Store
 8908                             	        movq	%r8, (%rdi)
 8909                             	        movq	%r9, 8(%rdi)
 8910                             	        movq	%r10, 16(%rdi)
 8911                             	        movq	%r11, 24(%rdi)
 8912                             	        movq	8(%rsp), %rdi
 8913                             	        movq	8(%rsp), %rsi
 8914                             	        movq	160(%rsp), %rbx
 8915                             	        # Multiply
 8916                             	        #  A[0] * B[0]
 8917                             	        movq	(%rbx), %rax
 8918                             	        mulq	(%rsi)
 8919                             	        movq	%rax, %r8
 8920                             	        movq	%rdx, %r9
 8921                             	        #  A[0] * B[1]
 8922                             	        movq	8(%rbx), %rax
 8923                             	        mulq	(%rsi)
 8924                             	        xorq	%r10, %r10
 8925                             	        addq	%rax, %r9
 8926                             	        adcq	%rdx, %r10
 8927                             	        #  A[1] * B[0]
 8928                             	        movq	(%rbx), %rax
 8929                             	        mulq	8(%rsi)
 8930                             	        xorq	%r11, %r11
 8931                             	        addq	%rax, %r9
 8932                             	        adcq	%rdx, %r10
 8933                             	        adcq	$0x00, %r11
 8934                             	        #  A[0] * B[2]
 8935                             	        movq	16(%rbx), %rax
 8936                             	        mulq	(%rsi)
 8937                             	        addq	%rax, %r10
 8938                             	        adcq	%rdx, %r11
 8939                             	        #  A[1] * B[1]
 8940                             	        movq	8(%rbx), %rax
 8941                             	        mulq	8(%rsi)
 8942                             	        xorq	%r12, %r12
 8943                             	        addq	%rax, %r10
 8944                             	        adcq	%rdx, %r11
 8945                             	        adcq	$0x00, %r12
 8946                             	        #  A[2] * B[0]
 8947                             	        movq	(%rbx), %rax
 8948                             	        mulq	16(%rsi)
 8949                             	        addq	%rax, %r10
 8950                             	        adcq	%rdx, %r11
 8951                             	        adcq	$0x00, %r12
 8952                             	        #  A[0] * B[3]
 8953                             	        movq	24(%rbx), %rax
 8954                             	        mulq	(%rsi)
 8955                             	        xorq	%r13, %r13
 8956                             	        addq	%rax, %r11
 8957                             	        adcq	%rdx, %r12
 8958                             	        adcq	$0x00, %r13
 8959                             	        #  A[1] * B[2]
 8960                             	        movq	16(%rbx), %rax
 8961                             	        mulq	8(%rsi)
 8962                             	        addq	%rax, %r11
 8963                             	        adcq	%rdx, %r12
 8964                             	        adcq	$0x00, %r13
 8965                             	        #  A[2] * B[1]
 8966                             	        movq	8(%rbx), %rax
 8967                             	        mulq	16(%rsi)
 8968                             	        addq	%rax, %r11
 8969                             	        adcq	%rdx, %r12
 8970                             	        adcq	$0x00, %r13
 8971                             	        #  A[3] * B[0]
 8972                             	        movq	(%rbx), %rax
 8973                             	        mulq	24(%rsi)
 8974                             	        addq	%rax, %r11
 8975                             	        adcq	%rdx, %r12
 8976                             	        adcq	$0x00, %r13
 8977                             	        #  A[1] * B[3]
 8978                             	        movq	24(%rbx), %rax
 8979                             	        mulq	8(%rsi)
 8980                             	        xorq	%r14, %r14
 8981                             	        addq	%rax, %r12
 8982                             	        adcq	%rdx, %r13
 8983                             	        adcq	$0x00, %r14
 8984                             	        #  A[2] * B[2]
 8985                             	        movq	16(%rbx), %rax
 8986                             	        mulq	16(%rsi)
 8987                             	        addq	%rax, %r12
 8988                             	        adcq	%rdx, %r13
 8989                             	        adcq	$0x00, %r14
 8990                             	        #  A[3] * B[1]
 8991                             	        movq	8(%rbx), %rax
 8992                             	        mulq	24(%rsi)
 8993                             	        addq	%rax, %r12
 8994                             	        adcq	%rdx, %r13
 8995                             	        adcq	$0x00, %r14
 8996                             	        #  A[2] * B[3]
 8997                             	        movq	24(%rbx), %rax
 8998                             	        mulq	16(%rsi)
 8999                             	        xorq	%r15, %r15
 9000                             	        addq	%rax, %r13
 9001                             	        adcq	%rdx, %r14
 9002                             	        adcq	$0x00, %r15
 9003                             	        #  A[3] * B[2]
 9004                             	        movq	16(%rbx), %rax
 9005                             	        mulq	24(%rsi)
 9006                             	        addq	%rax, %r13
 9007                             	        adcq	%rdx, %r14
 9008                             	        adcq	$0x00, %r15
 9009                             	        #  A[3] * B[3]
 9010                             	        movq	24(%rbx), %rax
 9011                             	        mulq	24(%rsi)
 9012                             	        addq	%rax, %r14
 9013                             	        adcq	%rdx, %r15
 9014                             	        # Reduce
 9015                             	        movq	$0x7fffffffffffffff, %rcx
 9016                             	        #  Move top half into t4-t7 and remove top bit from t3
 9017                             	        shldq	$0x01, %r14, %r15
 9018                             	        shldq	$0x01, %r13, %r14
 9019                             	        shldq	$0x01, %r12, %r13
 9020                             	        shldq	$0x01, %r11, %r12
 9021                             	        andq	%rcx, %r11
 9022                             	        #  Multiply top half by 19
 9023                             	        movq	$19, %rax
 9024                             	        mulq	%r12
 9025                             	        xorq	%r12, %r12
 9026                             	        addq	%rax, %r8
 9027                             	        movq	$19, %rax
 9028                             	        adcq	%rdx, %r12
 9029                             	        mulq	%r13
 9030                             	        xorq	%r13, %r13
 9031                             	        addq	%rax, %r9
 9032                             	        movq	$19, %rax
 9033                             	        adcq	%rdx, %r13
 9034                             	        mulq	%r14
 9035                             	        xorq	%r14, %r14
 9036                             	        addq	%rax, %r10
 9037                             	        movq	$19, %rax
 9038                             	        adcq	%rdx, %r14
 9039                             	        mulq	%r15
 9040                             	        #  Add remaining product results in
 9041                             	        addq	%r12, %r9
 9042                             	        adcq	%r13, %r10
 9043                             	        adcq	%r14, %r11
 9044                             	        adcq	%rax, %r11
 9045                             	        adcq	$0x00, %rdx
 9046                             	        #  Overflow
 9047                             	        shldq	$0x01, %r11, %rdx
 9048                             	        imulq	$19, %rdx, %rax
 9049                             	        andq	%rcx, %r11
 9050                             	        addq	%rax, %r8
 9051                             	        adcq	$0x00, %r9
 9052                             	        adcq	$0x00, %r10
 9053                             	        adcq	$0x00, %r11
 9054                             	        # Reduce if top bit set
 9055                             	        movq	%r11, %rdx
 9056                             	        sarq	$63, %rdx
 9057                             	        andq	$19, %rdx
 9058                             	        andq	%rcx, %r11
 9059                             	        addq	%rdx, %r8
 9060                             	        adcq	$0x00, %r9
 9061                             	        adcq	$0x00, %r10
 9062                             	        adcq	$0x00, %r11
 9063                             	        # Store
 9064                             	        movq	%r8, (%rdi)
 9065                             	        movq	%r9, 8(%rdi)
 9066                             	        movq	%r10, 16(%rdi)
 9067                             	        movq	%r11, 24(%rdi)
 9068                             	        movq	24(%rsp), %rdi
 9069                             	        movq	152(%rsp), %rsi
 9070                             	        movq	136(%rsp), %rbx
 9071                             	        # Multiply
 9072                             	        #  A[0] * B[0]
 9073                             	        movq	(%rbx), %rax
 9074                             	        mulq	(%rsi)
 9075                             	        movq	%rax, %r8
 9076                             	        movq	%rdx, %r9
 9077                             	        #  A[0] * B[1]
 9078                             	        movq	8(%rbx), %rax
 9079                             	        mulq	(%rsi)
 9080                             	        xorq	%r10, %r10
 9081                             	        addq	%rax, %r9
 9082                             	        adcq	%rdx, %r10
 9083                             	        #  A[1] * B[0]
 9084                             	        movq	(%rbx), %rax
 9085                             	        mulq	8(%rsi)
 9086                             	        xorq	%r11, %r11
 9087                             	        addq	%rax, %r9
 9088                             	        adcq	%rdx, %r10
 9089                             	        adcq	$0x00, %r11
 9090                             	        #  A[0] * B[2]
 9091                             	        movq	16(%rbx), %rax
 9092                             	        mulq	(%rsi)
 9093                             	        addq	%rax, %r10
 9094                             	        adcq	%rdx, %r11
 9095                             	        #  A[1] * B[1]
 9096                             	        movq	8(%rbx), %rax
 9097                             	        mulq	8(%rsi)
 9098                             	        xorq	%r12, %r12
 9099                             	        addq	%rax, %r10
 9100                             	        adcq	%rdx, %r11
 9101                             	        adcq	$0x00, %r12
 9102                             	        #  A[2] * B[0]
 9103                             	        movq	(%rbx), %rax
 9104                             	        mulq	16(%rsi)
 9105                             	        addq	%rax, %r10
 9106                             	        adcq	%rdx, %r11
 9107                             	        adcq	$0x00, %r12
 9108                             	        #  A[0] * B[3]
 9109                             	        movq	24(%rbx), %rax
 9110                             	        mulq	(%rsi)
 9111                             	        xorq	%r13, %r13
 9112                             	        addq	%rax, %r11
 9113                             	        adcq	%rdx, %r12
 9114                             	        adcq	$0x00, %r13
 9115                             	        #  A[1] * B[2]
 9116                             	        movq	16(%rbx), %rax
 9117                             	        mulq	8(%rsi)
 9118                             	        addq	%rax, %r11
 9119                             	        adcq	%rdx, %r12
 9120                             	        adcq	$0x00, %r13
 9121                             	        #  A[2] * B[1]
 9122                             	        movq	8(%rbx), %rax
 9123                             	        mulq	16(%rsi)
 9124                             	        addq	%rax, %r11
 9125                             	        adcq	%rdx, %r12
 9126                             	        adcq	$0x00, %r13
 9127                             	        #  A[3] * B[0]
 9128                             	        movq	(%rbx), %rax
 9129                             	        mulq	24(%rsi)
 9130                             	        addq	%rax, %r11
 9131                             	        adcq	%rdx, %r12
 9132                             	        adcq	$0x00, %r13
 9133                             	        #  A[1] * B[3]
 9134                             	        movq	24(%rbx), %rax
 9135                             	        mulq	8(%rsi)
 9136                             	        xorq	%r14, %r14
 9137                             	        addq	%rax, %r12
 9138                             	        adcq	%rdx, %r13
 9139                             	        adcq	$0x00, %r14
 9140                             	        #  A[2] * B[2]
 9141                             	        movq	16(%rbx), %rax
 9142                             	        mulq	16(%rsi)
 9143                             	        addq	%rax, %r12
 9144                             	        adcq	%rdx, %r13
 9145                             	        adcq	$0x00, %r14
 9146                             	        #  A[3] * B[1]
 9147                             	        movq	8(%rbx), %rax
 9148                             	        mulq	24(%rsi)
 9149                             	        addq	%rax, %r12
 9150                             	        adcq	%rdx, %r13
 9151                             	        adcq	$0x00, %r14
 9152                             	        #  A[2] * B[3]
 9153                             	        movq	24(%rbx), %rax
 9154                             	        mulq	16(%rsi)
 9155                             	        xorq	%r15, %r15
 9156                             	        addq	%rax, %r13
 9157                             	        adcq	%rdx, %r14
 9158                             	        adcq	$0x00, %r15
 9159                             	        #  A[3] * B[2]
 9160                             	        movq	16(%rbx), %rax
 9161                             	        mulq	24(%rsi)
 9162                             	        addq	%rax, %r13
 9163                             	        adcq	%rdx, %r14
 9164                             	        adcq	$0x00, %r15
 9165                             	        #  A[3] * B[3]
 9166                             	        movq	24(%rbx), %rax
 9167                             	        mulq	24(%rsi)
 9168                             	        addq	%rax, %r14
 9169                             	        adcq	%rdx, %r15
 9170                             	        # Reduce
 9171                             	        movq	$0x7fffffffffffffff, %rcx
 9172                             	        #  Move top half into t4-t7 and remove top bit from t3
 9173                             	        shldq	$0x01, %r14, %r15
 9174                             	        shldq	$0x01, %r13, %r14
 9175                             	        shldq	$0x01, %r12, %r13
 9176                             	        shldq	$0x01, %r11, %r12
 9177                             	        andq	%rcx, %r11
 9178                             	        #  Multiply top half by 19
 9179                             	        movq	$19, %rax
 9180                             	        mulq	%r12
 9181                             	        xorq	%r12, %r12
 9182                             	        addq	%rax, %r8
 9183                             	        movq	$19, %rax
 9184                             	        adcq	%rdx, %r12
 9185                             	        mulq	%r13
 9186                             	        xorq	%r13, %r13
 9187                             	        addq	%rax, %r9
 9188                             	        movq	$19, %rax
 9189                             	        adcq	%rdx, %r13
 9190                             	        mulq	%r14
 9191                             	        xorq	%r14, %r14
 9192                             	        addq	%rax, %r10
 9193                             	        movq	$19, %rax
 9194                             	        adcq	%rdx, %r14
 9195                             	        mulq	%r15
 9196                             	        #  Add remaining product results in
 9197                             	        addq	%r12, %r9
 9198                             	        adcq	%r13, %r10
 9199                             	        adcq	%r14, %r11
 9200                             	        adcq	%rax, %r11
 9201                             	        adcq	$0x00, %rdx
 9202                             	        #  Overflow
 9203                             	        shldq	$0x01, %r11, %rdx
 9204                             	        imulq	$19, %rdx, %rax
 9205                             	        andq	%rcx, %r11
 9206                             	        addq	%rax, %r8
 9207                             	        adcq	$0x00, %r9
 9208                             	        adcq	$0x00, %r10
 9209                             	        adcq	$0x00, %r11
 9210                             	        # Reduce if top bit set
 9211                             	        movq	%r11, %rdx
 9212                             	        sarq	$63, %rdx
 9213                             	        andq	$19, %rdx
 9214                             	        andq	%rcx, %r11
 9215                             	        addq	%rdx, %r8
 9216                             	        adcq	$0x00, %r9
 9217                             	        adcq	$0x00, %r10
 9218                             	        adcq	$0x00, %r11
 9219                             	        # Store
 9220                             	        movq	%r8, (%rdi)
 9221                             	        movq	%r9, 8(%rdi)
 9222                             	        movq	%r10, 16(%rdi)
 9223                             	        movq	%r11, 24(%rdi)
 9224                             	        movq	(%rsp), %rdi
 9225                             	        movq	128(%rsp), %rsi
 9226                             	        movq	144(%rsp), %rbx
 9227                             	        # Multiply
 9228                             	        #  A[0] * B[0]
 9229                             	        movq	(%rbx), %rax
 9230                             	        mulq	(%rsi)
 9231                             	        movq	%rax, %r8
 9232                             	        movq	%rdx, %r9
 9233                             	        #  A[0] * B[1]
 9234                             	        movq	8(%rbx), %rax
 9235                             	        mulq	(%rsi)
 9236                             	        xorq	%r10, %r10
 9237                             	        addq	%rax, %r9
 9238                             	        adcq	%rdx, %r10
 9239                             	        #  A[1] * B[0]
 9240                             	        movq	(%rbx), %rax
 9241                             	        mulq	8(%rsi)
 9242                             	        xorq	%r11, %r11
 9243                             	        addq	%rax, %r9
 9244                             	        adcq	%rdx, %r10
 9245                             	        adcq	$0x00, %r11
 9246                             	        #  A[0] * B[2]
 9247                             	        movq	16(%rbx), %rax
 9248                             	        mulq	(%rsi)
 9249                             	        addq	%rax, %r10
 9250                             	        adcq	%rdx, %r11
 9251                             	        #  A[1] * B[1]
 9252                             	        movq	8(%rbx), %rax
 9253                             	        mulq	8(%rsi)
 9254                             	        xorq	%r12, %r12
 9255                             	        addq	%rax, %r10
 9256                             	        adcq	%rdx, %r11
 9257                             	        adcq	$0x00, %r12
 9258                             	        #  A[2] * B[0]
 9259                             	        movq	(%rbx), %rax
 9260                             	        mulq	16(%rsi)
 9261                             	        addq	%rax, %r10
 9262                             	        adcq	%rdx, %r11
 9263                             	        adcq	$0x00, %r12
 9264                             	        #  A[0] * B[3]
 9265                             	        movq	24(%rbx), %rax
 9266                             	        mulq	(%rsi)
 9267                             	        xorq	%r13, %r13
 9268                             	        addq	%rax, %r11
 9269                             	        adcq	%rdx, %r12
 9270                             	        adcq	$0x00, %r13
 9271                             	        #  A[1] * B[2]
 9272                             	        movq	16(%rbx), %rax
 9273                             	        mulq	8(%rsi)
 9274                             	        addq	%rax, %r11
 9275                             	        adcq	%rdx, %r12
 9276                             	        adcq	$0x00, %r13
 9277                             	        #  A[2] * B[1]
 9278                             	        movq	8(%rbx), %rax
 9279                             	        mulq	16(%rsi)
 9280                             	        addq	%rax, %r11
 9281                             	        adcq	%rdx, %r12
 9282                             	        adcq	$0x00, %r13
 9283                             	        #  A[3] * B[0]
 9284                             	        movq	(%rbx), %rax
 9285                             	        mulq	24(%rsi)
 9286                             	        addq	%rax, %r11
 9287                             	        adcq	%rdx, %r12
 9288                             	        adcq	$0x00, %r13
 9289                             	        #  A[1] * B[3]
 9290                             	        movq	24(%rbx), %rax
 9291                             	        mulq	8(%rsi)
 9292                             	        xorq	%r14, %r14
 9293                             	        addq	%rax, %r12
 9294                             	        adcq	%rdx, %r13
 9295                             	        adcq	$0x00, %r14
 9296                             	        #  A[2] * B[2]
 9297                             	        movq	16(%rbx), %rax
 9298                             	        mulq	16(%rsi)
 9299                             	        addq	%rax, %r12
 9300                             	        adcq	%rdx, %r13
 9301                             	        adcq	$0x00, %r14
 9302                             	        #  A[3] * B[1]
 9303                             	        movq	8(%rbx), %rax
 9304                             	        mulq	24(%rsi)
 9305                             	        addq	%rax, %r12
 9306                             	        adcq	%rdx, %r13
 9307                             	        adcq	$0x00, %r14
 9308                             	        #  A[2] * B[3]
 9309                             	        movq	24(%rbx), %rax
 9310                             	        mulq	16(%rsi)
 9311                             	        xorq	%r15, %r15
 9312                             	        addq	%rax, %r13
 9313                             	        adcq	%rdx, %r14
 9314                             	        adcq	$0x00, %r15
 9315                             	        #  A[3] * B[2]
 9316                             	        movq	16(%rbx), %rax
 9317                             	        mulq	24(%rsi)
 9318                             	        addq	%rax, %r13
 9319                             	        adcq	%rdx, %r14
 9320                             	        adcq	$0x00, %r15
 9321                             	        #  A[3] * B[3]
 9322                             	        movq	24(%rbx), %rax
 9323                             	        mulq	24(%rsi)
 9324                             	        addq	%rax, %r14
 9325                             	        adcq	%rdx, %r15
 9326                             	        # Reduce
 9327                             	        movq	$0x7fffffffffffffff, %rcx
 9328                             	        #  Move top half into t4-t7 and remove top bit from t3
 9329                             	        shldq	$0x01, %r14, %r15
 9330                             	        shldq	$0x01, %r13, %r14
 9331                             	        shldq	$0x01, %r12, %r13
 9332                             	        shldq	$0x01, %r11, %r12
 9333                             	        andq	%rcx, %r11
 9334                             	        #  Multiply top half by 19
 9335                             	        movq	$19, %rax
 9336                             	        mulq	%r12
 9337                             	        xorq	%r12, %r12
 9338                             	        addq	%rax, %r8
 9339                             	        movq	$19, %rax
 9340                             	        adcq	%rdx, %r12
 9341                             	        mulq	%r13
 9342                             	        xorq	%r13, %r13
 9343                             	        addq	%rax, %r9
 9344                             	        movq	$19, %rax
 9345                             	        adcq	%rdx, %r13
 9346                             	        mulq	%r14
 9347                             	        xorq	%r14, %r14
 9348                             	        addq	%rax, %r10
 9349                             	        movq	$19, %rax
 9350                             	        adcq	%rdx, %r14
 9351                             	        mulq	%r15
 9352                             	        #  Add remaining product results in
 9353                             	        addq	%r12, %r9
 9354                             	        adcq	%r13, %r10
 9355                             	        adcq	%r14, %r11
 9356                             	        adcq	%rax, %r11
 9357                             	        adcq	$0x00, %rdx
 9358                             	        #  Overflow
 9359                             	        shldq	$0x01, %r11, %rdx
 9360                             	        imulq	$19, %rdx, %rax
 9361                             	        andq	%rcx, %r11
 9362                             	        addq	%rax, %r8
 9363                             	        adcq	$0x00, %r9
 9364                             	        adcq	$0x00, %r10
 9365                             	        adcq	$0x00, %r11
 9366                             	        # Reduce if top bit set
 9367                             	        movq	%r11, %rdx
 9368                             	        sarq	$63, %rdx
 9369                             	        andq	$19, %rdx
 9370                             	        andq	%rcx, %r11
 9371                             	        addq	%rdx, %r8
 9372                             	        adcq	$0x00, %r9
 9373                             	        adcq	$0x00, %r10
 9374                             	        adcq	$0x00, %r11
 9375                             	        # Store
 9376                             	        movq	%r8, (%rdi)
 9377                             	        movq	%r9, 8(%rdi)
 9378                             	        movq	%r10, 16(%rdi)
 9379                             	        movq	%r11, 24(%rdi)
 9380                             	        leaq	48(%rsp), %rdi
 9381                             	        movq	(%rsp), %rsi
 9382                             	        movq	(%rsp), %rbx
 9383                             	        # Add
 9384                             	        movq	(%rsi), %r8
 9385                             	        movq	8(%rsi), %r9
 9386                             	        addq	(%rbx), %r8
 9387                             	        movq	16(%rsi), %r10
 9388                             	        adcq	8(%rbx), %r9
 9389                             	        movq	24(%rsi), %rcx
 9390                             	        adcq	16(%rbx), %r10
 9391                             	        movq	$-19, %rax
 9392                             	        adcq	24(%rbx), %rcx
 9393                             	        movq	$0x7fffffffffffffff, %rdx
 9394                             	        movq	%rcx, %r11
 9395                             	        sarq	$63, %rcx
 9396                             	        #   Mask the modulus
 9397                             	        andq	%rcx, %rax
 9398                             	        andq	%rcx, %rdx
 9399                             	        #   Sub modulus (if overflow)
 9400                             	        subq	%rax, %r8
 9401                             	        sbbq	%rcx, %r9
 9402                             	        sbbq	%rcx, %r10
 9403                             	        sbbq	%rdx, %r11
 9404                             	        movq	%r8, (%rdi)
 9405                             	        movq	%r9, 8(%rdi)
 9406                             	        movq	%r10, 16(%rdi)
 9407                             	        movq	%r11, 24(%rdi)
 9408                             	        movq	(%rsp), %rdi
 9409                             	        movq	16(%rsp), %rsi
 9410                             	        movq	8(%rsp), %rbx
 9411                             	        # Sub
 9412                             	        movq	(%rsi), %r8
 9413                             	        movq	8(%rsi), %r9
 9414                             	        movq	16(%rsi), %r10
 9415                             	        movq	24(%rsi), %r11
 9416                             	        subq	(%rbx), %r8
 9417                             	        movq	$0x00, %rcx
 9418                             	        sbbq	8(%rbx), %r9
 9419                             	        movq	$-19, %rax
 9420                             	        sbbq	16(%rbx), %r10
 9421                             	        movq	$0x7fffffffffffffff, %rdx
 9422                             	        sbbq	24(%rbx), %r11
 9423                             	        sbbq	$0x00, %rcx
 9424                             	        #   Mask the modulus
 9425                             	        andq	%rcx, %rax
 9426                             	        andq	%rcx, %rdx
 9427                             	        #   Add modulus (if underflow)
 9428                             	        addq	%rax, %r8
 9429                             	        adcq	%rcx, %r9
 9430                             	        adcq	%rcx, %r10
 9431                             	        adcq	%rdx, %r11
 9432                             	        movq	%r8, (%rdi)
 9433                             	        movq	%r9, 8(%rdi)
 9434                             	        movq	%r10, 16(%rdi)
 9435                             	        movq	%r11, 24(%rdi)
 9436                             	        movq	8(%rsp), %rdi
 9437                             	        movq	16(%rsp), %rsi
 9438                             	        movq	8(%rsp), %rbx
 9439                             	        # Add
 9440                             	        movq	(%rsi), %r8
 9441                             	        movq	8(%rsi), %r9
 9442                             	        addq	(%rbx), %r8
 9443                             	        movq	16(%rsi), %r10
 9444                             	        adcq	8(%rbx), %r9
 9445                             	        movq	24(%rsi), %rcx
 9446                             	        adcq	16(%rbx), %r10
 9447                             	        movq	$-19, %rax
 9448                             	        adcq	24(%rbx), %rcx
 9449                             	        movq	$0x7fffffffffffffff, %rdx
 9450                             	        movq	%rcx, %r11
 9451                             	        sarq	$63, %rcx
 9452                             	        #   Mask the modulus
 9453                             	        andq	%rcx, %rax
 9454                             	        andq	%rcx, %rdx
 9455                             	        #   Sub modulus (if overflow)
 9456                             	        subq	%rax, %r8
 9457                             	        sbbq	%rcx, %r9
 9458                             	        sbbq	%rcx, %r10
 9459                             	        sbbq	%rdx, %r11
 9460                             	        movq	%r8, (%rdi)
 9461                             	        movq	%r9, 8(%rdi)
 9462                             	        movq	%r10, 16(%rdi)
 9463                             	        movq	%r11, 24(%rdi)
 9464                             	        movq	16(%rsp), %rdi
 9465                             	        leaq	48(%rsp), %rsi
 9466                             	        movq	24(%rsp), %rbx
 9467                             	        # Sub
 9468                             	        movq	(%rsi), %r8
 9469                             	        movq	8(%rsi), %r9
 9470                             	        movq	16(%rsi), %r10
 9471                             	        movq	24(%rsi), %r11
 9472                             	        subq	(%rbx), %r8
 9473                             	        movq	$0x00, %rcx
 9474                             	        sbbq	8(%rbx), %r9
 9475                             	        movq	$-19, %rax
 9476                             	        sbbq	16(%rbx), %r10
 9477                             	        movq	$0x7fffffffffffffff, %rdx
 9478                             	        sbbq	24(%rbx), %r11
 9479                             	        sbbq	$0x00, %rcx
 9480                             	        #   Mask the modulus
 9481                             	        andq	%rcx, %rax
 9482                             	        andq	%rcx, %rdx
 9483                             	        #   Add modulus (if underflow)
 9484                             	        addq	%rax, %r8
 9485                             	        adcq	%rcx, %r9
 9486                             	        adcq	%rcx, %r10
 9487                             	        adcq	%rdx, %r11
 9488                             	        movq	%r8, (%rdi)
 9489                             	        movq	%r9, 8(%rdi)
 9490                             	        movq	%r10, 16(%rdi)
 9491                             	        movq	%r11, 24(%rdi)
 9492                             	        movq	24(%rsp), %rdi
 9493                             	        leaq	48(%rsp), %rsi
 9494                             	        movq	24(%rsp), %rbx
 9495                             	        # Add
 9496                             	        movq	(%rsi), %r8
 9497                             	        movq	8(%rsi), %r9
 9498                             	        addq	(%rbx), %r8
 9499                             	        movq	16(%rsi), %r10
 9500                             	        adcq	8(%rbx), %r9
 9501                             	        movq	24(%rsi), %rcx
 9502                             	        adcq	16(%rbx), %r10
 9503                             	        movq	$-19, %rax
 9504                             	        adcq	24(%rbx), %rcx
 9505                             	        movq	$0x7fffffffffffffff, %rdx
 9506                             	        movq	%rcx, %r11
 9507                             	        sarq	$63, %rcx
 9508                             	        #   Mask the modulus
 9509                             	        andq	%rcx, %rax
 9510                             	        andq	%rcx, %rdx
 9511                             	        #   Sub modulus (if overflow)
 9512                             	        subq	%rax, %r8
 9513                             	        sbbq	%rcx, %r9
 9514                             	        sbbq	%rcx, %r10
 9515                             	        sbbq	%rdx, %r11
 9516                             	        movq	%r8, (%rdi)
 9517                             	        movq	%r9, 8(%rdi)
 9518                             	        movq	%r10, 16(%rdi)
 9519                             	        movq	%r11, 24(%rdi)
 9520                             	        addq	$0x50, %rsp
 9521                             	        popq	%r15
 9522                             	        popq	%r14
 9523                             	        popq	%r13
 9524                             	        popq	%r12
 9525                             	        popq	%rbx
 9526                             	        repz retq
 9527                             	#ifndef __APPLE__
 9529                             	#endif /* __APPLE__ */
 9530                             	#ifdef HAVE_INTEL_AVX2
 9531                             	#ifndef __APPLE__
 9532                             	.text
 9533                             	.globl	fe_mul_avx2
 9535                             	.align	16
 9536                             	fe_mul_avx2:
 9537                             	#else
 9538                             	.section	__TEXT,__text
 9539                             	.globl	_fe_mul_avx2
 9540                             	.p2align	4
 9541                             	_fe_mul_avx2:
 9542                             	#endif /* __APPLE__ */
 9543                             	        pushq	%r12
 9544                             	        pushq	%r13
 9545                             	        pushq	%r14
 9546                             	        pushq	%r15
 9547                             	        pushq	%rbx
 9548                             	        movq	%rdx, %rbx
 9549                             	        # Multiply
 9550                             	        # A[0] * B[0]
 9551                             	        movq	(%rbx), %rdx
 9552                             	        mulxq	(%rsi), %r8, %r9
 9553                             	        # A[2] * B[0]
 9554                             	        mulxq	16(%rsi), %r10, %r11
 9555                             	        # A[1] * B[0]
 9556                             	        mulxq	8(%rsi), %rax, %rcx
 9557                             	        xorq	%r15, %r15
 9558                             	        adcxq	%rax, %r9
 9559                             	        # A[1] * B[3]
 9560                             	        movq	24(%rbx), %rdx
 9561                             	        mulxq	8(%rsi), %r12, %r13
 9562                             	        adcxq	%rcx, %r10
 9563                             	        # A[0] * B[1]
 9564                             	        movq	8(%rbx), %rdx
 9565                             	        mulxq	(%rsi), %rax, %rcx
 9566                             	        adoxq	%rax, %r9
 9567                             	        # A[2] * B[1]
 9568                             	        mulxq	16(%rsi), %rax, %r14
 9569                             	        adoxq	%rcx, %r10
 9570                             	        adcxq	%rax, %r11
 9571                             	        # A[1] * B[2]
 9572                             	        movq	16(%rbx), %rdx
 9573                             	        mulxq	8(%rsi), %rax, %rcx
 9574                             	        adcxq	%r14, %r12
 9575                             	        adoxq	%rax, %r11
 9576                             	        adcxq	%r15, %r13
 9577                             	        adoxq	%rcx, %r12
 9578                             	        # A[0] * B[2]
 9579                             	        mulxq	(%rsi), %rax, %rcx
 9580                             	        adoxq	%r15, %r13
 9581                             	        xorq	%r14, %r14
 9582                             	        adcxq	%rax, %r10
 9583                             	        # A[1] * B[1]
 9584                             	        movq	8(%rbx), %rdx
 9585                             	        mulxq	8(%rsi), %rdx, %rax
 9586                             	        adcxq	%rcx, %r11
 9587                             	        adoxq	%rdx, %r10
 9588                             	        # A[3] * B[1]
 9589                             	        movq	8(%rbx), %rdx
 9590                             	        adoxq	%rax, %r11
 9591                             	        mulxq	24(%rsi), %rax, %rcx
 9592                             	        adcxq	%rax, %r12
 9593                             	        # A[2] * B[2]
 9594                             	        movq	16(%rbx), %rdx
 9595                             	        mulxq	16(%rsi), %rdx, %rax
 9596                             	        adcxq	%rcx, %r13
 9597                             	        adoxq	%rdx, %r12
 9598                             	        # A[3] * B[3]
 9599                             	        movq	24(%rbx), %rdx
 9600                             	        adoxq	%rax, %r13
 9601                             	        mulxq	24(%rsi), %rax, %rcx
 9602                             	        adoxq	%r15, %r14
 9603                             	        adcxq	%rax, %r14
 9604                             	        # A[0] * B[3]
 9605                             	        mulxq	(%rsi), %rdx, %rax
 9606                             	        adcxq	%rcx, %r15
 9607                             	        xorq	%rcx, %rcx
 9608                             	        adcxq	%rdx, %r11
 9609                             	        # A[3] * B[0]
 9610                             	        movq	(%rbx), %rdx
 9611                             	        adcxq	%rax, %r12
 9612                             	        mulxq	24(%rsi), %rdx, %rax
 9613                             	        adoxq	%rdx, %r11
 9614                             	        adoxq	%rax, %r12
 9615                             	        # A[2] * B[3]
 9616                             	        movq	24(%rbx), %rdx
 9617                             	        mulxq	16(%rsi), %rdx, %rax
 9618                             	        adcxq	%rdx, %r13
 9619                             	        # A[3] * B[2]
 9620                             	        movq	16(%rbx), %rdx
 9621                             	        adcxq	%rax, %r14
 9622                             	        mulxq	24(%rsi), %rax, %rdx
 9623                             	        adcxq	%rcx, %r15
 9624                             	        adoxq	%rax, %r13
 9625                             	        adoxq	%rdx, %r14
 9626                             	        adoxq	%rcx, %r15
 9627                             	        # Reduce
 9628                             	        movq	$0x7fffffffffffffff, %rcx
 9629                             	        #  Move top half into t4-t7 and remove top bit from t3
 9630                             	        shldq	$0x01, %r14, %r15
 9631                             	        shldq	$0x01, %r13, %r14
 9632                             	        shldq	$0x01, %r12, %r13
 9633                             	        shldq	$0x01, %r11, %r12
 9634                             	        andq	%rcx, %r11
 9635                             	        #  Multiply top half by 19
 9636                             	        movq	$19, %rdx
 9637                             	        xorq	%rcx, %rcx
 9638                             	        mulxq	%r12, %rax, %r12
 9639                             	        adcxq	%rax, %r8
 9640                             	        adoxq	%r12, %r9
 9641                             	        mulxq	%r13, %rax, %r13
 9642                             	        adcxq	%rax, %r9
 9643                             	        adoxq	%r13, %r10
 9644                             	        mulxq	%r14, %rax, %r14
 9645                             	        adcxq	%rax, %r10
 9646                             	        adoxq	%r14, %r11
 9647                             	        mulxq	%r15, %r15, %rdx
 9648                             	        adcxq	%r15, %r11
 9649                             	        adoxq	%rcx, %rdx
 9650                             	        adcxq	%rcx, %rdx
 9651                             	        #  Overflow
 9652                             	        shldq	$0x01, %r11, %rdx
 9653                             	        movq	$0x7fffffffffffffff, %rcx
 9654                             	        imulq	$19, %rdx, %rax
 9655                             	        andq	%rcx, %r11
 9656                             	        addq	%rax, %r8
 9657                             	        adcq	$0x00, %r9
 9658                             	        adcq	$0x00, %r10
 9659                             	        adcq	$0x00, %r11
 9660                             	        # Reduce if top bit set
 9661                             	        movq	%r11, %rdx
 9662                             	        sarq	$63, %rdx
 9663                             	        andq	$19, %rdx
 9664                             	        andq	%rcx, %r11
 9665                             	        addq	%rdx, %r8
 9666                             	        adcq	$0x00, %r9
 9667                             	        adcq	$0x00, %r10
 9668                             	        adcq	$0x00, %r11
 9669                             	        # Store
 9670                             	        movq	%r8, (%rdi)
 9671                             	        movq	%r9, 8(%rdi)
 9672                             	        movq	%r10, 16(%rdi)
 9673                             	        movq	%r11, 24(%rdi)
 9674                             	        popq	%rbx
 9675                             	        popq	%r15
 9676                             	        popq	%r14
 9677                             	        popq	%r13
 9678                             	        popq	%r12
 9679                             	        repz retq
 9680                             	#ifndef __APPLE__
 9682                             	#endif /* __APPLE__ */
 9683                             	#ifndef __APPLE__
 9684                             	.text
 9685                             	.globl	fe_sq_avx2
 9687                             	.align	16
 9688                             	fe_sq_avx2:
 9689                             	#else
 9690                             	.section	__TEXT,__text
 9691                             	.globl	_fe_sq_avx2
 9692                             	.p2align	4
 9693                             	_fe_sq_avx2:
 9694                             	#endif /* __APPLE__ */
 9695                             	        pushq	%rbx
 9696                             	        pushq	%r12
 9697                             	        pushq	%r13
 9698                             	        pushq	%r14
 9699                             	        pushq	%r15
 9700                             	        # Square
 9701                             	        # A[0] * A[1]
 9702                             	        movq	(%rsi), %rdx
 9703                             	        mulxq	8(%rsi), %r9, %r10
 9704                             	        # A[0] * A[3]
 9705                             	        mulxq	24(%rsi), %r11, %r12
 9706                             	        # A[2] * A[1]
 9707                             	        movq	16(%rsi), %rdx
 9708                             	        mulxq	8(%rsi), %rcx, %rbx
 9709                             	        xorq	%r15, %r15
 9710                             	        adoxq	%rcx, %r11
 9711                             	        # A[2] * A[3]
 9712                             	        mulxq	24(%rsi), %r13, %r14
 9713                             	        adoxq	%rbx, %r12
 9714                             	        # A[2] * A[0]
 9715                             	        mulxq	(%rsi), %rcx, %rbx
 9716                             	        adoxq	%r15, %r13
 9717                             	        adcxq	%rcx, %r10
 9718                             	        adoxq	%r15, %r14
 9719                             	        # A[1] * A[3]
 9720                             	        movq	8(%rsi), %rdx
 9721                             	        mulxq	24(%rsi), %rax, %r8
 9722                             	        adcxq	%rbx, %r11
 9723                             	        adcxq	%rax, %r12
 9724                             	        adcxq	%r8, %r13
 9725                             	        adcxq	%r15, %r14
 9726                             	        # Double with Carry Flag
 9727                             	        xorq	%r15, %r15
 9728                             	        # A[0] * A[0]
 9729                             	        movq	(%rsi), %rdx
 9730                             	        mulxq	%rdx, %r8, %rax
 9731                             	        adcxq	%r9, %r9
 9732                             	        # A[1] * A[1]
 9733                             	        movq	8(%rsi), %rdx
 9734                             	        mulxq	%rdx, %rcx, %rbx
 9735                             	        adcxq	%r10, %r10
 9736                             	        adoxq	%rax, %r9
 9737                             	        adcxq	%r11, %r11
 9738                             	        adoxq	%rcx, %r10
 9739                             	        # A[2] * A[2]
 9740                             	        movq	16(%rsi), %rdx
 9741                             	        mulxq	%rdx, %rax, %rcx
 9742                             	        adcxq	%r12, %r12
 9743                             	        adoxq	%rbx, %r11
 9744                             	        adcxq	%r13, %r13
 9745                             	        adoxq	%rax, %r12
 9746                             	        # A[3] * A[3]
 9747                             	        movq	24(%rsi), %rdx
 9748                             	        mulxq	%rdx, %rax, %rbx
 9749                             	        adcxq	%r14, %r14
 9750                             	        adoxq	%rcx, %r13
 9751                             	        adcxq	%r15, %r15
 9752                             	        adoxq	%rax, %r14
 9753                             	        adoxq	%rbx, %r15
 9754                             	        # Reduce
 9755                             	        movq	$0x7fffffffffffffff, %rcx
 9756                             	        #  Move top half into t4-t7 and remove top bit from t3
 9757                             	        shldq	$0x01, %r14, %r15
 9758                             	        shldq	$0x01, %r13, %r14
 9759                             	        shldq	$0x01, %r12, %r13
 9760                             	        shldq	$0x01, %r11, %r12
 9761                             	        andq	%rcx, %r11
 9762                             	        #  Multiply top half by 19
 9763                             	        movq	$19, %rdx
 9764                             	        xorq	%rcx, %rcx
 9765                             	        mulxq	%r12, %rax, %r12
 9766                             	        adcxq	%rax, %r8
 9767                             	        adoxq	%r12, %r9
 9768                             	        mulxq	%r13, %rax, %r13
 9769                             	        adcxq	%rax, %r9
 9770                             	        adoxq	%r13, %r10
 9771                             	        mulxq	%r14, %rax, %r14
 9772                             	        adcxq	%rax, %r10
 9773                             	        adoxq	%r14, %r11
 9774                             	        mulxq	%r15, %r15, %rdx
 9775                             	        adcxq	%r15, %r11
 9776                             	        adoxq	%rcx, %rdx
 9777                             	        adcxq	%rcx, %rdx
 9778                             	        #  Overflow
 9779                             	        shldq	$0x01, %r11, %rdx
 9780                             	        movq	$0x7fffffffffffffff, %rcx
 9781                             	        imulq	$19, %rdx, %rax
 9782                             	        andq	%rcx, %r11
 9783                             	        addq	%rax, %r8
 9784                             	        adcq	$0x00, %r9
 9785                             	        adcq	$0x00, %r10
 9786                             	        adcq	$0x00, %r11
 9787                             	        # Reduce if top bit set
 9788                             	        movq	%r11, %rdx
 9789                             	        sarq	$63, %rdx
 9790                             	        andq	$19, %rdx
 9791                             	        andq	%rcx, %r11
 9792                             	        addq	%rdx, %r8
 9793                             	        adcq	$0x00, %r9
 9794                             	        adcq	$0x00, %r10
 9795                             	        adcq	$0x00, %r11
 9796                             	        # Store
 9797                             	        movq	%r8, (%rdi)
 9798                             	        movq	%r9, 8(%rdi)
 9799                             	        movq	%r10, 16(%rdi)
 9800                             	        movq	%r11, 24(%rdi)
 9801                             	        popq	%r15
 9802                             	        popq	%r14
 9803                             	        popq	%r13
 9804                             	        popq	%r12
 9805                             	        popq	%rbx
 9806                             	        repz retq
 9807                             	#ifndef __APPLE__
 9809                             	#endif /* __APPLE__ */
 9810                             	#ifndef __APPLE__
 9811                             	.text
 9812                             	.globl	fe_sq_n_avx2
 9814                             	.align	16
 9815                             	fe_sq_n_avx2:
 9816                             	#else
 9817                             	.section	__TEXT,__text
 9818                             	.globl	_fe_sq_n_avx2
 9819                             	.p2align	4
 9820                             	_fe_sq_n_avx2:
 9821                             	#endif /* __APPLE__ */
 9822                             	        pushq	%rbx
 9823                             	        pushq	%r12
 9824                             	        pushq	%r13
 9825                             	        pushq	%r14
 9826                             	        pushq	%r15
 9827                             	        pushq	%rbp
 9828                             	        movq	%rdx, %rbp
 9829                             	L_fe_sq_n_avx2:
 9830                             	        # Square
 9831                             	        # A[0] * A[1]
 9832                             	        movq	(%rsi), %rdx
 9833                             	        mulxq	8(%rsi), %r9, %r10
 9834                             	        # A[0] * A[3]
 9835                             	        mulxq	24(%rsi), %r11, %r12
 9836                             	        # A[2] * A[1]
 9837                             	        movq	16(%rsi), %rdx
 9838                             	        mulxq	8(%rsi), %rcx, %rbx
 9839                             	        xorq	%r15, %r15
 9840                             	        adoxq	%rcx, %r11
 9841                             	        # A[2] * A[3]
 9842                             	        mulxq	24(%rsi), %r13, %r14
 9843                             	        adoxq	%rbx, %r12
 9844                             	        # A[2] * A[0]
 9845                             	        mulxq	(%rsi), %rcx, %rbx
 9846                             	        adoxq	%r15, %r13
 9847                             	        adcxq	%rcx, %r10
 9848                             	        adoxq	%r15, %r14
 9849                             	        # A[1] * A[3]
 9850                             	        movq	8(%rsi), %rdx
 9851                             	        mulxq	24(%rsi), %rax, %r8
 9852                             	        adcxq	%rbx, %r11
 9853                             	        adcxq	%rax, %r12
 9854                             	        adcxq	%r8, %r13
 9855                             	        adcxq	%r15, %r14
 9856                             	        # Double with Carry Flag
 9857                             	        xorq	%r15, %r15
 9858                             	        # A[0] * A[0]
 9859                             	        movq	(%rsi), %rdx
 9860                             	        mulxq	%rdx, %r8, %rax
 9861                             	        adcxq	%r9, %r9
 9862                             	        # A[1] * A[1]
 9863                             	        movq	8(%rsi), %rdx
 9864                             	        mulxq	%rdx, %rcx, %rbx
 9865                             	        adcxq	%r10, %r10
 9866                             	        adoxq	%rax, %r9
 9867                             	        adcxq	%r11, %r11
 9868                             	        adoxq	%rcx, %r10
 9869                             	        # A[2] * A[2]
 9870                             	        movq	16(%rsi), %rdx
 9871                             	        mulxq	%rdx, %rax, %rcx
 9872                             	        adcxq	%r12, %r12
 9873                             	        adoxq	%rbx, %r11
 9874                             	        adcxq	%r13, %r13
 9875                             	        adoxq	%rax, %r12
 9876                             	        # A[3] * A[3]
 9877                             	        movq	24(%rsi), %rdx
 9878                             	        mulxq	%rdx, %rax, %rbx
 9879                             	        adcxq	%r14, %r14
 9880                             	        adoxq	%rcx, %r13
 9881                             	        adcxq	%r15, %r15
 9882                             	        adoxq	%rax, %r14
 9883                             	        adoxq	%rbx, %r15
 9884                             	        # Reduce
 9885                             	        movq	$0x7fffffffffffffff, %rcx
 9886                             	        #  Move top half into t4-t7 and remove top bit from t3
 9887                             	        shldq	$0x01, %r14, %r15
 9888                             	        shldq	$0x01, %r13, %r14
 9889                             	        shldq	$0x01, %r12, %r13
 9890                             	        shldq	$0x01, %r11, %r12
 9891                             	        andq	%rcx, %r11
 9892                             	        #  Multiply top half by 19
 9893                             	        movq	$19, %rdx
 9894                             	        xorq	%rcx, %rcx
 9895                             	        mulxq	%r12, %rax, %r12
 9896                             	        adcxq	%rax, %r8
 9897                             	        adoxq	%r12, %r9
 9898                             	        mulxq	%r13, %rax, %r13
 9899                             	        adcxq	%rax, %r9
 9900                             	        adoxq	%r13, %r10
 9901                             	        mulxq	%r14, %rax, %r14
 9902                             	        adcxq	%rax, %r10
 9903                             	        adoxq	%r14, %r11
 9904                             	        mulxq	%r15, %r15, %rdx
 9905                             	        adcxq	%r15, %r11
 9906                             	        adoxq	%rcx, %rdx
 9907                             	        adcxq	%rcx, %rdx
 9908                             	        #  Overflow
 9909                             	        shldq	$0x01, %r11, %rdx
 9910                             	        movq	$0x7fffffffffffffff, %rcx
 9911                             	        imulq	$19, %rdx, %rax
 9912                             	        andq	%rcx, %r11
 9913                             	        addq	%rax, %r8
 9914                             	        adcq	$0x00, %r9
 9915                             	        adcq	$0x00, %r10
 9916                             	        adcq	$0x00, %r11
 9917                             	        # Reduce if top bit set
 9918                             	        movq	%r11, %rdx
 9919                             	        sarq	$63, %rdx
 9920                             	        andq	$19, %rdx
 9921                             	        andq	%rcx, %r11
 9922                             	        addq	%rdx, %r8
 9923                             	        adcq	$0x00, %r9
 9924                             	        adcq	$0x00, %r10
 9925                             	        adcq	$0x00, %r11
 9926                             	        # Store
 9927                             	        movq	%r8, (%rdi)
 9928                             	        movq	%r9, 8(%rdi)
 9929                             	        movq	%r10, 16(%rdi)
 9930                             	        movq	%r11, 24(%rdi)
 9931                             	        decb	%bpl
 9932                             	        jnz	L_fe_sq_n_avx2
 9933                             	        popq	%rbp
 9934                             	        popq	%r15
 9935                             	        popq	%r14
 9936                             	        popq	%r13
 9937                             	        popq	%r12
 9938                             	        popq	%rbx
 9939                             	        repz retq
 9940                             	#ifndef __APPLE__
 9942                             	#endif /* __APPLE__ */
 9943                             	#ifndef __APPLE__
 9944                             	.text
 9945                             	.globl	fe_mul121666_avx2
 9947                             	.align	16
 9948                             	fe_mul121666_avx2:
 9949                             	#else
 9950                             	.section	__TEXT,__text
 9951                             	.globl	_fe_mul121666_avx2
 9952                             	.p2align	4
 9953                             	_fe_mul121666_avx2:
 9954                             	#endif /* __APPLE__ */
 9955                             	        pushq	%r12
 9956                             	        pushq	%r13
 9957                             	        movq	$0x1db42, %rdx
 9958                             	        mulxq	(%rsi), %rax, %r13
 9959                             	        mulxq	8(%rsi), %rcx, %r12
 9960                             	        mulxq	16(%rsi), %r8, %r11
 9961                             	        mulxq	24(%rsi), %r9, %r10
 9962                             	        addq	%r13, %rcx
 9963                             	        adcq	%r12, %r8
 9964                             	        adcq	%r11, %r9
 9965                             	        adcq	$0x00, %r10
 9966                             	        movq	$0x7fffffffffffffff, %r13
 9967                             	        shldq	$0x01, %r9, %r10
 9968                             	        andq	%r13, %r9
 9969                             	        imulq	$19, %r10, %r10
 9970                             	        addq	%r10, %rax
 9971                             	        adcq	$0x00, %rcx
 9972                             	        adcq	$0x00, %r8
 9973                             	        adcq	$0x00, %r9
 9974                             	        movq	%rax, (%rdi)
 9975                             	        movq	%rcx, 8(%rdi)
 9976                             	        movq	%r8, 16(%rdi)
 9977                             	        movq	%r9, 24(%rdi)
 9978                             	        popq	%r13
 9979                             	        popq	%r12
 9980                             	        repz retq
 9981                             	#ifndef __APPLE__
 9983                             	#endif /* __APPLE__ */
 9984                             	#ifndef __APPLE__
 9985                             	.text
 9986                             	.globl	fe_sq2_avx2
 9988                             	.align	16
 9989                             	fe_sq2_avx2:
 9990                             	#else
 9991                             	.section	__TEXT,__text
 9992                             	.globl	_fe_sq2_avx2
 9993                             	.p2align	4
 9994                             	_fe_sq2_avx2:
 9995                             	#endif /* __APPLE__ */
 9996                             	        pushq	%rbx
 9997                             	        pushq	%r12
 9998                             	        pushq	%r13
 9999                             	        pushq	%r14
 10000                             	        pushq	%r15
 10001                             	        # Square * 2
 10002                             	        # A[0] * A[1]
 10003                             	        movq	(%rsi), %rdx
 10004                             	        mulxq	8(%rsi), %r9, %r10
 10005                             	        # A[0] * A[3]
 10006                             	        mulxq	24(%rsi), %r11, %r12
 10007                             	        # A[2] * A[1]
 10008                             	        movq	16(%rsi), %rdx
 10009                             	        mulxq	8(%rsi), %rcx, %rbx
 10010                             	        xorq	%r15, %r15
 10011                             	        adoxq	%rcx, %r11
 10012                             	        # A[2] * A[3]
 10013                             	        mulxq	24(%rsi), %r13, %r14
 10014                             	        adoxq	%rbx, %r12
 10015                             	        # A[2] * A[0]
 10016                             	        mulxq	(%rsi), %rcx, %rbx
 10017                             	        adoxq	%r15, %r13
 10018                             	        adcxq	%rcx, %r10
 10019                             	        adoxq	%r15, %r14
 10020                             	        # A[1] * A[3]
 10021                             	        movq	8(%rsi), %rdx
 10022                             	        mulxq	24(%rsi), %rax, %r8
 10023                             	        adcxq	%rbx, %r11
 10024                             	        adcxq	%rax, %r12
 10025                             	        adcxq	%r8, %r13
 10026                             	        adcxq	%r15, %r14
 10027                             	        # Double with Carry Flag
 10028                             	        xorq	%r15, %r15
 10029                             	        # A[0] * A[0]
 10030                             	        movq	(%rsi), %rdx
 10031                             	        mulxq	%rdx, %r8, %rax
 10032                             	        adcxq	%r9, %r9
 10033                             	        # A[1] * A[1]
 10034                             	        movq	8(%rsi), %rdx
 10035                             	        mulxq	%rdx, %rcx, %rbx
 10036                             	        adcxq	%r10, %r10
 10037                             	        adoxq	%rax, %r9
 10038                             	        adcxq	%r11, %r11
 10039                             	        adoxq	%rcx, %r10
 10040                             	        # A[2] * A[2]
 10041                             	        movq	16(%rsi), %rdx
 10042                             	        mulxq	%rdx, %rax, %rcx
 10043                             	        adcxq	%r12, %r12
 10044                             	        adoxq	%rbx, %r11
 10045                             	        adcxq	%r13, %r13
 10046                             	        adoxq	%rax, %r12
 10047                             	        # A[3] * A[3]
 10048                             	        movq	24(%rsi), %rdx
 10049                             	        mulxq	%rdx, %rax, %rbx
 10050                             	        adcxq	%r14, %r14
 10051                             	        adoxq	%rcx, %r13
 10052                             	        adcxq	%r15, %r15
 10053                             	        adoxq	%rax, %r14
 10054                             	        adoxq	%rbx, %r15
 10055                             	        # Reduce
 10056                             	        movq	$0x7fffffffffffffff, %rbx
 10057                             	        xorq	%rax, %rax
 10058                             	        #  Move top half into t4-t7 and remove top bit from t3 and double
 10059                             	        shldq	$3, %r15, %rax
 10060                             	        shldq	$2, %r14, %r15
 10061                             	        shldq	$2, %r13, %r14
 10062                             	        shldq	$2, %r12, %r13
 10063                             	        shldq	$2, %r11, %r12
 10064                             	        shldq	$0x01, %r10, %r11
 10065                             	        shldq	$0x01, %r9, %r10
 10066                             	        shldq	$0x01, %r8, %r9
 10067                             	        shlq	$0x01, %r8
 10068                             	        andq	%rbx, %r11
 10069                             	        #  Two out left, one in right
 10070                             	        andq	%rbx, %r15
 10071                             	        #  Multiply top bits by 19*19
 10072                             	        imulq	$0x169, %rax, %rcx
 10073                             	        xorq	%rbx, %rbx
 10074                             	        #  Multiply top half by 19
 10075                             	        movq	$19, %rdx
 10076                             	        adoxq	%rcx, %r8
 10077                             	        mulxq	%r12, %rax, %r12
 10078                             	        adcxq	%rax, %r8
 10079                             	        adoxq	%r12, %r9
 10080                             	        mulxq	%r13, %rax, %r13
 10081                             	        adcxq	%rax, %r9
 10082                             	        adoxq	%r13, %r10
 10083                             	        mulxq	%r14, %rax, %r14
 10084                             	        adcxq	%rax, %r10
 10085                             	        adoxq	%r14, %r11
 10086                             	        mulxq	%r15, %r15, %rdx
 10087                             	        adcxq	%r15, %r11
 10088                             	        adoxq	%rbx, %rdx
 10089                             	        adcxq	%rbx, %rdx
 10090                             	        #  Overflow
 10091                             	        shldq	$0x01, %r11, %rdx
 10092                             	        movq	$0x7fffffffffffffff, %rbx
 10093                             	        imulq	$19, %rdx, %rax
 10094                             	        andq	%rbx, %r11
 10095                             	        addq	%rax, %r8
 10096                             	        adcq	$0x00, %r9
 10097                             	        adcq	$0x00, %r10
 10098                             	        adcq	$0x00, %r11
 10099                             	        # Reduce if top bit set
 10100                             	        movq	%r11, %rdx
 10101                             	        sarq	$63, %rdx
 10102                             	        andq	$19, %rdx
 10103                             	        andq	%rbx, %r11
 10104                             	        addq	%rdx, %r8
 10105                             	        adcq	$0x00, %r9
 10106                             	        adcq	$0x00, %r10
 10107                             	        adcq	$0x00, %r11
 10108                             	        # Store
 10109                             	        movq	%r8, (%rdi)
 10110                             	        movq	%r9, 8(%rdi)
 10111                             	        movq	%r10, 16(%rdi)
 10112                             	        movq	%r11, 24(%rdi)
 10113                             	        popq	%r15
 10114                             	        popq	%r14
 10115                             	        popq	%r13
 10116                             	        popq	%r12
 10117                             	        popq	%rbx
 10118                             	        repz retq
 10119                             	#ifndef __APPLE__
 10121                             	#endif /* __APPLE__ */
 10122                             	#ifndef __APPLE__
 10123                             	.text
 10124                             	.globl	fe_invert_avx2
 10126                             	.align	16
 10127                             	fe_invert_avx2:
 10128                             	#else
 10129                             	.section	__TEXT,__text
 10130                             	.globl	_fe_invert_avx2
 10131                             	.p2align	4
 10132                             	_fe_invert_avx2:
 10133                             	#endif /* __APPLE__ */
 10134                             	        subq	$0x90, %rsp
 10135                             	        # Invert
 10136                             	        movq	%rdi, 128(%rsp)
 10137                             	        movq	%rsi, 136(%rsp)
 10138                             	        movq	%rsp, %rdi
 10139                             	        movq	136(%rsp), %rsi
 10140                             	#ifndef __APPLE__
 10141                             	        callq	fe_sq_avx2@plt
 10142                             	#else
 10143                             	        callq	_fe_sq_avx2
 10144                             	#endif /* __APPLE__ */
 10145                             	        leaq	32(%rsp), %rdi
 10146                             	        movq	%rsp, %rsi
 10147                             	#ifndef __APPLE__
 10148                             	        callq	fe_sq_avx2@plt
 10149                             	#else
 10150                             	        callq	_fe_sq_avx2
 10151                             	#endif /* __APPLE__ */
 10152                             	        leaq	32(%rsp), %rdi
 10153                             	        leaq	32(%rsp), %rsi
 10154                             	#ifndef __APPLE__
 10155                             	        callq	fe_sq_avx2@plt
 10156                             	#else
 10157                             	        callq	_fe_sq_avx2
 10158                             	#endif /* __APPLE__ */
 10159                             	        leaq	32(%rsp), %rdi
 10160                             	        movq	136(%rsp), %rsi
 10161                             	        leaq	32(%rsp), %rdx
 10162                             	#ifndef __APPLE__
 10163                             	        callq	fe_mul_avx2@plt
 10164                             	#else
 10165                             	        callq	_fe_mul_avx2
 10166                             	#endif /* __APPLE__ */
 10167                             	        movq	%rsp, %rdi
 10168                             	        movq	%rsp, %rsi
 10169                             	        leaq	32(%rsp), %rdx
 10170                             	#ifndef __APPLE__
 10171                             	        callq	fe_mul_avx2@plt
 10172                             	#else
 10173                             	        callq	_fe_mul_avx2
 10174                             	#endif /* __APPLE__ */
 10175                             	        leaq	64(%rsp), %rdi
 10176                             	        movq	%rsp, %rsi
 10177                             	#ifndef __APPLE__
 10178                             	        callq	fe_sq_avx2@plt
 10179                             	#else
 10180                             	        callq	_fe_sq_avx2
 10181                             	#endif /* __APPLE__ */
 10182                             	        leaq	32(%rsp), %rdi
 10183                             	        leaq	32(%rsp), %rsi
 10184                             	        leaq	64(%rsp), %rdx
 10185                             	#ifndef __APPLE__
 10186                             	        callq	fe_mul_avx2@plt
 10187                             	#else
 10188                             	        callq	_fe_mul_avx2
 10189                             	#endif /* __APPLE__ */
 10190                             	        leaq	64(%rsp), %rdi
 10191                             	        leaq	32(%rsp), %rsi
 10192                             	#ifndef __APPLE__
 10193                             	        callq	fe_sq_avx2@plt
 10194                             	#else
 10195                             	        callq	_fe_sq_avx2
 10196                             	#endif /* __APPLE__ */
 10197                             	        leaq	64(%rsp), %rdi
 10198                             	        leaq	64(%rsp), %rsi
 10199                             	        movq	$4, %rdx
 10200                             	#ifndef __APPLE__
 10201                             	        callq	fe_sq_n_avx2@plt
 10202                             	#else
 10203                             	        callq	_fe_sq_n_avx2
 10204                             	#endif /* __APPLE__ */
 10205                             	        leaq	32(%rsp), %rdi
 10206                             	        leaq	64(%rsp), %rsi
 10207                             	        leaq	32(%rsp), %rdx
 10208                             	#ifndef __APPLE__
 10209                             	        callq	fe_mul_avx2@plt
 10210                             	#else
 10211                             	        callq	_fe_mul_avx2
 10212                             	#endif /* __APPLE__ */
 10213                             	        leaq	64(%rsp), %rdi
 10214                             	        leaq	32(%rsp), %rsi
 10215                             	#ifndef __APPLE__
 10216                             	        callq	fe_sq_avx2@plt
 10217                             	#else
 10218                             	        callq	_fe_sq_avx2
 10219                             	#endif /* __APPLE__ */
 10220                             	        leaq	64(%rsp), %rdi
 10221                             	        leaq	64(%rsp), %rsi
 10222                             	        movq	$9, %rdx
 10223                             	#ifndef __APPLE__
 10224                             	        callq	fe_sq_n_avx2@plt
 10225                             	#else
 10226                             	        callq	_fe_sq_n_avx2
 10227                             	#endif /* __APPLE__ */
 10228                             	        leaq	64(%rsp), %rdi
 10229                             	        leaq	64(%rsp), %rsi
 10230                             	        leaq	32(%rsp), %rdx
 10231                             	#ifndef __APPLE__
 10232                             	        callq	fe_mul_avx2@plt
 10233                             	#else
 10234                             	        callq	_fe_mul_avx2
 10235                             	#endif /* __APPLE__ */
 10236                             	        leaq	96(%rsp), %rdi
 10237                             	        leaq	64(%rsp), %rsi
 10238                             	#ifndef __APPLE__
 10239                             	        callq	fe_sq_avx2@plt
 10240                             	#else
 10241                             	        callq	_fe_sq_avx2
 10242                             	#endif /* __APPLE__ */
 10243                             	        leaq	96(%rsp), %rdi
 10244                             	        leaq	96(%rsp), %rsi
 10245                             	        movq	$19, %rdx
 10246                             	#ifndef __APPLE__
 10247                             	        callq	fe_sq_n_avx2@plt
 10248                             	#else
 10249                             	        callq	_fe_sq_n_avx2
 10250                             	#endif /* __APPLE__ */
 10251                             	        leaq	64(%rsp), %rdi
 10252                             	        leaq	96(%rsp), %rsi
 10253                             	        leaq	64(%rsp), %rdx
 10254                             	#ifndef __APPLE__
 10255                             	        callq	fe_mul_avx2@plt
 10256                             	#else
 10257                             	        callq	_fe_mul_avx2
 10258                             	#endif /* __APPLE__ */
 10259                             	        leaq	64(%rsp), %rdi
 10260                             	        leaq	64(%rsp), %rsi
 10261                             	#ifndef __APPLE__
 10262                             	        callq	fe_sq_avx2@plt
 10263                             	#else
 10264                             	        callq	_fe_sq_avx2
 10265                             	#endif /* __APPLE__ */
 10266                             	        leaq	64(%rsp), %rdi
 10267                             	        leaq	64(%rsp), %rsi
 10268                             	        movq	$9, %rdx
 10269                             	#ifndef __APPLE__
 10270                             	        callq	fe_sq_n_avx2@plt
 10271                             	#else
 10272                             	        callq	_fe_sq_n_avx2
 10273                             	#endif /* __APPLE__ */
 10274                             	        leaq	32(%rsp), %rdi
 10275                             	        leaq	64(%rsp), %rsi
 10276                             	        leaq	32(%rsp), %rdx
 10277                             	#ifndef __APPLE__
 10278                             	        callq	fe_mul_avx2@plt
 10279                             	#else
 10280                             	        callq	_fe_mul_avx2
 10281                             	#endif /* __APPLE__ */
 10282                             	        leaq	64(%rsp), %rdi
 10283                             	        leaq	32(%rsp), %rsi
 10284                             	#ifndef __APPLE__
 10285                             	        callq	fe_sq_avx2@plt
 10286                             	#else
 10287                             	        callq	_fe_sq_avx2
 10288                             	#endif /* __APPLE__ */
 10289                             	        leaq	64(%rsp), %rdi
 10290                             	        leaq	64(%rsp), %rsi
 10291                             	        movq	$49, %rdx
 10292                             	#ifndef __APPLE__
 10293                             	        callq	fe_sq_n_avx2@plt
 10294                             	#else
 10295                             	        callq	_fe_sq_n_avx2
 10296                             	#endif /* __APPLE__ */
 10297                             	        leaq	64(%rsp), %rdi
 10298                             	        leaq	64(%rsp), %rsi
 10299                             	        leaq	32(%rsp), %rdx
 10300                             	#ifndef __APPLE__
 10301                             	        callq	fe_mul_avx2@plt
 10302                             	#else
 10303                             	        callq	_fe_mul_avx2
 10304                             	#endif /* __APPLE__ */
 10305                             	        leaq	96(%rsp), %rdi
 10306                             	        leaq	64(%rsp), %rsi
 10307                             	#ifndef __APPLE__
 10308                             	        callq	fe_sq_avx2@plt
 10309                             	#else
 10310                             	        callq	_fe_sq_avx2
 10311                             	#endif /* __APPLE__ */
 10312                             	        leaq	96(%rsp), %rdi
 10313                             	        leaq	96(%rsp), %rsi
 10314                             	        movq	$0x63, %rdx
 10315                             	#ifndef __APPLE__
 10316                             	        callq	fe_sq_n_avx2@plt
 10317                             	#else
 10318                             	        callq	_fe_sq_n_avx2
 10319                             	#endif /* __APPLE__ */
 10320                             	        leaq	64(%rsp), %rdi
 10321                             	        leaq	96(%rsp), %rsi
 10322                             	        leaq	64(%rsp), %rdx
 10323                             	#ifndef __APPLE__
 10324                             	        callq	fe_mul_avx2@plt
 10325                             	#else
 10326                             	        callq	_fe_mul_avx2
 10327                             	#endif /* __APPLE__ */
 10328                             	        leaq	64(%rsp), %rdi
 10329                             	        leaq	64(%rsp), %rsi
 10330                             	#ifndef __APPLE__
 10331                             	        callq	fe_sq_avx2@plt
 10332                             	#else
 10333                             	        callq	_fe_sq_avx2
 10334                             	#endif /* __APPLE__ */
 10335                             	        leaq	64(%rsp), %rdi
 10336                             	        leaq	64(%rsp), %rsi
 10337                             	        movq	$49, %rdx
 10338                             	#ifndef __APPLE__
 10339                             	        callq	fe_sq_n_avx2@plt
 10340                             	#else
 10341                             	        callq	_fe_sq_n_avx2
 10342                             	#endif /* __APPLE__ */
 10343                             	        leaq	32(%rsp), %rdi
 10344                             	        leaq	64(%rsp), %rsi
 10345                             	        leaq	32(%rsp), %rdx
 10346                             	#ifndef __APPLE__
 10347                             	        callq	fe_mul_avx2@plt
 10348                             	#else
 10349                             	        callq	_fe_mul_avx2
 10350                             	#endif /* __APPLE__ */
 10351                             	        leaq	32(%rsp), %rdi
 10352                             	        leaq	32(%rsp), %rsi
 10353                             	#ifndef __APPLE__
 10354                             	        callq	fe_sq_avx2@plt
 10355                             	#else
 10356                             	        callq	_fe_sq_avx2
 10357                             	#endif /* __APPLE__ */
 10358                             	        leaq	32(%rsp), %rdi
 10359                             	        leaq	32(%rsp), %rsi
 10360                             	        movq	$4, %rdx
 10361                             	#ifndef __APPLE__
 10362                             	        callq	fe_sq_n_avx2@plt
 10363                             	#else
 10364                             	        callq	_fe_sq_n_avx2
 10365                             	#endif /* __APPLE__ */
 10366                             	        movq	128(%rsp), %rdi
 10367                             	        leaq	32(%rsp), %rsi
 10368                             	        movq	%rsp, %rdx
 10369                             	#ifndef __APPLE__
 10370                             	        callq	fe_mul_avx2@plt
 10371                             	#else
 10372                             	        callq	_fe_mul_avx2
 10373                             	#endif /* __APPLE__ */
 10374                             	        movq	136(%rsp), %rsi
 10375                             	        movq	128(%rsp), %rdi
 10376                             	        addq	$0x90, %rsp
 10377                             	        repz retq
 10378                             	#ifndef __APPLE__
 10379                             	.text
 10380                             	.globl	curve25519_avx2
 10382                             	.align	16
 10383                             	curve25519_avx2:
 10384                             	#else
 10385                             	.section	__TEXT,__text
 10386                             	.globl	_curve25519_avx2
 10387                             	.p2align	4
 10388                             	_curve25519_avx2:
 10389                             	#endif /* __APPLE__ */
 10390                             	        pushq	%rbx
 10391                             	        pushq	%r12
 10392                             	        pushq	%r13
 10393                             	        pushq	%r14
 10394                             	        pushq	%r15
 10395                             	        pushq	%rbp
 10396                             	        movq	%rdx, %r8
 10397                             	        subq	$0xc0, %rsp
 10398                             	        movq	$0x00, 184(%rsp)
 10399                             	        movq	%rdi, 176(%rsp)
 10400                             	        # Set one
 10401                             	        movq	$0x01, (%rdi)
 10402                             	        movq	$0x00, 8(%rdi)
 10403                             	        movq	$0x00, 16(%rdi)
 10404                             	        movq	$0x00, 24(%rdi)
 10405                             	        # Set zero
 10406                             	        movq	$0x00, (%rsp)
 10407                             	        movq	$0x00, 8(%rsp)
 10408                             	        movq	$0x00, 16(%rsp)
 10409                             	        movq	$0x00, 24(%rsp)
 10410                             	        # Set one
 10411                             	        movq	$0x01, 32(%rsp)
 10412                             	        movq	$0x00, 40(%rsp)
 10413                             	        movq	$0x00, 48(%rsp)
 10414                             	        movq	$0x00, 56(%rsp)
 10415                             	        # Copy
 10416                             	        movq	(%r8), %r9
 10417                             	        movq	8(%r8), %r10
 10418                             	        movq	16(%r8), %r11
 10419                             	        movq	24(%r8), %r12
 10420                             	        movq	%r9, 64(%rsp)
 10421                             	        movq	%r10, 72(%rsp)
 10422                             	        movq	%r11, 80(%rsp)
 10423                             	        movq	%r12, 88(%rsp)
 10424                             	        movb	$62, 168(%rsp)
 10425                             	        movq	$3, 160(%rsp)
 10426                             	L_curve25519_avx2_words:
 10427                             	L_curve25519_avx2_bits:
 10428                             	        movq	184(%rsp), %rbx
 10429                             	        movq	160(%rsp), %r9
 10430                             	        movb	168(%rsp), %cl
 10431                             	        movq	(%rsi,%r9,8), %rax
 10432                             	        shrq	%cl, %rax
 10433                             	        andq	$0x01, %rax
 10434                             	        xorq	%rax, %rbx
 10435                             	        negq	%rbx
 10436                             	        # Conditional Swap
 10437                             	        movq	(%rdi), %r9
 10438                             	        movq	8(%rdi), %r10
 10439                             	        movq	16(%rdi), %r11
 10440                             	        movq	24(%rdi), %r12
 10441                             	        xorq	64(%rsp), %r9
 10442                             	        xorq	72(%rsp), %r10
 10443                             	        xorq	80(%rsp), %r11
 10444                             	        xorq	88(%rsp), %r12
 10445                             	        andq	%rbx, %r9
 10446                             	        andq	%rbx, %r10
 10447                             	        andq	%rbx, %r11
 10448                             	        andq	%rbx, %r12
 10449                             	        xorq	%r9, (%rdi)
 10450                             	        xorq	%r10, 8(%rdi)
 10451                             	        xorq	%r11, 16(%rdi)
 10452                             	        xorq	%r12, 24(%rdi)
 10453                             	        xorq	%r9, 64(%rsp)
 10454                             	        xorq	%r10, 72(%rsp)
 10455                             	        xorq	%r11, 80(%rsp)
 10456                             	        xorq	%r12, 88(%rsp)
 10457                             	        # Conditional Swap
 10458                             	        movq	(%rsp), %r9
 10459                             	        movq	8(%rsp), %r10
 10460                             	        movq	16(%rsp), %r11
 10461                             	        movq	24(%rsp), %r12
 10462                             	        xorq	32(%rsp), %r9
 10463                             	        xorq	40(%rsp), %r10
 10464                             	        xorq	48(%rsp), %r11
 10465                             	        xorq	56(%rsp), %r12
 10466                             	        andq	%rbx, %r9
 10467                             	        andq	%rbx, %r10
 10468                             	        andq	%rbx, %r11
 10469                             	        andq	%rbx, %r12
 10470                             	        xorq	%r9, (%rsp)
 10471                             	        xorq	%r10, 8(%rsp)
 10472                             	        xorq	%r11, 16(%rsp)
 10473                             	        xorq	%r12, 24(%rsp)
 10474                             	        xorq	%r9, 32(%rsp)
 10475                             	        xorq	%r10, 40(%rsp)
 10476                             	        xorq	%r11, 48(%rsp)
 10477                             	        xorq	%r12, 56(%rsp)
 10478                             	        movq	%rax, 184(%rsp)
 10479                             	        # Add
 10480                             	        movq	(%rdi), %r9
 10481                             	        movq	8(%rdi), %r10
 10482                             	        movq	16(%rdi), %r11
 10483                             	        movq	24(%rdi), %rax
 10484                             	        movq	%r9, %r13
 10485                             	        addq	(%rsp), %r9
 10486                             	        movq	%r10, %r14
 10487                             	        adcq	8(%rsp), %r10
 10488                             	        movq	%r11, %r15
 10489                             	        adcq	16(%rsp), %r11
 10490                             	        movq	%rax, %rbp
 10491                             	        adcq	24(%rsp), %rax
 10492                             	        movq	$-19, %rcx
 10493                             	        movq	%rax, %r12
 10494                             	        movq	$0x7fffffffffffffff, %rbx
 10495                             	        sarq	$63, %rax
 10496                             	        #   Mask the modulus
 10497                             	        andq	%rax, %rcx
 10498                             	        andq	%rax, %rbx
 10499                             	        #   Sub modulus (if overflow)
 10500                             	        subq	%rcx, %r9
 10501                             	        sbbq	%rax, %r10
 10502                             	        sbbq	%rax, %r11
 10503                             	        sbbq	%rbx, %r12
 10504                             	        # Sub
 10505                             	        subq	(%rsp), %r13
 10506                             	        movq	$0x00, %rax
 10507                             	        sbbq	8(%rsp), %r14
 10508                             	        movq	$-19, %rcx
 10509                             	        sbbq	16(%rsp), %r15
 10510                             	        movq	$0x7fffffffffffffff, %rbx
 10511                             	        sbbq	24(%rsp), %rbp
 10512                             	        sbbq	$0x00, %rax
 10513                             	        #   Mask the modulus
 10514                             	        andq	%rax, %rcx
 10515                             	        andq	%rax, %rbx
 10516                             	        #   Add modulus (if underflow)
 10517                             	        addq	%rcx, %r13
 10518                             	        adcq	%rax, %r14
 10519                             	        adcq	%rax, %r15
 10520                             	        adcq	%rbx, %rbp
 10521                             	        movq	%r9, (%rdi)
 10522                             	        movq	%r10, 8(%rdi)
 10523                             	        movq	%r11, 16(%rdi)
 10524                             	        movq	%r12, 24(%rdi)
 10525                             	        movq	%r13, 128(%rsp)
 10526                             	        movq	%r14, 136(%rsp)
 10527                             	        movq	%r15, 144(%rsp)
 10528                             	        movq	%rbp, 152(%rsp)
 10529                             	        # Add
 10530                             	        movq	64(%rsp), %r9
 10531                             	        movq	72(%rsp), %r10
 10532                             	        movq	80(%rsp), %r11
 10533                             	        movq	88(%rsp), %rax
 10534                             	        movq	%r9, %r13
 10535                             	        addq	32(%rsp), %r9
 10536                             	        movq	%r10, %r14
 10537                             	        adcq	40(%rsp), %r10
 10538                             	        movq	%r11, %r15
 10539                             	        adcq	48(%rsp), %r11
 10540                             	        movq	%rax, %rbp
 10541                             	        adcq	56(%rsp), %rax
 10542                             	        movq	$-19, %rcx
 10543                             	        movq	%rax, %r12
 10544                             	        movq	$0x7fffffffffffffff, %rbx
 10545                             	        sarq	$63, %rax
 10546                             	        #   Mask the modulus
 10547                             	        andq	%rax, %rcx
 10548                             	        andq	%rax, %rbx
 10549                             	        #   Sub modulus (if overflow)
 10550                             	        subq	%rcx, %r9
 10551                             	        sbbq	%rax, %r10
 10552                             	        sbbq	%rax, %r11
 10553                             	        sbbq	%rbx, %r12
 10554                             	        # Sub
 10555                             	        subq	32(%rsp), %r13
 10556                             	        movq	$0x00, %rax
 10557                             	        sbbq	40(%rsp), %r14
 10558                             	        movq	$-19, %rcx
 10559                             	        sbbq	48(%rsp), %r15
 10560                             	        movq	$0x7fffffffffffffff, %rbx
 10561                             	        sbbq	56(%rsp), %rbp
 10562                             	        sbbq	$0x00, %rax
 10563                             	        #   Mask the modulus
 10564                             	        andq	%rax, %rcx
 10565                             	        andq	%rax, %rbx
 10566                             	        #   Add modulus (if underflow)
 10567                             	        addq	%rcx, %r13
 10568                             	        adcq	%rax, %r14
 10569                             	        adcq	%rax, %r15
 10570                             	        adcq	%rbx, %rbp
 10571                             	        movq	%r9, (%rsp)
 10572                             	        movq	%r10, 8(%rsp)
 10573                             	        movq	%r11, 16(%rsp)
 10574                             	        movq	%r12, 24(%rsp)
 10575                             	        movq	%r13, 96(%rsp)
 10576                             	        movq	%r14, 104(%rsp)
 10577                             	        movq	%r15, 112(%rsp)
 10578                             	        movq	%rbp, 120(%rsp)
 10579                             	        # Multiply
 10580                             	        # A[0] * B[0]
 10581                             	        movq	(%rdi), %rdx
 10582                             	        mulxq	96(%rsp), %r9, %r10
 10583                             	        # A[2] * B[0]
 10584                             	        mulxq	112(%rsp), %r11, %r12
 10585                             	        # A[1] * B[0]
 10586                             	        mulxq	104(%rsp), %rcx, %rbx
 10587                             	        xorq	%rbp, %rbp
 10588                             	        adcxq	%rcx, %r10
 10589                             	        # A[1] * B[3]
 10590                             	        movq	24(%rdi), %rdx
 10591                             	        mulxq	104(%rsp), %r13, %r14
 10592                             	        adcxq	%rbx, %r11
 10593                             	        # A[0] * B[1]
 10594                             	        movq	8(%rdi), %rdx
 10595                             	        mulxq	96(%rsp), %rcx, %rbx
 10596                             	        adoxq	%rcx, %r10
 10597                             	        # A[2] * B[1]
 10598                             	        mulxq	112(%rsp), %rcx, %r15
 10599                             	        adoxq	%rbx, %r11
 10600                             	        adcxq	%rcx, %r12
 10601                             	        # A[1] * B[2]
 10602                             	        movq	16(%rdi), %rdx
 10603                             	        mulxq	104(%rsp), %rcx, %rbx
 10604                             	        adcxq	%r15, %r13
 10605                             	        adoxq	%rcx, %r12
 10606                             	        adcxq	%rbp, %r14
 10607                             	        adoxq	%rbx, %r13
 10608                             	        # A[0] * B[2]
 10609                             	        mulxq	96(%rsp), %rcx, %rbx
 10610                             	        adoxq	%rbp, %r14
 10611                             	        xorq	%r15, %r15
 10612                             	        adcxq	%rcx, %r11
 10613                             	        # A[1] * B[1]
 10614                             	        movq	8(%rdi), %rdx
 10615                             	        mulxq	104(%rsp), %rdx, %rcx
 10616                             	        adcxq	%rbx, %r12
 10617                             	        adoxq	%rdx, %r11
 10618                             	        # A[3] * B[1]
 10619                             	        movq	8(%rdi), %rdx
 10620                             	        adoxq	%rcx, %r12
 10621                             	        mulxq	120(%rsp), %rcx, %rbx
 10622                             	        adcxq	%rcx, %r13
 10623                             	        # A[2] * B[2]
 10624                             	        movq	16(%rdi), %rdx
 10625                             	        mulxq	112(%rsp), %rdx, %rcx
 10626                             	        adcxq	%rbx, %r14
 10627                             	        adoxq	%rdx, %r13
 10628                             	        # A[3] * B[3]
 10629                             	        movq	24(%rdi), %rdx
 10630                             	        adoxq	%rcx, %r14
 10631                             	        mulxq	120(%rsp), %rcx, %rbx
 10632                             	        adoxq	%rbp, %r15
 10633                             	        adcxq	%rcx, %r15
 10634                             	        # A[0] * B[3]
 10635                             	        mulxq	96(%rsp), %rdx, %rcx
 10636                             	        adcxq	%rbx, %rbp
 10637                             	        xorq	%rbx, %rbx
 10638                             	        adcxq	%rdx, %r12
 10639                             	        # A[3] * B[0]
 10640                             	        movq	(%rdi), %rdx
 10641                             	        adcxq	%rcx, %r13
 10642                             	        mulxq	120(%rsp), %rdx, %rcx
 10643                             	        adoxq	%rdx, %r12
 10644                             	        adoxq	%rcx, %r13
 10645                             	        # A[2] * B[3]
 10646                             	        movq	24(%rdi), %rdx
 10647                             	        mulxq	112(%rsp), %rdx, %rcx
 10648                             	        adcxq	%rdx, %r14
 10649                             	        # A[3] * B[2]
 10650                             	        movq	16(%rdi), %rdx
 10651                             	        adcxq	%rcx, %r15
 10652                             	        mulxq	120(%rsp), %rcx, %rdx
 10653                             	        adcxq	%rbx, %rbp
 10654                             	        adoxq	%rcx, %r14
 10655                             	        adoxq	%rdx, %r15
 10656                             	        adoxq	%rbx, %rbp
 10657                             	        # Reduce
 10658                             	        movq	$0x7fffffffffffffff, %rbx
 10659                             	        #  Move top half into t4-t7 and remove top bit from t3
 10660                             	        shldq	$0x01, %r15, %rbp
 10661                             	        shldq	$0x01, %r14, %r15
 10662                             	        shldq	$0x01, %r13, %r14
 10663                             	        shldq	$0x01, %r12, %r13
 10664                             	        andq	%rbx, %r12
 10665                             	        #  Multiply top half by 19
 10666                             	        movq	$19, %rdx
 10667                             	        xorq	%rbx, %rbx
 10668                             	        mulxq	%r13, %rcx, %r13
 10669                             	        adcxq	%rcx, %r9
 10670                             	        adoxq	%r13, %r10
 10671                             	        mulxq	%r14, %rcx, %r14
 10672                             	        adcxq	%rcx, %r10
 10673                             	        adoxq	%r14, %r11
 10674                             	        mulxq	%r15, %rcx, %r15
 10675                             	        adcxq	%rcx, %r11
 10676                             	        adoxq	%r15, %r12
 10677                             	        mulxq	%rbp, %rbp, %rdx
 10678                             	        adcxq	%rbp, %r12
 10679                             	        adoxq	%rbx, %rdx
 10680                             	        adcxq	%rbx, %rdx
 10681                             	        #  Overflow
 10682                             	        shldq	$0x01, %r12, %rdx
 10683                             	        movq	$0x7fffffffffffffff, %rbx
 10684                             	        imulq	$19, %rdx, %rcx
 10685                             	        andq	%rbx, %r12
 10686                             	        addq	%rcx, %r9
 10687                             	        adcq	$0x00, %r10
 10688                             	        adcq	$0x00, %r11
 10689                             	        adcq	$0x00, %r12
 10690                             	        # Reduce if top bit set
 10691                             	        movq	%r12, %rdx
 10692                             	        sarq	$63, %rdx
 10693                             	        andq	$19, %rdx
 10694                             	        andq	%rbx, %r12
 10695                             	        addq	%rdx, %r9
 10696                             	        adcq	$0x00, %r10
 10697                             	        adcq	$0x00, %r11
 10698                             	        adcq	$0x00, %r12
 10699                             	        # Store
 10700                             	        movq	%r9, 32(%rsp)
 10701                             	        movq	%r10, 40(%rsp)
 10702                             	        movq	%r11, 48(%rsp)
 10703                             	        movq	%r12, 56(%rsp)
 10704                             	        # Multiply
 10705                             	        # A[0] * B[0]
 10706                             	        movq	128(%rsp), %rdx
 10707                             	        mulxq	(%rsp), %r9, %r10
 10708                             	        # A[2] * B[0]
 10709                             	        mulxq	16(%rsp), %r11, %r12
 10710                             	        # A[1] * B[0]
 10711                             	        mulxq	8(%rsp), %rcx, %rbx
 10712                             	        xorq	%rbp, %rbp
 10713                             	        adcxq	%rcx, %r10
 10714                             	        # A[1] * B[3]
 10715                             	        movq	152(%rsp), %rdx
 10716                             	        mulxq	8(%rsp), %r13, %r14
 10717                             	        adcxq	%rbx, %r11
 10718                             	        # A[0] * B[1]
 10719                             	        movq	136(%rsp), %rdx
 10720                             	        mulxq	(%rsp), %rcx, %rbx
 10721                             	        adoxq	%rcx, %r10
 10722                             	        # A[2] * B[1]
 10723                             	        mulxq	16(%rsp), %rcx, %r15
 10724                             	        adoxq	%rbx, %r11
 10725                             	        adcxq	%rcx, %r12
 10726                             	        # A[1] * B[2]
 10727                             	        movq	144(%rsp), %rdx
 10728                             	        mulxq	8(%rsp), %rcx, %rbx
 10729                             	        adcxq	%r15, %r13
 10730                             	        adoxq	%rcx, %r12
 10731                             	        adcxq	%rbp, %r14
 10732                             	        adoxq	%rbx, %r13
 10733                             	        # A[0] * B[2]
 10734                             	        mulxq	(%rsp), %rcx, %rbx
 10735                             	        adoxq	%rbp, %r14
 10736                             	        xorq	%r15, %r15
 10737                             	        adcxq	%rcx, %r11
 10738                             	        # A[1] * B[1]
 10739                             	        movq	136(%rsp), %rdx
 10740                             	        mulxq	8(%rsp), %rdx, %rcx
 10741                             	        adcxq	%rbx, %r12
 10742                             	        adoxq	%rdx, %r11
 10743                             	        # A[3] * B[1]
 10744                             	        movq	136(%rsp), %rdx
 10745                             	        adoxq	%rcx, %r12
 10746                             	        mulxq	24(%rsp), %rcx, %rbx
 10747                             	        adcxq	%rcx, %r13
 10748                             	        # A[2] * B[2]
 10749                             	        movq	144(%rsp), %rdx
 10750                             	        mulxq	16(%rsp), %rdx, %rcx
 10751                             	        adcxq	%rbx, %r14
 10752                             	        adoxq	%rdx, %r13
 10753                             	        # A[3] * B[3]
 10754                             	        movq	152(%rsp), %rdx
 10755                             	        adoxq	%rcx, %r14
 10756                             	        mulxq	24(%rsp), %rcx, %rbx
 10757                             	        adoxq	%rbp, %r15
 10758                             	        adcxq	%rcx, %r15
 10759                             	        # A[0] * B[3]
 10760                             	        mulxq	(%rsp), %rdx, %rcx
 10761                             	        adcxq	%rbx, %rbp
 10762                             	        xorq	%rbx, %rbx
 10763                             	        adcxq	%rdx, %r12
 10764                             	        # A[3] * B[0]
 10765                             	        movq	128(%rsp), %rdx
 10766                             	        adcxq	%rcx, %r13
 10767                             	        mulxq	24(%rsp), %rdx, %rcx
 10768                             	        adoxq	%rdx, %r12
 10769                             	        adoxq	%rcx, %r13
 10770                             	        # A[2] * B[3]
 10771                             	        movq	152(%rsp), %rdx
 10772                             	        mulxq	16(%rsp), %rdx, %rcx
 10773                             	        adcxq	%rdx, %r14
 10774                             	        # A[3] * B[2]
 10775                             	        movq	144(%rsp), %rdx
 10776                             	        adcxq	%rcx, %r15
 10777                             	        mulxq	24(%rsp), %rcx, %rdx
 10778                             	        adcxq	%rbx, %rbp
 10779                             	        adoxq	%rcx, %r14
 10780                             	        adoxq	%rdx, %r15
 10781                             	        adoxq	%rbx, %rbp
 10782                             	        # Reduce
 10783                             	        movq	$0x7fffffffffffffff, %rbx
 10784                             	        #  Move top half into t4-t7 and remove top bit from t3
 10785                             	        shldq	$0x01, %r15, %rbp
 10786                             	        shldq	$0x01, %r14, %r15
 10787                             	        shldq	$0x01, %r13, %r14
 10788                             	        shldq	$0x01, %r12, %r13
 10789                             	        andq	%rbx, %r12
 10790                             	        #  Multiply top half by 19
 10791                             	        movq	$19, %rdx
 10792                             	        xorq	%rbx, %rbx
 10793                             	        mulxq	%r13, %rcx, %r13
 10794                             	        adcxq	%rcx, %r9
 10795                             	        adoxq	%r13, %r10
 10796                             	        mulxq	%r14, %rcx, %r14
 10797                             	        adcxq	%rcx, %r10
 10798                             	        adoxq	%r14, %r11
 10799                             	        mulxq	%r15, %rcx, %r15
 10800                             	        adcxq	%rcx, %r11
 10801                             	        adoxq	%r15, %r12
 10802                             	        mulxq	%rbp, %rbp, %rdx
 10803                             	        adcxq	%rbp, %r12
 10804                             	        adoxq	%rbx, %rdx
 10805                             	        adcxq	%rbx, %rdx
 10806                             	        #  Overflow
 10807                             	        shldq	$0x01, %r12, %rdx
 10808                             	        movq	$0x7fffffffffffffff, %rbx
 10809                             	        imulq	$19, %rdx, %rcx
 10810                             	        andq	%rbx, %r12
 10811                             	        addq	%rcx, %r9
 10812                             	        adcq	$0x00, %r10
 10813                             	        adcq	$0x00, %r11
 10814                             	        adcq	$0x00, %r12
 10815                             	        # Reduce if top bit set
 10816                             	        movq	%r12, %rdx
 10817                             	        sarq	$63, %rdx
 10818                             	        andq	$19, %rdx
 10819                             	        andq	%rbx, %r12
 10820                             	        addq	%rdx, %r9
 10821                             	        adcq	$0x00, %r10
 10822                             	        adcq	$0x00, %r11
 10823                             	        adcq	$0x00, %r12
 10824                             	        # Store
 10825                             	        movq	%r9, (%rsp)
 10826                             	        movq	%r10, 8(%rsp)
 10827                             	        movq	%r11, 16(%rsp)
 10828                             	        movq	%r12, 24(%rsp)
 10829                             	        # Square
 10830                             	        # A[0] * A[1]
 10831                             	        movq	128(%rsp), %rdx
 10832                             	        mulxq	136(%rsp), %r10, %r11
 10833                             	        # A[0] * A[3]
 10834                             	        mulxq	152(%rsp), %r12, %r13
 10835                             	        # A[2] * A[1]
 10836                             	        movq	144(%rsp), %rdx
 10837                             	        mulxq	136(%rsp), %rcx, %rbx
 10838                             	        xorq	%rbp, %rbp
 10839                             	        adoxq	%rcx, %r12
 10840                             	        # A[2] * A[3]
 10841                             	        mulxq	152(%rsp), %r14, %r15
 10842                             	        adoxq	%rbx, %r13
 10843                             	        # A[2] * A[0]
 10844                             	        mulxq	128(%rsp), %rcx, %rbx
 10845                             	        adoxq	%rbp, %r14
 10846                             	        adcxq	%rcx, %r11
 10847                             	        adoxq	%rbp, %r15
 10848                             	        # A[1] * A[3]
 10849                             	        movq	136(%rsp), %rdx
 10850                             	        mulxq	152(%rsp), %rax, %r9
 10851                             	        adcxq	%rbx, %r12
 10852                             	        adcxq	%rax, %r13
 10853                             	        adcxq	%r9, %r14
 10854                             	        adcxq	%rbp, %r15
 10855                             	        # Double with Carry Flag
 10856                             	        xorq	%rbp, %rbp
 10857                             	        # A[0] * A[0]
 10858                             	        movq	128(%rsp), %rdx
 10859                             	        mulxq	%rdx, %r9, %rax
 10860                             	        adcxq	%r10, %r10
 10861                             	        # A[1] * A[1]
 10862                             	        movq	136(%rsp), %rdx
 10863                             	        mulxq	%rdx, %rcx, %rbx
 10864                             	        adcxq	%r11, %r11
 10865                             	        adoxq	%rax, %r10
 10866                             	        adcxq	%r12, %r12
 10867                             	        adoxq	%rcx, %r11
 10868                             	        # A[2] * A[2]
 10869                             	        movq	144(%rsp), %rdx
 10870                             	        mulxq	%rdx, %rax, %rcx
 10871                             	        adcxq	%r13, %r13
 10872                             	        adoxq	%rbx, %r12
 10873                             	        adcxq	%r14, %r14
 10874                             	        adoxq	%rax, %r13
 10875                             	        # A[3] * A[3]
 10876                             	        movq	152(%rsp), %rdx
 10877                             	        mulxq	%rdx, %rax, %rbx
 10878                             	        adcxq	%r15, %r15
 10879                             	        adoxq	%rcx, %r14
 10880                             	        adcxq	%rbp, %rbp
 10881                             	        adoxq	%rax, %r15
 10882                             	        adoxq	%rbx, %rbp
 10883                             	        # Reduce
 10884                             	        movq	$0x7fffffffffffffff, %rcx
 10885                             	        #  Move top half into t4-t7 and remove top bit from t3
 10886                             	        shldq	$0x01, %r15, %rbp
 10887                             	        shldq	$0x01, %r14, %r15
 10888                             	        shldq	$0x01, %r13, %r14
 10889                             	        shldq	$0x01, %r12, %r13
 10890                             	        andq	%rcx, %r12
 10891                             	        #  Multiply top half by 19
 10892                             	        movq	$19, %rdx
 10893                             	        xorq	%rcx, %rcx
 10894                             	        mulxq	%r13, %rax, %r13
 10895                             	        adcxq	%rax, %r9
 10896                             	        adoxq	%r13, %r10
 10897                             	        mulxq	%r14, %rax, %r14
 10898                             	        adcxq	%rax, %r10
 10899                             	        adoxq	%r14, %r11
 10900                             	        mulxq	%r15, %rax, %r15
 10901                             	        adcxq	%rax, %r11
 10902                             	        adoxq	%r15, %r12
 10903                             	        mulxq	%rbp, %rbp, %rdx
 10904                             	        adcxq	%rbp, %r12
 10905                             	        adoxq	%rcx, %rdx
 10906                             	        adcxq	%rcx, %rdx
 10907                             	        #  Overflow
 10908                             	        shldq	$0x01, %r12, %rdx
 10909                             	        movq	$0x7fffffffffffffff, %rcx
 10910                             	        imulq	$19, %rdx, %rax
 10911                             	        andq	%rcx, %r12
 10912                             	        addq	%rax, %r9
 10913                             	        adcq	$0x00, %r10
 10914                             	        adcq	$0x00, %r11
 10915                             	        adcq	$0x00, %r12
 10916                             	        # Reduce if top bit set
 10917                             	        movq	%r12, %rdx
 10918                             	        sarq	$63, %rdx
 10919                             	        andq	$19, %rdx
 10920                             	        andq	%rcx, %r12
 10921                             	        addq	%rdx, %r9
 10922                             	        adcq	$0x00, %r10
 10923                             	        adcq	$0x00, %r11
 10924                             	        adcq	$0x00, %r12
 10925                             	        # Store
 10926                             	        movq	%r9, 96(%rsp)
 10927                             	        movq	%r10, 104(%rsp)
 10928                             	        movq	%r11, 112(%rsp)
 10929                             	        movq	%r12, 120(%rsp)
 10930                             	        # Square
 10931                             	        # A[0] * A[1]
 10932                             	        movq	(%rdi), %rdx
 10933                             	        mulxq	8(%rdi), %r10, %r11
 10934                             	        # A[0] * A[3]
 10935                             	        mulxq	24(%rdi), %r12, %r13
 10936                             	        # A[2] * A[1]
 10937                             	        movq	16(%rdi), %rdx
 10938                             	        mulxq	8(%rdi), %rcx, %rbx
 10939                             	        xorq	%rbp, %rbp
 10940                             	        adoxq	%rcx, %r12
 10941                             	        # A[2] * A[3]
 10942                             	        mulxq	24(%rdi), %r14, %r15
 10943                             	        adoxq	%rbx, %r13
 10944                             	        # A[2] * A[0]
 10945                             	        mulxq	(%rdi), %rcx, %rbx
 10946                             	        adoxq	%rbp, %r14
 10947                             	        adcxq	%rcx, %r11
 10948                             	        adoxq	%rbp, %r15
 10949                             	        # A[1] * A[3]
 10950                             	        movq	8(%rdi), %rdx
 10951                             	        mulxq	24(%rdi), %rax, %r9
 10952                             	        adcxq	%rbx, %r12
 10953                             	        adcxq	%rax, %r13
 10954                             	        adcxq	%r9, %r14
 10955                             	        adcxq	%rbp, %r15
 10956                             	        # Double with Carry Flag
 10957                             	        xorq	%rbp, %rbp
 10958                             	        # A[0] * A[0]
 10959                             	        movq	(%rdi), %rdx
 10960                             	        mulxq	%rdx, %r9, %rax
 10961                             	        adcxq	%r10, %r10
 10962                             	        # A[1] * A[1]
 10963                             	        movq	8(%rdi), %rdx
 10964                             	        mulxq	%rdx, %rcx, %rbx
 10965                             	        adcxq	%r11, %r11
 10966                             	        adoxq	%rax, %r10
 10967                             	        adcxq	%r12, %r12
 10968                             	        adoxq	%rcx, %r11
 10969                             	        # A[2] * A[2]
 10970                             	        movq	16(%rdi), %rdx
 10971                             	        mulxq	%rdx, %rax, %rcx
 10972                             	        adcxq	%r13, %r13
 10973                             	        adoxq	%rbx, %r12
 10974                             	        adcxq	%r14, %r14
 10975                             	        adoxq	%rax, %r13
 10976                             	        # A[3] * A[3]
 10977                             	        movq	24(%rdi), %rdx
 10978                             	        mulxq	%rdx, %rax, %rbx
 10979                             	        adcxq	%r15, %r15
 10980                             	        adoxq	%rcx, %r14
 10981                             	        adcxq	%rbp, %rbp
 10982                             	        adoxq	%rax, %r15
 10983                             	        adoxq	%rbx, %rbp
 10984                             	        # Reduce
 10985                             	        movq	$0x7fffffffffffffff, %rcx
 10986                             	        #  Move top half into t4-t7 and remove top bit from t3
 10987                             	        shldq	$0x01, %r15, %rbp
 10988                             	        shldq	$0x01, %r14, %r15
 10989                             	        shldq	$0x01, %r13, %r14
 10990                             	        shldq	$0x01, %r12, %r13
 10991                             	        andq	%rcx, %r12
 10992                             	        #  Multiply top half by 19
 10993                             	        movq	$19, %rdx
 10994                             	        xorq	%rcx, %rcx
 10995                             	        mulxq	%r13, %rax, %r13
 10996                             	        adcxq	%rax, %r9
 10997                             	        adoxq	%r13, %r10
 10998                             	        mulxq	%r14, %rax, %r14
 10999                             	        adcxq	%rax, %r10
 11000                             	        adoxq	%r14, %r11
 11001                             	        mulxq	%r15, %rax, %r15
 11002                             	        adcxq	%rax, %r11
 11003                             	        adoxq	%r15, %r12
 11004                             	        mulxq	%rbp, %rbp, %rdx
 11005                             	        adcxq	%rbp, %r12
 11006                             	        adoxq	%rcx, %rdx
 11007                             	        adcxq	%rcx, %rdx
 11008                             	        #  Overflow
 11009                             	        shldq	$0x01, %r12, %rdx
 11010                             	        movq	$0x7fffffffffffffff, %rcx
 11011                             	        imulq	$19, %rdx, %rax
 11012                             	        andq	%rcx, %r12
 11013                             	        addq	%rax, %r9
 11014                             	        adcq	$0x00, %r10
 11015                             	        adcq	$0x00, %r11
 11016                             	        adcq	$0x00, %r12
 11017                             	        # Reduce if top bit set
 11018                             	        movq	%r12, %rdx
 11019                             	        sarq	$63, %rdx
 11020                             	        andq	$19, %rdx
 11021                             	        andq	%rcx, %r12
 11022                             	        addq	%rdx, %r9
 11023                             	        adcq	$0x00, %r10
 11024                             	        adcq	$0x00, %r11
 11025                             	        adcq	$0x00, %r12
 11026                             	        # Store
 11027                             	        movq	%r9, 128(%rsp)
 11028                             	        movq	%r10, 136(%rsp)
 11029                             	        movq	%r11, 144(%rsp)
 11030                             	        movq	%r12, 152(%rsp)
 11031                             	        # Add
 11032                             	        movq	32(%rsp), %r9
 11033                             	        movq	40(%rsp), %r10
 11034                             	        movq	48(%rsp), %r11
 11035                             	        movq	56(%rsp), %rax
 11036                             	        movq	%r9, %r13
 11037                             	        addq	(%rsp), %r9
 11038                             	        movq	%r10, %r14
 11039                             	        adcq	8(%rsp), %r10
 11040                             	        movq	%r11, %r15
 11041                             	        adcq	16(%rsp), %r11
 11042                             	        movq	%rax, %rbp
 11043                             	        adcq	24(%rsp), %rax
 11044                             	        movq	$-19, %rcx
 11045                             	        movq	%rax, %r12
 11046                             	        movq	$0x7fffffffffffffff, %rbx
 11047                             	        sarq	$63, %rax
 11048                             	        #   Mask the modulus
 11049                             	        andq	%rax, %rcx
 11050                             	        andq	%rax, %rbx
 11051                             	        #   Sub modulus (if overflow)
 11052                             	        subq	%rcx, %r9
 11053                             	        sbbq	%rax, %r10
 11054                             	        sbbq	%rax, %r11
 11055                             	        sbbq	%rbx, %r12
 11056                             	        # Sub
 11057                             	        subq	(%rsp), %r13
 11058                             	        movq	$0x00, %rax
 11059                             	        sbbq	8(%rsp), %r14
 11060                             	        movq	$-19, %rcx
 11061                             	        sbbq	16(%rsp), %r15
 11062                             	        movq	$0x7fffffffffffffff, %rbx
 11063                             	        sbbq	24(%rsp), %rbp
 11064                             	        sbbq	$0x00, %rax
 11065                             	        #   Mask the modulus
 11066                             	        andq	%rax, %rcx
 11067                             	        andq	%rax, %rbx
 11068                             	        #   Add modulus (if underflow)
 11069                             	        addq	%rcx, %r13
 11070                             	        adcq	%rax, %r14
 11071                             	        adcq	%rax, %r15
 11072                             	        adcq	%rbx, %rbp
 11073                             	        movq	%r9, 64(%rsp)
 11074                             	        movq	%r10, 72(%rsp)
 11075                             	        movq	%r11, 80(%rsp)
 11076                             	        movq	%r12, 88(%rsp)
 11077                             	        movq	%r13, (%rsp)
 11078                             	        movq	%r14, 8(%rsp)
 11079                             	        movq	%r15, 16(%rsp)
 11080                             	        movq	%rbp, 24(%rsp)
 11081                             	        # Multiply
 11082                             	        # A[0] * B[0]
 11083                             	        movq	96(%rsp), %rdx
 11084                             	        mulxq	128(%rsp), %r9, %r10
 11085                             	        # A[2] * B[0]
 11086                             	        mulxq	144(%rsp), %r11, %r12
 11087                             	        # A[1] * B[0]
 11088                             	        mulxq	136(%rsp), %rcx, %rbx
 11089                             	        xorq	%rbp, %rbp
 11090                             	        adcxq	%rcx, %r10
 11091                             	        # A[1] * B[3]
 11092                             	        movq	120(%rsp), %rdx
 11093                             	        mulxq	136(%rsp), %r13, %r14
 11094                             	        adcxq	%rbx, %r11
 11095                             	        # A[0] * B[1]
 11096                             	        movq	104(%rsp), %rdx
 11097                             	        mulxq	128(%rsp), %rcx, %rbx
 11098                             	        adoxq	%rcx, %r10
 11099                             	        # A[2] * B[1]
 11100                             	        mulxq	144(%rsp), %rcx, %r15
 11101                             	        adoxq	%rbx, %r11
 11102                             	        adcxq	%rcx, %r12
 11103                             	        # A[1] * B[2]
 11104                             	        movq	112(%rsp), %rdx
 11105                             	        mulxq	136(%rsp), %rcx, %rbx
 11106                             	        adcxq	%r15, %r13
 11107                             	        adoxq	%rcx, %r12
 11108                             	        adcxq	%rbp, %r14
 11109                             	        adoxq	%rbx, %r13
 11110                             	        # A[0] * B[2]
 11111                             	        mulxq	128(%rsp), %rcx, %rbx
 11112                             	        adoxq	%rbp, %r14
 11113                             	        xorq	%r15, %r15
 11114                             	        adcxq	%rcx, %r11
 11115                             	        # A[1] * B[1]
 11116                             	        movq	104(%rsp), %rdx
 11117                             	        mulxq	136(%rsp), %rdx, %rcx
 11118                             	        adcxq	%rbx, %r12
 11119                             	        adoxq	%rdx, %r11
 11120                             	        # A[3] * B[1]
 11121                             	        movq	104(%rsp), %rdx
 11122                             	        adoxq	%rcx, %r12
 11123                             	        mulxq	152(%rsp), %rcx, %rbx
 11124                             	        adcxq	%rcx, %r13
 11125                             	        # A[2] * B[2]
 11126                             	        movq	112(%rsp), %rdx
 11127                             	        mulxq	144(%rsp), %rdx, %rcx
 11128                             	        adcxq	%rbx, %r14
 11129                             	        adoxq	%rdx, %r13
 11130                             	        # A[3] * B[3]
 11131                             	        movq	120(%rsp), %rdx
 11132                             	        adoxq	%rcx, %r14
 11133                             	        mulxq	152(%rsp), %rcx, %rbx
 11134                             	        adoxq	%rbp, %r15
 11135                             	        adcxq	%rcx, %r15
 11136                             	        # A[0] * B[3]
 11137                             	        mulxq	128(%rsp), %rdx, %rcx
 11138                             	        adcxq	%rbx, %rbp
 11139                             	        xorq	%rbx, %rbx
 11140                             	        adcxq	%rdx, %r12
 11141                             	        # A[3] * B[0]
 11142                             	        movq	96(%rsp), %rdx
 11143                             	        adcxq	%rcx, %r13
 11144                             	        mulxq	152(%rsp), %rdx, %rcx
 11145                             	        adoxq	%rdx, %r12
 11146                             	        adoxq	%rcx, %r13
 11147                             	        # A[2] * B[3]
 11148                             	        movq	120(%rsp), %rdx
 11149                             	        mulxq	144(%rsp), %rdx, %rcx
 11150                             	        adcxq	%rdx, %r14
 11151                             	        # A[3] * B[2]
 11152                             	        movq	112(%rsp), %rdx
 11153                             	        adcxq	%rcx, %r15
 11154                             	        mulxq	152(%rsp), %rcx, %rdx
 11155                             	        adcxq	%rbx, %rbp
 11156                             	        adoxq	%rcx, %r14
 11157                             	        adoxq	%rdx, %r15
 11158                             	        adoxq	%rbx, %rbp
 11159                             	        # Reduce
 11160                             	        movq	$0x7fffffffffffffff, %rbx
 11161                             	        #  Move top half into t4-t7 and remove top bit from t3
 11162                             	        shldq	$0x01, %r15, %rbp
 11163                             	        shldq	$0x01, %r14, %r15
 11164                             	        shldq	$0x01, %r13, %r14
 11165                             	        shldq	$0x01, %r12, %r13
 11166                             	        andq	%rbx, %r12
 11167                             	        #  Multiply top half by 19
 11168                             	        movq	$19, %rdx
 11169                             	        xorq	%rbx, %rbx
 11170                             	        mulxq	%r13, %rcx, %r13
 11171                             	        adcxq	%rcx, %r9
 11172                             	        adoxq	%r13, %r10
 11173                             	        mulxq	%r14, %rcx, %r14
 11174                             	        adcxq	%rcx, %r10
 11175                             	        adoxq	%r14, %r11
 11176                             	        mulxq	%r15, %rcx, %r15
 11177                             	        adcxq	%rcx, %r11
 11178                             	        adoxq	%r15, %r12
 11179                             	        mulxq	%rbp, %rbp, %rdx
 11180                             	        adcxq	%rbp, %r12
 11181                             	        adoxq	%rbx, %rdx
 11182                             	        adcxq	%rbx, %rdx
 11183                             	        #  Overflow
 11184                             	        shldq	$0x01, %r12, %rdx
 11185                             	        movq	$0x7fffffffffffffff, %rbx
 11186                             	        imulq	$19, %rdx, %rcx
 11187                             	        andq	%rbx, %r12
 11188                             	        addq	%rcx, %r9
 11189                             	        adcq	$0x00, %r10
 11190                             	        adcq	$0x00, %r11
 11191                             	        adcq	$0x00, %r12
 11192                             	        # Reduce if top bit set
 11193                             	        movq	%r12, %rdx
 11194                             	        sarq	$63, %rdx
 11195                             	        andq	$19, %rdx
 11196                             	        andq	%rbx, %r12
 11197                             	        addq	%rdx, %r9
 11198                             	        adcq	$0x00, %r10
 11199                             	        adcq	$0x00, %r11
 11200                             	        adcq	$0x00, %r12
 11201                             	        # Store
 11202                             	        movq	%r9, (%rdi)
 11203                             	        movq	%r10, 8(%rdi)
 11204                             	        movq	%r11, 16(%rdi)
 11205                             	        movq	%r12, 24(%rdi)
 11206                             	        # Sub
 11207                             	        movq	128(%rsp), %r9
 11208                             	        movq	136(%rsp), %r10
 11209                             	        movq	144(%rsp), %r11
 11210                             	        movq	152(%rsp), %r12
 11211                             	        subq	96(%rsp), %r9
 11212                             	        movq	$0x00, %rax
 11213                             	        sbbq	104(%rsp), %r10
 11214                             	        movq	$-19, %rcx
 11215                             	        sbbq	112(%rsp), %r11
 11216                             	        movq	$0x7fffffffffffffff, %rbx
 11217                             	        sbbq	120(%rsp), %r12
 11218                             	        sbbq	$0x00, %rax
 11219                             	        #   Mask the modulus
 11220                             	        andq	%rax, %rcx
 11221                             	        andq	%rax, %rbx
 11222                             	        #   Add modulus (if underflow)
 11223                             	        addq	%rcx, %r9
 11224                             	        adcq	%rax, %r10
 11225                             	        adcq	%rax, %r11
 11226                             	        adcq	%rbx, %r12
 11227                             	        movq	%r9, 128(%rsp)
 11228                             	        movq	%r10, 136(%rsp)
 11229                             	        movq	%r11, 144(%rsp)
 11230                             	        movq	%r12, 152(%rsp)
 11231                             	        # Square
 11232                             	        # A[0] * A[1]
 11233                             	        movq	(%rsp), %rdx
 11234                             	        mulxq	8(%rsp), %r10, %r11
 11235                             	        # A[0] * A[3]
 11236                             	        mulxq	24(%rsp), %r12, %r13
 11237                             	        # A[2] * A[1]
 11238                             	        movq	16(%rsp), %rdx
 11239                             	        mulxq	8(%rsp), %rcx, %rbx
 11240                             	        xorq	%rbp, %rbp
 11241                             	        adoxq	%rcx, %r12
 11242                             	        # A[2] * A[3]
 11243                             	        mulxq	24(%rsp), %r14, %r15
 11244                             	        adoxq	%rbx, %r13
 11245                             	        # A[2] * A[0]
 11246                             	        mulxq	(%rsp), %rcx, %rbx
 11247                             	        adoxq	%rbp, %r14
 11248                             	        adcxq	%rcx, %r11
 11249                             	        adoxq	%rbp, %r15
 11250                             	        # A[1] * A[3]
 11251                             	        movq	8(%rsp), %rdx
 11252                             	        mulxq	24(%rsp), %rax, %r9
 11253                             	        adcxq	%rbx, %r12
 11254                             	        adcxq	%rax, %r13
 11255                             	        adcxq	%r9, %r14
 11256                             	        adcxq	%rbp, %r15
 11257                             	        # Double with Carry Flag
 11258                             	        xorq	%rbp, %rbp
 11259                             	        # A[0] * A[0]
 11260                             	        movq	(%rsp), %rdx
 11261                             	        mulxq	%rdx, %r9, %rax
 11262                             	        adcxq	%r10, %r10
 11263                             	        # A[1] * A[1]
 11264                             	        movq	8(%rsp), %rdx
 11265                             	        mulxq	%rdx, %rcx, %rbx
 11266                             	        adcxq	%r11, %r11
 11267                             	        adoxq	%rax, %r10
 11268                             	        adcxq	%r12, %r12
 11269                             	        adoxq	%rcx, %r11
 11270                             	        # A[2] * A[2]
 11271                             	        movq	16(%rsp), %rdx
 11272                             	        mulxq	%rdx, %rax, %rcx
 11273                             	        adcxq	%r13, %r13
 11274                             	        adoxq	%rbx, %r12
 11275                             	        adcxq	%r14, %r14
 11276                             	        adoxq	%rax, %r13
 11277                             	        # A[3] * A[3]
 11278                             	        movq	24(%rsp), %rdx
 11279                             	        mulxq	%rdx, %rax, %rbx
 11280                             	        adcxq	%r15, %r15
 11281                             	        adoxq	%rcx, %r14
 11282                             	        adcxq	%rbp, %rbp
 11283                             	        adoxq	%rax, %r15
 11284                             	        adoxq	%rbx, %rbp
 11285                             	        # Reduce
 11286                             	        movq	$0x7fffffffffffffff, %rcx
 11287                             	        #  Move top half into t4-t7 and remove top bit from t3
 11288                             	        shldq	$0x01, %r15, %rbp
 11289                             	        shldq	$0x01, %r14, %r15
 11290                             	        shldq	$0x01, %r13, %r14
 11291                             	        shldq	$0x01, %r12, %r13
 11292                             	        andq	%rcx, %r12
 11293                             	        #  Multiply top half by 19
 11294                             	        movq	$19, %rdx
 11295                             	        xorq	%rcx, %rcx
 11296                             	        mulxq	%r13, %rax, %r13
 11297                             	        adcxq	%rax, %r9
 11298                             	        adoxq	%r13, %r10
 11299                             	        mulxq	%r14, %rax, %r14
 11300                             	        adcxq	%rax, %r10
 11301                             	        adoxq	%r14, %r11
 11302                             	        mulxq	%r15, %rax, %r15
 11303                             	        adcxq	%rax, %r11
 11304                             	        adoxq	%r15, %r12
 11305                             	        mulxq	%rbp, %rbp, %rdx
 11306                             	        adcxq	%rbp, %r12
 11307                             	        adoxq	%rcx, %rdx
 11308                             	        adcxq	%rcx, %rdx
 11309                             	        #  Overflow
 11310                             	        shldq	$0x01, %r12, %rdx
 11311                             	        movq	$0x7fffffffffffffff, %rcx
 11312                             	        imulq	$19, %rdx, %rax
 11313                             	        andq	%rcx, %r12
 11314                             	        addq	%rax, %r9
 11315                             	        adcq	$0x00, %r10
 11316                             	        adcq	$0x00, %r11
 11317                             	        adcq	$0x00, %r12
 11318                             	        # Reduce if top bit set
 11319                             	        movq	%r12, %rdx
 11320                             	        sarq	$63, %rdx
 11321                             	        andq	$19, %rdx
 11322                             	        andq	%rcx, %r12
 11323                             	        addq	%rdx, %r9
 11324                             	        adcq	$0x00, %r10
 11325                             	        adcq	$0x00, %r11
 11326                             	        adcq	$0x00, %r12
 11327                             	        # Store
 11328                             	        movq	%r9, (%rsp)
 11329                             	        movq	%r10, 8(%rsp)
 11330                             	        movq	%r11, 16(%rsp)
 11331                             	        movq	%r12, 24(%rsp)
 11332                             	        movq	$0x1db42, %rdx
 11333                             	        mulxq	128(%rsp), %r9, %rbp
 11334                             	        mulxq	136(%rsp), %r10, %r15
 11335                             	        mulxq	144(%rsp), %r11, %r14
 11336                             	        mulxq	152(%rsp), %r12, %r13
 11337                             	        addq	%rbp, %r10
 11338                             	        adcq	%r15, %r11
 11339                             	        adcq	%r14, %r12
 11340                             	        adcq	$0x00, %r13
 11341                             	        movq	$0x7fffffffffffffff, %rbp
 11342                             	        shldq	$0x01, %r12, %r13
 11343                             	        andq	%rbp, %r12
 11344                             	        imulq	$19, %r13, %r13
 11345                             	        addq	%r13, %r9
 11346                             	        adcq	$0x00, %r10
 11347                             	        adcq	$0x00, %r11
 11348                             	        adcq	$0x00, %r12
 11349                             	        movq	%r9, 32(%rsp)
 11350                             	        movq	%r10, 40(%rsp)
 11351                             	        movq	%r11, 48(%rsp)
 11352                             	        movq	%r12, 56(%rsp)
 11353                             	        # Square
 11354                             	        # A[0] * A[1]
 11355                             	        movq	64(%rsp), %rdx
 11356                             	        mulxq	72(%rsp), %r10, %r11
 11357                             	        # A[0] * A[3]
 11358                             	        mulxq	88(%rsp), %r12, %r13
 11359                             	        # A[2] * A[1]
 11360                             	        movq	80(%rsp), %rdx
 11361                             	        mulxq	72(%rsp), %rcx, %rbx
 11362                             	        xorq	%rbp, %rbp
 11363                             	        adoxq	%rcx, %r12
 11364                             	        # A[2] * A[3]
 11365                             	        mulxq	88(%rsp), %r14, %r15
 11366                             	        adoxq	%rbx, %r13
 11367                             	        # A[2] * A[0]
 11368                             	        mulxq	64(%rsp), %rcx, %rbx
 11369                             	        adoxq	%rbp, %r14
 11370                             	        adcxq	%rcx, %r11
 11371                             	        adoxq	%rbp, %r15
 11372                             	        # A[1] * A[3]
 11373                             	        movq	72(%rsp), %rdx
 11374                             	        mulxq	88(%rsp), %rax, %r9
 11375                             	        adcxq	%rbx, %r12
 11376                             	        adcxq	%rax, %r13
 11377                             	        adcxq	%r9, %r14
 11378                             	        adcxq	%rbp, %r15
 11379                             	        # Double with Carry Flag
 11380                             	        xorq	%rbp, %rbp
 11381                             	        # A[0] * A[0]
 11382                             	        movq	64(%rsp), %rdx
 11383                             	        mulxq	%rdx, %r9, %rax
 11384                             	        adcxq	%r10, %r10
 11385                             	        # A[1] * A[1]
 11386                             	        movq	72(%rsp), %rdx
 11387                             	        mulxq	%rdx, %rcx, %rbx
 11388                             	        adcxq	%r11, %r11
 11389                             	        adoxq	%rax, %r10
 11390                             	        adcxq	%r12, %r12
 11391                             	        adoxq	%rcx, %r11
 11392                             	        # A[2] * A[2]
 11393                             	        movq	80(%rsp), %rdx
 11394                             	        mulxq	%rdx, %rax, %rcx
 11395                             	        adcxq	%r13, %r13
 11396                             	        adoxq	%rbx, %r12
 11397                             	        adcxq	%r14, %r14
 11398                             	        adoxq	%rax, %r13
 11399                             	        # A[3] * A[3]
 11400                             	        movq	88(%rsp), %rdx
 11401                             	        mulxq	%rdx, %rax, %rbx
 11402                             	        adcxq	%r15, %r15
 11403                             	        adoxq	%rcx, %r14
 11404                             	        adcxq	%rbp, %rbp
 11405                             	        adoxq	%rax, %r15
 11406                             	        adoxq	%rbx, %rbp
 11407                             	        # Reduce
 11408                             	        movq	$0x7fffffffffffffff, %rcx
 11409                             	        #  Move top half into t4-t7 and remove top bit from t3
 11410                             	        shldq	$0x01, %r15, %rbp
 11411                             	        shldq	$0x01, %r14, %r15
 11412                             	        shldq	$0x01, %r13, %r14
 11413                             	        shldq	$0x01, %r12, %r13
 11414                             	        andq	%rcx, %r12
 11415                             	        #  Multiply top half by 19
 11416                             	        movq	$19, %rdx
 11417                             	        xorq	%rcx, %rcx
 11418                             	        mulxq	%r13, %rax, %r13
 11419                             	        adcxq	%rax, %r9
 11420                             	        adoxq	%r13, %r10
 11421                             	        mulxq	%r14, %rax, %r14
 11422                             	        adcxq	%rax, %r10
 11423                             	        adoxq	%r14, %r11
 11424                             	        mulxq	%r15, %rax, %r15
 11425                             	        adcxq	%rax, %r11
 11426                             	        adoxq	%r15, %r12
 11427                             	        mulxq	%rbp, %rbp, %rdx
 11428                             	        adcxq	%rbp, %r12
 11429                             	        adoxq	%rcx, %rdx
 11430                             	        adcxq	%rcx, %rdx
 11431                             	        #  Overflow
 11432                             	        shldq	$0x01, %r12, %rdx
 11433                             	        movq	$0x7fffffffffffffff, %rcx
 11434                             	        imulq	$19, %rdx, %rax
 11435                             	        andq	%rcx, %r12
 11436                             	        addq	%rax, %r9
 11437                             	        adcq	$0x00, %r10
 11438                             	        adcq	$0x00, %r11
 11439                             	        adcq	$0x00, %r12
 11440                             	        # Reduce if top bit set
 11441                             	        movq	%r12, %rdx
 11442                             	        sarq	$63, %rdx
 11443                             	        andq	$19, %rdx
 11444                             	        andq	%rcx, %r12
 11445                             	        addq	%rdx, %r9
 11446                             	        adcq	$0x00, %r10
 11447                             	        adcq	$0x00, %r11
 11448                             	        adcq	$0x00, %r12
 11449                             	        # Store
 11450                             	        movq	%r9, 64(%rsp)
 11451                             	        movq	%r10, 72(%rsp)
 11452                             	        movq	%r11, 80(%rsp)
 11453                             	        movq	%r12, 88(%rsp)
 11454                             	        # Add
 11455                             	        movq	96(%rsp), %r9
 11456                             	        movq	104(%rsp), %r10
 11457                             	        addq	32(%rsp), %r9
 11458                             	        movq	112(%rsp), %r11
 11459                             	        adcq	40(%rsp), %r10
 11460                             	        movq	120(%rsp), %rax
 11461                             	        adcq	48(%rsp), %r11
 11462                             	        movq	$-19, %rcx
 11463                             	        adcq	56(%rsp), %rax
 11464                             	        movq	$0x7fffffffffffffff, %rbx
 11465                             	        movq	%rax, %r12
 11466                             	        sarq	$63, %rax
 11467                             	        #   Mask the modulus
 11468                             	        andq	%rax, %rcx
 11469                             	        andq	%rax, %rbx
 11470                             	        #   Sub modulus (if overflow)
 11471                             	        subq	%rcx, %r9
 11472                             	        sbbq	%rax, %r10
 11473                             	        sbbq	%rax, %r11
 11474                             	        sbbq	%rbx, %r12
 11475                             	        movq	%r9, 96(%rsp)
 11476                             	        movq	%r10, 104(%rsp)
 11477                             	        movq	%r11, 112(%rsp)
 11478                             	        movq	%r12, 120(%rsp)
 11479                             	        # Multiply
 11480                             	        # A[0] * B[0]
 11481                             	        movq	(%rsp), %rdx
 11482                             	        mulxq	(%r8), %r9, %r10
 11483                             	        # A[2] * B[0]
 11484                             	        mulxq	16(%r8), %r11, %r12
 11485                             	        # A[1] * B[0]
 11486                             	        mulxq	8(%r8), %rcx, %rbx
 11487                             	        xorq	%rbp, %rbp
 11488                             	        adcxq	%rcx, %r10
 11489                             	        # A[1] * B[3]
 11490                             	        movq	24(%rsp), %rdx
 11491                             	        mulxq	8(%r8), %r13, %r14
 11492                             	        adcxq	%rbx, %r11
 11493                             	        # A[0] * B[1]
 11494                             	        movq	8(%rsp), %rdx
 11495                             	        mulxq	(%r8), %rcx, %rbx
 11496                             	        adoxq	%rcx, %r10
 11497                             	        # A[2] * B[1]
 11498                             	        mulxq	16(%r8), %rcx, %r15
 11499                             	        adoxq	%rbx, %r11
 11500                             	        adcxq	%rcx, %r12
 11501                             	        # A[1] * B[2]
 11502                             	        movq	16(%rsp), %rdx
 11503                             	        mulxq	8(%r8), %rcx, %rbx
 11504                             	        adcxq	%r15, %r13
 11505                             	        adoxq	%rcx, %r12
 11506                             	        adcxq	%rbp, %r14
 11507                             	        adoxq	%rbx, %r13
 11508                             	        # A[0] * B[2]
 11509                             	        mulxq	(%r8), %rcx, %rbx
 11510                             	        adoxq	%rbp, %r14
 11511                             	        xorq	%r15, %r15
 11512                             	        adcxq	%rcx, %r11
 11513                             	        # A[1] * B[1]
 11514                             	        movq	8(%rsp), %rdx
 11515                             	        mulxq	8(%r8), %rdx, %rcx
 11516                             	        adcxq	%rbx, %r12
 11517                             	        adoxq	%rdx, %r11
 11518                             	        # A[3] * B[1]
 11519                             	        movq	8(%rsp), %rdx
 11520                             	        adoxq	%rcx, %r12
 11521                             	        mulxq	24(%r8), %rcx, %rbx
 11522                             	        adcxq	%rcx, %r13
 11523                             	        # A[2] * B[2]
 11524                             	        movq	16(%rsp), %rdx
 11525                             	        mulxq	16(%r8), %rdx, %rcx
 11526                             	        adcxq	%rbx, %r14
 11527                             	        adoxq	%rdx, %r13
 11528                             	        # A[3] * B[3]
 11529                             	        movq	24(%rsp), %rdx
 11530                             	        adoxq	%rcx, %r14
 11531                             	        mulxq	24(%r8), %rcx, %rbx
 11532                             	        adoxq	%rbp, %r15
 11533                             	        adcxq	%rcx, %r15
 11534                             	        # A[0] * B[3]
 11535                             	        mulxq	(%r8), %rdx, %rcx
 11536                             	        adcxq	%rbx, %rbp
 11537                             	        xorq	%rbx, %rbx
 11538                             	        adcxq	%rdx, %r12
 11539                             	        # A[3] * B[0]
 11540                             	        movq	(%rsp), %rdx
 11541                             	        adcxq	%rcx, %r13
 11542                             	        mulxq	24(%r8), %rdx, %rcx
 11543                             	        adoxq	%rdx, %r12
 11544                             	        adoxq	%rcx, %r13
 11545                             	        # A[2] * B[3]
 11546                             	        movq	24(%rsp), %rdx
 11547                             	        mulxq	16(%r8), %rdx, %rcx
 11548                             	        adcxq	%rdx, %r14
 11549                             	        # A[3] * B[2]
 11550                             	        movq	16(%rsp), %rdx
 11551                             	        adcxq	%rcx, %r15
 11552                             	        mulxq	24(%r8), %rcx, %rdx
 11553                             	        adcxq	%rbx, %rbp
 11554                             	        adoxq	%rcx, %r14
 11555                             	        adoxq	%rdx, %r15
 11556                             	        adoxq	%rbx, %rbp
 11557                             	        # Reduce
 11558                             	        movq	$0x7fffffffffffffff, %rbx
 11559                             	        #  Move top half into t4-t7 and remove top bit from t3
 11560                             	        shldq	$0x01, %r15, %rbp
 11561                             	        shldq	$0x01, %r14, %r15
 11562                             	        shldq	$0x01, %r13, %r14
 11563                             	        shldq	$0x01, %r12, %r13
 11564                             	        andq	%rbx, %r12
 11565                             	        #  Multiply top half by 19
 11566                             	        movq	$19, %rdx
 11567                             	        xorq	%rbx, %rbx
 11568                             	        mulxq	%r13, %rcx, %r13
 11569                             	        adcxq	%rcx, %r9
 11570                             	        adoxq	%r13, %r10
 11571                             	        mulxq	%r14, %rcx, %r14
 11572                             	        adcxq	%rcx, %r10
 11573                             	        adoxq	%r14, %r11
 11574                             	        mulxq	%r15, %rcx, %r15
 11575                             	        adcxq	%rcx, %r11
 11576                             	        adoxq	%r15, %r12
 11577                             	        mulxq	%rbp, %rbp, %rdx
 11578                             	        adcxq	%rbp, %r12
 11579                             	        adoxq	%rbx, %rdx
 11580                             	        adcxq	%rbx, %rdx
 11581                             	        #  Overflow
 11582                             	        shldq	$0x01, %r12, %rdx
 11583                             	        movq	$0x7fffffffffffffff, %rbx
 11584                             	        imulq	$19, %rdx, %rcx
 11585                             	        andq	%rbx, %r12
 11586                             	        addq	%rcx, %r9
 11587                             	        adcq	$0x00, %r10
 11588                             	        adcq	$0x00, %r11
 11589                             	        adcq	$0x00, %r12
 11590                             	        # Reduce if top bit set
 11591                             	        movq	%r12, %rdx
 11592                             	        sarq	$63, %rdx
 11593                             	        andq	$19, %rdx
 11594                             	        andq	%rbx, %r12
 11595                             	        addq	%rdx, %r9
 11596                             	        adcq	$0x00, %r10
 11597                             	        adcq	$0x00, %r11
 11598                             	        adcq	$0x00, %r12
 11599                             	        # Store
 11600                             	        movq	%r9, 32(%rsp)
 11601                             	        movq	%r10, 40(%rsp)
 11602                             	        movq	%r11, 48(%rsp)
 11603                             	        movq	%r12, 56(%rsp)
 11604                             	        # Multiply
 11605                             	        # A[0] * B[0]
 11606                             	        movq	96(%rsp), %rdx
 11607                             	        mulxq	128(%rsp), %r9, %r10
 11608                             	        # A[2] * B[0]
 11609                             	        mulxq	144(%rsp), %r11, %r12
 11610                             	        # A[1] * B[0]
 11611                             	        mulxq	136(%rsp), %rcx, %rbx
 11612                             	        xorq	%rbp, %rbp
 11613                             	        adcxq	%rcx, %r10
 11614                             	        # A[1] * B[3]
 11615                             	        movq	120(%rsp), %rdx
 11616                             	        mulxq	136(%rsp), %r13, %r14
 11617                             	        adcxq	%rbx, %r11
 11618                             	        # A[0] * B[1]
 11619                             	        movq	104(%rsp), %rdx
 11620                             	        mulxq	128(%rsp), %rcx, %rbx
 11621                             	        adoxq	%rcx, %r10
 11622                             	        # A[2] * B[1]
 11623                             	        mulxq	144(%rsp), %rcx, %r15
 11624                             	        adoxq	%rbx, %r11
 11625                             	        adcxq	%rcx, %r12
 11626                             	        # A[1] * B[2]
 11627                             	        movq	112(%rsp), %rdx
 11628                             	        mulxq	136(%rsp), %rcx, %rbx
 11629                             	        adcxq	%r15, %r13
 11630                             	        adoxq	%rcx, %r12
 11631                             	        adcxq	%rbp, %r14
 11632                             	        adoxq	%rbx, %r13
 11633                             	        # A[0] * B[2]
 11634                             	        mulxq	128(%rsp), %rcx, %rbx
 11635                             	        adoxq	%rbp, %r14
 11636                             	        xorq	%r15, %r15
 11637                             	        adcxq	%rcx, %r11
 11638                             	        # A[1] * B[1]
 11639                             	        movq	104(%rsp), %rdx
 11640                             	        mulxq	136(%rsp), %rdx, %rcx
 11641                             	        adcxq	%rbx, %r12
 11642                             	        adoxq	%rdx, %r11
 11643                             	        # A[3] * B[1]
 11644                             	        movq	104(%rsp), %rdx
 11645                             	        adoxq	%rcx, %r12
 11646                             	        mulxq	152(%rsp), %rcx, %rbx
 11647                             	        adcxq	%rcx, %r13
 11648                             	        # A[2] * B[2]
 11649                             	        movq	112(%rsp), %rdx
 11650                             	        mulxq	144(%rsp), %rdx, %rcx
 11651                             	        adcxq	%rbx, %r14
 11652                             	        adoxq	%rdx, %r13
 11653                             	        # A[3] * B[3]
 11654                             	        movq	120(%rsp), %rdx
 11655                             	        adoxq	%rcx, %r14
 11656                             	        mulxq	152(%rsp), %rcx, %rbx
 11657                             	        adoxq	%rbp, %r15
 11658                             	        adcxq	%rcx, %r15
 11659                             	        # A[0] * B[3]
 11660                             	        mulxq	128(%rsp), %rdx, %rcx
 11661                             	        adcxq	%rbx, %rbp
 11662                             	        xorq	%rbx, %rbx
 11663                             	        adcxq	%rdx, %r12
 11664                             	        # A[3] * B[0]
 11665                             	        movq	96(%rsp), %rdx
 11666                             	        adcxq	%rcx, %r13
 11667                             	        mulxq	152(%rsp), %rdx, %rcx
 11668                             	        adoxq	%rdx, %r12
 11669                             	        adoxq	%rcx, %r13
 11670                             	        # A[2] * B[3]
 11671                             	        movq	120(%rsp), %rdx
 11672                             	        mulxq	144(%rsp), %rdx, %rcx
 11673                             	        adcxq	%rdx, %r14
 11674                             	        # A[3] * B[2]
 11675                             	        movq	112(%rsp), %rdx
 11676                             	        adcxq	%rcx, %r15
 11677                             	        mulxq	152(%rsp), %rcx, %rdx
 11678                             	        adcxq	%rbx, %rbp
 11679                             	        adoxq	%rcx, %r14
 11680                             	        adoxq	%rdx, %r15
 11681                             	        adoxq	%rbx, %rbp
 11682                             	        # Reduce
 11683                             	        movq	$0x7fffffffffffffff, %rbx
 11684                             	        #  Move top half into t4-t7 and remove top bit from t3
 11685                             	        shldq	$0x01, %r15, %rbp
 11686                             	        shldq	$0x01, %r14, %r15
 11687                             	        shldq	$0x01, %r13, %r14
 11688                             	        shldq	$0x01, %r12, %r13
 11689                             	        andq	%rbx, %r12
 11690                             	        #  Multiply top half by 19
 11691                             	        movq	$19, %rdx
 11692                             	        xorq	%rbx, %rbx
 11693                             	        mulxq	%r13, %rcx, %r13
 11694                             	        adcxq	%rcx, %r9
 11695                             	        adoxq	%r13, %r10
 11696                             	        mulxq	%r14, %rcx, %r14
 11697                             	        adcxq	%rcx, %r10
 11698                             	        adoxq	%r14, %r11
 11699                             	        mulxq	%r15, %rcx, %r15
 11700                             	        adcxq	%rcx, %r11
 11701                             	        adoxq	%r15, %r12
 11702                             	        mulxq	%rbp, %rbp, %rdx
 11703                             	        adcxq	%rbp, %r12
 11704                             	        adoxq	%rbx, %rdx
 11705                             	        adcxq	%rbx, %rdx
 11706                             	        #  Overflow
 11707                             	        shldq	$0x01, %r12, %rdx
 11708                             	        movq	$0x7fffffffffffffff, %rbx
 11709                             	        imulq	$19, %rdx, %rcx
 11710                             	        andq	%rbx, %r12
 11711                             	        addq	%rcx, %r9
 11712                             	        adcq	$0x00, %r10
 11713                             	        adcq	$0x00, %r11
 11714                             	        adcq	$0x00, %r12
 11715                             	        # Reduce if top bit set
 11716                             	        movq	%r12, %rdx
 11717                             	        sarq	$63, %rdx
 11718                             	        andq	$19, %rdx
 11719                             	        andq	%rbx, %r12
 11720                             	        addq	%rdx, %r9
 11721                             	        adcq	$0x00, %r10
 11722                             	        adcq	$0x00, %r11
 11723                             	        adcq	$0x00, %r12
 11724                             	        # Store
 11725                             	        movq	%r9, (%rsp)
 11726                             	        movq	%r10, 8(%rsp)
 11727                             	        movq	%r11, 16(%rsp)
 11728                             	        movq	%r12, 24(%rsp)
 11729                             	        decb	168(%rsp)
 11730                             	        jge	L_curve25519_avx2_bits
 11731                             	        movq	$63, 168(%rsp)
 11732                             	        decb	160(%rsp)
 11733                             	        jge	L_curve25519_avx2_words
 11734                             	        # Invert
 11735                             	        leaq	32(%rsp), %rdi
 11736                             	        movq	%rsp, %rsi
 11737                             	#ifndef __APPLE__
 11738                             	        callq	fe_sq_avx2@plt
 11739                             	#else
 11740                             	        callq	_fe_sq_avx2
 11741                             	#endif /* __APPLE__ */
 11742                             	        leaq	64(%rsp), %rdi
 11743                             	        leaq	32(%rsp), %rsi
 11744                             	#ifndef __APPLE__
 11745                             	        callq	fe_sq_avx2@plt
 11746                             	#else
 11747                             	        callq	_fe_sq_avx2
 11748                             	#endif /* __APPLE__ */
 11749                             	        leaq	64(%rsp), %rdi
 11750                             	        leaq	64(%rsp), %rsi
 11751                             	#ifndef __APPLE__
 11752                             	        callq	fe_sq_avx2@plt
 11753                             	#else
 11754                             	        callq	_fe_sq_avx2
 11755                             	#endif /* __APPLE__ */
 11756                             	        leaq	64(%rsp), %rdi
 11757                             	        movq	%rsp, %rsi
 11758                             	        leaq	64(%rsp), %rdx
 11759                             	#ifndef __APPLE__
 11760                             	        callq	fe_mul_avx2@plt
 11761                             	#else
 11762                             	        callq	_fe_mul_avx2
 11763                             	#endif /* __APPLE__ */
 11764                             	        leaq	32(%rsp), %rdi
 11765                             	        leaq	32(%rsp), %rsi
 11766                             	        leaq	64(%rsp), %rdx
 11767                             	#ifndef __APPLE__
 11768                             	        callq	fe_mul_avx2@plt
 11769                             	#else
 11770                             	        callq	_fe_mul_avx2
 11771                             	#endif /* __APPLE__ */
 11772                             	        leaq	96(%rsp), %rdi
 11773                             	        leaq	32(%rsp), %rsi
 11774                             	#ifndef __APPLE__
 11775                             	        callq	fe_sq_avx2@plt
 11776                             	#else
 11777                             	        callq	_fe_sq_avx2
 11778                             	#endif /* __APPLE__ */
 11779                             	        leaq	64(%rsp), %rdi
 11780                             	        leaq	64(%rsp), %rsi
 11781                             	        leaq	96(%rsp), %rdx
 11782                             	#ifndef __APPLE__
 11783                             	        callq	fe_mul_avx2@plt
 11784                             	#else
 11785                             	        callq	_fe_mul_avx2
 11786                             	#endif /* __APPLE__ */
 11787                             	        leaq	96(%rsp), %rdi
 11788                             	        leaq	64(%rsp), %rsi
 11789                             	#ifndef __APPLE__
 11790                             	        callq	fe_sq_avx2@plt
 11791                             	#else
 11792                             	        callq	_fe_sq_avx2
 11793                             	#endif /* __APPLE__ */
 11794                             	        leaq	96(%rsp), %rdi
 11795                             	        leaq	96(%rsp), %rsi
 11796                             	        movq	$4, %rdx
 11797                             	#ifndef __APPLE__
 11798                             	        callq	fe_sq_n_avx2@plt
 11799                             	#else
 11800                             	        callq	_fe_sq_n_avx2
 11801                             	#endif /* __APPLE__ */
 11802                             	        leaq	64(%rsp), %rdi
 11803                             	        leaq	96(%rsp), %rsi
 11804                             	        leaq	64(%rsp), %rdx
 11805                             	#ifndef __APPLE__
 11806                             	        callq	fe_mul_avx2@plt
 11807                             	#else
 11808                             	        callq	_fe_mul_avx2
 11809                             	#endif /* __APPLE__ */
 11810                             	        leaq	96(%rsp), %rdi
 11811                             	        leaq	64(%rsp), %rsi
 11812                             	#ifndef __APPLE__
 11813                             	        callq	fe_sq_avx2@plt
 11814                             	#else
 11815                             	        callq	_fe_sq_avx2
 11816                             	#endif /* __APPLE__ */
 11817                             	        leaq	96(%rsp), %rdi
 11818                             	        leaq	96(%rsp), %rsi
 11819                             	        movq	$9, %rdx
 11820                             	#ifndef __APPLE__
 11821                             	        callq	fe_sq_n_avx2@plt
 11822                             	#else
 11823                             	        callq	_fe_sq_n_avx2
 11824                             	#endif /* __APPLE__ */
 11825                             	        leaq	96(%rsp), %rdi
 11826                             	        leaq	96(%rsp), %rsi
 11827                             	        leaq	64(%rsp), %rdx
 11828                             	#ifndef __APPLE__
 11829                             	        callq	fe_mul_avx2@plt
 11830                             	#else
 11831                             	        callq	_fe_mul_avx2
 11832                             	#endif /* __APPLE__ */
 11833                             	        leaq	128(%rsp), %rdi
 11834                             	        leaq	96(%rsp), %rsi
 11835                             	#ifndef __APPLE__
 11836                             	        callq	fe_sq_avx2@plt
 11837                             	#else
 11838                             	        callq	_fe_sq_avx2
 11839                             	#endif /* __APPLE__ */
 11840                             	        leaq	128(%rsp), %rdi
 11841                             	        leaq	128(%rsp), %rsi
 11842                             	        movq	$19, %rdx
 11843                             	#ifndef __APPLE__
 11844                             	        callq	fe_sq_n_avx2@plt
 11845                             	#else
 11846                             	        callq	_fe_sq_n_avx2
 11847                             	#endif /* __APPLE__ */
 11848                             	        leaq	96(%rsp), %rdi
 11849                             	        leaq	128(%rsp), %rsi
 11850                             	        leaq	96(%rsp), %rdx
 11851                             	#ifndef __APPLE__
 11852                             	        callq	fe_mul_avx2@plt
 11853                             	#else
 11854                             	        callq	_fe_mul_avx2
 11855                             	#endif /* __APPLE__ */
 11856                             	        leaq	96(%rsp), %rdi
 11857                             	        leaq	96(%rsp), %rsi
 11858                             	#ifndef __APPLE__
 11859                             	        callq	fe_sq_avx2@plt
 11860                             	#else
 11861                             	        callq	_fe_sq_avx2
 11862                             	#endif /* __APPLE__ */
 11863                             	        leaq	96(%rsp), %rdi
 11864                             	        leaq	96(%rsp), %rsi
 11865                             	        movq	$9, %rdx
 11866                             	#ifndef __APPLE__
 11867                             	        callq	fe_sq_n_avx2@plt
 11868                             	#else
 11869                             	        callq	_fe_sq_n_avx2
 11870                             	#endif /* __APPLE__ */
 11871                             	        leaq	64(%rsp), %rdi
 11872                             	        leaq	96(%rsp), %rsi
 11873                             	        leaq	64(%rsp), %rdx
 11874                             	#ifndef __APPLE__
 11875                             	        callq	fe_mul_avx2@plt
 11876                             	#else
 11877                             	        callq	_fe_mul_avx2
 11878                             	#endif /* __APPLE__ */
 11879                             	        leaq	96(%rsp), %rdi
 11880                             	        leaq	64(%rsp), %rsi
 11881                             	#ifndef __APPLE__
 11882                             	        callq	fe_sq_avx2@plt
 11883                             	#else
 11884                             	        callq	_fe_sq_avx2
 11885                             	#endif /* __APPLE__ */
 11886                             	        leaq	96(%rsp), %rdi
 11887                             	        leaq	96(%rsp), %rsi
 11888                             	        movq	$49, %rdx
 11889                             	#ifndef __APPLE__
 11890                             	        callq	fe_sq_n_avx2@plt
 11891                             	#else
 11892                             	        callq	_fe_sq_n_avx2
 11893                             	#endif /* __APPLE__ */
 11894                             	        leaq	96(%rsp), %rdi
 11895                             	        leaq	96(%rsp), %rsi
 11896                             	        leaq	64(%rsp), %rdx
 11897                             	#ifndef __APPLE__
 11898                             	        callq	fe_mul_avx2@plt
 11899                             	#else
 11900                             	        callq	_fe_mul_avx2
 11901                             	#endif /* __APPLE__ */
 11902                             	        leaq	128(%rsp), %rdi
 11903                             	        leaq	96(%rsp), %rsi
 11904                             	#ifndef __APPLE__
 11905                             	        callq	fe_sq_avx2@plt
 11906                             	#else
 11907                             	        callq	_fe_sq_avx2
 11908                             	#endif /* __APPLE__ */
 11909                             	        leaq	128(%rsp), %rdi
 11910                             	        leaq	128(%rsp), %rsi
 11911                             	        movq	$0x63, %rdx
 11912                             	#ifndef __APPLE__
 11913                             	        callq	fe_sq_n_avx2@plt
 11914                             	#else
 11915                             	        callq	_fe_sq_n_avx2
 11916                             	#endif /* __APPLE__ */
 11917                             	        leaq	96(%rsp), %rdi
 11918                             	        leaq	128(%rsp), %rsi
 11919                             	        leaq	96(%rsp), %rdx
 11920                             	#ifndef __APPLE__
 11921                             	        callq	fe_mul_avx2@plt
 11922                             	#else
 11923                             	        callq	_fe_mul_avx2
 11924                             	#endif /* __APPLE__ */
 11925                             	        leaq	96(%rsp), %rdi
 11926                             	        leaq	96(%rsp), %rsi
 11927                             	#ifndef __APPLE__
 11928                             	        callq	fe_sq_avx2@plt
 11929                             	#else
 11930                             	        callq	_fe_sq_avx2
 11931                             	#endif /* __APPLE__ */
 11932                             	        leaq	96(%rsp), %rdi
 11933                             	        leaq	96(%rsp), %rsi
 11934                             	        movq	$49, %rdx
 11935                             	#ifndef __APPLE__
 11936                             	        callq	fe_sq_n_avx2@plt
 11937                             	#else
 11938                             	        callq	_fe_sq_n_avx2
 11939                             	#endif /* __APPLE__ */
 11940                             	        leaq	64(%rsp), %rdi
 11941                             	        leaq	96(%rsp), %rsi
 11942                             	        leaq	64(%rsp), %rdx
 11943                             	#ifndef __APPLE__
 11944                             	        callq	fe_mul_avx2@plt
 11945                             	#else
 11946                             	        callq	_fe_mul_avx2
 11947                             	#endif /* __APPLE__ */
 11948                             	        leaq	64(%rsp), %rdi
 11949                             	        leaq	64(%rsp), %rsi
 11950                             	#ifndef __APPLE__
 11951                             	        callq	fe_sq_avx2@plt
 11952                             	#else
 11953                             	        callq	_fe_sq_avx2
 11954                             	#endif /* __APPLE__ */
 11955                             	        leaq	64(%rsp), %rdi
 11956                             	        leaq	64(%rsp), %rsi
 11957                             	        movq	$4, %rdx
 11958                             	#ifndef __APPLE__
 11959                             	        callq	fe_sq_n_avx2@plt
 11960                             	#else
 11961                             	        callq	_fe_sq_n_avx2
 11962                             	#endif /* __APPLE__ */
 11963                             	        movq	%rsp, %rdi
 11964                             	        leaq	64(%rsp), %rsi
 11965                             	        leaq	32(%rsp), %rdx
 11966                             	#ifndef __APPLE__
 11967                             	        callq	fe_mul_avx2@plt
 11968                             	#else
 11969                             	        callq	_fe_mul_avx2
 11970                             	#endif /* __APPLE__ */
 11971                             	        movq	176(%rsp), %rdi
 11972                             	        # Multiply
 11973                             	        # A[0] * B[0]
 11974                             	        movq	(%rsp), %rdx
 11975                             	        mulxq	(%rdi), %r9, %r10
 11976                             	        # A[2] * B[0]
 11977                             	        mulxq	16(%rdi), %r11, %r12
 11978                             	        # A[1] * B[0]
 11979                             	        mulxq	8(%rdi), %rcx, %rbx
 11980                             	        xorq	%rbp, %rbp
 11981                             	        adcxq	%rcx, %r10
 11982                             	        # A[1] * B[3]
 11983                             	        movq	24(%rsp), %rdx
 11984                             	        mulxq	8(%rdi), %r13, %r14
 11985                             	        adcxq	%rbx, %r11
 11986                             	        # A[0] * B[1]
 11987                             	        movq	8(%rsp), %rdx
 11988                             	        mulxq	(%rdi), %rcx, %rbx
 11989                             	        adoxq	%rcx, %r10
 11990                             	        # A[2] * B[1]
 11991                             	        mulxq	16(%rdi), %rcx, %r15
 11992                             	        adoxq	%rbx, %r11
 11993                             	        adcxq	%rcx, %r12
 11994                             	        # A[1] * B[2]
 11995                             	        movq	16(%rsp), %rdx
 11996                             	        mulxq	8(%rdi), %rcx, %rbx
 11997                             	        adcxq	%r15, %r13
 11998                             	        adoxq	%rcx, %r12
 11999                             	        adcxq	%rbp, %r14
 12000                             	        adoxq	%rbx, %r13
 12001                             	        # A[0] * B[2]
 12002                             	        mulxq	(%rdi), %rcx, %rbx
 12003                             	        adoxq	%rbp, %r14
 12004                             	        xorq	%r15, %r15
 12005                             	        adcxq	%rcx, %r11
 12006                             	        # A[1] * B[1]
 12007                             	        movq	8(%rsp), %rdx
 12008                             	        mulxq	8(%rdi), %rdx, %rcx
 12009                             	        adcxq	%rbx, %r12
 12010                             	        adoxq	%rdx, %r11
 12011                             	        # A[3] * B[1]
 12012                             	        movq	8(%rsp), %rdx
 12013                             	        adoxq	%rcx, %r12
 12014                             	        mulxq	24(%rdi), %rcx, %rbx
 12015                             	        adcxq	%rcx, %r13
 12016                             	        # A[2] * B[2]
 12017                             	        movq	16(%rsp), %rdx
 12018                             	        mulxq	16(%rdi), %rdx, %rcx
 12019                             	        adcxq	%rbx, %r14
 12020                             	        adoxq	%rdx, %r13
 12021                             	        # A[3] * B[3]
 12022                             	        movq	24(%rsp), %rdx
 12023                             	        adoxq	%rcx, %r14
 12024                             	        mulxq	24(%rdi), %rcx, %rbx
 12025                             	        adoxq	%rbp, %r15
 12026                             	        adcxq	%rcx, %r15
 12027                             	        # A[0] * B[3]
 12028                             	        mulxq	(%rdi), %rdx, %rcx
 12029                             	        adcxq	%rbx, %rbp
 12030                             	        xorq	%rbx, %rbx
 12031                             	        adcxq	%rdx, %r12
 12032                             	        # A[3] * B[0]
 12033                             	        movq	(%rsp), %rdx
 12034                             	        adcxq	%rcx, %r13
 12035                             	        mulxq	24(%rdi), %rdx, %rcx
 12036                             	        adoxq	%rdx, %r12
 12037                             	        adoxq	%rcx, %r13
 12038                             	        # A[2] * B[3]
 12039                             	        movq	24(%rsp), %rdx
 12040                             	        mulxq	16(%rdi), %rdx, %rcx
 12041                             	        adcxq	%rdx, %r14
 12042                             	        # A[3] * B[2]
 12043                             	        movq	16(%rsp), %rdx
 12044                             	        adcxq	%rcx, %r15
 12045                             	        mulxq	24(%rdi), %rcx, %rdx
 12046                             	        adcxq	%rbx, %rbp
 12047                             	        adoxq	%rcx, %r14
 12048                             	        adoxq	%rdx, %r15
 12049                             	        adoxq	%rbx, %rbp
 12050                             	        # Reduce
 12051                             	        movq	$0x7fffffffffffffff, %rbx
 12052                             	        #  Move top half into t4-t7 and remove top bit from t3
 12053                             	        shldq	$0x01, %r15, %rbp
 12054                             	        shldq	$0x01, %r14, %r15
 12055                             	        shldq	$0x01, %r13, %r14
 12056                             	        shldq	$0x01, %r12, %r13
 12057                             	        andq	%rbx, %r12
 12058                             	        #  Multiply top half by 19
 12059                             	        movq	$19, %rdx
 12060                             	        xorq	%rbx, %rbx
 12061                             	        mulxq	%r13, %rcx, %r13
 12062                             	        adcxq	%rcx, %r9
 12063                             	        adoxq	%r13, %r10
 12064                             	        mulxq	%r14, %rcx, %r14
 12065                             	        adcxq	%rcx, %r10
 12066                             	        adoxq	%r14, %r11
 12067                             	        mulxq	%r15, %rcx, %r15
 12068                             	        adcxq	%rcx, %r11
 12069                             	        adoxq	%r15, %r12
 12070                             	        mulxq	%rbp, %rbp, %rdx
 12071                             	        adcxq	%rbp, %r12
 12072                             	        adoxq	%rbx, %rdx
 12073                             	        adcxq	%rbx, %rdx
 12074                             	        #  Overflow
 12075                             	        shldq	$0x01, %r12, %rdx
 12076                             	        movq	$0x7fffffffffffffff, %rbx
 12077                             	        imulq	$19, %rdx, %rcx
 12078                             	        andq	%rbx, %r12
 12079                             	        addq	%rcx, %r9
 12080                             	        adcq	$0x00, %r10
 12081                             	        adcq	$0x00, %r11
 12082                             	        adcq	$0x00, %r12
 12083                             	        # Reduce if top bit set
 12084                             	        movq	%r12, %rdx
 12085                             	        sarq	$63, %rdx
 12086                             	        andq	$19, %rdx
 12087                             	        andq	%rbx, %r12
 12088                             	        addq	%rdx, %r9
 12089                             	        adcq	$0x00, %r10
 12090                             	        adcq	$0x00, %r11
 12091                             	        adcq	$0x00, %r12
 12092                             	        movq	$0x7fffffffffffffff, %rbx
 12093                             	        movq	%r9, %rdx
 12094                             	        addq	$19, %rdx
 12095                             	        movq	%r10, %rdx
 12096                             	        adcq	$0x00, %rdx
 12097                             	        movq	%r11, %rdx
 12098                             	        adcq	$0x00, %rdx
 12099                             	        movq	%r12, %rdx
 12100                             	        adcq	$0x00, %rdx
 12101                             	        sarq	$63, %rdx
 12102                             	        andq	$19, %rdx
 12103                             	        addq	%rdx, %r9
 12104                             	        adcq	$0x00, %r10
 12105                             	        adcq	$0x00, %r11
 12106                             	        adcq	$0x00, %r12
 12107                             	        andq	%rbx, %r12
 12108                             	        # Store
 12109                             	        movq	%r9, (%rdi)
 12110                             	        movq	%r10, 8(%rdi)
 12111                             	        movq	%r11, 16(%rdi)
 12112                             	        movq	%r12, 24(%rdi)
 12113                             	        xorq	%rax, %rax
 12114                             	        addq	$0xc0, %rsp
 12115                             	        popq	%rbp
 12116                             	        popq	%r15
 12117                             	        popq	%r14
 12118                             	        popq	%r13
 12119                             	        popq	%r12
 12120                             	        popq	%rbx
 12121                             	        repz retq
 12122                             	#ifndef __APPLE__
 12124                             	#endif /* __APPLE__ */
 12125                             	#ifndef __APPLE__
 12126                             	.text
 12127                             	.globl	fe_pow22523_avx2
 12129                             	.align	16
 12130                             	fe_pow22523_avx2:
 12131                             	#else
 12132                             	.section	__TEXT,__text
 12133                             	.globl	_fe_pow22523_avx2
 12134                             	.p2align	4
 12135                             	_fe_pow22523_avx2:
 12136                             	#endif /* __APPLE__ */
 12137                             	        subq	$0x70, %rsp
 12138                             	        # pow22523
 12139                             	        movq	%rdi, 96(%rsp)
 12140                             	        movq	%rsi, 104(%rsp)
 12141                             	        movq	%rsp, %rdi
 12142                             	        movq	104(%rsp), %rsi
 12143                             	#ifndef __APPLE__
 12144                             	        callq	fe_sq_avx2@plt
 12145                             	#else
 12146                             	        callq	_fe_sq_avx2
 12147                             	#endif /* __APPLE__ */
 12148                             	        leaq	32(%rsp), %rdi
 12149                             	        movq	%rsp, %rsi
 12150                             	#ifndef __APPLE__
 12151                             	        callq	fe_sq_avx2@plt
 12152                             	#else
 12153                             	        callq	_fe_sq_avx2
 12154                             	#endif /* __APPLE__ */
 12155                             	        leaq	32(%rsp), %rdi
 12156                             	        leaq	32(%rsp), %rsi
 12157                             	#ifndef __APPLE__
 12158                             	        callq	fe_sq_avx2@plt
 12159                             	#else
 12160                             	        callq	_fe_sq_avx2
 12161                             	#endif /* __APPLE__ */
 12162                             	        leaq	32(%rsp), %rdi
 12163                             	        movq	104(%rsp), %rsi
 12164                             	        leaq	32(%rsp), %rdx
 12165                             	#ifndef __APPLE__
 12166                             	        callq	fe_mul_avx2@plt
 12167                             	#else
 12168                             	        callq	_fe_mul_avx2
 12169                             	#endif /* __APPLE__ */
 12170                             	        movq	%rsp, %rdi
 12171                             	        movq	%rsp, %rsi
 12172                             	        leaq	32(%rsp), %rdx
 12173                             	#ifndef __APPLE__
 12174                             	        callq	fe_mul_avx2@plt
 12175                             	#else
 12176                             	        callq	_fe_mul_avx2
 12177                             	#endif /* __APPLE__ */
 12178                             	        movq	%rsp, %rdi
 12179                             	        movq	%rsp, %rsi
 12180                             	#ifndef __APPLE__
 12181                             	        callq	fe_sq_avx2@plt
 12182                             	#else
 12183                             	        callq	_fe_sq_avx2
 12184                             	#endif /* __APPLE__ */
 12185                             	        movq	%rsp, %rdi
 12186                             	        leaq	32(%rsp), %rsi
 12187                             	        movq	%rsp, %rdx
 12188                             	#ifndef __APPLE__
 12189                             	        callq	fe_mul_avx2@plt
 12190                             	#else
 12191                             	        callq	_fe_mul_avx2
 12192                             	#endif /* __APPLE__ */
 12193                             	        leaq	32(%rsp), %rdi
 12194                             	        movq	%rsp, %rsi
 12195                             	#ifndef __APPLE__
 12196                             	        callq	fe_sq_avx2@plt
 12197                             	#else
 12198                             	        callq	_fe_sq_avx2
 12199                             	#endif /* __APPLE__ */
 12200                             	        leaq	32(%rsp), %rdi
 12201                             	        leaq	32(%rsp), %rsi
 12202                             	        movb	$4, %dl
 12203                             	#ifndef __APPLE__
 12204                             	        callq	fe_sq_n_avx2@plt
 12205                             	#else
 12206                             	        callq	_fe_sq_n_avx2
 12207                             	#endif /* __APPLE__ */
 12208                             	        movq	%rsp, %rdi
 12209                             	        leaq	32(%rsp), %rsi
 12210                             	        movq	%rsp, %rdx
 12211                             	#ifndef __APPLE__
 12212                             	        callq	fe_mul_avx2@plt
 12213                             	#else
 12214                             	        callq	_fe_mul_avx2
 12215                             	#endif /* __APPLE__ */
 12216                             	        leaq	32(%rsp), %rdi
 12217                             	        movq	%rsp, %rsi
 12218                             	#ifndef __APPLE__
 12219                             	        callq	fe_sq_avx2@plt
 12220                             	#else
 12221                             	        callq	_fe_sq_avx2
 12222                             	#endif /* __APPLE__ */
 12223                             	        leaq	32(%rsp), %rdi
 12224                             	        leaq	32(%rsp), %rsi
 12225                             	        movb	$9, %dl
 12226                             	#ifndef __APPLE__
 12227                             	        callq	fe_sq_n_avx2@plt
 12228                             	#else
 12229                             	        callq	_fe_sq_n_avx2
 12230                             	#endif /* __APPLE__ */
 12231                             	        leaq	32(%rsp), %rdi
 12232                             	        leaq	32(%rsp), %rsi
 12233                             	        movq	%rsp, %rdx
 12234                             	#ifndef __APPLE__
 12235                             	        callq	fe_mul_avx2@plt
 12236                             	#else
 12237                             	        callq	_fe_mul_avx2
 12238                             	#endif /* __APPLE__ */
 12239                             	        leaq	64(%rsp), %rdi
 12240                             	        leaq	32(%rsp), %rsi
 12241                             	#ifndef __APPLE__
 12242                             	        callq	fe_sq_avx2@plt
 12243                             	#else
 12244                             	        callq	_fe_sq_avx2
 12245                             	#endif /* __APPLE__ */
 12246                             	        leaq	64(%rsp), %rdi
 12247                             	        leaq	64(%rsp), %rsi
 12248                             	        movb	$19, %dl
 12249                             	#ifndef __APPLE__
 12250                             	        callq	fe_sq_n_avx2@plt
 12251                             	#else
 12252                             	        callq	_fe_sq_n_avx2
 12253                             	#endif /* __APPLE__ */
 12254                             	        leaq	32(%rsp), %rdi
 12255                             	        leaq	64(%rsp), %rsi
 12256                             	        leaq	32(%rsp), %rdx
 12257                             	#ifndef __APPLE__
 12258                             	        callq	fe_mul_avx2@plt
 12259                             	#else
 12260                             	        callq	_fe_mul_avx2
 12261                             	#endif /* __APPLE__ */
 12262                             	        leaq	32(%rsp), %rdi
 12263                             	        leaq	32(%rsp), %rsi
 12264                             	#ifndef __APPLE__
 12265                             	        callq	fe_sq_avx2@plt
 12266                             	#else
 12267                             	        callq	_fe_sq_avx2
 12268                             	#endif /* __APPLE__ */
 12269                             	        leaq	32(%rsp), %rdi
 12270                             	        leaq	32(%rsp), %rsi
 12271                             	        movb	$9, %dl
 12272                             	#ifndef __APPLE__
 12273                             	        callq	fe_sq_n_avx2@plt
 12274                             	#else
 12275                             	        callq	_fe_sq_n_avx2
 12276                             	#endif /* __APPLE__ */
 12277                             	        movq	%rsp, %rdi
 12278                             	        leaq	32(%rsp), %rsi
 12279                             	        movq	%rsp, %rdx
 12280                             	#ifndef __APPLE__
 12281                             	        callq	fe_mul_avx2@plt
 12282                             	#else
 12283                             	        callq	_fe_mul_avx2
 12284                             	#endif /* __APPLE__ */
 12285                             	        leaq	32(%rsp), %rdi
 12286                             	        movq	%rsp, %rsi
 12287                             	#ifndef __APPLE__
 12288                             	        callq	fe_sq_avx2@plt
 12289                             	#else
 12290                             	        callq	_fe_sq_avx2
 12291                             	#endif /* __APPLE__ */
 12292                             	        leaq	32(%rsp), %rdi
 12293                             	        leaq	32(%rsp), %rsi
 12294                             	        movb	$49, %dl
 12295                             	#ifndef __APPLE__
 12296                             	        callq	fe_sq_n_avx2@plt
 12297                             	#else
 12298                             	        callq	_fe_sq_n_avx2
 12299                             	#endif /* __APPLE__ */
 12300                             	        leaq	32(%rsp), %rdi
 12301                             	        leaq	32(%rsp), %rsi
 12302                             	        movq	%rsp, %rdx
 12303                             	#ifndef __APPLE__
 12304                             	        callq	fe_mul_avx2@plt
 12305                             	#else
 12306                             	        callq	_fe_mul_avx2
 12307                             	#endif /* __APPLE__ */
 12308                             	        leaq	64(%rsp), %rdi
 12309                             	        leaq	32(%rsp), %rsi
 12310                             	#ifndef __APPLE__
 12311                             	        callq	fe_sq_avx2@plt
 12312                             	#else
 12313                             	        callq	_fe_sq_avx2
 12314                             	#endif /* __APPLE__ */
 12315                             	        leaq	64(%rsp), %rdi
 12316                             	        leaq	64(%rsp), %rsi
 12317                             	        movb	$0x63, %dl
 12318                             	#ifndef __APPLE__
 12319                             	        callq	fe_sq_n_avx2@plt
 12320                             	#else
 12321                             	        callq	_fe_sq_n_avx2
 12322                             	#endif /* __APPLE__ */
 12323                             	        leaq	32(%rsp), %rdi
 12324                             	        leaq	64(%rsp), %rsi
 12325                             	        leaq	32(%rsp), %rdx
 12326                             	#ifndef __APPLE__
 12327                             	        callq	fe_mul_avx2@plt
 12328                             	#else
 12329                             	        callq	_fe_mul_avx2
 12330                             	#endif /* __APPLE__ */
 12331                             	        leaq	32(%rsp), %rdi
 12332                             	        leaq	32(%rsp), %rsi
 12333                             	#ifndef __APPLE__
 12334                             	        callq	fe_sq_avx2@plt
 12335                             	#else
 12336                             	        callq	_fe_sq_avx2
 12337                             	#endif /* __APPLE__ */
 12338                             	        leaq	32(%rsp), %rdi
 12339                             	        leaq	32(%rsp), %rsi
 12340                             	        movb	$49, %dl
 12341                             	#ifndef __APPLE__
 12342                             	        callq	fe_sq_n_avx2@plt
 12343                             	#else
 12344                             	        callq	_fe_sq_n_avx2
 12345                             	#endif /* __APPLE__ */
 12346                             	        movq	%rsp, %rdi
 12347                             	        leaq	32(%rsp), %rsi
 12348                             	        movq	%rsp, %rdx
 12349                             	#ifndef __APPLE__
 12350                             	        callq	fe_mul_avx2@plt
 12351                             	#else
 12352                             	        callq	_fe_mul_avx2
 12353                             	#endif /* __APPLE__ */
 12354                             	        movq	%rsp, %rdi
 12355                             	        movq	%rsp, %rsi
 12356                             	#ifndef __APPLE__
 12357                             	        callq	fe_sq_avx2@plt
 12358                             	#else
 12359                             	        callq	_fe_sq_avx2
 12360                             	#endif /* __APPLE__ */
 12361                             	        movq	%rsp, %rdi
 12362                             	        movq	%rsp, %rsi
 12363                             	#ifndef __APPLE__
 12364                             	        callq	fe_sq_avx2@plt
 12365                             	#else
 12366                             	        callq	_fe_sq_avx2
 12367                             	#endif /* __APPLE__ */
 12368                             	        movq	96(%rsp), %rdi
 12369                             	        movq	%rsp, %rsi
 12370                             	        movq	104(%rsp), %rdx
 12371                             	#ifndef __APPLE__
 12372                             	        callq	fe_mul_avx2@plt
 12373                             	#else
 12374                             	        callq	_fe_mul_avx2
 12375                             	#endif /* __APPLE__ */
 12376                             	        movq	104(%rsp), %rsi
 12377                             	        movq	96(%rsp), %rdi
 12378                             	        addq	$0x70, %rsp
 12379                             	        repz retq
 12380                             	#ifndef __APPLE__
 12381                             	.text
 12382                             	.globl	fe_ge_to_p2_avx2
 12384                             	.align	16
 12385                             	fe_ge_to_p2_avx2:
 12386                             	#else
 12387                             	.section	__TEXT,__text
 12388                             	.globl	_fe_ge_to_p2_avx2
 12389                             	.p2align	4
 12390                             	_fe_ge_to_p2_avx2:
 12391                             	#endif /* __APPLE__ */
 12392                             	        pushq	%rbx
 12393                             	        pushq	%r12
 12394                             	        pushq	%r13
 12395                             	        pushq	%r14
 12396                             	        pushq	%r15
 12397                             	        subq	$40, %rsp
 12398                             	        movq	%rsi, (%rsp)
 12399                             	        movq	%rdx, 8(%rsp)
 12400                             	        movq	%rcx, 16(%rsp)
 12401                             	        movq	%r8, 24(%rsp)
 12402                             	        movq	%r9, 32(%rsp)
 12403                             	        movq	16(%rsp), %rsi
 12404                             	        movq	88(%rsp), %rbx
 12405                             	        # Multiply
 12406                             	        # A[0] * B[0]
 12407                             	        movq	(%rbx), %rdx
 12408                             	        mulxq	(%rsi), %r8, %r9
 12409                             	        # A[2] * B[0]
 12410                             	        mulxq	16(%rsi), %r10, %r11
 12411                             	        # A[1] * B[0]
 12412                             	        mulxq	8(%rsi), %rcx, %rax
 12413                             	        xorq	%r15, %r15
 12414                             	        adcxq	%rcx, %r9
 12415                             	        # A[1] * B[3]
 12416                             	        movq	24(%rbx), %rdx
 12417                             	        mulxq	8(%rsi), %r12, %r13
 12418                             	        adcxq	%rax, %r10
 12419                             	        # A[0] * B[1]
 12420                             	        movq	8(%rbx), %rdx
 12421                             	        mulxq	(%rsi), %rcx, %rax
 12422                             	        adoxq	%rcx, %r9
 12423                             	        # A[2] * B[1]
 12424                             	        mulxq	16(%rsi), %rcx, %r14
 12425                             	        adoxq	%rax, %r10
 12426                             	        adcxq	%rcx, %r11
 12427                             	        # A[1] * B[2]
 12428                             	        movq	16(%rbx), %rdx
 12429                             	        mulxq	8(%rsi), %rcx, %rax
 12430                             	        adcxq	%r14, %r12
 12431                             	        adoxq	%rcx, %r11
 12432                             	        adcxq	%r15, %r13
 12433                             	        adoxq	%rax, %r12
 12434                             	        # A[0] * B[2]
 12435                             	        mulxq	(%rsi), %rcx, %rax
 12436                             	        adoxq	%r15, %r13
 12437                             	        xorq	%r14, %r14
 12438                             	        adcxq	%rcx, %r10
 12439                             	        # A[1] * B[1]
 12440                             	        movq	8(%rbx), %rdx
 12441                             	        mulxq	8(%rsi), %rdx, %rcx
 12442                             	        adcxq	%rax, %r11
 12443                             	        adoxq	%rdx, %r10
 12444                             	        # A[3] * B[1]
 12445                             	        movq	8(%rbx), %rdx
 12446                             	        adoxq	%rcx, %r11
 12447                             	        mulxq	24(%rsi), %rcx, %rax
 12448                             	        adcxq	%rcx, %r12
 12449                             	        # A[2] * B[2]
 12450                             	        movq	16(%rbx), %rdx
 12451                             	        mulxq	16(%rsi), %rdx, %rcx
 12452                             	        adcxq	%rax, %r13
 12453                             	        adoxq	%rdx, %r12
 12454                             	        # A[3] * B[3]
 12455                             	        movq	24(%rbx), %rdx
 12456                             	        adoxq	%rcx, %r13
 12457                             	        mulxq	24(%rsi), %rcx, %rax
 12458                             	        adoxq	%r15, %r14
 12459                             	        adcxq	%rcx, %r14
 12460                             	        # A[0] * B[3]
 12461                             	        mulxq	(%rsi), %rdx, %rcx
 12462                             	        adcxq	%rax, %r15
 12463                             	        xorq	%rax, %rax
 12464                             	        adcxq	%rdx, %r11
 12465                             	        # A[3] * B[0]
 12466                             	        movq	(%rbx), %rdx
 12467                             	        adcxq	%rcx, %r12
 12468                             	        mulxq	24(%rsi), %rdx, %rcx
 12469                             	        adoxq	%rdx, %r11
 12470                             	        adoxq	%rcx, %r12
 12471                             	        # A[2] * B[3]
 12472                             	        movq	24(%rbx), %rdx
 12473                             	        mulxq	16(%rsi), %rdx, %rcx
 12474                             	        adcxq	%rdx, %r13
 12475                             	        # A[3] * B[2]
 12476                             	        movq	16(%rbx), %rdx
 12477                             	        adcxq	%rcx, %r14
 12478                             	        mulxq	24(%rsi), %rcx, %rdx
 12479                             	        adcxq	%rax, %r15
 12480                             	        adoxq	%rcx, %r13
 12481                             	        adoxq	%rdx, %r14
 12482                             	        adoxq	%rax, %r15
 12483                             	        # Reduce
 12484                             	        movq	$0x7fffffffffffffff, %rax
 12485                             	        #  Move top half into t4-t7 and remove top bit from t3
 12486                             	        shldq	$0x01, %r14, %r15
 12487                             	        shldq	$0x01, %r13, %r14
 12488                             	        shldq	$0x01, %r12, %r13
 12489                             	        shldq	$0x01, %r11, %r12
 12490                             	        andq	%rax, %r11
 12491                             	        #  Multiply top half by 19
 12492                             	        movq	$19, %rdx
 12493                             	        xorq	%rax, %rax
 12494                             	        mulxq	%r12, %rcx, %r12
 12495                             	        adcxq	%rcx, %r8
 12496                             	        adoxq	%r12, %r9
 12497                             	        mulxq	%r13, %rcx, %r13
 12498                             	        adcxq	%rcx, %r9
 12499                             	        adoxq	%r13, %r10
 12500                             	        mulxq	%r14, %rcx, %r14
 12501                             	        adcxq	%rcx, %r10
 12502                             	        adoxq	%r14, %r11
 12503                             	        mulxq	%r15, %r15, %rdx
 12504                             	        adcxq	%r15, %r11
 12505                             	        adoxq	%rax, %rdx
 12506                             	        adcxq	%rax, %rdx
 12507                             	        #  Overflow
 12508                             	        shldq	$0x01, %r11, %rdx
 12509                             	        movq	$0x7fffffffffffffff, %rax
 12510                             	        imulq	$19, %rdx, %rcx
 12511                             	        andq	%rax, %r11
 12512                             	        addq	%rcx, %r8
 12513                             	        adcq	$0x00, %r9
 12514                             	        adcq	$0x00, %r10
 12515                             	        adcq	$0x00, %r11
 12516                             	        # Reduce if top bit set
 12517                             	        movq	%r11, %rdx
 12518                             	        sarq	$63, %rdx
 12519                             	        andq	$19, %rdx
 12520                             	        andq	%rax, %r11
 12521                             	        addq	%rdx, %r8
 12522                             	        adcq	$0x00, %r9
 12523                             	        adcq	$0x00, %r10
 12524                             	        adcq	$0x00, %r11
 12525                             	        # Store
 12526                             	        movq	%r8, (%rdi)
 12527                             	        movq	%r9, 8(%rdi)
 12528                             	        movq	%r10, 16(%rdi)
 12529                             	        movq	%r11, 24(%rdi)
 12530                             	        movq	(%rsp), %rdi
 12531                             	        movq	24(%rsp), %rsi
 12532                             	        movq	32(%rsp), %rbx
 12533                             	        # Multiply
 12534                             	        # A[0] * B[0]
 12535                             	        movq	(%rbx), %rdx
 12536                             	        mulxq	(%rsi), %r8, %r9
 12537                             	        # A[2] * B[0]
 12538                             	        mulxq	16(%rsi), %r10, %r11
 12539                             	        # A[1] * B[0]
 12540                             	        mulxq	8(%rsi), %rcx, %rax
 12541                             	        xorq	%r15, %r15
 12542                             	        adcxq	%rcx, %r9
 12543                             	        # A[1] * B[3]
 12544                             	        movq	24(%rbx), %rdx
 12545                             	        mulxq	8(%rsi), %r12, %r13
 12546                             	        adcxq	%rax, %r10
 12547                             	        # A[0] * B[1]
 12548                             	        movq	8(%rbx), %rdx
 12549                             	        mulxq	(%rsi), %rcx, %rax
 12550                             	        adoxq	%rcx, %r9
 12551                             	        # A[2] * B[1]
 12552                             	        mulxq	16(%rsi), %rcx, %r14
 12553                             	        adoxq	%rax, %r10
 12554                             	        adcxq	%rcx, %r11
 12555                             	        # A[1] * B[2]
 12556                             	        movq	16(%rbx), %rdx
 12557                             	        mulxq	8(%rsi), %rcx, %rax
 12558                             	        adcxq	%r14, %r12
 12559                             	        adoxq	%rcx, %r11
 12560                             	        adcxq	%r15, %r13
 12561                             	        adoxq	%rax, %r12
 12562                             	        # A[0] * B[2]
 12563                             	        mulxq	(%rsi), %rcx, %rax
 12564                             	        adoxq	%r15, %r13
 12565                             	        xorq	%r14, %r14
 12566                             	        adcxq	%rcx, %r10
 12567                             	        # A[1] * B[1]
 12568                             	        movq	8(%rbx), %rdx
 12569                             	        mulxq	8(%rsi), %rdx, %rcx
 12570                             	        adcxq	%rax, %r11
 12571                             	        adoxq	%rdx, %r10
 12572                             	        # A[3] * B[1]
 12573                             	        movq	8(%rbx), %rdx
 12574                             	        adoxq	%rcx, %r11
 12575                             	        mulxq	24(%rsi), %rcx, %rax
 12576                             	        adcxq	%rcx, %r12
 12577                             	        # A[2] * B[2]
 12578                             	        movq	16(%rbx), %rdx
 12579                             	        mulxq	16(%rsi), %rdx, %rcx
 12580                             	        adcxq	%rax, %r13
 12581                             	        adoxq	%rdx, %r12
 12582                             	        # A[3] * B[3]
 12583                             	        movq	24(%rbx), %rdx
 12584                             	        adoxq	%rcx, %r13
 12585                             	        mulxq	24(%rsi), %rcx, %rax
 12586                             	        adoxq	%r15, %r14
 12587                             	        adcxq	%rcx, %r14
 12588                             	        # A[0] * B[3]
 12589                             	        mulxq	(%rsi), %rdx, %rcx
 12590                             	        adcxq	%rax, %r15
 12591                             	        xorq	%rax, %rax
 12592                             	        adcxq	%rdx, %r11
 12593                             	        # A[3] * B[0]
 12594                             	        movq	(%rbx), %rdx
 12595                             	        adcxq	%rcx, %r12
 12596                             	        mulxq	24(%rsi), %rdx, %rcx
 12597                             	        adoxq	%rdx, %r11
 12598                             	        adoxq	%rcx, %r12
 12599                             	        # A[2] * B[3]
 12600                             	        movq	24(%rbx), %rdx
 12601                             	        mulxq	16(%rsi), %rdx, %rcx
 12602                             	        adcxq	%rdx, %r13
 12603                             	        # A[3] * B[2]
 12604                             	        movq	16(%rbx), %rdx
 12605                             	        adcxq	%rcx, %r14
 12606                             	        mulxq	24(%rsi), %rcx, %rdx
 12607                             	        adcxq	%rax, %r15
 12608                             	        adoxq	%rcx, %r13
 12609                             	        adoxq	%rdx, %r14
 12610                             	        adoxq	%rax, %r15
 12611                             	        # Reduce
 12612                             	        movq	$0x7fffffffffffffff, %rax
 12613                             	        #  Move top half into t4-t7 and remove top bit from t3
 12614                             	        shldq	$0x01, %r14, %r15
 12615                             	        shldq	$0x01, %r13, %r14
 12616                             	        shldq	$0x01, %r12, %r13
 12617                             	        shldq	$0x01, %r11, %r12
 12618                             	        andq	%rax, %r11
 12619                             	        #  Multiply top half by 19
 12620                             	        movq	$19, %rdx
 12621                             	        xorq	%rax, %rax
 12622                             	        mulxq	%r12, %rcx, %r12
 12623                             	        adcxq	%rcx, %r8
 12624                             	        adoxq	%r12, %r9
 12625                             	        mulxq	%r13, %rcx, %r13
 12626                             	        adcxq	%rcx, %r9
 12627                             	        adoxq	%r13, %r10
 12628                             	        mulxq	%r14, %rcx, %r14
 12629                             	        adcxq	%rcx, %r10
 12630                             	        adoxq	%r14, %r11
 12631                             	        mulxq	%r15, %r15, %rdx
 12632                             	        adcxq	%r15, %r11
 12633                             	        adoxq	%rax, %rdx
 12634                             	        adcxq	%rax, %rdx
 12635                             	        #  Overflow
 12636                             	        shldq	$0x01, %r11, %rdx
 12637                             	        movq	$0x7fffffffffffffff, %rax
 12638                             	        imulq	$19, %rdx, %rcx
 12639                             	        andq	%rax, %r11
 12640                             	        addq	%rcx, %r8
 12641                             	        adcq	$0x00, %r9
 12642                             	        adcq	$0x00, %r10
 12643                             	        adcq	$0x00, %r11
 12644                             	        # Reduce if top bit set
 12645                             	        movq	%r11, %rdx
 12646                             	        sarq	$63, %rdx
 12647                             	        andq	$19, %rdx
 12648                             	        andq	%rax, %r11
 12649                             	        addq	%rdx, %r8
 12650                             	        adcq	$0x00, %r9
 12651                             	        adcq	$0x00, %r10
 12652                             	        adcq	$0x00, %r11
 12653                             	        # Store
 12654                             	        movq	%r8, (%rdi)
 12655                             	        movq	%r9, 8(%rdi)
 12656                             	        movq	%r10, 16(%rdi)
 12657                             	        movq	%r11, 24(%rdi)
 12658                             	        movq	8(%rsp), %rdi
 12659                             	        movq	88(%rsp), %rsi
 12660                             	        # Multiply
 12661                             	        # A[0] * B[0]
 12662                             	        movq	(%rsi), %rdx
 12663                             	        mulxq	(%rbx), %r8, %r9
 12664                             	        # A[2] * B[0]
 12665                             	        mulxq	16(%rbx), %r10, %r11
 12666                             	        # A[1] * B[0]
 12667                             	        mulxq	8(%rbx), %rcx, %rax
 12668                             	        xorq	%r15, %r15
 12669                             	        adcxq	%rcx, %r9
 12670                             	        # A[1] * B[3]
 12671                             	        movq	24(%rsi), %rdx
 12672                             	        mulxq	8(%rbx), %r12, %r13
 12673                             	        adcxq	%rax, %r10
 12674                             	        # A[0] * B[1]
 12675                             	        movq	8(%rsi), %rdx
 12676                             	        mulxq	(%rbx), %rcx, %rax
 12677                             	        adoxq	%rcx, %r9
 12678                             	        # A[2] * B[1]
 12679                             	        mulxq	16(%rbx), %rcx, %r14
 12680                             	        adoxq	%rax, %r10
 12681                             	        adcxq	%rcx, %r11
 12682                             	        # A[1] * B[2]
 12683                             	        movq	16(%rsi), %rdx
 12684                             	        mulxq	8(%rbx), %rcx, %rax
 12685                             	        adcxq	%r14, %r12
 12686                             	        adoxq	%rcx, %r11
 12687                             	        adcxq	%r15, %r13
 12688                             	        adoxq	%rax, %r12
 12689                             	        # A[0] * B[2]
 12690                             	        mulxq	(%rbx), %rcx, %rax
 12691                             	        adoxq	%r15, %r13
 12692                             	        xorq	%r14, %r14
 12693                             	        adcxq	%rcx, %r10
 12694                             	        # A[1] * B[1]
 12695                             	        movq	8(%rsi), %rdx
 12696                             	        mulxq	8(%rbx), %rdx, %rcx
 12697                             	        adcxq	%rax, %r11
 12698                             	        adoxq	%rdx, %r10
 12699                             	        # A[3] * B[1]
 12700                             	        movq	8(%rsi), %rdx
 12701                             	        adoxq	%rcx, %r11
 12702                             	        mulxq	24(%rbx), %rcx, %rax
 12703                             	        adcxq	%rcx, %r12
 12704                             	        # A[2] * B[2]
 12705                             	        movq	16(%rsi), %rdx
 12706                             	        mulxq	16(%rbx), %rdx, %rcx
 12707                             	        adcxq	%rax, %r13
 12708                             	        adoxq	%rdx, %r12
 12709                             	        # A[3] * B[3]
 12710                             	        movq	24(%rsi), %rdx
 12711                             	        adoxq	%rcx, %r13
 12712                             	        mulxq	24(%rbx), %rcx, %rax
 12713                             	        adoxq	%r15, %r14
 12714                             	        adcxq	%rcx, %r14
 12715                             	        # A[0] * B[3]
 12716                             	        mulxq	(%rbx), %rdx, %rcx
 12717                             	        adcxq	%rax, %r15
 12718                             	        xorq	%rax, %rax
 12719                             	        adcxq	%rdx, %r11
 12720                             	        # A[3] * B[0]
 12721                             	        movq	(%rsi), %rdx
 12722                             	        adcxq	%rcx, %r12
 12723                             	        mulxq	24(%rbx), %rdx, %rcx
 12724                             	        adoxq	%rdx, %r11
 12725                             	        adoxq	%rcx, %r12
 12726                             	        # A[2] * B[3]
 12727                             	        movq	24(%rsi), %rdx
 12728                             	        mulxq	16(%rbx), %rdx, %rcx
 12729                             	        adcxq	%rdx, %r13
 12730                             	        # A[3] * B[2]
 12731                             	        movq	16(%rsi), %rdx
 12732                             	        adcxq	%rcx, %r14
 12733                             	        mulxq	24(%rbx), %rcx, %rdx
 12734                             	        adcxq	%rax, %r15
 12735                             	        adoxq	%rcx, %r13
 12736                             	        adoxq	%rdx, %r14
 12737                             	        adoxq	%rax, %r15
 12738                             	        # Reduce
 12739                             	        movq	$0x7fffffffffffffff, %rax
 12740                             	        #  Move top half into t4-t7 and remove top bit from t3
 12741                             	        shldq	$0x01, %r14, %r15
 12742                             	        shldq	$0x01, %r13, %r14
 12743                             	        shldq	$0x01, %r12, %r13
 12744                             	        shldq	$0x01, %r11, %r12
 12745                             	        andq	%rax, %r11
 12746                             	        #  Multiply top half by 19
 12747                             	        movq	$19, %rdx
 12748                             	        xorq	%rax, %rax
 12749                             	        mulxq	%r12, %rcx, %r12
 12750                             	        adcxq	%rcx, %r8
 12751                             	        adoxq	%r12, %r9
 12752                             	        mulxq	%r13, %rcx, %r13
 12753                             	        adcxq	%rcx, %r9
 12754                             	        adoxq	%r13, %r10
 12755                             	        mulxq	%r14, %rcx, %r14
 12756                             	        adcxq	%rcx, %r10
 12757                             	        adoxq	%r14, %r11
 12758                             	        mulxq	%r15, %r15, %rdx
 12759                             	        adcxq	%r15, %r11
 12760                             	        adoxq	%rax, %rdx
 12761                             	        adcxq	%rax, %rdx
 12762                             	        #  Overflow
 12763                             	        shldq	$0x01, %r11, %rdx
 12764                             	        movq	$0x7fffffffffffffff, %rax
 12765                             	        imulq	$19, %rdx, %rcx
 12766                             	        andq	%rax, %r11
 12767                             	        addq	%rcx, %r8
 12768                             	        adcq	$0x00, %r9
 12769                             	        adcq	$0x00, %r10
 12770                             	        adcq	$0x00, %r11
 12771                             	        # Reduce if top bit set
 12772                             	        movq	%r11, %rdx
 12773                             	        sarq	$63, %rdx
 12774                             	        andq	$19, %rdx
 12775                             	        andq	%rax, %r11
 12776                             	        addq	%rdx, %r8
 12777                             	        adcq	$0x00, %r9
 12778                             	        adcq	$0x00, %r10
 12779                             	        adcq	$0x00, %r11
 12780                             	        # Store
 12781                             	        movq	%r8, (%rdi)
 12782                             	        movq	%r9, 8(%rdi)
 12783                             	        movq	%r10, 16(%rdi)
 12784                             	        movq	%r11, 24(%rdi)
 12785                             	        addq	$40, %rsp
 12786                             	        popq	%r15
 12787                             	        popq	%r14
 12788                             	        popq	%r13
 12789                             	        popq	%r12
 12790                             	        popq	%rbx
 12791                             	        repz retq
 12792                             	#ifndef __APPLE__
 12794                             	#endif /* __APPLE__ */
 12795                             	#ifndef __APPLE__
 12796                             	.text
 12797                             	.globl	fe_ge_to_p3_avx2
 12799                             	.align	16
 12800                             	fe_ge_to_p3_avx2:
 12801                             	#else
 12802                             	.section	__TEXT,__text
 12803                             	.globl	_fe_ge_to_p3_avx2
 12804                             	.p2align	4
 12805                             	_fe_ge_to_p3_avx2:
 12806                             	#endif /* __APPLE__ */
 12807                             	        pushq	%rbx
 12808                             	        pushq	%r12
 12809                             	        pushq	%r13
 12810                             	        pushq	%r14
 12811                             	        pushq	%r15
 12812                             	        subq	$40, %rsp
 12813                             	        movq	%rsi, (%rsp)
 12814                             	        movq	%rdx, 8(%rsp)
 12815                             	        movq	%rcx, 16(%rsp)
 12816                             	        movq	%r8, 24(%rsp)
 12817                             	        movq	%r9, 32(%rsp)
 12818                             	        movq	24(%rsp), %rsi
 12819                             	        movq	96(%rsp), %rbx
 12820                             	        # Multiply
 12821                             	        # A[0] * B[0]
 12822                             	        movq	(%rbx), %rdx
 12823                             	        mulxq	(%rsi), %r8, %r9
 12824                             	        # A[2] * B[0]
 12825                             	        mulxq	16(%rsi), %r10, %r11
 12826                             	        # A[1] * B[0]
 12827                             	        mulxq	8(%rsi), %rcx, %rax
 12828                             	        xorq	%r15, %r15
 12829                             	        adcxq	%rcx, %r9
 12830                             	        # A[1] * B[3]
 12831                             	        movq	24(%rbx), %rdx
 12832                             	        mulxq	8(%rsi), %r12, %r13
 12833                             	        adcxq	%rax, %r10
 12834                             	        # A[0] * B[1]
 12835                             	        movq	8(%rbx), %rdx
 12836                             	        mulxq	(%rsi), %rcx, %rax
 12837                             	        adoxq	%rcx, %r9
 12838                             	        # A[2] * B[1]
 12839                             	        mulxq	16(%rsi), %rcx, %r14
 12840                             	        adoxq	%rax, %r10
 12841                             	        adcxq	%rcx, %r11
 12842                             	        # A[1] * B[2]
 12843                             	        movq	16(%rbx), %rdx
 12844                             	        mulxq	8(%rsi), %rcx, %rax
 12845                             	        adcxq	%r14, %r12
 12846                             	        adoxq	%rcx, %r11
 12847                             	        adcxq	%r15, %r13
 12848                             	        adoxq	%rax, %r12
 12849                             	        # A[0] * B[2]
 12850                             	        mulxq	(%rsi), %rcx, %rax
 12851                             	        adoxq	%r15, %r13
 12852                             	        xorq	%r14, %r14
 12853                             	        adcxq	%rcx, %r10
 12854                             	        # A[1] * B[1]
 12855                             	        movq	8(%rbx), %rdx
 12856                             	        mulxq	8(%rsi), %rdx, %rcx
 12857                             	        adcxq	%rax, %r11
 12858                             	        adoxq	%rdx, %r10
 12859                             	        # A[3] * B[1]
 12860                             	        movq	8(%rbx), %rdx
 12861                             	        adoxq	%rcx, %r11
 12862                             	        mulxq	24(%rsi), %rcx, %rax
 12863                             	        adcxq	%rcx, %r12
 12864                             	        # A[2] * B[2]
 12865                             	        movq	16(%rbx), %rdx
 12866                             	        mulxq	16(%rsi), %rdx, %rcx
 12867                             	        adcxq	%rax, %r13
 12868                             	        adoxq	%rdx, %r12
 12869                             	        # A[3] * B[3]
 12870                             	        movq	24(%rbx), %rdx
 12871                             	        adoxq	%rcx, %r13
 12872                             	        mulxq	24(%rsi), %rcx, %rax
 12873                             	        adoxq	%r15, %r14
 12874                             	        adcxq	%rcx, %r14
 12875                             	        # A[0] * B[3]
 12876                             	        mulxq	(%rsi), %rdx, %rcx
 12877                             	        adcxq	%rax, %r15
 12878                             	        xorq	%rax, %rax
 12879                             	        adcxq	%rdx, %r11
 12880                             	        # A[3] * B[0]
 12881                             	        movq	(%rbx), %rdx
 12882                             	        adcxq	%rcx, %r12
 12883                             	        mulxq	24(%rsi), %rdx, %rcx
 12884                             	        adoxq	%rdx, %r11
 12885                             	        adoxq	%rcx, %r12
 12886                             	        # A[2] * B[3]
 12887                             	        movq	24(%rbx), %rdx
 12888                             	        mulxq	16(%rsi), %rdx, %rcx
 12889                             	        adcxq	%rdx, %r13
 12890                             	        # A[3] * B[2]
 12891                             	        movq	16(%rbx), %rdx
 12892                             	        adcxq	%rcx, %r14
 12893                             	        mulxq	24(%rsi), %rcx, %rdx
 12894                             	        adcxq	%rax, %r15
 12895                             	        adoxq	%rcx, %r13
 12896                             	        adoxq	%rdx, %r14
 12897                             	        adoxq	%rax, %r15
 12898                             	        # Reduce
 12899                             	        movq	$0x7fffffffffffffff, %rax
 12900                             	        #  Move top half into t4-t7 and remove top bit from t3
 12901                             	        shldq	$0x01, %r14, %r15
 12902                             	        shldq	$0x01, %r13, %r14
 12903                             	        shldq	$0x01, %r12, %r13
 12904                             	        shldq	$0x01, %r11, %r12
 12905                             	        andq	%rax, %r11
 12906                             	        #  Multiply top half by 19
 12907                             	        movq	$19, %rdx
 12908                             	        xorq	%rax, %rax
 12909                             	        mulxq	%r12, %rcx, %r12
 12910                             	        adcxq	%rcx, %r8
 12911                             	        adoxq	%r12, %r9
 12912                             	        mulxq	%r13, %rcx, %r13
 12913                             	        adcxq	%rcx, %r9
 12914                             	        adoxq	%r13, %r10
 12915                             	        mulxq	%r14, %rcx, %r14
 12916                             	        adcxq	%rcx, %r10
 12917                             	        adoxq	%r14, %r11
 12918                             	        mulxq	%r15, %r15, %rdx
 12919                             	        adcxq	%r15, %r11
 12920                             	        adoxq	%rax, %rdx
 12921                             	        adcxq	%rax, %rdx
 12922                             	        #  Overflow
 12923                             	        shldq	$0x01, %r11, %rdx
 12924                             	        movq	$0x7fffffffffffffff, %rax
 12925                             	        imulq	$19, %rdx, %rcx
 12926                             	        andq	%rax, %r11
 12927                             	        addq	%rcx, %r8
 12928                             	        adcq	$0x00, %r9
 12929                             	        adcq	$0x00, %r10
 12930                             	        adcq	$0x00, %r11
 12931                             	        # Reduce if top bit set
 12932                             	        movq	%r11, %rdx
 12933                             	        sarq	$63, %rdx
 12934                             	        andq	$19, %rdx
 12935                             	        andq	%rax, %r11
 12936                             	        addq	%rdx, %r8
 12937                             	        adcq	$0x00, %r9
 12938                             	        adcq	$0x00, %r10
 12939                             	        adcq	$0x00, %r11
 12940                             	        # Store
 12941                             	        movq	%r8, (%rdi)
 12942                             	        movq	%r9, 8(%rdi)
 12943                             	        movq	%r10, 16(%rdi)
 12944                             	        movq	%r11, 24(%rdi)
 12945                             	        movq	(%rsp), %rdi
 12946                             	        movq	32(%rsp), %rsi
 12947                             	        movq	88(%rsp), %rbx
 12948                             	        # Multiply
 12949                             	        # A[0] * B[0]
 12950                             	        movq	(%rbx), %rdx
 12951                             	        mulxq	(%rsi), %r8, %r9
 12952                             	        # A[2] * B[0]
 12953                             	        mulxq	16(%rsi), %r10, %r11
 12954                             	        # A[1] * B[0]
 12955                             	        mulxq	8(%rsi), %rcx, %rax
 12956                             	        xorq	%r15, %r15
 12957                             	        adcxq	%rcx, %r9
 12958                             	        # A[1] * B[3]
 12959                             	        movq	24(%rbx), %rdx
 12960                             	        mulxq	8(%rsi), %r12, %r13
 12961                             	        adcxq	%rax, %r10
 12962                             	        # A[0] * B[1]
 12963                             	        movq	8(%rbx), %rdx
 12964                             	        mulxq	(%rsi), %rcx, %rax
 12965                             	        adoxq	%rcx, %r9
 12966                             	        # A[2] * B[1]
 12967                             	        mulxq	16(%rsi), %rcx, %r14
 12968                             	        adoxq	%rax, %r10
 12969                             	        adcxq	%rcx, %r11
 12970                             	        # A[1] * B[2]
 12971                             	        movq	16(%rbx), %rdx
 12972                             	        mulxq	8(%rsi), %rcx, %rax
 12973                             	        adcxq	%r14, %r12
 12974                             	        adoxq	%rcx, %r11
 12975                             	        adcxq	%r15, %r13
 12976                             	        adoxq	%rax, %r12
 12977                             	        # A[0] * B[2]
 12978                             	        mulxq	(%rsi), %rcx, %rax
 12979                             	        adoxq	%r15, %r13
 12980                             	        xorq	%r14, %r14
 12981                             	        adcxq	%rcx, %r10
 12982                             	        # A[1] * B[1]
 12983                             	        movq	8(%rbx), %rdx
 12984                             	        mulxq	8(%rsi), %rdx, %rcx
 12985                             	        adcxq	%rax, %r11
 12986                             	        adoxq	%rdx, %r10
 12987                             	        # A[3] * B[1]
 12988                             	        movq	8(%rbx), %rdx
 12989                             	        adoxq	%rcx, %r11
 12990                             	        mulxq	24(%rsi), %rcx, %rax
 12991                             	        adcxq	%rcx, %r12
 12992                             	        # A[2] * B[2]
 12993                             	        movq	16(%rbx), %rdx
 12994                             	        mulxq	16(%rsi), %rdx, %rcx
 12995                             	        adcxq	%rax, %r13
 12996                             	        adoxq	%rdx, %r12
 12997                             	        # A[3] * B[3]
 12998                             	        movq	24(%rbx), %rdx
 12999                             	        adoxq	%rcx, %r13
 13000                             	        mulxq	24(%rsi), %rcx, %rax
 13001                             	        adoxq	%r15, %r14
 13002                             	        adcxq	%rcx, %r14
 13003                             	        # A[0] * B[3]
 13004                             	        mulxq	(%rsi), %rdx, %rcx
 13005                             	        adcxq	%rax, %r15
 13006                             	        xorq	%rax, %rax
 13007                             	        adcxq	%rdx, %r11
 13008                             	        # A[3] * B[0]
 13009                             	        movq	(%rbx), %rdx
 13010                             	        adcxq	%rcx, %r12
 13011                             	        mulxq	24(%rsi), %rdx, %rcx
 13012                             	        adoxq	%rdx, %r11
 13013                             	        adoxq	%rcx, %r12
 13014                             	        # A[2] * B[3]
 13015                             	        movq	24(%rbx), %rdx
 13016                             	        mulxq	16(%rsi), %rdx, %rcx
 13017                             	        adcxq	%rdx, %r13
 13018                             	        # A[3] * B[2]
 13019                             	        movq	16(%rbx), %rdx
 13020                             	        adcxq	%rcx, %r14
 13021                             	        mulxq	24(%rsi), %rcx, %rdx
 13022                             	        adcxq	%rax, %r15
 13023                             	        adoxq	%rcx, %r13
 13024                             	        adoxq	%rdx, %r14
 13025                             	        adoxq	%rax, %r15
 13026                             	        # Reduce
 13027                             	        movq	$0x7fffffffffffffff, %rax
 13028                             	        #  Move top half into t4-t7 and remove top bit from t3
 13029                             	        shldq	$0x01, %r14, %r15
 13030                             	        shldq	$0x01, %r13, %r14
 13031                             	        shldq	$0x01, %r12, %r13
 13032                             	        shldq	$0x01, %r11, %r12
 13033                             	        andq	%rax, %r11
 13034                             	        #  Multiply top half by 19
 13035                             	        movq	$19, %rdx
 13036                             	        xorq	%rax, %rax
 13037                             	        mulxq	%r12, %rcx, %r12
 13038                             	        adcxq	%rcx, %r8
 13039                             	        adoxq	%r12, %r9
 13040                             	        mulxq	%r13, %rcx, %r13
 13041                             	        adcxq	%rcx, %r9
 13042                             	        adoxq	%r13, %r10
 13043                             	        mulxq	%r14, %rcx, %r14
 13044                             	        adcxq	%rcx, %r10
 13045                             	        adoxq	%r14, %r11
 13046                             	        mulxq	%r15, %r15, %rdx
 13047                             	        adcxq	%r15, %r11
 13048                             	        adoxq	%rax, %rdx
 13049                             	        adcxq	%rax, %rdx
 13050                             	        #  Overflow
 13051                             	        shldq	$0x01, %r11, %rdx
 13052                             	        movq	$0x7fffffffffffffff, %rax
 13053                             	        imulq	$19, %rdx, %rcx
 13054                             	        andq	%rax, %r11
 13055                             	        addq	%rcx, %r8
 13056                             	        adcq	$0x00, %r9
 13057                             	        adcq	$0x00, %r10
 13058                             	        adcq	$0x00, %r11
 13059                             	        # Reduce if top bit set
 13060                             	        movq	%r11, %rdx
 13061                             	        sarq	$63, %rdx
 13062                             	        andq	$19, %rdx
 13063                             	        andq	%rax, %r11
 13064                             	        addq	%rdx, %r8
 13065                             	        adcq	$0x00, %r9
 13066                             	        adcq	$0x00, %r10
 13067                             	        adcq	$0x00, %r11
 13068                             	        # Store
 13069                             	        movq	%r8, (%rdi)
 13070                             	        movq	%r9, 8(%rdi)
 13071                             	        movq	%r10, 16(%rdi)
 13072                             	        movq	%r11, 24(%rdi)
 13073                             	        movq	8(%rsp), %rdi
 13074                             	        movq	96(%rsp), %rsi
 13075                             	        # Multiply
 13076                             	        # A[0] * B[0]
 13077                             	        movq	(%rsi), %rdx
 13078                             	        mulxq	(%rbx), %r8, %r9
 13079                             	        # A[2] * B[0]
 13080                             	        mulxq	16(%rbx), %r10, %r11
 13081                             	        # A[1] * B[0]
 13082                             	        mulxq	8(%rbx), %rcx, %rax
 13083                             	        xorq	%r15, %r15
 13084                             	        adcxq	%rcx, %r9
 13085                             	        # A[1] * B[3]
 13086                             	        movq	24(%rsi), %rdx
 13087                             	        mulxq	8(%rbx), %r12, %r13
 13088                             	        adcxq	%rax, %r10
 13089                             	        # A[0] * B[1]
 13090                             	        movq	8(%rsi), %rdx
 13091                             	        mulxq	(%rbx), %rcx, %rax
 13092                             	        adoxq	%rcx, %r9
 13093                             	        # A[2] * B[1]
 13094                             	        mulxq	16(%rbx), %rcx, %r14
 13095                             	        adoxq	%rax, %r10
 13096                             	        adcxq	%rcx, %r11
 13097                             	        # A[1] * B[2]
 13098                             	        movq	16(%rsi), %rdx
 13099                             	        mulxq	8(%rbx), %rcx, %rax
 13100                             	        adcxq	%r14, %r12
 13101                             	        adoxq	%rcx, %r11
 13102                             	        adcxq	%r15, %r13
 13103                             	        adoxq	%rax, %r12
 13104                             	        # A[0] * B[2]
 13105                             	        mulxq	(%rbx), %rcx, %rax
 13106                             	        adoxq	%r15, %r13
 13107                             	        xorq	%r14, %r14
 13108                             	        adcxq	%rcx, %r10
 13109                             	        # A[1] * B[1]
 13110                             	        movq	8(%rsi), %rdx
 13111                             	        mulxq	8(%rbx), %rdx, %rcx
 13112                             	        adcxq	%rax, %r11
 13113                             	        adoxq	%rdx, %r10
 13114                             	        # A[3] * B[1]
 13115                             	        movq	8(%rsi), %rdx
 13116                             	        adoxq	%rcx, %r11
 13117                             	        mulxq	24(%rbx), %rcx, %rax
 13118                             	        adcxq	%rcx, %r12
 13119                             	        # A[2] * B[2]
 13120                             	        movq	16(%rsi), %rdx
 13121                             	        mulxq	16(%rbx), %rdx, %rcx
 13122                             	        adcxq	%rax, %r13
 13123                             	        adoxq	%rdx, %r12
 13124                             	        # A[3] * B[3]
 13125                             	        movq	24(%rsi), %rdx
 13126                             	        adoxq	%rcx, %r13
 13127                             	        mulxq	24(%rbx), %rcx, %rax
 13128                             	        adoxq	%r15, %r14
 13129                             	        adcxq	%rcx, %r14
 13130                             	        # A[0] * B[3]
 13131                             	        mulxq	(%rbx), %rdx, %rcx
 13132                             	        adcxq	%rax, %r15
 13133                             	        xorq	%rax, %rax
 13134                             	        adcxq	%rdx, %r11
 13135                             	        # A[3] * B[0]
 13136                             	        movq	(%rsi), %rdx
 13137                             	        adcxq	%rcx, %r12
 13138                             	        mulxq	24(%rbx), %rdx, %rcx
 13139                             	        adoxq	%rdx, %r11
 13140                             	        adoxq	%rcx, %r12
 13141                             	        # A[2] * B[3]
 13142                             	        movq	24(%rsi), %rdx
 13143                             	        mulxq	16(%rbx), %rdx, %rcx
 13144                             	        adcxq	%rdx, %r13
 13145                             	        # A[3] * B[2]
 13146                             	        movq	16(%rsi), %rdx
 13147                             	        adcxq	%rcx, %r14
 13148                             	        mulxq	24(%rbx), %rcx, %rdx
 13149                             	        adcxq	%rax, %r15
 13150                             	        adoxq	%rcx, %r13
 13151                             	        adoxq	%rdx, %r14
 13152                             	        adoxq	%rax, %r15
 13153                             	        # Reduce
 13154                             	        movq	$0x7fffffffffffffff, %rax
 13155                             	        #  Move top half into t4-t7 and remove top bit from t3
 13156                             	        shldq	$0x01, %r14, %r15
 13157                             	        shldq	$0x01, %r13, %r14
 13158                             	        shldq	$0x01, %r12, %r13
 13159                             	        shldq	$0x01, %r11, %r12
 13160                             	        andq	%rax, %r11
 13161                             	        #  Multiply top half by 19
 13162                             	        movq	$19, %rdx
 13163                             	        xorq	%rax, %rax
 13164                             	        mulxq	%r12, %rcx, %r12
 13165                             	        adcxq	%rcx, %r8
 13166                             	        adoxq	%r12, %r9
 13167                             	        mulxq	%r13, %rcx, %r13
 13168                             	        adcxq	%rcx, %r9
 13169                             	        adoxq	%r13, %r10
 13170                             	        mulxq	%r14, %rcx, %r14
 13171                             	        adcxq	%rcx, %r10
 13172                             	        adoxq	%r14, %r11
 13173                             	        mulxq	%r15, %r15, %rdx
 13174                             	        adcxq	%r15, %r11
 13175                             	        adoxq	%rax, %rdx
 13176                             	        adcxq	%rax, %rdx
 13177                             	        #  Overflow
 13178                             	        shldq	$0x01, %r11, %rdx
 13179                             	        movq	$0x7fffffffffffffff, %rax
 13180                             	        imulq	$19, %rdx, %rcx
 13181                             	        andq	%rax, %r11
 13182                             	        addq	%rcx, %r8
 13183                             	        adcq	$0x00, %r9
 13184                             	        adcq	$0x00, %r10
 13185                             	        adcq	$0x00, %r11
 13186                             	        # Reduce if top bit set
 13187                             	        movq	%r11, %rdx
 13188                             	        sarq	$63, %rdx
 13189                             	        andq	$19, %rdx
 13190                             	        andq	%rax, %r11
 13191                             	        addq	%rdx, %r8
 13192                             	        adcq	$0x00, %r9
 13193                             	        adcq	$0x00, %r10
 13194                             	        adcq	$0x00, %r11
 13195                             	        # Store
 13196                             	        movq	%r8, (%rdi)
 13197                             	        movq	%r9, 8(%rdi)
 13198                             	        movq	%r10, 16(%rdi)
 13199                             	        movq	%r11, 24(%rdi)
 13200                             	        movq	16(%rsp), %rdi
 13201                             	        movq	24(%rsp), %rsi
 13202                             	        movq	32(%rsp), %rbx
 13203                             	        # Multiply
 13204                             	        # A[0] * B[0]
 13205                             	        movq	(%rbx), %rdx
 13206                             	        mulxq	(%rsi), %r8, %r9
 13207                             	        # A[2] * B[0]
 13208                             	        mulxq	16(%rsi), %r10, %r11
 13209                             	        # A[1] * B[0]
 13210                             	        mulxq	8(%rsi), %rcx, %rax
 13211                             	        xorq	%r15, %r15
 13212                             	        adcxq	%rcx, %r9
 13213                             	        # A[1] * B[3]
 13214                             	        movq	24(%rbx), %rdx
 13215                             	        mulxq	8(%rsi), %r12, %r13
 13216                             	        adcxq	%rax, %r10
 13217                             	        # A[0] * B[1]
 13218                             	        movq	8(%rbx), %rdx
 13219                             	        mulxq	(%rsi), %rcx, %rax
 13220                             	        adoxq	%rcx, %r9
 13221                             	        # A[2] * B[1]
 13222                             	        mulxq	16(%rsi), %rcx, %r14
 13223                             	        adoxq	%rax, %r10
 13224                             	        adcxq	%rcx, %r11
 13225                             	        # A[1] * B[2]
 13226                             	        movq	16(%rbx), %rdx
 13227                             	        mulxq	8(%rsi), %rcx, %rax
 13228                             	        adcxq	%r14, %r12
 13229                             	        adoxq	%rcx, %r11
 13230                             	        adcxq	%r15, %r13
 13231                             	        adoxq	%rax, %r12
 13232                             	        # A[0] * B[2]
 13233                             	        mulxq	(%rsi), %rcx, %rax
 13234                             	        adoxq	%r15, %r13
 13235                             	        xorq	%r14, %r14
 13236                             	        adcxq	%rcx, %r10
 13237                             	        # A[1] * B[1]
 13238                             	        movq	8(%rbx), %rdx
 13239                             	        mulxq	8(%rsi), %rdx, %rcx
 13240                             	        adcxq	%rax, %r11
 13241                             	        adoxq	%rdx, %r10
 13242                             	        # A[3] * B[1]
 13243                             	        movq	8(%rbx), %rdx
 13244                             	        adoxq	%rcx, %r11
 13245                             	        mulxq	24(%rsi), %rcx, %rax
 13246                             	        adcxq	%rcx, %r12
 13247                             	        # A[2] * B[2]
 13248                             	        movq	16(%rbx), %rdx
 13249                             	        mulxq	16(%rsi), %rdx, %rcx
 13250                             	        adcxq	%rax, %r13
 13251                             	        adoxq	%rdx, %r12
 13252                             	        # A[3] * B[3]
 13253                             	        movq	24(%rbx), %rdx
 13254                             	        adoxq	%rcx, %r13
 13255                             	        mulxq	24(%rsi), %rcx, %rax
 13256                             	        adoxq	%r15, %r14
 13257                             	        adcxq	%rcx, %r14
 13258                             	        # A[0] * B[3]
 13259                             	        mulxq	(%rsi), %rdx, %rcx
 13260                             	        adcxq	%rax, %r15
 13261                             	        xorq	%rax, %rax
 13262                             	        adcxq	%rdx, %r11
 13263                             	        # A[3] * B[0]
 13264                             	        movq	(%rbx), %rdx
 13265                             	        adcxq	%rcx, %r12
 13266                             	        mulxq	24(%rsi), %rdx, %rcx
 13267                             	        adoxq	%rdx, %r11
 13268                             	        adoxq	%rcx, %r12
 13269                             	        # A[2] * B[3]
 13270                             	        movq	24(%rbx), %rdx
 13271                             	        mulxq	16(%rsi), %rdx, %rcx
 13272                             	        adcxq	%rdx, %r13
 13273                             	        # A[3] * B[2]
 13274                             	        movq	16(%rbx), %rdx
 13275                             	        adcxq	%rcx, %r14
 13276                             	        mulxq	24(%rsi), %rcx, %rdx
 13277                             	        adcxq	%rax, %r15
 13278                             	        adoxq	%rcx, %r13
 13279                             	        adoxq	%rdx, %r14
 13280                             	        adoxq	%rax, %r15
 13281                             	        # Reduce
 13282                             	        movq	$0x7fffffffffffffff, %rax
 13283                             	        #  Move top half into t4-t7 and remove top bit from t3
 13284                             	        shldq	$0x01, %r14, %r15
 13285                             	        shldq	$0x01, %r13, %r14
 13286                             	        shldq	$0x01, %r12, %r13
 13287                             	        shldq	$0x01, %r11, %r12
 13288                             	        andq	%rax, %r11
 13289                             	        #  Multiply top half by 19
 13290                             	        movq	$19, %rdx
 13291                             	        xorq	%rax, %rax
 13292                             	        mulxq	%r12, %rcx, %r12
 13293                             	        adcxq	%rcx, %r8
 13294                             	        adoxq	%r12, %r9
 13295                             	        mulxq	%r13, %rcx, %r13
 13296                             	        adcxq	%rcx, %r9
 13297                             	        adoxq	%r13, %r10
 13298                             	        mulxq	%r14, %rcx, %r14
 13299                             	        adcxq	%rcx, %r10
 13300                             	        adoxq	%r14, %r11
 13301                             	        mulxq	%r15, %r15, %rdx
 13302                             	        adcxq	%r15, %r11
 13303                             	        adoxq	%rax, %rdx
 13304                             	        adcxq	%rax, %rdx
 13305                             	        #  Overflow
 13306                             	        shldq	$0x01, %r11, %rdx
 13307                             	        movq	$0x7fffffffffffffff, %rax
 13308                             	        imulq	$19, %rdx, %rcx
 13309                             	        andq	%rax, %r11
 13310                             	        addq	%rcx, %r8
 13311                             	        adcq	$0x00, %r9
 13312                             	        adcq	$0x00, %r10
 13313                             	        adcq	$0x00, %r11
 13314                             	        # Reduce if top bit set
 13315                             	        movq	%r11, %rdx
 13316                             	        sarq	$63, %rdx
 13317                             	        andq	$19, %rdx
 13318                             	        andq	%rax, %r11
 13319                             	        addq	%rdx, %r8
 13320                             	        adcq	$0x00, %r9
 13321                             	        adcq	$0x00, %r10
 13322                             	        adcq	$0x00, %r11
 13323                             	        # Store
 13324                             	        movq	%r8, (%rdi)
 13325                             	        movq	%r9, 8(%rdi)
 13326                             	        movq	%r10, 16(%rdi)
 13327                             	        movq	%r11, 24(%rdi)
 13328                             	        addq	$40, %rsp
 13329                             	        popq	%r15
 13330                             	        popq	%r14
 13331                             	        popq	%r13
 13332                             	        popq	%r12
 13333                             	        popq	%rbx
 13334                             	        repz retq
 13335                             	#ifndef __APPLE__
 13337                             	#endif /* __APPLE__ */
 13338                             	#ifndef __APPLE__
 13339                             	.text
 13340                             	.globl	fe_ge_dbl_avx2
 13342                             	.align	16
 13343                             	fe_ge_dbl_avx2:
 13344                             	#else
 13345                             	.section	__TEXT,__text
 13346                             	.globl	_fe_ge_dbl_avx2
 13347                             	.p2align	4
 13348                             	_fe_ge_dbl_avx2:
 13349                             	#endif /* __APPLE__ */
 13350                             	        pushq	%rbp
 13351                             	        pushq	%rbx
 13352                             	        pushq	%r12
 13353                             	        pushq	%r13
 13354                             	        pushq	%r14
 13355                             	        pushq	%r15
 13356                             	        subq	$48, %rsp
 13357                             	        movq	%rdi, (%rsp)
 13358                             	        movq	%rsi, 8(%rsp)
 13359                             	        movq	%rdx, 16(%rsp)
 13360                             	        movq	%rcx, 24(%rsp)
 13361                             	        movq	%r8, 32(%rsp)
 13362                             	        movq	%r9, 40(%rsp)
 13363                             	        movq	32(%rsp), %rsi
 13364                             	        # Square
 13365                             	        # A[0] * A[1]
 13366                             	        movq	(%rsi), %rdx
 13367                             	        mulxq	8(%rsi), %r9, %r10
 13368                             	        # A[0] * A[3]
 13369                             	        mulxq	24(%rsi), %r11, %r12
 13370                             	        # A[2] * A[1]
 13371                             	        movq	16(%rsi), %rdx
 13372                             	        mulxq	8(%rsi), %rcx, %rax
 13373                             	        xorq	%r15, %r15
 13374                             	        adoxq	%rcx, %r11
 13375                             	        # A[2] * A[3]
 13376                             	        mulxq	24(%rsi), %r13, %r14
 13377                             	        adoxq	%rax, %r12
 13378                             	        # A[2] * A[0]
 13379                             	        mulxq	(%rsi), %rcx, %rax
 13380                             	        adoxq	%r15, %r13
 13381                             	        adcxq	%rcx, %r10
 13382                             	        adoxq	%r15, %r14
 13383                             	        # A[1] * A[3]
 13384                             	        movq	8(%rsi), %rdx
 13385                             	        mulxq	24(%rsi), %rbp, %r8
 13386                             	        adcxq	%rax, %r11
 13387                             	        adcxq	%rbp, %r12
 13388                             	        adcxq	%r8, %r13
 13389                             	        adcxq	%r15, %r14
 13390                             	        # Double with Carry Flag
 13391                             	        xorq	%r15, %r15
 13392                             	        # A[0] * A[0]
 13393                             	        movq	(%rsi), %rdx
 13394                             	        mulxq	%rdx, %r8, %rbp
 13395                             	        adcxq	%r9, %r9
 13396                             	        # A[1] * A[1]
 13397                             	        movq	8(%rsi), %rdx
 13398                             	        mulxq	%rdx, %rcx, %rax
 13399                             	        adcxq	%r10, %r10
 13400                             	        adoxq	%rbp, %r9
 13401                             	        adcxq	%r11, %r11
 13402                             	        adoxq	%rcx, %r10
 13403                             	        # A[2] * A[2]
 13404                             	        movq	16(%rsi), %rdx
 13405                             	        mulxq	%rdx, %rbp, %rcx
 13406                             	        adcxq	%r12, %r12
 13407                             	        adoxq	%rax, %r11
 13408                             	        adcxq	%r13, %r13
 13409                             	        adoxq	%rbp, %r12
 13410                             	        # A[3] * A[3]
 13411                             	        movq	24(%rsi), %rdx
 13412                             	        mulxq	%rdx, %rbp, %rax
 13413                             	        adcxq	%r14, %r14
 13414                             	        adoxq	%rcx, %r13
 13415                             	        adcxq	%r15, %r15
 13416                             	        adoxq	%rbp, %r14
 13417                             	        adoxq	%rax, %r15
 13418                             	        # Reduce
 13419                             	        movq	$0x7fffffffffffffff, %rcx
 13420                             	        #  Move top half into t4-t7 and remove top bit from t3
 13421                             	        shldq	$0x01, %r14, %r15
 13422                             	        shldq	$0x01, %r13, %r14
 13423                             	        shldq	$0x01, %r12, %r13
 13424                             	        shldq	$0x01, %r11, %r12
 13425                             	        andq	%rcx, %r11
 13426                             	        #  Multiply top half by 19
 13427                             	        movq	$19, %rdx
 13428                             	        xorq	%rcx, %rcx
 13429                             	        mulxq	%r12, %rbp, %r12
 13430                             	        adcxq	%rbp, %r8
 13431                             	        adoxq	%r12, %r9
 13432                             	        mulxq	%r13, %rbp, %r13
 13433                             	        adcxq	%rbp, %r9
 13434                             	        adoxq	%r13, %r10
 13435                             	        mulxq	%r14, %rbp, %r14
 13436                             	        adcxq	%rbp, %r10
 13437                             	        adoxq	%r14, %r11
 13438                             	        mulxq	%r15, %r15, %rdx
 13439                             	        adcxq	%r15, %r11
 13440                             	        adoxq	%rcx, %rdx
 13441                             	        adcxq	%rcx, %rdx
 13442                             	        #  Overflow
 13443                             	        shldq	$0x01, %r11, %rdx
 13444                             	        movq	$0x7fffffffffffffff, %rcx
 13445                             	        imulq	$19, %rdx, %rbp
 13446                             	        andq	%rcx, %r11
 13447                             	        addq	%rbp, %r8
 13448                             	        adcq	$0x00, %r9
 13449                             	        adcq	$0x00, %r10
 13450                             	        adcq	$0x00, %r11
 13451                             	        # Reduce if top bit set
 13452                             	        movq	%r11, %rdx
 13453                             	        sarq	$63, %rdx
 13454                             	        andq	$19, %rdx
 13455                             	        andq	%rcx, %r11
 13456                             	        addq	%rdx, %r8
 13457                             	        adcq	$0x00, %r9
 13458                             	        adcq	$0x00, %r10
 13459                             	        adcq	$0x00, %r11
 13460                             	        # Store
 13461                             	        movq	%r8, (%rdi)
 13462                             	        movq	%r9, 8(%rdi)
 13463                             	        movq	%r10, 16(%rdi)
 13464                             	        movq	%r11, 24(%rdi)
 13465                             	        movq	16(%rsp), %rdi
 13466                             	        movq	40(%rsp), %rbx
 13467                             	        # Square
 13468                             	        # A[0] * A[1]
 13469                             	        movq	(%rbx), %rdx
 13470                             	        mulxq	8(%rbx), %r9, %r10
 13471                             	        # A[0] * A[3]
 13472                             	        mulxq	24(%rbx), %r11, %r12
 13473                             	        # A[2] * A[1]
 13474                             	        movq	16(%rbx), %rdx
 13475                             	        mulxq	8(%rbx), %rcx, %rax
 13476                             	        xorq	%r15, %r15
 13477                             	        adoxq	%rcx, %r11
 13478                             	        # A[2] * A[3]
 13479                             	        mulxq	24(%rbx), %r13, %r14
 13480                             	        adoxq	%rax, %r12
 13481                             	        # A[2] * A[0]
 13482                             	        mulxq	(%rbx), %rcx, %rax
 13483                             	        adoxq	%r15, %r13
 13484                             	        adcxq	%rcx, %r10
 13485                             	        adoxq	%r15, %r14
 13486                             	        # A[1] * A[3]
 13487                             	        movq	8(%rbx), %rdx
 13488                             	        mulxq	24(%rbx), %rbp, %r8
 13489                             	        adcxq	%rax, %r11
 13490                             	        adcxq	%rbp, %r12
 13491                             	        adcxq	%r8, %r13
 13492                             	        adcxq	%r15, %r14
 13493                             	        # Double with Carry Flag
 13494                             	        xorq	%r15, %r15
 13495                             	        # A[0] * A[0]
 13496                             	        movq	(%rbx), %rdx
 13497                             	        mulxq	%rdx, %r8, %rbp
 13498                             	        adcxq	%r9, %r9
 13499                             	        # A[1] * A[1]
 13500                             	        movq	8(%rbx), %rdx
 13501                             	        mulxq	%rdx, %rcx, %rax
 13502                             	        adcxq	%r10, %r10
 13503                             	        adoxq	%rbp, %r9
 13504                             	        adcxq	%r11, %r11
 13505                             	        adoxq	%rcx, %r10
 13506                             	        # A[2] * A[2]
 13507                             	        movq	16(%rbx), %rdx
 13508                             	        mulxq	%rdx, %rbp, %rcx
 13509                             	        adcxq	%r12, %r12
 13510                             	        adoxq	%rax, %r11
 13511                             	        adcxq	%r13, %r13
 13512                             	        adoxq	%rbp, %r12
 13513                             	        # A[3] * A[3]
 13514                             	        movq	24(%rbx), %rdx
 13515                             	        mulxq	%rdx, %rbp, %rax
 13516                             	        adcxq	%r14, %r14
 13517                             	        adoxq	%rcx, %r13
 13518                             	        adcxq	%r15, %r15
 13519                             	        adoxq	%rbp, %r14
 13520                             	        adoxq	%rax, %r15
 13521                             	        # Reduce
 13522                             	        movq	$0x7fffffffffffffff, %rcx
 13523                             	        #  Move top half into t4-t7 and remove top bit from t3
 13524                             	        shldq	$0x01, %r14, %r15
 13525                             	        shldq	$0x01, %r13, %r14
 13526                             	        shldq	$0x01, %r12, %r13
 13527                             	        shldq	$0x01, %r11, %r12
 13528                             	        andq	%rcx, %r11
 13529                             	        #  Multiply top half by 19
 13530                             	        movq	$19, %rdx
 13531                             	        xorq	%rcx, %rcx
 13532                             	        mulxq	%r12, %rbp, %r12
 13533                             	        adcxq	%rbp, %r8
 13534                             	        adoxq	%r12, %r9
 13535                             	        mulxq	%r13, %rbp, %r13
 13536                             	        adcxq	%rbp, %r9
 13537                             	        adoxq	%r13, %r10
 13538                             	        mulxq	%r14, %rbp, %r14
 13539                             	        adcxq	%rbp, %r10
 13540                             	        adoxq	%r14, %r11
 13541                             	        mulxq	%r15, %r15, %rdx
 13542                             	        adcxq	%r15, %r11
 13543                             	        adoxq	%rcx, %rdx
 13544                             	        adcxq	%rcx, %rdx
 13545                             	        #  Overflow
 13546                             	        shldq	$0x01, %r11, %rdx
 13547                             	        movq	$0x7fffffffffffffff, %rcx
 13548                             	        imulq	$19, %rdx, %rbp
 13549                             	        andq	%rcx, %r11
 13550                             	        addq	%rbp, %r8
 13551                             	        adcq	$0x00, %r9
 13552                             	        adcq	$0x00, %r10
 13553                             	        adcq	$0x00, %r11
 13554                             	        # Reduce if top bit set
 13555                             	        movq	%r11, %rdx
 13556                             	        sarq	$63, %rdx
 13557                             	        andq	$19, %rdx
 13558                             	        andq	%rcx, %r11
 13559                             	        addq	%rdx, %r8
 13560                             	        adcq	$0x00, %r9
 13561                             	        adcq	$0x00, %r10
 13562                             	        adcq	$0x00, %r11
 13563                             	        # Store
 13564                             	        movq	%r8, (%rdi)
 13565                             	        movq	%r9, 8(%rdi)
 13566                             	        movq	%r10, 16(%rdi)
 13567                             	        movq	%r11, 24(%rdi)
 13568                             	        movq	8(%rsp), %rdi
 13569                             	        # Add
 13570                             	        movq	(%rsi), %r8
 13571                             	        movq	8(%rsi), %r9
 13572                             	        addq	(%rbx), %r8
 13573                             	        movq	16(%rsi), %r10
 13574                             	        adcq	8(%rbx), %r9
 13575                             	        movq	24(%rsi), %rdx
 13576                             	        adcq	16(%rbx), %r10
 13577                             	        movq	$-19, %rcx
 13578                             	        adcq	24(%rbx), %rdx
 13579                             	        movq	$0x7fffffffffffffff, %rax
 13580                             	        movq	%rdx, %r11
 13581                             	        sarq	$63, %rdx
 13582                             	        #   Mask the modulus
 13583                             	        andq	%rdx, %rcx
 13584                             	        andq	%rdx, %rax
 13585                             	        #   Sub modulus (if overflow)
 13586                             	        subq	%rcx, %r8
 13587                             	        sbbq	%rdx, %r9
 13588                             	        sbbq	%rdx, %r10
 13589                             	        sbbq	%rax, %r11
 13590                             	        movq	%r8, (%rdi)
 13591                             	        movq	%r9, 8(%rdi)
 13592                             	        movq	%r10, 16(%rdi)
 13593                             	        movq	%r11, 24(%rdi)
 13594                             	        movq	24(%rsp), %rsi
 13595                             	        # Square
 13596                             	        # A[0] * A[1]
 13597                             	        movq	(%rdi), %rdx
 13598                             	        mulxq	8(%rdi), %r9, %r10
 13599                             	        # A[0] * A[3]
 13600                             	        mulxq	24(%rdi), %r11, %r12
 13601                             	        # A[2] * A[1]
 13602                             	        movq	16(%rdi), %rdx
 13603                             	        mulxq	8(%rdi), %rcx, %rax
 13604                             	        xorq	%r15, %r15
 13605                             	        adoxq	%rcx, %r11
 13606                             	        # A[2] * A[3]
 13607                             	        mulxq	24(%rdi), %r13, %r14
 13608                             	        adoxq	%rax, %r12
 13609                             	        # A[2] * A[0]
 13610                             	        mulxq	(%rdi), %rcx, %rax
 13611                             	        adoxq	%r15, %r13
 13612                             	        adcxq	%rcx, %r10
 13613                             	        adoxq	%r15, %r14
 13614                             	        # A[1] * A[3]
 13615                             	        movq	8(%rdi), %rdx
 13616                             	        mulxq	24(%rdi), %rbp, %r8
 13617                             	        adcxq	%rax, %r11
 13618                             	        adcxq	%rbp, %r12
 13619                             	        adcxq	%r8, %r13
 13620                             	        adcxq	%r15, %r14
 13621                             	        # Double with Carry Flag
 13622                             	        xorq	%r15, %r15
 13623                             	        # A[0] * A[0]
 13624                             	        movq	(%rdi), %rdx
 13625                             	        mulxq	%rdx, %r8, %rbp
 13626                             	        adcxq	%r9, %r9
 13627                             	        # A[1] * A[1]
 13628                             	        movq	8(%rdi), %rdx
 13629                             	        mulxq	%rdx, %rcx, %rax
 13630                             	        adcxq	%r10, %r10
 13631                             	        adoxq	%rbp, %r9
 13632                             	        adcxq	%r11, %r11
 13633                             	        adoxq	%rcx, %r10
 13634                             	        # A[2] * A[2]
 13635                             	        movq	16(%rdi), %rdx
 13636                             	        mulxq	%rdx, %rbp, %rcx
 13637                             	        adcxq	%r12, %r12
 13638                             	        adoxq	%rax, %r11
 13639                             	        adcxq	%r13, %r13
 13640                             	        adoxq	%rbp, %r12
 13641                             	        # A[3] * A[3]
 13642                             	        movq	24(%rdi), %rdx
 13643                             	        mulxq	%rdx, %rbp, %rax
 13644                             	        adcxq	%r14, %r14
 13645                             	        adoxq	%rcx, %r13
 13646                             	        adcxq	%r15, %r15
 13647                             	        adoxq	%rbp, %r14
 13648                             	        adoxq	%rax, %r15
 13649                             	        # Reduce
 13650                             	        movq	$0x7fffffffffffffff, %rcx
 13651                             	        #  Move top half into t4-t7 and remove top bit from t3
 13652                             	        shldq	$0x01, %r14, %r15
 13653                             	        shldq	$0x01, %r13, %r14
 13654                             	        shldq	$0x01, %r12, %r13
 13655                             	        shldq	$0x01, %r11, %r12
 13656                             	        andq	%rcx, %r11
 13657                             	        #  Multiply top half by 19
 13658                             	        movq	$19, %rdx
 13659                             	        xorq	%rcx, %rcx
 13660                             	        mulxq	%r12, %rbp, %r12
 13661                             	        adcxq	%rbp, %r8
 13662                             	        adoxq	%r12, %r9
 13663                             	        mulxq	%r13, %rbp, %r13
 13664                             	        adcxq	%rbp, %r9
 13665                             	        adoxq	%r13, %r10
 13666                             	        mulxq	%r14, %rbp, %r14
 13667                             	        adcxq	%rbp, %r10
 13668                             	        adoxq	%r14, %r11
 13669                             	        mulxq	%r15, %r15, %rdx
 13670                             	        adcxq	%r15, %r11
 13671                             	        adoxq	%rcx, %rdx
 13672                             	        adcxq	%rcx, %rdx
 13673                             	        #  Overflow
 13674                             	        shldq	$0x01, %r11, %rdx
 13675                             	        movq	$0x7fffffffffffffff, %rcx
 13676                             	        imulq	$19, %rdx, %rbp
 13677                             	        andq	%rcx, %r11
 13678                             	        addq	%rbp, %r8
 13679                             	        adcq	$0x00, %r9
 13680                             	        adcq	$0x00, %r10
 13681                             	        adcq	$0x00, %r11
 13682                             	        # Reduce if top bit set
 13683                             	        movq	%r11, %rdx
 13684                             	        sarq	$63, %rdx
 13685                             	        andq	$19, %rdx
 13686                             	        andq	%rcx, %r11
 13687                             	        addq	%rdx, %r8
 13688                             	        adcq	$0x00, %r9
 13689                             	        adcq	$0x00, %r10
 13690                             	        adcq	$0x00, %r11
 13691                             	        # Store
 13692                             	        movq	%r8, (%rsi)
 13693                             	        movq	%r9, 8(%rsi)
 13694                             	        movq	%r10, 16(%rsi)
 13695                             	        movq	%r11, 24(%rsi)
 13696                             	        movq	16(%rsp), %rsi
 13697                             	        movq	(%rsp), %rbx
 13698                             	        # Add
 13699                             	        movq	(%rsi), %r8
 13700                             	        movq	8(%rsi), %r9
 13701                             	        movq	16(%rsi), %r10
 13702                             	        movq	24(%rsi), %rdx
 13703                             	        movq	%r8, %r12
 13704                             	        addq	(%rbx), %r8
 13705                             	        movq	%r9, %r13
 13706                             	        adcq	8(%rbx), %r9
 13707                             	        movq	%r10, %r14
 13708                             	        adcq	16(%rbx), %r10
 13709                             	        movq	%rdx, %r15
 13710                             	        adcq	24(%rbx), %rdx
 13711                             	        movq	$-19, %rcx
 13712                             	        movq	%rdx, %r11
 13713                             	        movq	$0x7fffffffffffffff, %rax
 13714                             	        sarq	$63, %rdx
 13715                             	        #   Mask the modulus
 13716                             	        andq	%rdx, %rcx
 13717                             	        andq	%rdx, %rax
 13718                             	        #   Sub modulus (if overflow)
 13719                             	        subq	%rcx, %r8
 13720                             	        sbbq	%rdx, %r9
 13721                             	        sbbq	%rdx, %r10
 13722                             	        sbbq	%rax, %r11
 13723                             	        # Sub
 13724                             	        subq	(%rbx), %r12
 13725                             	        movq	$0x00, %rdx
 13726                             	        sbbq	8(%rbx), %r13
 13727                             	        movq	$-19, %rcx
 13728                             	        sbbq	16(%rbx), %r14
 13729                             	        movq	$0x7fffffffffffffff, %rax
 13730                             	        sbbq	24(%rbx), %r15
 13731                             	        sbbq	$0x00, %rdx
 13732                             	        #   Mask the modulus
 13733                             	        andq	%rdx, %rcx
 13734                             	        andq	%rdx, %rax
 13735                             	        #   Add modulus (if underflow)
 13736                             	        addq	%rcx, %r12
 13737                             	        adcq	%rdx, %r13
 13738                             	        adcq	%rdx, %r14
 13739                             	        adcq	%rax, %r15
 13740                             	        movq	%r8, (%rdi)
 13741                             	        movq	%r9, 8(%rdi)
 13742                             	        movq	%r10, 16(%rdi)
 13743                             	        movq	%r11, 24(%rdi)
 13744                             	        movq	%r12, (%rsi)
 13745                             	        movq	%r13, 8(%rsi)
 13746                             	        movq	%r14, 16(%rsi)
 13747                             	        movq	%r15, 24(%rsi)
 13748                             	        movq	24(%rsp), %rsi
 13749                             	        # Sub
 13750                             	        movq	(%rsi), %r8
 13751                             	        movq	8(%rsi), %r9
 13752                             	        movq	16(%rsi), %r10
 13753                             	        movq	24(%rsi), %r11
 13754                             	        subq	(%rdi), %r8
 13755                             	        movq	$0x00, %rdx
 13756                             	        sbbq	8(%rdi), %r9
 13757                             	        movq	$-19, %rcx
 13758                             	        sbbq	16(%rdi), %r10
 13759                             	        movq	$0x7fffffffffffffff, %rax
 13760                             	        sbbq	24(%rdi), %r11
 13761                             	        sbbq	$0x00, %rdx
 13762                             	        #   Mask the modulus
 13763                             	        andq	%rdx, %rcx
 13764                             	        andq	%rdx, %rax
 13765                             	        #   Add modulus (if underflow)
 13766                             	        addq	%rcx, %r8
 13767                             	        adcq	%rdx, %r9
 13768                             	        adcq	%rdx, %r10
 13769                             	        adcq	%rax, %r11
 13770                             	        movq	%r8, (%rbx)
 13771                             	        movq	%r9, 8(%rbx)
 13772                             	        movq	%r10, 16(%rbx)
 13773                             	        movq	%r11, 24(%rbx)
 13774                             	        movq	104(%rsp), %rdi
 13775                             	        # Square * 2
 13776                             	        # A[0] * A[1]
 13777                             	        movq	(%rdi), %rdx
 13778                             	        mulxq	8(%rdi), %r9, %r10
 13779                             	        # A[0] * A[3]
 13780                             	        mulxq	24(%rdi), %r11, %r12
 13781                             	        # A[2] * A[1]
 13782                             	        movq	16(%rdi), %rdx
 13783                             	        mulxq	8(%rdi), %rcx, %rax
 13784                             	        xorq	%r15, %r15
 13785                             	        adoxq	%rcx, %r11
 13786                             	        # A[2] * A[3]
 13787                             	        mulxq	24(%rdi), %r13, %r14
 13788                             	        adoxq	%rax, %r12
 13789                             	        # A[2] * A[0]
 13790                             	        mulxq	(%rdi), %rcx, %rax
 13791                             	        adoxq	%r15, %r13
 13792                             	        adcxq	%rcx, %r10
 13793                             	        adoxq	%r15, %r14
 13794                             	        # A[1] * A[3]
 13795                             	        movq	8(%rdi), %rdx
 13796                             	        mulxq	24(%rdi), %rbp, %r8
 13797                             	        adcxq	%rax, %r11
 13798                             	        adcxq	%rbp, %r12
 13799                             	        adcxq	%r8, %r13
 13800                             	        adcxq	%r15, %r14
 13801                             	        # Double with Carry Flag
 13802                             	        xorq	%r15, %r15
 13803                             	        # A[0] * A[0]
 13804                             	        movq	(%rdi), %rdx
 13805                             	        mulxq	%rdx, %r8, %rbp
 13806                             	        adcxq	%r9, %r9
 13807                             	        # A[1] * A[1]
 13808                             	        movq	8(%rdi), %rdx
 13809                             	        mulxq	%rdx, %rcx, %rax
 13810                             	        adcxq	%r10, %r10
 13811                             	        adoxq	%rbp, %r9
 13812                             	        adcxq	%r11, %r11
 13813                             	        adoxq	%rcx, %r10
 13814                             	        # A[2] * A[2]
 13815                             	        movq	16(%rdi), %rdx
 13816                             	        mulxq	%rdx, %rbp, %rcx
 13817                             	        adcxq	%r12, %r12
 13818                             	        adoxq	%rax, %r11
 13819                             	        adcxq	%r13, %r13
 13820                             	        adoxq	%rbp, %r12
 13821                             	        # A[3] * A[3]
 13822                             	        movq	24(%rdi), %rdx
 13823                             	        mulxq	%rdx, %rbp, %rax
 13824                             	        adcxq	%r14, %r14
 13825                             	        adoxq	%rcx, %r13
 13826                             	        adcxq	%r15, %r15
 13827                             	        adoxq	%rbp, %r14
 13828                             	        adoxq	%rax, %r15
 13829                             	        # Reduce
 13830                             	        movq	$0x7fffffffffffffff, %rax
 13831                             	        xorq	%rbp, %rbp
 13832                             	        #  Move top half into t4-t7 and remove top bit from t3 and double
 13833                             	        shldq	$3, %r15, %rbp
 13834                             	        shldq	$2, %r14, %r15
 13835                             	        shldq	$2, %r13, %r14
 13836                             	        shldq	$2, %r12, %r13
 13837                             	        shldq	$2, %r11, %r12
 13838                             	        shldq	$0x01, %r10, %r11
 13839                             	        shldq	$0x01, %r9, %r10
 13840                             	        shldq	$0x01, %r8, %r9
 13841                             	        shlq	$0x01, %r8
 13842                             	        andq	%rax, %r11
 13843                             	        #  Two out left, one in right
 13844                             	        andq	%rax, %r15
 13845                             	        #  Multiply top bits by 19*19
 13846                             	        imulq	$0x169, %rbp, %rcx
 13847                             	        xorq	%rax, %rax
 13848                             	        #  Multiply top half by 19
 13849                             	        movq	$19, %rdx
 13850                             	        adoxq	%rcx, %r8
 13851                             	        mulxq	%r12, %rbp, %r12
 13852                             	        adcxq	%rbp, %r8
 13853                             	        adoxq	%r12, %r9
 13854                             	        mulxq	%r13, %rbp, %r13
 13855                             	        adcxq	%rbp, %r9
 13856                             	        adoxq	%r13, %r10
 13857                             	        mulxq	%r14, %rbp, %r14
 13858                             	        adcxq	%rbp, %r10
 13859                             	        adoxq	%r14, %r11
 13860                             	        mulxq	%r15, %r15, %rdx
 13861                             	        adcxq	%r15, %r11
 13862                             	        adoxq	%rax, %rdx
 13863                             	        adcxq	%rax, %rdx
 13864                             	        #  Overflow
 13865                             	        shldq	$0x01, %r11, %rdx
 13866                             	        movq	$0x7fffffffffffffff, %rax
 13867                             	        imulq	$19, %rdx, %rbp
 13868                             	        andq	%rax, %r11
 13869                             	        addq	%rbp, %r8
 13870                             	        adcq	$0x00, %r9
 13871                             	        adcq	$0x00, %r10
 13872                             	        adcq	$0x00, %r11
 13873                             	        # Reduce if top bit set
 13874                             	        movq	%r11, %rdx
 13875                             	        sarq	$63, %rdx
 13876                             	        andq	$19, %rdx
 13877                             	        andq	%rax, %r11
 13878                             	        addq	%rdx, %r8
 13879                             	        adcq	$0x00, %r9
 13880                             	        adcq	$0x00, %r10
 13881                             	        adcq	$0x00, %r11
 13882                             	        # Store
 13883                             	        movq	%r8, (%rsi)
 13884                             	        movq	%r9, 8(%rsi)
 13885                             	        movq	%r10, 16(%rsi)
 13886                             	        movq	%r11, 24(%rsi)
 13887                             	        movq	16(%rsp), %rdi
 13888                             	        # Sub
 13889                             	        movq	(%rsi), %r8
 13890                             	        movq	8(%rsi), %r9
 13891                             	        movq	16(%rsi), %r10
 13892                             	        movq	24(%rsi), %r11
 13893                             	        subq	(%rdi), %r8
 13894                             	        movq	$0x00, %rdx
 13895                             	        sbbq	8(%rdi), %r9
 13896                             	        movq	$-19, %rcx
 13897                             	        sbbq	16(%rdi), %r10
 13898                             	        movq	$0x7fffffffffffffff, %rax
 13899                             	        sbbq	24(%rdi), %r11
 13900                             	        sbbq	$0x00, %rdx
 13901                             	        #   Mask the modulus
 13902                             	        andq	%rdx, %rcx
 13903                             	        andq	%rdx, %rax
 13904                             	        #   Add modulus (if underflow)
 13905                             	        addq	%rcx, %r8
 13906                             	        adcq	%rdx, %r9
 13907                             	        adcq	%rdx, %r10
 13908                             	        adcq	%rax, %r11
 13909                             	        movq	%r8, (%rsi)
 13910                             	        movq	%r9, 8(%rsi)
 13911                             	        movq	%r10, 16(%rsi)
 13912                             	        movq	%r11, 24(%rsi)
 13913                             	        addq	$48, %rsp
 13914                             	        popq	%r15
 13915                             	        popq	%r14
 13916                             	        popq	%r13
 13917                             	        popq	%r12
 13918                             	        popq	%rbx
 13919                             	        popq	%rbp
 13920                             	        repz retq
 13921                             	#ifndef __APPLE__
 13923                             	#endif /* __APPLE__ */
 13924                             	#ifndef __APPLE__
 13925                             	.text
 13926                             	.globl	fe_ge_madd_avx2
 13928                             	.align	16
 13929                             	fe_ge_madd_avx2:
 13930                             	#else
 13931                             	.section	__TEXT,__text
 13932                             	.globl	_fe_ge_madd_avx2
 13933                             	.p2align	4
 13934                             	_fe_ge_madd_avx2:
 13935                             	#endif /* __APPLE__ */
 13936                             	        pushq	%rbp
 13937                             	        pushq	%rbx
 13938                             	        pushq	%r12
 13939                             	        pushq	%r13
 13940                             	        pushq	%r14
 13941                             	        pushq	%r15
 13942                             	        subq	$48, %rsp
 13943                             	        movq	%rdi, (%rsp)
 13944                             	        movq	%rsi, 8(%rsp)
 13945                             	        movq	%rdx, 16(%rsp)
 13946                             	        movq	%rcx, 24(%rsp)
 13947                             	        movq	%r8, 32(%rsp)
 13948                             	        movq	%r9, 40(%rsp)
 13949                             	        movq	8(%rsp), %rsi
 13950                             	        movq	40(%rsp), %rbx
 13951                             	        movq	32(%rsp), %rbp
 13952                             	        # Add
 13953                             	        movq	(%rbx), %r8
 13954                             	        movq	8(%rbx), %r9
 13955                             	        movq	16(%rbx), %r10
 13956                             	        movq	24(%rbx), %rdx
 13957                             	        movq	%r8, %r12
 13958                             	        addq	(%rbp), %r8
 13959                             	        movq	%r9, %r13
 13960                             	        adcq	8(%rbp), %r9
 13961                             	        movq	%r10, %r14
 13962                             	        adcq	16(%rbp), %r10
 13963                             	        movq	%rdx, %r15
 13964                             	        adcq	24(%rbp), %rdx
 13965                             	        movq	$-19, %rcx
 13966                             	        movq	%rdx, %r11
 13967                             	        movq	$0x7fffffffffffffff, %rax
 13968                             	        sarq	$63, %rdx
 13969                             	        #   Mask the modulus
 13970                             	        andq	%rdx, %rcx
 13971                             	        andq	%rdx, %rax
 13972                             	        #   Sub modulus (if overflow)
 13973                             	        subq	%rcx, %r8
 13974                             	        sbbq	%rdx, %r9
 13975                             	        sbbq	%rdx, %r10
 13976                             	        sbbq	%rax, %r11
 13977                             	        # Sub
 13978                             	        subq	(%rbp), %r12
 13979                             	        movq	$0x00, %rdx
 13980                             	        sbbq	8(%rbp), %r13
 13981                             	        movq	$-19, %rcx
 13982                             	        sbbq	16(%rbp), %r14
 13983                             	        movq	$0x7fffffffffffffff, %rax
 13984                             	        sbbq	24(%rbp), %r15
 13985                             	        sbbq	$0x00, %rdx
 13986                             	        #   Mask the modulus
 13987                             	        andq	%rdx, %rcx
 13988                             	        andq	%rdx, %rax
 13989                             	        #   Add modulus (if underflow)
 13990                             	        addq	%rcx, %r12
 13991                             	        adcq	%rdx, %r13
 13992                             	        adcq	%rdx, %r14
 13993                             	        adcq	%rax, %r15
 13994                             	        movq	%r8, (%rdi)
 13995                             	        movq	%r9, 8(%rdi)
 13996                             	        movq	%r10, 16(%rdi)
 13997                             	        movq	%r11, 24(%rdi)
 13998                             	        movq	%r12, (%rsi)
 13999                             	        movq	%r13, 8(%rsi)
 14000                             	        movq	%r14, 16(%rsi)
 14001                             	        movq	%r15, 24(%rsi)
 14002                             	        movq	16(%rsp), %rbx
 14003                             	        movq	128(%rsp), %rbp
 14004                             	        # Multiply
 14005                             	        # A[0] * B[0]
 14006                             	        movq	(%rbp), %rdx
 14007                             	        mulxq	(%rdi), %r8, %r9
 14008                             	        # A[2] * B[0]
 14009                             	        mulxq	16(%rdi), %r10, %r11
 14010                             	        # A[1] * B[0]
 14011                             	        mulxq	8(%rdi), %rcx, %rax
 14012                             	        xorq	%r15, %r15
 14013                             	        adcxq	%rcx, %r9
 14014                             	        # A[1] * B[3]
 14015                             	        movq	24(%rbp), %rdx
 14016                             	        mulxq	8(%rdi), %r12, %r13
 14017                             	        adcxq	%rax, %r10
 14018                             	        # A[0] * B[1]
 14019                             	        movq	8(%rbp), %rdx
 14020                             	        mulxq	(%rdi), %rcx, %rax
 14021                             	        adoxq	%rcx, %r9
 14022                             	        # A[2] * B[1]
 14023                             	        mulxq	16(%rdi), %rcx, %r14
 14024                             	        adoxq	%rax, %r10
 14025                             	        adcxq	%rcx, %r11
 14026                             	        # A[1] * B[2]
 14027                             	        movq	16(%rbp), %rdx
 14028                             	        mulxq	8(%rdi), %rcx, %rax
 14029                             	        adcxq	%r14, %r12
 14030                             	        adoxq	%rcx, %r11
 14031                             	        adcxq	%r15, %r13
 14032                             	        adoxq	%rax, %r12
 14033                             	        # A[0] * B[2]
 14034                             	        mulxq	(%rdi), %rcx, %rax
 14035                             	        adoxq	%r15, %r13
 14036                             	        xorq	%r14, %r14
 14037                             	        adcxq	%rcx, %r10
 14038                             	        # A[1] * B[1]
 14039                             	        movq	8(%rbp), %rdx
 14040                             	        mulxq	8(%rdi), %rdx, %rcx
 14041                             	        adcxq	%rax, %r11
 14042                             	        adoxq	%rdx, %r10
 14043                             	        # A[3] * B[1]
 14044                             	        movq	8(%rbp), %rdx
 14045                             	        adoxq	%rcx, %r11
 14046                             	        mulxq	24(%rdi), %rcx, %rax
 14047                             	        adcxq	%rcx, %r12
 14048                             	        # A[2] * B[2]
 14049                             	        movq	16(%rbp), %rdx
 14050                             	        mulxq	16(%rdi), %rdx, %rcx
 14051                             	        adcxq	%rax, %r13
 14052                             	        adoxq	%rdx, %r12
 14053                             	        # A[3] * B[3]
 14054                             	        movq	24(%rbp), %rdx
 14055                             	        adoxq	%rcx, %r13
 14056                             	        mulxq	24(%rdi), %rcx, %rax
 14057                             	        adoxq	%r15, %r14
 14058                             	        adcxq	%rcx, %r14
 14059                             	        # A[0] * B[3]
 14060                             	        mulxq	(%rdi), %rdx, %rcx
 14061                             	        adcxq	%rax, %r15
 14062                             	        xorq	%rax, %rax
 14063                             	        adcxq	%rdx, %r11
 14064                             	        # A[3] * B[0]
 14065                             	        movq	(%rbp), %rdx
 14066                             	        adcxq	%rcx, %r12
 14067                             	        mulxq	24(%rdi), %rdx, %rcx
 14068                             	        adoxq	%rdx, %r11
 14069                             	        adoxq	%rcx, %r12
 14070                             	        # A[2] * B[3]
 14071                             	        movq	24(%rbp), %rdx
 14072                             	        mulxq	16(%rdi), %rdx, %rcx
 14073                             	        adcxq	%rdx, %r13
 14074                             	        # A[3] * B[2]
 14075                             	        movq	16(%rbp), %rdx
 14076                             	        adcxq	%rcx, %r14
 14077                             	        mulxq	24(%rdi), %rcx, %rdx
 14078                             	        adcxq	%rax, %r15
 14079                             	        adoxq	%rcx, %r13
 14080                             	        adoxq	%rdx, %r14
 14081                             	        adoxq	%rax, %r15
 14082                             	        # Reduce
 14083                             	        movq	$0x7fffffffffffffff, %rax
 14084                             	        #  Move top half into t4-t7 and remove top bit from t3
 14085                             	        shldq	$0x01, %r14, %r15
 14086                             	        shldq	$0x01, %r13, %r14
 14087                             	        shldq	$0x01, %r12, %r13
 14088                             	        shldq	$0x01, %r11, %r12
 14089                             	        andq	%rax, %r11
 14090                             	        #  Multiply top half by 19
 14091                             	        movq	$19, %rdx
 14092                             	        xorq	%rax, %rax
 14093                             	        mulxq	%r12, %rcx, %r12
 14094                             	        adcxq	%rcx, %r8
 14095                             	        adoxq	%r12, %r9
 14096                             	        mulxq	%r13, %rcx, %r13
 14097                             	        adcxq	%rcx, %r9
 14098                             	        adoxq	%r13, %r10
 14099                             	        mulxq	%r14, %rcx, %r14
 14100                             	        adcxq	%rcx, %r10
 14101                             	        adoxq	%r14, %r11
 14102                             	        mulxq	%r15, %r15, %rdx
 14103                             	        adcxq	%r15, %r11
 14104                             	        adoxq	%rax, %rdx
 14105                             	        adcxq	%rax, %rdx
 14106                             	        #  Overflow
 14107                             	        shldq	$0x01, %r11, %rdx
 14108                             	        movq	$0x7fffffffffffffff, %rax
 14109                             	        imulq	$19, %rdx, %rcx
 14110                             	        andq	%rax, %r11
 14111                             	        addq	%rcx, %r8
 14112                             	        adcq	$0x00, %r9
 14113                             	        adcq	$0x00, %r10
 14114                             	        adcq	$0x00, %r11
 14115                             	        # Reduce if top bit set
 14116                             	        movq	%r11, %rdx
 14117                             	        sarq	$63, %rdx
 14118                             	        andq	$19, %rdx
 14119                             	        andq	%rax, %r11
 14120                             	        addq	%rdx, %r8
 14121                             	        adcq	$0x00, %r9
 14122                             	        adcq	$0x00, %r10
 14123                             	        adcq	$0x00, %r11
 14124                             	        # Store
 14125                             	        movq	%r8, (%rbx)
 14126                             	        movq	%r9, 8(%rbx)
 14127                             	        movq	%r10, 16(%rbx)
 14128                             	        movq	%r11, 24(%rbx)
 14129                             	        movq	136(%rsp), %rdi
 14130                             	        # Multiply
 14131                             	        # A[0] * B[0]
 14132                             	        movq	(%rdi), %rdx
 14133                             	        mulxq	(%rsi), %r8, %r9
 14134                             	        # A[2] * B[0]
 14135                             	        mulxq	16(%rsi), %r10, %r11
 14136                             	        # A[1] * B[0]
 14137                             	        mulxq	8(%rsi), %rcx, %rax
 14138                             	        xorq	%r15, %r15
 14139                             	        adcxq	%rcx, %r9
 14140                             	        # A[1] * B[3]
 14141                             	        movq	24(%rdi), %rdx
 14142                             	        mulxq	8(%rsi), %r12, %r13
 14143                             	        adcxq	%rax, %r10
 14144                             	        # A[0] * B[1]
 14145                             	        movq	8(%rdi), %rdx
 14146                             	        mulxq	(%rsi), %rcx, %rax
 14147                             	        adoxq	%rcx, %r9
 14148                             	        # A[2] * B[1]
 14149                             	        mulxq	16(%rsi), %rcx, %r14
 14150                             	        adoxq	%rax, %r10
 14151                             	        adcxq	%rcx, %r11
 14152                             	        # A[1] * B[2]
 14153                             	        movq	16(%rdi), %rdx
 14154                             	        mulxq	8(%rsi), %rcx, %rax
 14155                             	        adcxq	%r14, %r12
 14156                             	        adoxq	%rcx, %r11
 14157                             	        adcxq	%r15, %r13
 14158                             	        adoxq	%rax, %r12
 14159                             	        # A[0] * B[2]
 14160                             	        mulxq	(%rsi), %rcx, %rax
 14161                             	        adoxq	%r15, %r13
 14162                             	        xorq	%r14, %r14
 14163                             	        adcxq	%rcx, %r10
 14164                             	        # A[1] * B[1]
 14165                             	        movq	8(%rdi), %rdx
 14166                             	        mulxq	8(%rsi), %rdx, %rcx
 14167                             	        adcxq	%rax, %r11
 14168                             	        adoxq	%rdx, %r10
 14169                             	        # A[3] * B[1]
 14170                             	        movq	8(%rdi), %rdx
 14171                             	        adoxq	%rcx, %r11
 14172                             	        mulxq	24(%rsi), %rcx, %rax
 14173                             	        adcxq	%rcx, %r12
 14174                             	        # A[2] * B[2]
 14175                             	        movq	16(%rdi), %rdx
 14176                             	        mulxq	16(%rsi), %rdx, %rcx
 14177                             	        adcxq	%rax, %r13
 14178                             	        adoxq	%rdx, %r12
 14179                             	        # A[3] * B[3]
 14180                             	        movq	24(%rdi), %rdx
 14181                             	        adoxq	%rcx, %r13
 14182                             	        mulxq	24(%rsi), %rcx, %rax
 14183                             	        adoxq	%r15, %r14
 14184                             	        adcxq	%rcx, %r14
 14185                             	        # A[0] * B[3]
 14186                             	        mulxq	(%rsi), %rdx, %rcx
 14187                             	        adcxq	%rax, %r15
 14188                             	        xorq	%rax, %rax
 14189                             	        adcxq	%rdx, %r11
 14190                             	        # A[3] * B[0]
 14191                             	        movq	(%rdi), %rdx
 14192                             	        adcxq	%rcx, %r12
 14193                             	        mulxq	24(%rsi), %rdx, %rcx
 14194                             	        adoxq	%rdx, %r11
 14195                             	        adoxq	%rcx, %r12
 14196                             	        # A[2] * B[3]
 14197                             	        movq	24(%rdi), %rdx
 14198                             	        mulxq	16(%rsi), %rdx, %rcx
 14199                             	        adcxq	%rdx, %r13
 14200                             	        # A[3] * B[2]
 14201                             	        movq	16(%rdi), %rdx
 14202                             	        adcxq	%rcx, %r14
 14203                             	        mulxq	24(%rsi), %rcx, %rdx
 14204                             	        adcxq	%rax, %r15
 14205                             	        adoxq	%rcx, %r13
 14206                             	        adoxq	%rdx, %r14
 14207                             	        adoxq	%rax, %r15
 14208                             	        # Reduce
 14209                             	        movq	$0x7fffffffffffffff, %rax
 14210                             	        #  Move top half into t4-t7 and remove top bit from t3
 14211                             	        shldq	$0x01, %r14, %r15
 14212                             	        shldq	$0x01, %r13, %r14
 14213                             	        shldq	$0x01, %r12, %r13
 14214                             	        shldq	$0x01, %r11, %r12
 14215                             	        andq	%rax, %r11
 14216                             	        #  Multiply top half by 19
 14217                             	        movq	$19, %rdx
 14218                             	        xorq	%rax, %rax
 14219                             	        mulxq	%r12, %rcx, %r12
 14220                             	        adcxq	%rcx, %r8
 14221                             	        adoxq	%r12, %r9
 14222                             	        mulxq	%r13, %rcx, %r13
 14223                             	        adcxq	%rcx, %r9
 14224                             	        adoxq	%r13, %r10
 14225                             	        mulxq	%r14, %rcx, %r14
 14226                             	        adcxq	%rcx, %r10
 14227                             	        adoxq	%r14, %r11
 14228                             	        mulxq	%r15, %r15, %rdx
 14229                             	        adcxq	%r15, %r11
 14230                             	        adoxq	%rax, %rdx
 14231                             	        adcxq	%rax, %rdx
 14232                             	        #  Overflow
 14233                             	        shldq	$0x01, %r11, %rdx
 14234                             	        movq	$0x7fffffffffffffff, %rax
 14235                             	        imulq	$19, %rdx, %rcx
 14236                             	        andq	%rax, %r11
 14237                             	        addq	%rcx, %r8
 14238                             	        adcq	$0x00, %r9
 14239                             	        adcq	$0x00, %r10
 14240                             	        adcq	$0x00, %r11
 14241                             	        # Reduce if top bit set
 14242                             	        movq	%r11, %rdx
 14243                             	        sarq	$63, %rdx
 14244                             	        andq	$19, %rdx
 14245                             	        andq	%rax, %r11
 14246                             	        addq	%rdx, %r8
 14247                             	        adcq	$0x00, %r9
 14248                             	        adcq	$0x00, %r10
 14249                             	        adcq	$0x00, %r11
 14250                             	        # Store
 14251                             	        movq	%r8, (%rsi)
 14252                             	        movq	%r9, 8(%rsi)
 14253                             	        movq	%r10, 16(%rsi)
 14254                             	        movq	%r11, 24(%rsi)
 14255                             	        movq	24(%rsp), %rdi
 14256                             	        movq	120(%rsp), %rsi
 14257                             	        movq	112(%rsp), %rbp
 14258                             	        # Multiply
 14259                             	        # A[0] * B[0]
 14260                             	        movq	(%rbp), %rdx
 14261                             	        mulxq	(%rsi), %r8, %r9
 14262                             	        # A[2] * B[0]
 14263                             	        mulxq	16(%rsi), %r10, %r11
 14264                             	        # A[1] * B[0]
 14265                             	        mulxq	8(%rsi), %rcx, %rax
 14266                             	        xorq	%r15, %r15
 14267                             	        adcxq	%rcx, %r9
 14268                             	        # A[1] * B[3]
 14269                             	        movq	24(%rbp), %rdx
 14270                             	        mulxq	8(%rsi), %r12, %r13
 14271                             	        adcxq	%rax, %r10
 14272                             	        # A[0] * B[1]
 14273                             	        movq	8(%rbp), %rdx
 14274                             	        mulxq	(%rsi), %rcx, %rax
 14275                             	        adoxq	%rcx, %r9
 14276                             	        # A[2] * B[1]
 14277                             	        mulxq	16(%rsi), %rcx, %r14
 14278                             	        adoxq	%rax, %r10
 14279                             	        adcxq	%rcx, %r11
 14280                             	        # A[1] * B[2]
 14281                             	        movq	16(%rbp), %rdx
 14282                             	        mulxq	8(%rsi), %rcx, %rax
 14283                             	        adcxq	%r14, %r12
 14284                             	        adoxq	%rcx, %r11
 14285                             	        adcxq	%r15, %r13
 14286                             	        adoxq	%rax, %r12
 14287                             	        # A[0] * B[2]
 14288                             	        mulxq	(%rsi), %rcx, %rax
 14289                             	        adoxq	%r15, %r13
 14290                             	        xorq	%r14, %r14
 14291                             	        adcxq	%rcx, %r10
 14292                             	        # A[1] * B[1]
 14293                             	        movq	8(%rbp), %rdx
 14294                             	        mulxq	8(%rsi), %rdx, %rcx
 14295                             	        adcxq	%rax, %r11
 14296                             	        adoxq	%rdx, %r10
 14297                             	        # A[3] * B[1]
 14298                             	        movq	8(%rbp), %rdx
 14299                             	        adoxq	%rcx, %r11
 14300                             	        mulxq	24(%rsi), %rcx, %rax
 14301                             	        adcxq	%rcx, %r12
 14302                             	        # A[2] * B[2]
 14303                             	        movq	16(%rbp), %rdx
 14304                             	        mulxq	16(%rsi), %rdx, %rcx
 14305                             	        adcxq	%rax, %r13
 14306                             	        adoxq	%rdx, %r12
 14307                             	        # A[3] * B[3]
 14308                             	        movq	24(%rbp), %rdx
 14309                             	        adoxq	%rcx, %r13
 14310                             	        mulxq	24(%rsi), %rcx, %rax
 14311                             	        adoxq	%r15, %r14
 14312                             	        adcxq	%rcx, %r14
 14313                             	        # A[0] * B[3]
 14314                             	        mulxq	(%rsi), %rdx, %rcx
 14315                             	        adcxq	%rax, %r15
 14316                             	        xorq	%rax, %rax
 14317                             	        adcxq	%rdx, %r11
 14318                             	        # A[3] * B[0]
 14319                             	        movq	(%rbp), %rdx
 14320                             	        adcxq	%rcx, %r12
 14321                             	        mulxq	24(%rsi), %rdx, %rcx
 14322                             	        adoxq	%rdx, %r11
 14323                             	        adoxq	%rcx, %r12
 14324                             	        # A[2] * B[3]
 14325                             	        movq	24(%rbp), %rdx
 14326                             	        mulxq	16(%rsi), %rdx, %rcx
 14327                             	        adcxq	%rdx, %r13
 14328                             	        # A[3] * B[2]
 14329                             	        movq	16(%rbp), %rdx
 14330                             	        adcxq	%rcx, %r14
 14331                             	        mulxq	24(%rsi), %rcx, %rdx
 14332                             	        adcxq	%rax, %r15
 14333                             	        adoxq	%rcx, %r13
 14334                             	        adoxq	%rdx, %r14
 14335                             	        adoxq	%rax, %r15
 14336                             	        # Reduce
 14337                             	        movq	$0x7fffffffffffffff, %rax
 14338                             	        #  Move top half into t4-t7 and remove top bit from t3
 14339                             	        shldq	$0x01, %r14, %r15
 14340                             	        shldq	$0x01, %r13, %r14
 14341                             	        shldq	$0x01, %r12, %r13
 14342                             	        shldq	$0x01, %r11, %r12
 14343                             	        andq	%rax, %r11
 14344                             	        #  Multiply top half by 19
 14345                             	        movq	$19, %rdx
 14346                             	        xorq	%rax, %rax
 14347                             	        mulxq	%r12, %rcx, %r12
 14348                             	        adcxq	%rcx, %r8
 14349                             	        adoxq	%r12, %r9
 14350                             	        mulxq	%r13, %rcx, %r13
 14351                             	        adcxq	%rcx, %r9
 14352                             	        adoxq	%r13, %r10
 14353                             	        mulxq	%r14, %rcx, %r14
 14354                             	        adcxq	%rcx, %r10
 14355                             	        adoxq	%r14, %r11
 14356                             	        mulxq	%r15, %r15, %rdx
 14357                             	        adcxq	%r15, %r11
 14358                             	        adoxq	%rax, %rdx
 14359                             	        adcxq	%rax, %rdx
 14360                             	        #  Overflow
 14361                             	        shldq	$0x01, %r11, %rdx
 14362                             	        movq	$0x7fffffffffffffff, %rax
 14363                             	        imulq	$19, %rdx, %rcx
 14364                             	        andq	%rax, %r11
 14365                             	        addq	%rcx, %r8
 14366                             	        adcq	$0x00, %r9
 14367                             	        adcq	$0x00, %r10
 14368                             	        adcq	$0x00, %r11
 14369                             	        # Reduce if top bit set
 14370                             	        movq	%r11, %rdx
 14371                             	        sarq	$63, %rdx
 14372                             	        andq	$19, %rdx
 14373                             	        andq	%rax, %r11
 14374                             	        addq	%rdx, %r8
 14375                             	        adcq	$0x00, %r9
 14376                             	        adcq	$0x00, %r10
 14377                             	        adcq	$0x00, %r11
 14378                             	        # Store
 14379                             	        movq	%r8, (%rdi)
 14380                             	        movq	%r9, 8(%rdi)
 14381                             	        movq	%r10, 16(%rdi)
 14382                             	        movq	%r11, 24(%rdi)
 14383                             	        movq	8(%rsp), %rdi
 14384                             	        movq	(%rsp), %rsi
 14385                             	        # Add
 14386                             	        movq	(%rbx), %r8
 14387                             	        movq	8(%rbx), %r9
 14388                             	        movq	16(%rbx), %r10
 14389                             	        movq	24(%rbx), %rdx
 14390                             	        movq	%r8, %r12
 14391                             	        addq	(%rdi), %r8
 14392                             	        movq	%r9, %r13
 14393                             	        adcq	8(%rdi), %r9
 14394                             	        movq	%r10, %r14
 14395                             	        adcq	16(%rdi), %r10
 14396                             	        movq	%rdx, %r15
 14397                             	        adcq	24(%rdi), %rdx
 14398                             	        movq	$-19, %rcx
 14399                             	        movq	%rdx, %r11
 14400                             	        movq	$0x7fffffffffffffff, %rax
 14401                             	        sarq	$63, %rdx
 14402                             	        #   Mask the modulus
 14403                             	        andq	%rdx, %rcx
 14404                             	        andq	%rdx, %rax
 14405                             	        #   Sub modulus (if overflow)
 14406                             	        subq	%rcx, %r8
 14407                             	        sbbq	%rdx, %r9
 14408                             	        sbbq	%rdx, %r10
 14409                             	        sbbq	%rax, %r11
 14410                             	        # Sub
 14411                             	        subq	(%rdi), %r12
 14412                             	        movq	$0x00, %rdx
 14413                             	        sbbq	8(%rdi), %r13
 14414                             	        movq	$-19, %rcx
 14415                             	        sbbq	16(%rdi), %r14
 14416                             	        movq	$0x7fffffffffffffff, %rax
 14417                             	        sbbq	24(%rdi), %r15
 14418                             	        sbbq	$0x00, %rdx
 14419                             	        #   Mask the modulus
 14420                             	        andq	%rdx, %rcx
 14421                             	        andq	%rdx, %rax
 14422                             	        #   Add modulus (if underflow)
 14423                             	        addq	%rcx, %r12
 14424                             	        adcq	%rdx, %r13
 14425                             	        adcq	%rdx, %r14
 14426                             	        adcq	%rax, %r15
 14427                             	        movq	%r8, (%rdi)
 14428                             	        movq	%r9, 8(%rdi)
 14429                             	        movq	%r10, 16(%rdi)
 14430                             	        movq	%r11, 24(%rdi)
 14431                             	        movq	%r12, (%rsi)
 14432                             	        movq	%r13, 8(%rsi)
 14433                             	        movq	%r14, 16(%rsi)
 14434                             	        movq	%r15, 24(%rsi)
 14435                             	        movq	104(%rsp), %rdi
 14436                             	        # Double
 14437                             	        movq	(%rdi), %r8
 14438                             	        movq	8(%rdi), %r9
 14439                             	        addq	%r8, %r8
 14440                             	        movq	16(%rdi), %r10
 14441                             	        adcq	%r9, %r9
 14442                             	        movq	24(%rdi), %rdx
 14443                             	        adcq	%r10, %r10
 14444                             	        movq	$-19, %rcx
 14445                             	        adcq	%rdx, %rdx
 14446                             	        movq	$0x7fffffffffffffff, %rax
 14447                             	        movq	%rdx, %r11
 14448                             	        sarq	$63, %rdx
 14449                             	        #   Mask the modulus
 14450                             	        andq	%rdx, %rcx
 14451                             	        andq	%rdx, %rax
 14452                             	        #   Sub modulus (if overflow)
 14453                             	        subq	%rcx, %r8
 14454                             	        sbbq	%rdx, %r9
 14455                             	        sbbq	%rdx, %r10
 14456                             	        sbbq	%rax, %r11
 14457                             	        movq	%r8, (%rbx)
 14458                             	        movq	%r9, 8(%rbx)
 14459                             	        movq	%r10, 16(%rbx)
 14460                             	        movq	%r11, 24(%rbx)
 14461                             	        movq	24(%rsp), %rdi
 14462                             	        # Add
 14463                             	        movq	(%rbx), %r8
 14464                             	        movq	8(%rbx), %r9
 14465                             	        movq	16(%rbx), %r10
 14466                             	        movq	24(%rbx), %rdx
 14467                             	        movq	%r8, %r12
 14468                             	        addq	(%rdi), %r8
 14469                             	        movq	%r9, %r13
 14470                             	        adcq	8(%rdi), %r9
 14471                             	        movq	%r10, %r14
 14472                             	        adcq	16(%rdi), %r10
 14473                             	        movq	%rdx, %r15
 14474                             	        adcq	24(%rdi), %rdx
 14475                             	        movq	$-19, %rcx
 14476                             	        movq	%rdx, %r11
 14477                             	        movq	$0x7fffffffffffffff, %rax
 14478                             	        sarq	$63, %rdx
 14479                             	        #   Mask the modulus
 14480                             	        andq	%rdx, %rcx
 14481                             	        andq	%rdx, %rax
 14482                             	        #   Sub modulus (if overflow)
 14483                             	        subq	%rcx, %r8
 14484                             	        sbbq	%rdx, %r9
 14485                             	        sbbq	%rdx, %r10
 14486                             	        sbbq	%rax, %r11
 14487                             	        # Sub
 14488                             	        subq	(%rdi), %r12
 14489                             	        movq	$0x00, %rdx
 14490                             	        sbbq	8(%rdi), %r13
 14491                             	        movq	$-19, %rcx
 14492                             	        sbbq	16(%rdi), %r14
 14493                             	        movq	$0x7fffffffffffffff, %rax
 14494                             	        sbbq	24(%rdi), %r15
 14495                             	        sbbq	$0x00, %rdx
 14496                             	        #   Mask the modulus
 14497                             	        andq	%rdx, %rcx
 14498                             	        andq	%rdx, %rax
 14499                             	        #   Add modulus (if underflow)
 14500                             	        addq	%rcx, %r12
 14501                             	        adcq	%rdx, %r13
 14502                             	        adcq	%rdx, %r14
 14503                             	        adcq	%rax, %r15
 14504                             	        movq	%r8, (%rbx)
 14505                             	        movq	%r9, 8(%rbx)
 14506                             	        movq	%r10, 16(%rbx)
 14507                             	        movq	%r11, 24(%rbx)
 14508                             	        movq	%r12, (%rdi)
 14509                             	        movq	%r13, 8(%rdi)
 14510                             	        movq	%r14, 16(%rdi)
 14511                             	        movq	%r15, 24(%rdi)
 14512                             	        addq	$48, %rsp
 14513                             	        popq	%r15
 14514                             	        popq	%r14
 14515                             	        popq	%r13
 14516                             	        popq	%r12
 14517                             	        popq	%rbx
 14518                             	        popq	%rbp
 14519                             	        repz retq
 14520                             	#ifndef __APPLE__
 14522                             	#endif /* __APPLE__ */
 14523                             	#ifndef __APPLE__
 14524                             	.text
 14525                             	.globl	fe_ge_msub_avx2
 14527                             	.align	16
 14528                             	fe_ge_msub_avx2:
 14529                             	#else
 14530                             	.section	__TEXT,__text
 14531                             	.globl	_fe_ge_msub_avx2
 14532                             	.p2align	4
 14533                             	_fe_ge_msub_avx2:
 14534                             	#endif /* __APPLE__ */
 14535                             	        pushq	%rbp
 14536                             	        pushq	%rbx
 14537                             	        pushq	%r12
 14538                             	        pushq	%r13
 14539                             	        pushq	%r14
 14540                             	        pushq	%r15
 14541                             	        subq	$48, %rsp
 14542                             	        movq	%rdi, (%rsp)
 14543                             	        movq	%rsi, 8(%rsp)
 14544                             	        movq	%rdx, 16(%rsp)
 14545                             	        movq	%rcx, 24(%rsp)
 14546                             	        movq	%r8, 32(%rsp)
 14547                             	        movq	%r9, 40(%rsp)
 14548                             	        movq	8(%rsp), %rsi
 14549                             	        movq	40(%rsp), %rbx
 14550                             	        movq	32(%rsp), %rbp
 14551                             	        # Add
 14552                             	        movq	(%rbx), %r8
 14553                             	        movq	8(%rbx), %r9
 14554                             	        movq	16(%rbx), %r10
 14555                             	        movq	24(%rbx), %rdx
 14556                             	        movq	%r8, %r12
 14557                             	        addq	(%rbp), %r8
 14558                             	        movq	%r9, %r13
 14559                             	        adcq	8(%rbp), %r9
 14560                             	        movq	%r10, %r14
 14561                             	        adcq	16(%rbp), %r10
 14562                             	        movq	%rdx, %r15
 14563                             	        adcq	24(%rbp), %rdx
 14564                             	        movq	$-19, %rcx
 14565                             	        movq	%rdx, %r11
 14566                             	        movq	$0x7fffffffffffffff, %rax
 14567                             	        sarq	$63, %rdx
 14568                             	        #   Mask the modulus
 14569                             	        andq	%rdx, %rcx
 14570                             	        andq	%rdx, %rax
 14571                             	        #   Sub modulus (if overflow)
 14572                             	        subq	%rcx, %r8
 14573                             	        sbbq	%rdx, %r9
 14574                             	        sbbq	%rdx, %r10
 14575                             	        sbbq	%rax, %r11
 14576                             	        # Sub
 14577                             	        subq	(%rbp), %r12
 14578                             	        movq	$0x00, %rdx
 14579                             	        sbbq	8(%rbp), %r13
 14580                             	        movq	$-19, %rcx
 14581                             	        sbbq	16(%rbp), %r14
 14582                             	        movq	$0x7fffffffffffffff, %rax
 14583                             	        sbbq	24(%rbp), %r15
 14584                             	        sbbq	$0x00, %rdx
 14585                             	        #   Mask the modulus
 14586                             	        andq	%rdx, %rcx
 14587                             	        andq	%rdx, %rax
 14588                             	        #   Add modulus (if underflow)
 14589                             	        addq	%rcx, %r12
 14590                             	        adcq	%rdx, %r13
 14591                             	        adcq	%rdx, %r14
 14592                             	        adcq	%rax, %r15
 14593                             	        movq	%r8, (%rdi)
 14594                             	        movq	%r9, 8(%rdi)
 14595                             	        movq	%r10, 16(%rdi)
 14596                             	        movq	%r11, 24(%rdi)
 14597                             	        movq	%r12, (%rsi)
 14598                             	        movq	%r13, 8(%rsi)
 14599                             	        movq	%r14, 16(%rsi)
 14600                             	        movq	%r15, 24(%rsi)
 14601                             	        movq	16(%rsp), %rbx
 14602                             	        movq	136(%rsp), %rbp
 14603                             	        # Multiply
 14604                             	        # A[0] * B[0]
 14605                             	        movq	(%rbp), %rdx
 14606                             	        mulxq	(%rdi), %r8, %r9
 14607                             	        # A[2] * B[0]
 14608                             	        mulxq	16(%rdi), %r10, %r11
 14609                             	        # A[1] * B[0]
 14610                             	        mulxq	8(%rdi), %rcx, %rax
 14611                             	        xorq	%r15, %r15
 14612                             	        adcxq	%rcx, %r9
 14613                             	        # A[1] * B[3]
 14614                             	        movq	24(%rbp), %rdx
 14615                             	        mulxq	8(%rdi), %r12, %r13
 14616                             	        adcxq	%rax, %r10
 14617                             	        # A[0] * B[1]
 14618                             	        movq	8(%rbp), %rdx
 14619                             	        mulxq	(%rdi), %rcx, %rax
 14620                             	        adoxq	%rcx, %r9
 14621                             	        # A[2] * B[1]
 14622                             	        mulxq	16(%rdi), %rcx, %r14
 14623                             	        adoxq	%rax, %r10
 14624                             	        adcxq	%rcx, %r11
 14625                             	        # A[1] * B[2]
 14626                             	        movq	16(%rbp), %rdx
 14627                             	        mulxq	8(%rdi), %rcx, %rax
 14628                             	        adcxq	%r14, %r12
 14629                             	        adoxq	%rcx, %r11
 14630                             	        adcxq	%r15, %r13
 14631                             	        adoxq	%rax, %r12
 14632                             	        # A[0] * B[2]
 14633                             	        mulxq	(%rdi), %rcx, %rax
 14634                             	        adoxq	%r15, %r13
 14635                             	        xorq	%r14, %r14
 14636                             	        adcxq	%rcx, %r10
 14637                             	        # A[1] * B[1]
 14638                             	        movq	8(%rbp), %rdx
 14639                             	        mulxq	8(%rdi), %rdx, %rcx
 14640                             	        adcxq	%rax, %r11
 14641                             	        adoxq	%rdx, %r10
 14642                             	        # A[3] * B[1]
 14643                             	        movq	8(%rbp), %rdx
 14644                             	        adoxq	%rcx, %r11
 14645                             	        mulxq	24(%rdi), %rcx, %rax
 14646                             	        adcxq	%rcx, %r12
 14647                             	        # A[2] * B[2]
 14648                             	        movq	16(%rbp), %rdx
 14649                             	        mulxq	16(%rdi), %rdx, %rcx
 14650                             	        adcxq	%rax, %r13
 14651                             	        adoxq	%rdx, %r12
 14652                             	        # A[3] * B[3]
 14653                             	        movq	24(%rbp), %rdx
 14654                             	        adoxq	%rcx, %r13
 14655                             	        mulxq	24(%rdi), %rcx, %rax
 14656                             	        adoxq	%r15, %r14
 14657                             	        adcxq	%rcx, %r14
 14658                             	        # A[0] * B[3]
 14659                             	        mulxq	(%rdi), %rdx, %rcx
 14660                             	        adcxq	%rax, %r15
 14661                             	        xorq	%rax, %rax
 14662                             	        adcxq	%rdx, %r11
 14663                             	        # A[3] * B[0]
 14664                             	        movq	(%rbp), %rdx
 14665                             	        adcxq	%rcx, %r12
 14666                             	        mulxq	24(%rdi), %rdx, %rcx
 14667                             	        adoxq	%rdx, %r11
 14668                             	        adoxq	%rcx, %r12
 14669                             	        # A[2] * B[3]
 14670                             	        movq	24(%rbp), %rdx
 14671                             	        mulxq	16(%rdi), %rdx, %rcx
 14672                             	        adcxq	%rdx, %r13
 14673                             	        # A[3] * B[2]
 14674                             	        movq	16(%rbp), %rdx
 14675                             	        adcxq	%rcx, %r14
 14676                             	        mulxq	24(%rdi), %rcx, %rdx
 14677                             	        adcxq	%rax, %r15
 14678                             	        adoxq	%rcx, %r13
 14679                             	        adoxq	%rdx, %r14
 14680                             	        adoxq	%rax, %r15
 14681                             	        # Reduce
 14682                             	        movq	$0x7fffffffffffffff, %rax
 14683                             	        #  Move top half into t4-t7 and remove top bit from t3
 14684                             	        shldq	$0x01, %r14, %r15
 14685                             	        shldq	$0x01, %r13, %r14
 14686                             	        shldq	$0x01, %r12, %r13
 14687                             	        shldq	$0x01, %r11, %r12
 14688                             	        andq	%rax, %r11
 14689                             	        #  Multiply top half by 19
 14690                             	        movq	$19, %rdx
 14691                             	        xorq	%rax, %rax
 14692                             	        mulxq	%r12, %rcx, %r12
 14693                             	        adcxq	%rcx, %r8
 14694                             	        adoxq	%r12, %r9
 14695                             	        mulxq	%r13, %rcx, %r13
 14696                             	        adcxq	%rcx, %r9
 14697                             	        adoxq	%r13, %r10
 14698                             	        mulxq	%r14, %rcx, %r14
 14699                             	        adcxq	%rcx, %r10
 14700                             	        adoxq	%r14, %r11
 14701                             	        mulxq	%r15, %r15, %rdx
 14702                             	        adcxq	%r15, %r11
 14703                             	        adoxq	%rax, %rdx
 14704                             	        adcxq	%rax, %rdx
 14705                             	        #  Overflow
 14706                             	        shldq	$0x01, %r11, %rdx
 14707                             	        movq	$0x7fffffffffffffff, %rax
 14708                             	        imulq	$19, %rdx, %rcx
 14709                             	        andq	%rax, %r11
 14710                             	        addq	%rcx, %r8
 14711                             	        adcq	$0x00, %r9
 14712                             	        adcq	$0x00, %r10
 14713                             	        adcq	$0x00, %r11
 14714                             	        # Reduce if top bit set
 14715                             	        movq	%r11, %rdx
 14716                             	        sarq	$63, %rdx
 14717                             	        andq	$19, %rdx
 14718                             	        andq	%rax, %r11
 14719                             	        addq	%rdx, %r8
 14720                             	        adcq	$0x00, %r9
 14721                             	        adcq	$0x00, %r10
 14722                             	        adcq	$0x00, %r11
 14723                             	        # Store
 14724                             	        movq	%r8, (%rbx)
 14725                             	        movq	%r9, 8(%rbx)
 14726                             	        movq	%r10, 16(%rbx)
 14727                             	        movq	%r11, 24(%rbx)
 14728                             	        movq	128(%rsp), %rdi
 14729                             	        # Multiply
 14730                             	        # A[0] * B[0]
 14731                             	        movq	(%rdi), %rdx
 14732                             	        mulxq	(%rsi), %r8, %r9
 14733                             	        # A[2] * B[0]
 14734                             	        mulxq	16(%rsi), %r10, %r11
 14735                             	        # A[1] * B[0]
 14736                             	        mulxq	8(%rsi), %rcx, %rax
 14737                             	        xorq	%r15, %r15
 14738                             	        adcxq	%rcx, %r9
 14739                             	        # A[1] * B[3]
 14740                             	        movq	24(%rdi), %rdx
 14741                             	        mulxq	8(%rsi), %r12, %r13
 14742                             	        adcxq	%rax, %r10
 14743                             	        # A[0] * B[1]
 14744                             	        movq	8(%rdi), %rdx
 14745                             	        mulxq	(%rsi), %rcx, %rax
 14746                             	        adoxq	%rcx, %r9
 14747                             	        # A[2] * B[1]
 14748                             	        mulxq	16(%rsi), %rcx, %r14
 14749                             	        adoxq	%rax, %r10
 14750                             	        adcxq	%rcx, %r11
 14751                             	        # A[1] * B[2]
 14752                             	        movq	16(%rdi), %rdx
 14753                             	        mulxq	8(%rsi), %rcx, %rax
 14754                             	        adcxq	%r14, %r12
 14755                             	        adoxq	%rcx, %r11
 14756                             	        adcxq	%r15, %r13
 14757                             	        adoxq	%rax, %r12
 14758                             	        # A[0] * B[2]
 14759                             	        mulxq	(%rsi), %rcx, %rax
 14760                             	        adoxq	%r15, %r13
 14761                             	        xorq	%r14, %r14
 14762                             	        adcxq	%rcx, %r10
 14763                             	        # A[1] * B[1]
 14764                             	        movq	8(%rdi), %rdx
 14765                             	        mulxq	8(%rsi), %rdx, %rcx
 14766                             	        adcxq	%rax, %r11
 14767                             	        adoxq	%rdx, %r10
 14768                             	        # A[3] * B[1]
 14769                             	        movq	8(%rdi), %rdx
 14770                             	        adoxq	%rcx, %r11
 14771                             	        mulxq	24(%rsi), %rcx, %rax
 14772                             	        adcxq	%rcx, %r12
 14773                             	        # A[2] * B[2]
 14774                             	        movq	16(%rdi), %rdx
 14775                             	        mulxq	16(%rsi), %rdx, %rcx
 14776                             	        adcxq	%rax, %r13
 14777                             	        adoxq	%rdx, %r12
 14778                             	        # A[3] * B[3]
 14779                             	        movq	24(%rdi), %rdx
 14780                             	        adoxq	%rcx, %r13
 14781                             	        mulxq	24(%rsi), %rcx, %rax
 14782                             	        adoxq	%r15, %r14
 14783                             	        adcxq	%rcx, %r14
 14784                             	        # A[0] * B[3]
 14785                             	        mulxq	(%rsi), %rdx, %rcx
 14786                             	        adcxq	%rax, %r15
 14787                             	        xorq	%rax, %rax
 14788                             	        adcxq	%rdx, %r11
 14789                             	        # A[3] * B[0]
 14790                             	        movq	(%rdi), %rdx
 14791                             	        adcxq	%rcx, %r12
 14792                             	        mulxq	24(%rsi), %rdx, %rcx
 14793                             	        adoxq	%rdx, %r11
 14794                             	        adoxq	%rcx, %r12
 14795                             	        # A[2] * B[3]
 14796                             	        movq	24(%rdi), %rdx
 14797                             	        mulxq	16(%rsi), %rdx, %rcx
 14798                             	        adcxq	%rdx, %r13
 14799                             	        # A[3] * B[2]
 14800                             	        movq	16(%rdi), %rdx
 14801                             	        adcxq	%rcx, %r14
 14802                             	        mulxq	24(%rsi), %rcx, %rdx
 14803                             	        adcxq	%rax, %r15
 14804                             	        adoxq	%rcx, %r13
 14805                             	        adoxq	%rdx, %r14
 14806                             	        adoxq	%rax, %r15
 14807                             	        # Reduce
 14808                             	        movq	$0x7fffffffffffffff, %rax
 14809                             	        #  Move top half into t4-t7 and remove top bit from t3
 14810                             	        shldq	$0x01, %r14, %r15
 14811                             	        shldq	$0x01, %r13, %r14
 14812                             	        shldq	$0x01, %r12, %r13
 14813                             	        shldq	$0x01, %r11, %r12
 14814                             	        andq	%rax, %r11
 14815                             	        #  Multiply top half by 19
 14816                             	        movq	$19, %rdx
 14817                             	        xorq	%rax, %rax
 14818                             	        mulxq	%r12, %rcx, %r12
 14819                             	        adcxq	%rcx, %r8
 14820                             	        adoxq	%r12, %r9
 14821                             	        mulxq	%r13, %rcx, %r13
 14822                             	        adcxq	%rcx, %r9
 14823                             	        adoxq	%r13, %r10
 14824                             	        mulxq	%r14, %rcx, %r14
 14825                             	        adcxq	%rcx, %r10
 14826                             	        adoxq	%r14, %r11
 14827                             	        mulxq	%r15, %r15, %rdx
 14828                             	        adcxq	%r15, %r11
 14829                             	        adoxq	%rax, %rdx
 14830                             	        adcxq	%rax, %rdx
 14831                             	        #  Overflow
 14832                             	        shldq	$0x01, %r11, %rdx
 14833                             	        movq	$0x7fffffffffffffff, %rax
 14834                             	        imulq	$19, %rdx, %rcx
 14835                             	        andq	%rax, %r11
 14836                             	        addq	%rcx, %r8
 14837                             	        adcq	$0x00, %r9
 14838                             	        adcq	$0x00, %r10
 14839                             	        adcq	$0x00, %r11
 14840                             	        # Reduce if top bit set
 14841                             	        movq	%r11, %rdx
 14842                             	        sarq	$63, %rdx
 14843                             	        andq	$19, %rdx
 14844                             	        andq	%rax, %r11
 14845                             	        addq	%rdx, %r8
 14846                             	        adcq	$0x00, %r9
 14847                             	        adcq	$0x00, %r10
 14848                             	        adcq	$0x00, %r11
 14849                             	        # Store
 14850                             	        movq	%r8, (%rsi)
 14851                             	        movq	%r9, 8(%rsi)
 14852                             	        movq	%r10, 16(%rsi)
 14853                             	        movq	%r11, 24(%rsi)
 14854                             	        movq	24(%rsp), %rdi
 14855                             	        movq	120(%rsp), %rsi
 14856                             	        movq	112(%rsp), %rbp
 14857                             	        # Multiply
 14858                             	        # A[0] * B[0]
 14859                             	        movq	(%rbp), %rdx
 14860                             	        mulxq	(%rsi), %r8, %r9
 14861                             	        # A[2] * B[0]
 14862                             	        mulxq	16(%rsi), %r10, %r11
 14863                             	        # A[1] * B[0]
 14864                             	        mulxq	8(%rsi), %rcx, %rax
 14865                             	        xorq	%r15, %r15
 14866                             	        adcxq	%rcx, %r9
 14867                             	        # A[1] * B[3]
 14868                             	        movq	24(%rbp), %rdx
 14869                             	        mulxq	8(%rsi), %r12, %r13
 14870                             	        adcxq	%rax, %r10
 14871                             	        # A[0] * B[1]
 14872                             	        movq	8(%rbp), %rdx
 14873                             	        mulxq	(%rsi), %rcx, %rax
 14874                             	        adoxq	%rcx, %r9
 14875                             	        # A[2] * B[1]
 14876                             	        mulxq	16(%rsi), %rcx, %r14
 14877                             	        adoxq	%rax, %r10
 14878                             	        adcxq	%rcx, %r11
 14879                             	        # A[1] * B[2]
 14880                             	        movq	16(%rbp), %rdx
 14881                             	        mulxq	8(%rsi), %rcx, %rax
 14882                             	        adcxq	%r14, %r12
 14883                             	        adoxq	%rcx, %r11
 14884                             	        adcxq	%r15, %r13
 14885                             	        adoxq	%rax, %r12
 14886                             	        # A[0] * B[2]
 14887                             	        mulxq	(%rsi), %rcx, %rax
 14888                             	        adoxq	%r15, %r13
 14889                             	        xorq	%r14, %r14
 14890                             	        adcxq	%rcx, %r10
 14891                             	        # A[1] * B[1]
 14892                             	        movq	8(%rbp), %rdx
 14893                             	        mulxq	8(%rsi), %rdx, %rcx
 14894                             	        adcxq	%rax, %r11
 14895                             	        adoxq	%rdx, %r10
 14896                             	        # A[3] * B[1]
 14897                             	        movq	8(%rbp), %rdx
 14898                             	        adoxq	%rcx, %r11
 14899                             	        mulxq	24(%rsi), %rcx, %rax
 14900                             	        adcxq	%rcx, %r12
 14901                             	        # A[2] * B[2]
 14902                             	        movq	16(%rbp), %rdx
 14903                             	        mulxq	16(%rsi), %rdx, %rcx
 14904                             	        adcxq	%rax, %r13
 14905                             	        adoxq	%rdx, %r12
 14906                             	        # A[3] * B[3]
 14907                             	        movq	24(%rbp), %rdx
 14908                             	        adoxq	%rcx, %r13
 14909                             	        mulxq	24(%rsi), %rcx, %rax
 14910                             	        adoxq	%r15, %r14
 14911                             	        adcxq	%rcx, %r14
 14912                             	        # A[0] * B[3]
 14913                             	        mulxq	(%rsi), %rdx, %rcx
 14914                             	        adcxq	%rax, %r15
 14915                             	        xorq	%rax, %rax
 14916                             	        adcxq	%rdx, %r11
 14917                             	        # A[3] * B[0]
 14918                             	        movq	(%rbp), %rdx
 14919                             	        adcxq	%rcx, %r12
 14920                             	        mulxq	24(%rsi), %rdx, %rcx
 14921                             	        adoxq	%rdx, %r11
 14922                             	        adoxq	%rcx, %r12
 14923                             	        # A[2] * B[3]
 14924                             	        movq	24(%rbp), %rdx
 14925                             	        mulxq	16(%rsi), %rdx, %rcx
 14926                             	        adcxq	%rdx, %r13
 14927                             	        # A[3] * B[2]
 14928                             	        movq	16(%rbp), %rdx
 14929                             	        adcxq	%rcx, %r14
 14930                             	        mulxq	24(%rsi), %rcx, %rdx
 14931                             	        adcxq	%rax, %r15
 14932                             	        adoxq	%rcx, %r13
 14933                             	        adoxq	%rdx, %r14
 14934                             	        adoxq	%rax, %r15
 14935                             	        # Reduce
 14936                             	        movq	$0x7fffffffffffffff, %rax
 14937                             	        #  Move top half into t4-t7 and remove top bit from t3
 14938                             	        shldq	$0x01, %r14, %r15
 14939                             	        shldq	$0x01, %r13, %r14
 14940                             	        shldq	$0x01, %r12, %r13
 14941                             	        shldq	$0x01, %r11, %r12
 14942                             	        andq	%rax, %r11
 14943                             	        #  Multiply top half by 19
 14944                             	        movq	$19, %rdx
 14945                             	        xorq	%rax, %rax
 14946                             	        mulxq	%r12, %rcx, %r12
 14947                             	        adcxq	%rcx, %r8
 14948                             	        adoxq	%r12, %r9
 14949                             	        mulxq	%r13, %rcx, %r13
 14950                             	        adcxq	%rcx, %r9
 14951                             	        adoxq	%r13, %r10
 14952                             	        mulxq	%r14, %rcx, %r14
 14953                             	        adcxq	%rcx, %r10
 14954                             	        adoxq	%r14, %r11
 14955                             	        mulxq	%r15, %r15, %rdx
 14956                             	        adcxq	%r15, %r11
 14957                             	        adoxq	%rax, %rdx
 14958                             	        adcxq	%rax, %rdx
 14959                             	        #  Overflow
 14960                             	        shldq	$0x01, %r11, %rdx
 14961                             	        movq	$0x7fffffffffffffff, %rax
 14962                             	        imulq	$19, %rdx, %rcx
 14963                             	        andq	%rax, %r11
 14964                             	        addq	%rcx, %r8
 14965                             	        adcq	$0x00, %r9
 14966                             	        adcq	$0x00, %r10
 14967                             	        adcq	$0x00, %r11
 14968                             	        # Reduce if top bit set
 14969                             	        movq	%r11, %rdx
 14970                             	        sarq	$63, %rdx
 14971                             	        andq	$19, %rdx
 14972                             	        andq	%rax, %r11
 14973                             	        addq	%rdx, %r8
 14974                             	        adcq	$0x00, %r9
 14975                             	        adcq	$0x00, %r10
 14976                             	        adcq	$0x00, %r11
 14977                             	        # Store
 14978                             	        movq	%r8, (%rdi)
 14979                             	        movq	%r9, 8(%rdi)
 14980                             	        movq	%r10, 16(%rdi)
 14981                             	        movq	%r11, 24(%rdi)
 14982                             	        movq	8(%rsp), %rsi
 14983                             	        movq	(%rsp), %rbp
 14984                             	        # Add
 14985                             	        movq	(%rbx), %r8
 14986                             	        movq	8(%rbx), %r9
 14987                             	        movq	16(%rbx), %r10
 14988                             	        movq	24(%rbx), %rdx
 14989                             	        movq	%r8, %r12
 14990                             	        addq	(%rsi), %r8
 14991                             	        movq	%r9, %r13
 14992                             	        adcq	8(%rsi), %r9
 14993                             	        movq	%r10, %r14
 14994                             	        adcq	16(%rsi), %r10
 14995                             	        movq	%rdx, %r15
 14996                             	        adcq	24(%rsi), %rdx
 14997                             	        movq	$-19, %rcx
 14998                             	        movq	%rdx, %r11
 14999                             	        movq	$0x7fffffffffffffff, %rax
 15000                             	        sarq	$63, %rdx
 15001                             	        #   Mask the modulus
 15002                             	        andq	%rdx, %rcx
 15003                             	        andq	%rdx, %rax
 15004                             	        #   Sub modulus (if overflow)
 15005                             	        subq	%rcx, %r8
 15006                             	        sbbq	%rdx, %r9
 15007                             	        sbbq	%rdx, %r10
 15008                             	        sbbq	%rax, %r11
 15009                             	        # Sub
 15010                             	        subq	(%rsi), %r12
 15011                             	        movq	$0x00, %rdx
 15012                             	        sbbq	8(%rsi), %r13
 15013                             	        movq	$-19, %rcx
 15014                             	        sbbq	16(%rsi), %r14
 15015                             	        movq	$0x7fffffffffffffff, %rax
 15016                             	        sbbq	24(%rsi), %r15
 15017                             	        sbbq	$0x00, %rdx
 15018                             	        #   Mask the modulus
 15019                             	        andq	%rdx, %rcx
 15020                             	        andq	%rdx, %rax
 15021                             	        #   Add modulus (if underflow)
 15022                             	        addq	%rcx, %r12
 15023                             	        adcq	%rdx, %r13
 15024                             	        adcq	%rdx, %r14
 15025                             	        adcq	%rax, %r15
 15026                             	        movq	%r8, (%rsi)
 15027                             	        movq	%r9, 8(%rsi)
 15028                             	        movq	%r10, 16(%rsi)
 15029                             	        movq	%r11, 24(%rsi)
 15030                             	        movq	%r12, (%rbp)
 15031                             	        movq	%r13, 8(%rbp)
 15032                             	        movq	%r14, 16(%rbp)
 15033                             	        movq	%r15, 24(%rbp)
 15034                             	        movq	104(%rsp), %rsi
 15035                             	        # Double
 15036                             	        movq	(%rsi), %r8
 15037                             	        movq	8(%rsi), %r9
 15038                             	        addq	%r8, %r8
 15039                             	        movq	16(%rsi), %r10
 15040                             	        adcq	%r9, %r9
 15041                             	        movq	24(%rsi), %rdx
 15042                             	        adcq	%r10, %r10
 15043                             	        movq	$-19, %rcx
 15044                             	        adcq	%rdx, %rdx
 15045                             	        movq	$0x7fffffffffffffff, %rax
 15046                             	        movq	%rdx, %r11
 15047                             	        sarq	$63, %rdx
 15048                             	        #   Mask the modulus
 15049                             	        andq	%rdx, %rcx
 15050                             	        andq	%rdx, %rax
 15051                             	        #   Sub modulus (if overflow)
 15052                             	        subq	%rcx, %r8
 15053                             	        sbbq	%rdx, %r9
 15054                             	        sbbq	%rdx, %r10
 15055                             	        sbbq	%rax, %r11
 15056                             	        movq	%r8, (%rbx)
 15057                             	        movq	%r9, 8(%rbx)
 15058                             	        movq	%r10, 16(%rbx)
 15059                             	        movq	%r11, 24(%rbx)
 15060                             	        # Add
 15061                             	        movq	(%rbx), %r8
 15062                             	        movq	8(%rbx), %r9
 15063                             	        movq	16(%rbx), %r10
 15064                             	        movq	24(%rbx), %rdx
 15065                             	        movq	%r8, %r12
 15066                             	        addq	(%rdi), %r8
 15067                             	        movq	%r9, %r13
 15068                             	        adcq	8(%rdi), %r9
 15069                             	        movq	%r10, %r14
 15070                             	        adcq	16(%rdi), %r10
 15071                             	        movq	%rdx, %r15
 15072                             	        adcq	24(%rdi), %rdx
 15073                             	        movq	$-19, %rcx
 15074                             	        movq	%rdx, %r11
 15075                             	        movq	$0x7fffffffffffffff, %rax
 15076                             	        sarq	$63, %rdx
 15077                             	        #   Mask the modulus
 15078                             	        andq	%rdx, %rcx
 15079                             	        andq	%rdx, %rax
 15080                             	        #   Sub modulus (if overflow)
 15081                             	        subq	%rcx, %r8
 15082                             	        sbbq	%rdx, %r9
 15083                             	        sbbq	%rdx, %r10
 15084                             	        sbbq	%rax, %r11
 15085                             	        # Sub
 15086                             	        subq	(%rdi), %r12
 15087                             	        movq	$0x00, %rdx
 15088                             	        sbbq	8(%rdi), %r13
 15089                             	        movq	$-19, %rcx
 15090                             	        sbbq	16(%rdi), %r14
 15091                             	        movq	$0x7fffffffffffffff, %rax
 15092                             	        sbbq	24(%rdi), %r15
 15093                             	        sbbq	$0x00, %rdx
 15094                             	        #   Mask the modulus
 15095                             	        andq	%rdx, %rcx
 15096                             	        andq	%rdx, %rax
 15097                             	        #   Add modulus (if underflow)
 15098                             	        addq	%rcx, %r12
 15099                             	        adcq	%rdx, %r13
 15100                             	        adcq	%rdx, %r14
 15101                             	        adcq	%rax, %r15
 15102                             	        movq	%r8, (%rdi)
 15103                             	        movq	%r9, 8(%rdi)
 15104                             	        movq	%r10, 16(%rdi)
 15105                             	        movq	%r11, 24(%rdi)
 15106                             	        movq	%r12, (%rbx)
 15107                             	        movq	%r13, 8(%rbx)
 15108                             	        movq	%r14, 16(%rbx)
 15109                             	        movq	%r15, 24(%rbx)
 15110                             	        addq	$48, %rsp
 15111                             	        popq	%r15
 15112                             	        popq	%r14
 15113                             	        popq	%r13
 15114                             	        popq	%r12
 15115                             	        popq	%rbx
 15116                             	        popq	%rbp
 15117                             	        repz retq
 15118                             	#ifndef __APPLE__
 15120                             	#endif /* __APPLE__ */
 15121                             	#ifndef __APPLE__
 15122                             	.text
 15123                             	.globl	fe_ge_add_avx2
 15125                             	.align	16
 15126                             	fe_ge_add_avx2:
 15127                             	#else
 15128                             	.section	__TEXT,__text
 15129                             	.globl	_fe_ge_add_avx2
 15130                             	.p2align	4
 15131                             	_fe_ge_add_avx2:
 15132                             	#endif /* __APPLE__ */
 15133                             	        pushq	%rbx
 15134                             	        pushq	%rbp
 15135                             	        pushq	%r12
 15136                             	        pushq	%r13
 15137                             	        pushq	%r14
 15138                             	        pushq	%r15
 15139                             	        subq	$0x50, %rsp
 15140                             	        movq	%rdi, (%rsp)
 15141                             	        movq	%rsi, 8(%rsp)
 15142                             	        movq	%rdx, 16(%rsp)
 15143                             	        movq	%rcx, 24(%rsp)
 15144                             	        movq	%r8, 32(%rsp)
 15145                             	        movq	%r9, 40(%rsp)
 15146                             	        movq	8(%rsp), %rsi
 15147                             	        movq	40(%rsp), %rbx
 15148                             	        movq	32(%rsp), %rbp
 15149                             	        # Add
 15150                             	        movq	(%rbx), %r8
 15151                             	        movq	8(%rbx), %r9
 15152                             	        movq	16(%rbx), %r10
 15153                             	        movq	24(%rbx), %rdx
 15154                             	        movq	%r8, %r12
 15155                             	        addq	(%rbp), %r8
 15156                             	        movq	%r9, %r13
 15157                             	        adcq	8(%rbp), %r9
 15158                             	        movq	%r10, %r14
 15159                             	        adcq	16(%rbp), %r10
 15160                             	        movq	%rdx, %r15
 15161                             	        adcq	24(%rbp), %rdx
 15162                             	        movq	$-19, %rcx
 15163                             	        movq	%rdx, %r11
 15164                             	        movq	$0x7fffffffffffffff, %rax
 15165                             	        sarq	$63, %rdx
 15166                             	        #   Mask the modulus
 15167                             	        andq	%rdx, %rcx
 15168                             	        andq	%rdx, %rax
 15169                             	        #   Sub modulus (if overflow)
 15170                             	        subq	%rcx, %r8
 15171                             	        sbbq	%rdx, %r9
 15172                             	        sbbq	%rdx, %r10
 15173                             	        sbbq	%rax, %r11
 15174                             	        # Sub
 15175                             	        subq	(%rbp), %r12
 15176                             	        movq	$0x00, %rdx
 15177                             	        sbbq	8(%rbp), %r13
 15178                             	        movq	$-19, %rcx
 15179                             	        sbbq	16(%rbp), %r14
 15180                             	        movq	$0x7fffffffffffffff, %rax
 15181                             	        sbbq	24(%rbp), %r15
 15182                             	        sbbq	$0x00, %rdx
 15183                             	        #   Mask the modulus
 15184                             	        andq	%rdx, %rcx
 15185                             	        andq	%rdx, %rax
 15186                             	        #   Add modulus (if underflow)
 15187                             	        addq	%rcx, %r12
 15188                             	        adcq	%rdx, %r13
 15189                             	        adcq	%rdx, %r14
 15190                             	        adcq	%rax, %r15
 15191                             	        movq	%r8, (%rdi)
 15192                             	        movq	%r9, 8(%rdi)
 15193                             	        movq	%r10, 16(%rdi)
 15194                             	        movq	%r11, 24(%rdi)
 15195                             	        movq	%r12, (%rsi)
 15196                             	        movq	%r13, 8(%rsi)
 15197                             	        movq	%r14, 16(%rsi)
 15198                             	        movq	%r15, 24(%rsi)
 15199                             	        movq	16(%rsp), %rbx
 15200                             	        movq	168(%rsp), %rbp
 15201                             	        # Multiply
 15202                             	        # A[0] * B[0]
 15203                             	        movq	(%rbp), %rdx
 15204                             	        mulxq	(%rdi), %r8, %r9
 15205                             	        # A[2] * B[0]
 15206                             	        mulxq	16(%rdi), %r10, %r11
 15207                             	        # A[1] * B[0]
 15208                             	        mulxq	8(%rdi), %rcx, %rax
 15209                             	        xorq	%r15, %r15
 15210                             	        adcxq	%rcx, %r9
 15211                             	        # A[1] * B[3]
 15212                             	        movq	24(%rbp), %rdx
 15213                             	        mulxq	8(%rdi), %r12, %r13
 15214                             	        adcxq	%rax, %r10
 15215                             	        # A[0] * B[1]
 15216                             	        movq	8(%rbp), %rdx
 15217                             	        mulxq	(%rdi), %rcx, %rax
 15218                             	        adoxq	%rcx, %r9
 15219                             	        # A[2] * B[1]
 15220                             	        mulxq	16(%rdi), %rcx, %r14
 15221                             	        adoxq	%rax, %r10
 15222                             	        adcxq	%rcx, %r11
 15223                             	        # A[1] * B[2]
 15224                             	        movq	16(%rbp), %rdx
 15225                             	        mulxq	8(%rdi), %rcx, %rax
 15226                             	        adcxq	%r14, %r12
 15227                             	        adoxq	%rcx, %r11
 15228                             	        adcxq	%r15, %r13
 15229                             	        adoxq	%rax, %r12
 15230                             	        # A[0] * B[2]
 15231                             	        mulxq	(%rdi), %rcx, %rax
 15232                             	        adoxq	%r15, %r13
 15233                             	        xorq	%r14, %r14
 15234                             	        adcxq	%rcx, %r10
 15235                             	        # A[1] * B[1]
 15236                             	        movq	8(%rbp), %rdx
 15237                             	        mulxq	8(%rdi), %rdx, %rcx
 15238                             	        adcxq	%rax, %r11
 15239                             	        adoxq	%rdx, %r10
 15240                             	        # A[3] * B[1]
 15241                             	        movq	8(%rbp), %rdx
 15242                             	        adoxq	%rcx, %r11
 15243                             	        mulxq	24(%rdi), %rcx, %rax
 15244                             	        adcxq	%rcx, %r12
 15245                             	        # A[2] * B[2]
 15246                             	        movq	16(%rbp), %rdx
 15247                             	        mulxq	16(%rdi), %rdx, %rcx
 15248                             	        adcxq	%rax, %r13
 15249                             	        adoxq	%rdx, %r12
 15250                             	        # A[3] * B[3]
 15251                             	        movq	24(%rbp), %rdx
 15252                             	        adoxq	%rcx, %r13
 15253                             	        mulxq	24(%rdi), %rcx, %rax
 15254                             	        adoxq	%r15, %r14
 15255                             	        adcxq	%rcx, %r14
 15256                             	        # A[0] * B[3]
 15257                             	        mulxq	(%rdi), %rdx, %rcx
 15258                             	        adcxq	%rax, %r15
 15259                             	        xorq	%rax, %rax
 15260                             	        adcxq	%rdx, %r11
 15261                             	        # A[3] * B[0]
 15262                             	        movq	(%rbp), %rdx
 15263                             	        adcxq	%rcx, %r12
 15264                             	        mulxq	24(%rdi), %rdx, %rcx
 15265                             	        adoxq	%rdx, %r11
 15266                             	        adoxq	%rcx, %r12
 15267                             	        # A[2] * B[3]
 15268                             	        movq	24(%rbp), %rdx
 15269                             	        mulxq	16(%rdi), %rdx, %rcx
 15270                             	        adcxq	%rdx, %r13
 15271                             	        # A[3] * B[2]
 15272                             	        movq	16(%rbp), %rdx
 15273                             	        adcxq	%rcx, %r14
 15274                             	        mulxq	24(%rdi), %rcx, %rdx
 15275                             	        adcxq	%rax, %r15
 15276                             	        adoxq	%rcx, %r13
 15277                             	        adoxq	%rdx, %r14
 15278                             	        adoxq	%rax, %r15
 15279                             	        # Reduce
 15280                             	        movq	$0x7fffffffffffffff, %rax
 15281                             	        #  Move top half into t4-t7 and remove top bit from t3
 15282                             	        shldq	$0x01, %r14, %r15
 15283                             	        shldq	$0x01, %r13, %r14
 15284                             	        shldq	$0x01, %r12, %r13
 15285                             	        shldq	$0x01, %r11, %r12
 15286                             	        andq	%rax, %r11
 15287                             	        #  Multiply top half by 19
 15288                             	        movq	$19, %rdx
 15289                             	        xorq	%rax, %rax
 15290                             	        mulxq	%r12, %rcx, %r12
 15291                             	        adcxq	%rcx, %r8
 15292                             	        adoxq	%r12, %r9
 15293                             	        mulxq	%r13, %rcx, %r13
 15294                             	        adcxq	%rcx, %r9
 15295                             	        adoxq	%r13, %r10
 15296                             	        mulxq	%r14, %rcx, %r14
 15297                             	        adcxq	%rcx, %r10
 15298                             	        adoxq	%r14, %r11
 15299                             	        mulxq	%r15, %r15, %rdx
 15300                             	        adcxq	%r15, %r11
 15301                             	        adoxq	%rax, %rdx
 15302                             	        adcxq	%rax, %rdx
 15303                             	        #  Overflow
 15304                             	        shldq	$0x01, %r11, %rdx
 15305                             	        movq	$0x7fffffffffffffff, %rax
 15306                             	        imulq	$19, %rdx, %rcx
 15307                             	        andq	%rax, %r11
 15308                             	        addq	%rcx, %r8
 15309                             	        adcq	$0x00, %r9
 15310                             	        adcq	$0x00, %r10
 15311                             	        adcq	$0x00, %r11
 15312                             	        # Reduce if top bit set
 15313                             	        movq	%r11, %rdx
 15314                             	        sarq	$63, %rdx
 15315                             	        andq	$19, %rdx
 15316                             	        andq	%rax, %r11
 15317                             	        addq	%rdx, %r8
 15318                             	        adcq	$0x00, %r9
 15319                             	        adcq	$0x00, %r10
 15320                             	        adcq	$0x00, %r11
 15321                             	        # Store
 15322                             	        movq	%r8, (%rbx)
 15323                             	        movq	%r9, 8(%rbx)
 15324                             	        movq	%r10, 16(%rbx)
 15325                             	        movq	%r11, 24(%rbx)
 15326                             	        movq	176(%rsp), %rbx
 15327                             	        # Multiply
 15328                             	        # A[0] * B[0]
 15329                             	        movq	(%rbx), %rdx
 15330                             	        mulxq	(%rsi), %r8, %r9
 15331                             	        # A[2] * B[0]
 15332                             	        mulxq	16(%rsi), %r10, %r11
 15333                             	        # A[1] * B[0]
 15334                             	        mulxq	8(%rsi), %rcx, %rax
 15335                             	        xorq	%r15, %r15
 15336                             	        adcxq	%rcx, %r9
 15337                             	        # A[1] * B[3]
 15338                             	        movq	24(%rbx), %rdx
 15339                             	        mulxq	8(%rsi), %r12, %r13
 15340                             	        adcxq	%rax, %r10
 15341                             	        # A[0] * B[1]
 15342                             	        movq	8(%rbx), %rdx
 15343                             	        mulxq	(%rsi), %rcx, %rax
 15344                             	        adoxq	%rcx, %r9
 15345                             	        # A[2] * B[1]
 15346                             	        mulxq	16(%rsi), %rcx, %r14
 15347                             	        adoxq	%rax, %r10
 15348                             	        adcxq	%rcx, %r11
 15349                             	        # A[1] * B[2]
 15350                             	        movq	16(%rbx), %rdx
 15351                             	        mulxq	8(%rsi), %rcx, %rax
 15352                             	        adcxq	%r14, %r12
 15353                             	        adoxq	%rcx, %r11
 15354                             	        adcxq	%r15, %r13
 15355                             	        adoxq	%rax, %r12
 15356                             	        # A[0] * B[2]
 15357                             	        mulxq	(%rsi), %rcx, %rax
 15358                             	        adoxq	%r15, %r13
 15359                             	        xorq	%r14, %r14
 15360                             	        adcxq	%rcx, %r10
 15361                             	        # A[1] * B[1]
 15362                             	        movq	8(%rbx), %rdx
 15363                             	        mulxq	8(%rsi), %rdx, %rcx
 15364                             	        adcxq	%rax, %r11
 15365                             	        adoxq	%rdx, %r10
 15366                             	        # A[3] * B[1]
 15367                             	        movq	8(%rbx), %rdx
 15368                             	        adoxq	%rcx, %r11
 15369                             	        mulxq	24(%rsi), %rcx, %rax
 15370                             	        adcxq	%rcx, %r12
 15371                             	        # A[2] * B[2]
 15372                             	        movq	16(%rbx), %rdx
 15373                             	        mulxq	16(%rsi), %rdx, %rcx
 15374                             	        adcxq	%rax, %r13
 15375                             	        adoxq	%rdx, %r12
 15376                             	        # A[3] * B[3]
 15377                             	        movq	24(%rbx), %rdx
 15378                             	        adoxq	%rcx, %r13
 15379                             	        mulxq	24(%rsi), %rcx, %rax
 15380                             	        adoxq	%r15, %r14
 15381                             	        adcxq	%rcx, %r14
 15382                             	        # A[0] * B[3]
 15383                             	        mulxq	(%rsi), %rdx, %rcx
 15384                             	        adcxq	%rax, %r15
 15385                             	        xorq	%rax, %rax
 15386                             	        adcxq	%rdx, %r11
 15387                             	        # A[3] * B[0]
 15388                             	        movq	(%rbx), %rdx
 15389                             	        adcxq	%rcx, %r12
 15390                             	        mulxq	24(%rsi), %rdx, %rcx
 15391                             	        adoxq	%rdx, %r11
 15392                             	        adoxq	%rcx, %r12
 15393                             	        # A[2] * B[3]
 15394                             	        movq	24(%rbx), %rdx
 15395                             	        mulxq	16(%rsi), %rdx, %rcx
 15396                             	        adcxq	%rdx, %r13
 15397                             	        # A[3] * B[2]
 15398                             	        movq	16(%rbx), %rdx
 15399                             	        adcxq	%rcx, %r14
 15400                             	        mulxq	24(%rsi), %rcx, %rdx
 15401                             	        adcxq	%rax, %r15
 15402                             	        adoxq	%rcx, %r13
 15403                             	        adoxq	%rdx, %r14
 15404                             	        adoxq	%rax, %r15
 15405                             	        # Reduce
 15406                             	        movq	$0x7fffffffffffffff, %rax
 15407                             	        #  Move top half into t4-t7 and remove top bit from t3
 15408                             	        shldq	$0x01, %r14, %r15
 15409                             	        shldq	$0x01, %r13, %r14
 15410                             	        shldq	$0x01, %r12, %r13
 15411                             	        shldq	$0x01, %r11, %r12
 15412                             	        andq	%rax, %r11
 15413                             	        #  Multiply top half by 19
 15414                             	        movq	$19, %rdx
 15415                             	        xorq	%rax, %rax
 15416                             	        mulxq	%r12, %rcx, %r12
 15417                             	        adcxq	%rcx, %r8
 15418                             	        adoxq	%r12, %r9
 15419                             	        mulxq	%r13, %rcx, %r13
 15420                             	        adcxq	%rcx, %r9
 15421                             	        adoxq	%r13, %r10
 15422                             	        mulxq	%r14, %rcx, %r14
 15423                             	        adcxq	%rcx, %r10
 15424                             	        adoxq	%r14, %r11
 15425                             	        mulxq	%r15, %r15, %rdx
 15426                             	        adcxq	%r15, %r11
 15427                             	        adoxq	%rax, %rdx
 15428                             	        adcxq	%rax, %rdx
 15429                             	        #  Overflow
 15430                             	        shldq	$0x01, %r11, %rdx
 15431                             	        movq	$0x7fffffffffffffff, %rax
 15432                             	        imulq	$19, %rdx, %rcx
 15433                             	        andq	%rax, %r11
 15434                             	        addq	%rcx, %r8
 15435                             	        adcq	$0x00, %r9
 15436                             	        adcq	$0x00, %r10
 15437                             	        adcq	$0x00, %r11
 15438                             	        # Reduce if top bit set
 15439                             	        movq	%r11, %rdx
 15440                             	        sarq	$63, %rdx
 15441                             	        andq	$19, %rdx
 15442                             	        andq	%rax, %r11
 15443                             	        addq	%rdx, %r8
 15444                             	        adcq	$0x00, %r9
 15445                             	        adcq	$0x00, %r10
 15446                             	        adcq	$0x00, %r11
 15447                             	        # Store
 15448                             	        movq	%r8, (%rsi)
 15449                             	        movq	%r9, 8(%rsi)
 15450                             	        movq	%r10, 16(%rsi)
 15451                             	        movq	%r11, 24(%rsi)
 15452                             	        movq	24(%rsp), %rsi
 15453                             	        movq	160(%rsp), %rbx
 15454                             	        movq	144(%rsp), %rbp
 15455                             	        # Multiply
 15456                             	        # A[0] * B[0]
 15457                             	        movq	(%rbp), %rdx
 15458                             	        mulxq	(%rbx), %r8, %r9
 15459                             	        # A[2] * B[0]
 15460                             	        mulxq	16(%rbx), %r10, %r11
 15461                             	        # A[1] * B[0]
 15462                             	        mulxq	8(%rbx), %rcx, %rax
 15463                             	        xorq	%r15, %r15
 15464                             	        adcxq	%rcx, %r9
 15465                             	        # A[1] * B[3]
 15466                             	        movq	24(%rbp), %rdx
 15467                             	        mulxq	8(%rbx), %r12, %r13
 15468                             	        adcxq	%rax, %r10
 15469                             	        # A[0] * B[1]
 15470                             	        movq	8(%rbp), %rdx
 15471                             	        mulxq	(%rbx), %rcx, %rax
 15472                             	        adoxq	%rcx, %r9
 15473                             	        # A[2] * B[1]
 15474                             	        mulxq	16(%rbx), %rcx, %r14
 15475                             	        adoxq	%rax, %r10
 15476                             	        adcxq	%rcx, %r11
 15477                             	        # A[1] * B[2]
 15478                             	        movq	16(%rbp), %rdx
 15479                             	        mulxq	8(%rbx), %rcx, %rax
 15480                             	        adcxq	%r14, %r12
 15481                             	        adoxq	%rcx, %r11
 15482                             	        adcxq	%r15, %r13
 15483                             	        adoxq	%rax, %r12
 15484                             	        # A[0] * B[2]
 15485                             	        mulxq	(%rbx), %rcx, %rax
 15486                             	        adoxq	%r15, %r13
 15487                             	        xorq	%r14, %r14
 15488                             	        adcxq	%rcx, %r10
 15489                             	        # A[1] * B[1]
 15490                             	        movq	8(%rbp), %rdx
 15491                             	        mulxq	8(%rbx), %rdx, %rcx
 15492                             	        adcxq	%rax, %r11
 15493                             	        adoxq	%rdx, %r10
 15494                             	        # A[3] * B[1]
 15495                             	        movq	8(%rbp), %rdx
 15496                             	        adoxq	%rcx, %r11
 15497                             	        mulxq	24(%rbx), %rcx, %rax
 15498                             	        adcxq	%rcx, %r12
 15499                             	        # A[2] * B[2]
 15500                             	        movq	16(%rbp), %rdx
 15501                             	        mulxq	16(%rbx), %rdx, %rcx
 15502                             	        adcxq	%rax, %r13
 15503                             	        adoxq	%rdx, %r12
 15504                             	        # A[3] * B[3]
 15505                             	        movq	24(%rbp), %rdx
 15506                             	        adoxq	%rcx, %r13
 15507                             	        mulxq	24(%rbx), %rcx, %rax
 15508                             	        adoxq	%r15, %r14
 15509                             	        adcxq	%rcx, %r14
 15510                             	        # A[0] * B[3]
 15511                             	        mulxq	(%rbx), %rdx, %rcx
 15512                             	        adcxq	%rax, %r15
 15513                             	        xorq	%rax, %rax
 15514                             	        adcxq	%rdx, %r11
 15515                             	        # A[3] * B[0]
 15516                             	        movq	(%rbp), %rdx
 15517                             	        adcxq	%rcx, %r12
 15518                             	        mulxq	24(%rbx), %rdx, %rcx
 15519                             	        adoxq	%rdx, %r11
 15520                             	        adoxq	%rcx, %r12
 15521                             	        # A[2] * B[3]
 15522                             	        movq	24(%rbp), %rdx
 15523                             	        mulxq	16(%rbx), %rdx, %rcx
 15524                             	        adcxq	%rdx, %r13
 15525                             	        # A[3] * B[2]
 15526                             	        movq	16(%rbp), %rdx
 15527                             	        adcxq	%rcx, %r14
 15528                             	        mulxq	24(%rbx), %rcx, %rdx
 15529                             	        adcxq	%rax, %r15
 15530                             	        adoxq	%rcx, %r13
 15531                             	        adoxq	%rdx, %r14
 15532                             	        adoxq	%rax, %r15
 15533                             	        # Reduce
 15534                             	        movq	$0x7fffffffffffffff, %rax
 15535                             	        #  Move top half into t4-t7 and remove top bit from t3
 15536                             	        shldq	$0x01, %r14, %r15
 15537                             	        shldq	$0x01, %r13, %r14
 15538                             	        shldq	$0x01, %r12, %r13
 15539                             	        shldq	$0x01, %r11, %r12
 15540                             	        andq	%rax, %r11
 15541                             	        #  Multiply top half by 19
 15542                             	        movq	$19, %rdx
 15543                             	        xorq	%rax, %rax
 15544                             	        mulxq	%r12, %rcx, %r12
 15545                             	        adcxq	%rcx, %r8
 15546                             	        adoxq	%r12, %r9
 15547                             	        mulxq	%r13, %rcx, %r13
 15548                             	        adcxq	%rcx, %r9
 15549                             	        adoxq	%r13, %r10
 15550                             	        mulxq	%r14, %rcx, %r14
 15551                             	        adcxq	%rcx, %r10
 15552                             	        adoxq	%r14, %r11
 15553                             	        mulxq	%r15, %r15, %rdx
 15554                             	        adcxq	%r15, %r11
 15555                             	        adoxq	%rax, %rdx
 15556                             	        adcxq	%rax, %rdx
 15557                             	        #  Overflow
 15558                             	        shldq	$0x01, %r11, %rdx
 15559                             	        movq	$0x7fffffffffffffff, %rax
 15560                             	        imulq	$19, %rdx, %rcx
 15561                             	        andq	%rax, %r11
 15562                             	        addq	%rcx, %r8
 15563                             	        adcq	$0x00, %r9
 15564                             	        adcq	$0x00, %r10
 15565                             	        adcq	$0x00, %r11
 15566                             	        # Reduce if top bit set
 15567                             	        movq	%r11, %rdx
 15568                             	        sarq	$63, %rdx
 15569                             	        andq	$19, %rdx
 15570                             	        andq	%rax, %r11
 15571                             	        addq	%rdx, %r8
 15572                             	        adcq	$0x00, %r9
 15573                             	        adcq	$0x00, %r10
 15574                             	        adcq	$0x00, %r11
 15575                             	        # Store
 15576                             	        movq	%r8, (%rsi)
 15577                             	        movq	%r9, 8(%rsi)
 15578                             	        movq	%r10, 16(%rsi)
 15579                             	        movq	%r11, 24(%rsi)
 15580                             	        movq	136(%rsp), %rsi
 15581                             	        movq	152(%rsp), %rbx
 15582                             	        # Multiply
 15583                             	        # A[0] * B[0]
 15584                             	        movq	(%rbx), %rdx
 15585                             	        mulxq	(%rsi), %r8, %r9
 15586                             	        # A[2] * B[0]
 15587                             	        mulxq	16(%rsi), %r10, %r11
 15588                             	        # A[1] * B[0]
 15589                             	        mulxq	8(%rsi), %rcx, %rax
 15590                             	        xorq	%r15, %r15
 15591                             	        adcxq	%rcx, %r9
 15592                             	        # A[1] * B[3]
 15593                             	        movq	24(%rbx), %rdx
 15594                             	        mulxq	8(%rsi), %r12, %r13
 15595                             	        adcxq	%rax, %r10
 15596                             	        # A[0] * B[1]
 15597                             	        movq	8(%rbx), %rdx
 15598                             	        mulxq	(%rsi), %rcx, %rax
 15599                             	        adoxq	%rcx, %r9
 15600                             	        # A[2] * B[1]
 15601                             	        mulxq	16(%rsi), %rcx, %r14
 15602                             	        adoxq	%rax, %r10
 15603                             	        adcxq	%rcx, %r11
 15604                             	        # A[1] * B[2]
 15605                             	        movq	16(%rbx), %rdx
 15606                             	        mulxq	8(%rsi), %rcx, %rax
 15607                             	        adcxq	%r14, %r12
 15608                             	        adoxq	%rcx, %r11
 15609                             	        adcxq	%r15, %r13
 15610                             	        adoxq	%rax, %r12
 15611                             	        # A[0] * B[2]
 15612                             	        mulxq	(%rsi), %rcx, %rax
 15613                             	        adoxq	%r15, %r13
 15614                             	        xorq	%r14, %r14
 15615                             	        adcxq	%rcx, %r10
 15616                             	        # A[1] * B[1]
 15617                             	        movq	8(%rbx), %rdx
 15618                             	        mulxq	8(%rsi), %rdx, %rcx
 15619                             	        adcxq	%rax, %r11
 15620                             	        adoxq	%rdx, %r10
 15621                             	        # A[3] * B[1]
 15622                             	        movq	8(%rbx), %rdx
 15623                             	        adoxq	%rcx, %r11
 15624                             	        mulxq	24(%rsi), %rcx, %rax
 15625                             	        adcxq	%rcx, %r12
 15626                             	        # A[2] * B[2]
 15627                             	        movq	16(%rbx), %rdx
 15628                             	        mulxq	16(%rsi), %rdx, %rcx
 15629                             	        adcxq	%rax, %r13
 15630                             	        adoxq	%rdx, %r12
 15631                             	        # A[3] * B[3]
 15632                             	        movq	24(%rbx), %rdx
 15633                             	        adoxq	%rcx, %r13
 15634                             	        mulxq	24(%rsi), %rcx, %rax
 15635                             	        adoxq	%r15, %r14
 15636                             	        adcxq	%rcx, %r14
 15637                             	        # A[0] * B[3]
 15638                             	        mulxq	(%rsi), %rdx, %rcx
 15639                             	        adcxq	%rax, %r15
 15640                             	        xorq	%rax, %rax
 15641                             	        adcxq	%rdx, %r11
 15642                             	        # A[3] * B[0]
 15643                             	        movq	(%rbx), %rdx
 15644                             	        adcxq	%rcx, %r12
 15645                             	        mulxq	24(%rsi), %rdx, %rcx
 15646                             	        adoxq	%rdx, %r11
 15647                             	        adoxq	%rcx, %r12
 15648                             	        # A[2] * B[3]
 15649                             	        movq	24(%rbx), %rdx
 15650                             	        mulxq	16(%rsi), %rdx, %rcx
 15651                             	        adcxq	%rdx, %r13
 15652                             	        # A[3] * B[2]
 15653                             	        movq	16(%rbx), %rdx
 15654                             	        adcxq	%rcx, %r14
 15655                             	        mulxq	24(%rsi), %rcx, %rdx
 15656                             	        adcxq	%rax, %r15
 15657                             	        adoxq	%rcx, %r13
 15658                             	        adoxq	%rdx, %r14
 15659                             	        adoxq	%rax, %r15
 15660                             	        # Reduce
 15661                             	        movq	$0x7fffffffffffffff, %rax
 15662                             	        #  Move top half into t4-t7 and remove top bit from t3
 15663                             	        shldq	$0x01, %r14, %r15
 15664                             	        shldq	$0x01, %r13, %r14
 15665                             	        shldq	$0x01, %r12, %r13
 15666                             	        shldq	$0x01, %r11, %r12
 15667                             	        andq	%rax, %r11
 15668                             	        #  Multiply top half by 19
 15669                             	        movq	$19, %rdx
 15670                             	        xorq	%rax, %rax
 15671                             	        mulxq	%r12, %rcx, %r12
 15672                             	        adcxq	%rcx, %r8
 15673                             	        adoxq	%r12, %r9
 15674                             	        mulxq	%r13, %rcx, %r13
 15675                             	        adcxq	%rcx, %r9
 15676                             	        adoxq	%r13, %r10
 15677                             	        mulxq	%r14, %rcx, %r14
 15678                             	        adcxq	%rcx, %r10
 15679                             	        adoxq	%r14, %r11
 15680                             	        mulxq	%r15, %r15, %rdx
 15681                             	        adcxq	%r15, %r11
 15682                             	        adoxq	%rax, %rdx
 15683                             	        adcxq	%rax, %rdx
 15684                             	        #  Overflow
 15685                             	        shldq	$0x01, %r11, %rdx
 15686                             	        movq	$0x7fffffffffffffff, %rax
 15687                             	        imulq	$19, %rdx, %rcx
 15688                             	        andq	%rax, %r11
 15689                             	        addq	%rcx, %r8
 15690                             	        adcq	$0x00, %r9
 15691                             	        adcq	$0x00, %r10
 15692                             	        adcq	$0x00, %r11
 15693                             	        # Reduce if top bit set
 15694                             	        movq	%r11, %rdx
 15695                             	        sarq	$63, %rdx
 15696                             	        andq	$19, %rdx
 15697                             	        andq	%rax, %r11
 15698                             	        addq	%rdx, %r8
 15699                             	        adcq	$0x00, %r9
 15700                             	        adcq	$0x00, %r10
 15701                             	        adcq	$0x00, %r11
 15702                             	        # Store
 15703                             	        movq	%r8, (%rdi)
 15704                             	        movq	%r9, 8(%rdi)
 15705                             	        movq	%r10, 16(%rdi)
 15706                             	        movq	%r11, 24(%rdi)
 15707                             	        leaq	48(%rsp), %rsi
 15708                             	        # Double
 15709                             	        movq	(%rdi), %r8
 15710                             	        movq	8(%rdi), %r9
 15711                             	        addq	%r8, %r8
 15712                             	        movq	16(%rdi), %r10
 15713                             	        adcq	%r9, %r9
 15714                             	        movq	24(%rdi), %rdx
 15715                             	        adcq	%r10, %r10
 15716                             	        movq	$-19, %rcx
 15717                             	        adcq	%rdx, %rdx
 15718                             	        movq	$0x7fffffffffffffff, %rax
 15719                             	        movq	%rdx, %r11
 15720                             	        sarq	$63, %rdx
 15721                             	        #   Mask the modulus
 15722                             	        andq	%rdx, %rcx
 15723                             	        andq	%rdx, %rax
 15724                             	        #   Sub modulus (if overflow)
 15725                             	        subq	%rcx, %r8
 15726                             	        sbbq	%rdx, %r9
 15727                             	        sbbq	%rdx, %r10
 15728                             	        sbbq	%rax, %r11
 15729                             	        movq	%r8, (%rsi)
 15730                             	        movq	%r9, 8(%rsi)
 15731                             	        movq	%r10, 16(%rsi)
 15732                             	        movq	%r11, 24(%rsi)
 15733                             	        movq	8(%rsp), %rbx
 15734                             	        movq	16(%rsp), %rbp
 15735                             	        # Add
 15736                             	        movq	(%rbp), %r8
 15737                             	        movq	8(%rbp), %r9
 15738                             	        movq	16(%rbp), %r10
 15739                             	        movq	24(%rbp), %rdx
 15740                             	        movq	%r8, %r12
 15741                             	        addq	(%rbx), %r8
 15742                             	        movq	%r9, %r13
 15743                             	        adcq	8(%rbx), %r9
 15744                             	        movq	%r10, %r14
 15745                             	        adcq	16(%rbx), %r10
 15746                             	        movq	%rdx, %r15
 15747                             	        adcq	24(%rbx), %rdx
 15748                             	        movq	$-19, %rcx
 15749                             	        movq	%rdx, %r11
 15750                             	        movq	$0x7fffffffffffffff, %rax
 15751                             	        sarq	$63, %rdx
 15752                             	        #   Mask the modulus
 15753                             	        andq	%rdx, %rcx
 15754                             	        andq	%rdx, %rax
 15755                             	        #   Sub modulus (if overflow)
 15756                             	        subq	%rcx, %r8
 15757                             	        sbbq	%rdx, %r9
 15758                             	        sbbq	%rdx, %r10
 15759                             	        sbbq	%rax, %r11
 15760                             	        # Sub
 15761                             	        subq	(%rbx), %r12
 15762                             	        movq	$0x00, %rdx
 15763                             	        sbbq	8(%rbx), %r13
 15764                             	        movq	$-19, %rcx
 15765                             	        sbbq	16(%rbx), %r14
 15766                             	        movq	$0x7fffffffffffffff, %rax
 15767                             	        sbbq	24(%rbx), %r15
 15768                             	        sbbq	$0x00, %rdx
 15769                             	        #   Mask the modulus
 15770                             	        andq	%rdx, %rcx
 15771                             	        andq	%rdx, %rax
 15772                             	        #   Add modulus (if underflow)
 15773                             	        addq	%rcx, %r12
 15774                             	        adcq	%rdx, %r13
 15775                             	        adcq	%rdx, %r14
 15776                             	        adcq	%rax, %r15
 15777                             	        movq	%r8, (%rbx)
 15778                             	        movq	%r9, 8(%rbx)
 15779                             	        movq	%r10, 16(%rbx)
 15780                             	        movq	%r11, 24(%rbx)
 15781                             	        movq	%r12, (%rdi)
 15782                             	        movq	%r13, 8(%rdi)
 15783                             	        movq	%r14, 16(%rdi)
 15784                             	        movq	%r15, 24(%rdi)
 15785                             	        movq	24(%rsp), %rdi
 15786                             	        # Add
 15787                             	        movq	(%rsi), %r8
 15788                             	        movq	8(%rsi), %r9
 15789                             	        movq	16(%rsi), %r10
 15790                             	        movq	24(%rsi), %rdx
 15791                             	        movq	%r8, %r12
 15792                             	        addq	(%rdi), %r8
 15793                             	        movq	%r9, %r13
 15794                             	        adcq	8(%rdi), %r9
 15795                             	        movq	%r10, %r14
 15796                             	        adcq	16(%rdi), %r10
 15797                             	        movq	%rdx, %r15
 15798                             	        adcq	24(%rdi), %rdx
 15799                             	        movq	$-19, %rcx
 15800                             	        movq	%rdx, %r11
 15801                             	        movq	$0x7fffffffffffffff, %rax
 15802                             	        sarq	$63, %rdx
 15803                             	        #   Mask the modulus
 15804                             	        andq	%rdx, %rcx
 15805                             	        andq	%rdx, %rax
 15806                             	        #   Sub modulus (if overflow)
 15807                             	        subq	%rcx, %r8
 15808                             	        sbbq	%rdx, %r9
 15809                             	        sbbq	%rdx, %r10
 15810                             	        sbbq	%rax, %r11
 15811                             	        # Sub
 15812                             	        subq	(%rdi), %r12
 15813                             	        movq	$0x00, %rdx
 15814                             	        sbbq	8(%rdi), %r13
 15815                             	        movq	$-19, %rcx
 15816                             	        sbbq	16(%rdi), %r14
 15817                             	        movq	$0x7fffffffffffffff, %rax
 15818                             	        sbbq	24(%rdi), %r15
 15819                             	        sbbq	$0x00, %rdx
 15820                             	        #   Mask the modulus
 15821                             	        andq	%rdx, %rcx
 15822                             	        andq	%rdx, %rax
 15823                             	        #   Add modulus (if underflow)
 15824                             	        addq	%rcx, %r12
 15825                             	        adcq	%rdx, %r13
 15826                             	        adcq	%rdx, %r14
 15827                             	        adcq	%rax, %r15
 15828                             	        movq	%r8, (%rbp)
 15829                             	        movq	%r9, 8(%rbp)
 15830                             	        movq	%r10, 16(%rbp)
 15831                             	        movq	%r11, 24(%rbp)
 15832                             	        movq	%r12, (%rdi)
 15833                             	        movq	%r13, 8(%rdi)
 15834                             	        movq	%r14, 16(%rdi)
 15835                             	        movq	%r15, 24(%rdi)
 15836                             	        addq	$0x50, %rsp
 15837                             	        popq	%r15
 15838                             	        popq	%r14
 15839                             	        popq	%r13
 15840                             	        popq	%r12
 15841                             	        popq	%rbp
 15842                             	        popq	%rbx
 15843                             	        repz retq
 15844                             	#ifndef __APPLE__
 15846                             	#endif /* __APPLE__ */
 15847                             	#ifndef __APPLE__
 15848                             	.text
 15849                             	.globl	fe_ge_sub_avx2
 15851                             	.align	16
 15852                             	fe_ge_sub_avx2:
 15853                             	#else
 15854                             	.section	__TEXT,__text
 15855                             	.globl	_fe_ge_sub_avx2
 15856                             	.p2align	4
 15857                             	_fe_ge_sub_avx2:
 15858                             	#endif /* __APPLE__ */
 15859                             	        pushq	%rbx
 15860                             	        pushq	%rbp
 15861                             	        pushq	%r12
 15862                             	        pushq	%r13
 15863                             	        pushq	%r14
 15864                             	        pushq	%r15
 15865                             	        subq	$0x50, %rsp
 15866                             	        movq	%rdi, (%rsp)
 15867                             	        movq	%rsi, 8(%rsp)
 15868                             	        movq	%rdx, 16(%rsp)
 15869                             	        movq	%rcx, 24(%rsp)
 15870                             	        movq	%r8, 32(%rsp)
 15871                             	        movq	%r9, 40(%rsp)
 15872                             	        movq	8(%rsp), %rsi
 15873                             	        movq	40(%rsp), %rbx
 15874                             	        movq	32(%rsp), %rbp
 15875                             	        # Add
 15876                             	        movq	(%rbx), %r8
 15877                             	        movq	8(%rbx), %r9
 15878                             	        movq	16(%rbx), %r10
 15879                             	        movq	24(%rbx), %rdx
 15880                             	        movq	%r8, %r12
 15881                             	        addq	(%rbp), %r8
 15882                             	        movq	%r9, %r13
 15883                             	        adcq	8(%rbp), %r9
 15884                             	        movq	%r10, %r14
 15885                             	        adcq	16(%rbp), %r10
 15886                             	        movq	%rdx, %r15
 15887                             	        adcq	24(%rbp), %rdx
 15888                             	        movq	$-19, %rcx
 15889                             	        movq	%rdx, %r11
 15890                             	        movq	$0x7fffffffffffffff, %rax
 15891                             	        sarq	$63, %rdx
 15892                             	        #   Mask the modulus
 15893                             	        andq	%rdx, %rcx
 15894                             	        andq	%rdx, %rax
 15895                             	        #   Sub modulus (if overflow)
 15896                             	        subq	%rcx, %r8
 15897                             	        sbbq	%rdx, %r9
 15898                             	        sbbq	%rdx, %r10
 15899                             	        sbbq	%rax, %r11
 15900                             	        # Sub
 15901                             	        subq	(%rbp), %r12
 15902                             	        movq	$0x00, %rdx
 15903                             	        sbbq	8(%rbp), %r13
 15904                             	        movq	$-19, %rcx
 15905                             	        sbbq	16(%rbp), %r14
 15906                             	        movq	$0x7fffffffffffffff, %rax
 15907                             	        sbbq	24(%rbp), %r15
 15908                             	        sbbq	$0x00, %rdx
 15909                             	        #   Mask the modulus
 15910                             	        andq	%rdx, %rcx
 15911                             	        andq	%rdx, %rax
 15912                             	        #   Add modulus (if underflow)
 15913                             	        addq	%rcx, %r12
 15914                             	        adcq	%rdx, %r13
 15915                             	        adcq	%rdx, %r14
 15916                             	        adcq	%rax, %r15
 15917                             	        movq	%r8, (%rdi)
 15918                             	        movq	%r9, 8(%rdi)
 15919                             	        movq	%r10, 16(%rdi)
 15920                             	        movq	%r11, 24(%rdi)
 15921                             	        movq	%r12, (%rsi)
 15922                             	        movq	%r13, 8(%rsi)
 15923                             	        movq	%r14, 16(%rsi)
 15924                             	        movq	%r15, 24(%rsi)
 15925                             	        movq	16(%rsp), %rbx
 15926                             	        movq	176(%rsp), %rbp
 15927                             	        # Multiply
 15928                             	        # A[0] * B[0]
 15929                             	        movq	(%rbp), %rdx
 15930                             	        mulxq	(%rdi), %r8, %r9
 15931                             	        # A[2] * B[0]
 15932                             	        mulxq	16(%rdi), %r10, %r11
 15933                             	        # A[1] * B[0]
 15934                             	        mulxq	8(%rdi), %rcx, %rax
 15935                             	        xorq	%r15, %r15
 15936                             	        adcxq	%rcx, %r9
 15937                             	        # A[1] * B[3]
 15938                             	        movq	24(%rbp), %rdx
 15939                             	        mulxq	8(%rdi), %r12, %r13
 15940                             	        adcxq	%rax, %r10
 15941                             	        # A[0] * B[1]
 15942                             	        movq	8(%rbp), %rdx
 15943                             	        mulxq	(%rdi), %rcx, %rax
 15944                             	        adoxq	%rcx, %r9
 15945                             	        # A[2] * B[1]
 15946                             	        mulxq	16(%rdi), %rcx, %r14
 15947                             	        adoxq	%rax, %r10
 15948                             	        adcxq	%rcx, %r11
 15949                             	        # A[1] * B[2]
 15950                             	        movq	16(%rbp), %rdx
 15951                             	        mulxq	8(%rdi), %rcx, %rax
 15952                             	        adcxq	%r14, %r12
 15953                             	        adoxq	%rcx, %r11
 15954                             	        adcxq	%r15, %r13
 15955                             	        adoxq	%rax, %r12
 15956                             	        # A[0] * B[2]
 15957                             	        mulxq	(%rdi), %rcx, %rax
 15958                             	        adoxq	%r15, %r13
 15959                             	        xorq	%r14, %r14
 15960                             	        adcxq	%rcx, %r10
 15961                             	        # A[1] * B[1]
 15962                             	        movq	8(%rbp), %rdx
 15963                             	        mulxq	8(%rdi), %rdx, %rcx
 15964                             	        adcxq	%rax, %r11
 15965                             	        adoxq	%rdx, %r10
 15966                             	        # A[3] * B[1]
 15967                             	        movq	8(%rbp), %rdx
 15968                             	        adoxq	%rcx, %r11
 15969                             	        mulxq	24(%rdi), %rcx, %rax
 15970                             	        adcxq	%rcx, %r12
 15971                             	        # A[2] * B[2]
 15972                             	        movq	16(%rbp), %rdx
 15973                             	        mulxq	16(%rdi), %rdx, %rcx
 15974                             	        adcxq	%rax, %r13
 15975                             	        adoxq	%rdx, %r12
 15976                             	        # A[3] * B[3]
 15977                             	        movq	24(%rbp), %rdx
 15978                             	        adoxq	%rcx, %r13
 15979                             	        mulxq	24(%rdi), %rcx, %rax
 15980                             	        adoxq	%r15, %r14
 15981                             	        adcxq	%rcx, %r14
 15982                             	        # A[0] * B[3]
 15983                             	        mulxq	(%rdi), %rdx, %rcx
 15984                             	        adcxq	%rax, %r15
 15985                             	        xorq	%rax, %rax
 15986                             	        adcxq	%rdx, %r11
 15987                             	        # A[3] * B[0]
 15988                             	        movq	(%rbp), %rdx
 15989                             	        adcxq	%rcx, %r12
 15990                             	        mulxq	24(%rdi), %rdx, %rcx
 15991                             	        adoxq	%rdx, %r11
 15992                             	        adoxq	%rcx, %r12
 15993                             	        # A[2] * B[3]
 15994                             	        movq	24(%rbp), %rdx
 15995                             	        mulxq	16(%rdi), %rdx, %rcx
 15996                             	        adcxq	%rdx, %r13
 15997                             	        # A[3] * B[2]
 15998                             	        movq	16(%rbp), %rdx
 15999                             	        adcxq	%rcx, %r14
 16000                             	        mulxq	24(%rdi), %rcx, %rdx
 16001                             	        adcxq	%rax, %r15
 16002                             	        adoxq	%rcx, %r13
 16003                             	        adoxq	%rdx, %r14
 16004                             	        adoxq	%rax, %r15
 16005                             	        # Reduce
 16006                             	        movq	$0x7fffffffffffffff, %rax
 16007                             	        #  Move top half into t4-t7 and remove top bit from t3
 16008                             	        shldq	$0x01, %r14, %r15
 16009                             	        shldq	$0x01, %r13, %r14
 16010                             	        shldq	$0x01, %r12, %r13
 16011                             	        shldq	$0x01, %r11, %r12
 16012                             	        andq	%rax, %r11
 16013                             	        #  Multiply top half by 19
 16014                             	        movq	$19, %rdx
 16015                             	        xorq	%rax, %rax
 16016                             	        mulxq	%r12, %rcx, %r12
 16017                             	        adcxq	%rcx, %r8
 16018                             	        adoxq	%r12, %r9
 16019                             	        mulxq	%r13, %rcx, %r13
 16020                             	        adcxq	%rcx, %r9
 16021                             	        adoxq	%r13, %r10
 16022                             	        mulxq	%r14, %rcx, %r14
 16023                             	        adcxq	%rcx, %r10
 16024                             	        adoxq	%r14, %r11
 16025                             	        mulxq	%r15, %r15, %rdx
 16026                             	        adcxq	%r15, %r11
 16027                             	        adoxq	%rax, %rdx
 16028                             	        adcxq	%rax, %rdx
 16029                             	        #  Overflow
 16030                             	        shldq	$0x01, %r11, %rdx
 16031                             	        movq	$0x7fffffffffffffff, %rax
 16032                             	        imulq	$19, %rdx, %rcx
 16033                             	        andq	%rax, %r11
 16034                             	        addq	%rcx, %r8
 16035                             	        adcq	$0x00, %r9
 16036                             	        adcq	$0x00, %r10
 16037                             	        adcq	$0x00, %r11
 16038                             	        # Reduce if top bit set
 16039                             	        movq	%r11, %rdx
 16040                             	        sarq	$63, %rdx
 16041                             	        andq	$19, %rdx
 16042                             	        andq	%rax, %r11
 16043                             	        addq	%rdx, %r8
 16044                             	        adcq	$0x00, %r9
 16045                             	        adcq	$0x00, %r10
 16046                             	        adcq	$0x00, %r11
 16047                             	        # Store
 16048                             	        movq	%r8, (%rbx)
 16049                             	        movq	%r9, 8(%rbx)
 16050                             	        movq	%r10, 16(%rbx)
 16051                             	        movq	%r11, 24(%rbx)
 16052                             	        movq	168(%rsp), %rbx
 16053                             	        # Multiply
 16054                             	        # A[0] * B[0]
 16055                             	        movq	(%rbx), %rdx
 16056                             	        mulxq	(%rsi), %r8, %r9
 16057                             	        # A[2] * B[0]
 16058                             	        mulxq	16(%rsi), %r10, %r11
 16059                             	        # A[1] * B[0]
 16060                             	        mulxq	8(%rsi), %rcx, %rax
 16061                             	        xorq	%r15, %r15
 16062                             	        adcxq	%rcx, %r9
 16063                             	        # A[1] * B[3]
 16064                             	        movq	24(%rbx), %rdx
 16065                             	        mulxq	8(%rsi), %r12, %r13
 16066                             	        adcxq	%rax, %r10
 16067                             	        # A[0] * B[1]
 16068                             	        movq	8(%rbx), %rdx
 16069                             	        mulxq	(%rsi), %rcx, %rax
 16070                             	        adoxq	%rcx, %r9
 16071                             	        # A[2] * B[1]
 16072                             	        mulxq	16(%rsi), %rcx, %r14
 16073                             	        adoxq	%rax, %r10
 16074                             	        adcxq	%rcx, %r11
 16075                             	        # A[1] * B[2]
 16076                             	        movq	16(%rbx), %rdx
 16077                             	        mulxq	8(%rsi), %rcx, %rax
 16078                             	        adcxq	%r14, %r12
 16079                             	        adoxq	%rcx, %r11
 16080                             	        adcxq	%r15, %r13
 16081                             	        adoxq	%rax, %r12
 16082                             	        # A[0] * B[2]
 16083                             	        mulxq	(%rsi), %rcx, %rax
 16084                             	        adoxq	%r15, %r13
 16085                             	        xorq	%r14, %r14
 16086                             	        adcxq	%rcx, %r10
 16087                             	        # A[1] * B[1]
 16088                             	        movq	8(%rbx), %rdx
 16089                             	        mulxq	8(%rsi), %rdx, %rcx
 16090                             	        adcxq	%rax, %r11
 16091                             	        adoxq	%rdx, %r10
 16092                             	        # A[3] * B[1]
 16093                             	        movq	8(%rbx), %rdx
 16094                             	        adoxq	%rcx, %r11
 16095                             	        mulxq	24(%rsi), %rcx, %rax
 16096                             	        adcxq	%rcx, %r12
 16097                             	        # A[2] * B[2]
 16098                             	        movq	16(%rbx), %rdx
 16099                             	        mulxq	16(%rsi), %rdx, %rcx
 16100                             	        adcxq	%rax, %r13
 16101                             	        adoxq	%rdx, %r12
 16102                             	        # A[3] * B[3]
 16103                             	        movq	24(%rbx), %rdx
 16104                             	        adoxq	%rcx, %r13
 16105                             	        mulxq	24(%rsi), %rcx, %rax
 16106                             	        adoxq	%r15, %r14
 16107                             	        adcxq	%rcx, %r14
 16108                             	        # A[0] * B[3]
 16109                             	        mulxq	(%rsi), %rdx, %rcx
 16110                             	        adcxq	%rax, %r15
 16111                             	        xorq	%rax, %rax
 16112                             	        adcxq	%rdx, %r11
 16113                             	        # A[3] * B[0]
 16114                             	        movq	(%rbx), %rdx
 16115                             	        adcxq	%rcx, %r12
 16116                             	        mulxq	24(%rsi), %rdx, %rcx
 16117                             	        adoxq	%rdx, %r11
 16118                             	        adoxq	%rcx, %r12
 16119                             	        # A[2] * B[3]
 16120                             	        movq	24(%rbx), %rdx
 16121                             	        mulxq	16(%rsi), %rdx, %rcx
 16122                             	        adcxq	%rdx, %r13
 16123                             	        # A[3] * B[2]
 16124                             	        movq	16(%rbx), %rdx
 16125                             	        adcxq	%rcx, %r14
 16126                             	        mulxq	24(%rsi), %rcx, %rdx
 16127                             	        adcxq	%rax, %r15
 16128                             	        adoxq	%rcx, %r13
 16129                             	        adoxq	%rdx, %r14
 16130                             	        adoxq	%rax, %r15
 16131                             	        # Reduce
 16132                             	        movq	$0x7fffffffffffffff, %rax
 16133                             	        #  Move top half into t4-t7 and remove top bit from t3
 16134                             	        shldq	$0x01, %r14, %r15
 16135                             	        shldq	$0x01, %r13, %r14
 16136                             	        shldq	$0x01, %r12, %r13
 16137                             	        shldq	$0x01, %r11, %r12
 16138                             	        andq	%rax, %r11
 16139                             	        #  Multiply top half by 19
 16140                             	        movq	$19, %rdx
 16141                             	        xorq	%rax, %rax
 16142                             	        mulxq	%r12, %rcx, %r12
 16143                             	        adcxq	%rcx, %r8
 16144                             	        adoxq	%r12, %r9
 16145                             	        mulxq	%r13, %rcx, %r13
 16146                             	        adcxq	%rcx, %r9
 16147                             	        adoxq	%r13, %r10
 16148                             	        mulxq	%r14, %rcx, %r14
 16149                             	        adcxq	%rcx, %r10
 16150                             	        adoxq	%r14, %r11
 16151                             	        mulxq	%r15, %r15, %rdx
 16152                             	        adcxq	%r15, %r11
 16153                             	        adoxq	%rax, %rdx
 16154                             	        adcxq	%rax, %rdx
 16155                             	        #  Overflow
 16156                             	        shldq	$0x01, %r11, %rdx
 16157                             	        movq	$0x7fffffffffffffff, %rax
 16158                             	        imulq	$19, %rdx, %rcx
 16159                             	        andq	%rax, %r11
 16160                             	        addq	%rcx, %r8
 16161                             	        adcq	$0x00, %r9
 16162                             	        adcq	$0x00, %r10
 16163                             	        adcq	$0x00, %r11
 16164                             	        # Reduce if top bit set
 16165                             	        movq	%r11, %rdx
 16166                             	        sarq	$63, %rdx
 16167                             	        andq	$19, %rdx
 16168                             	        andq	%rax, %r11
 16169                             	        addq	%rdx, %r8
 16170                             	        adcq	$0x00, %r9
 16171                             	        adcq	$0x00, %r10
 16172                             	        adcq	$0x00, %r11
 16173                             	        # Store
 16174                             	        movq	%r8, (%rsi)
 16175                             	        movq	%r9, 8(%rsi)
 16176                             	        movq	%r10, 16(%rsi)
 16177                             	        movq	%r11, 24(%rsi)
 16178                             	        movq	24(%rsp), %rsi
 16179                             	        movq	160(%rsp), %rbx
 16180                             	        movq	144(%rsp), %rbp
 16181                             	        # Multiply
 16182                             	        # A[0] * B[0]
 16183                             	        movq	(%rbp), %rdx
 16184                             	        mulxq	(%rbx), %r8, %r9
 16185                             	        # A[2] * B[0]
 16186                             	        mulxq	16(%rbx), %r10, %r11
 16187                             	        # A[1] * B[0]
 16188                             	        mulxq	8(%rbx), %rcx, %rax
 16189                             	        xorq	%r15, %r15
 16190                             	        adcxq	%rcx, %r9
 16191                             	        # A[1] * B[3]
 16192                             	        movq	24(%rbp), %rdx
 16193                             	        mulxq	8(%rbx), %r12, %r13
 16194                             	        adcxq	%rax, %r10
 16195                             	        # A[0] * B[1]
 16196                             	        movq	8(%rbp), %rdx
 16197                             	        mulxq	(%rbx), %rcx, %rax
 16198                             	        adoxq	%rcx, %r9
 16199                             	        # A[2] * B[1]
 16200                             	        mulxq	16(%rbx), %rcx, %r14
 16201                             	        adoxq	%rax, %r10
 16202                             	        adcxq	%rcx, %r11
 16203                             	        # A[1] * B[2]
 16204                             	        movq	16(%rbp), %rdx
 16205                             	        mulxq	8(%rbx), %rcx, %rax
 16206                             	        adcxq	%r14, %r12
 16207                             	        adoxq	%rcx, %r11
 16208                             	        adcxq	%r15, %r13
 16209                             	        adoxq	%rax, %r12
 16210                             	        # A[0] * B[2]
 16211                             	        mulxq	(%rbx), %rcx, %rax
 16212                             	        adoxq	%r15, %r13
 16213                             	        xorq	%r14, %r14
 16214                             	        adcxq	%rcx, %r10
 16215                             	        # A[1] * B[1]
 16216                             	        movq	8(%rbp), %rdx
 16217                             	        mulxq	8(%rbx), %rdx, %rcx
 16218                             	        adcxq	%rax, %r11
 16219                             	        adoxq	%rdx, %r10
 16220                             	        # A[3] * B[1]
 16221                             	        movq	8(%rbp), %rdx
 16222                             	        adoxq	%rcx, %r11
 16223                             	        mulxq	24(%rbx), %rcx, %rax
 16224                             	        adcxq	%rcx, %r12
 16225                             	        # A[2] * B[2]
 16226                             	        movq	16(%rbp), %rdx
 16227                             	        mulxq	16(%rbx), %rdx, %rcx
 16228                             	        adcxq	%rax, %r13
 16229                             	        adoxq	%rdx, %r12
 16230                             	        # A[3] * B[3]
 16231                             	        movq	24(%rbp), %rdx
 16232                             	        adoxq	%rcx, %r13
 16233                             	        mulxq	24(%rbx), %rcx, %rax
 16234                             	        adoxq	%r15, %r14
 16235                             	        adcxq	%rcx, %r14
 16236                             	        # A[0] * B[3]
 16237                             	        mulxq	(%rbx), %rdx, %rcx
 16238                             	        adcxq	%rax, %r15
 16239                             	        xorq	%rax, %rax
 16240                             	        adcxq	%rdx, %r11
 16241                             	        # A[3] * B[0]
 16242                             	        movq	(%rbp), %rdx
 16243                             	        adcxq	%rcx, %r12
 16244                             	        mulxq	24(%rbx), %rdx, %rcx
 16245                             	        adoxq	%rdx, %r11
 16246                             	        adoxq	%rcx, %r12
 16247                             	        # A[2] * B[3]
 16248                             	        movq	24(%rbp), %rdx
 16249                             	        mulxq	16(%rbx), %rdx, %rcx
 16250                             	        adcxq	%rdx, %r13
 16251                             	        # A[3] * B[2]
 16252                             	        movq	16(%rbp), %rdx
 16253                             	        adcxq	%rcx, %r14
 16254                             	        mulxq	24(%rbx), %rcx, %rdx
 16255                             	        adcxq	%rax, %r15
 16256                             	        adoxq	%rcx, %r13
 16257                             	        adoxq	%rdx, %r14
 16258                             	        adoxq	%rax, %r15
 16259                             	        # Reduce
 16260                             	        movq	$0x7fffffffffffffff, %rax
 16261                             	        #  Move top half into t4-t7 and remove top bit from t3
 16262                             	        shldq	$0x01, %r14, %r15
 16263                             	        shldq	$0x01, %r13, %r14
 16264                             	        shldq	$0x01, %r12, %r13
 16265                             	        shldq	$0x01, %r11, %r12
 16266                             	        andq	%rax, %r11
 16267                             	        #  Multiply top half by 19
 16268                             	        movq	$19, %rdx
 16269                             	        xorq	%rax, %rax
 16270                             	        mulxq	%r12, %rcx, %r12
 16271                             	        adcxq	%rcx, %r8
 16272                             	        adoxq	%r12, %r9
 16273                             	        mulxq	%r13, %rcx, %r13
 16274                             	        adcxq	%rcx, %r9
 16275                             	        adoxq	%r13, %r10
 16276                             	        mulxq	%r14, %rcx, %r14
 16277                             	        adcxq	%rcx, %r10
 16278                             	        adoxq	%r14, %r11
 16279                             	        mulxq	%r15, %r15, %rdx
 16280                             	        adcxq	%r15, %r11
 16281                             	        adoxq	%rax, %rdx
 16282                             	        adcxq	%rax, %rdx
 16283                             	        #  Overflow
 16284                             	        shldq	$0x01, %r11, %rdx
 16285                             	        movq	$0x7fffffffffffffff, %rax
 16286                             	        imulq	$19, %rdx, %rcx
 16287                             	        andq	%rax, %r11
 16288                             	        addq	%rcx, %r8
 16289                             	        adcq	$0x00, %r9
 16290                             	        adcq	$0x00, %r10
 16291                             	        adcq	$0x00, %r11
 16292                             	        # Reduce if top bit set
 16293                             	        movq	%r11, %rdx
 16294                             	        sarq	$63, %rdx
 16295                             	        andq	$19, %rdx
 16296                             	        andq	%rax, %r11
 16297                             	        addq	%rdx, %r8
 16298                             	        adcq	$0x00, %r9
 16299                             	        adcq	$0x00, %r10
 16300                             	        adcq	$0x00, %r11
 16301                             	        # Store
 16302                             	        movq	%r8, (%rsi)
 16303                             	        movq	%r9, 8(%rsi)
 16304                             	        movq	%r10, 16(%rsi)
 16305                             	        movq	%r11, 24(%rsi)
 16306                             	        movq	136(%rsp), %rsi
 16307                             	        movq	152(%rsp), %rbx
 16308                             	        # Multiply
 16309                             	        # A[0] * B[0]
 16310                             	        movq	(%rbx), %rdx
 16311                             	        mulxq	(%rsi), %r8, %r9
 16312                             	        # A[2] * B[0]
 16313                             	        mulxq	16(%rsi), %r10, %r11
 16314                             	        # A[1] * B[0]
 16315                             	        mulxq	8(%rsi), %rcx, %rax
 16316                             	        xorq	%r15, %r15
 16317                             	        adcxq	%rcx, %r9
 16318                             	        # A[1] * B[3]
 16319                             	        movq	24(%rbx), %rdx
 16320                             	        mulxq	8(%rsi), %r12, %r13
 16321                             	        adcxq	%rax, %r10
 16322                             	        # A[0] * B[1]
 16323                             	        movq	8(%rbx), %rdx
 16324                             	        mulxq	(%rsi), %rcx, %rax
 16325                             	        adoxq	%rcx, %r9
 16326                             	        # A[2] * B[1]
 16327                             	        mulxq	16(%rsi), %rcx, %r14
 16328                             	        adoxq	%rax, %r10
 16329                             	        adcxq	%rcx, %r11
 16330                             	        # A[1] * B[2]
 16331                             	        movq	16(%rbx), %rdx
 16332                             	        mulxq	8(%rsi), %rcx, %rax
 16333                             	        adcxq	%r14, %r12
 16334                             	        adoxq	%rcx, %r11
 16335                             	        adcxq	%r15, %r13
 16336                             	        adoxq	%rax, %r12
 16337                             	        # A[0] * B[2]
 16338                             	        mulxq	(%rsi), %rcx, %rax
 16339                             	        adoxq	%r15, %r13
 16340                             	        xorq	%r14, %r14
 16341                             	        adcxq	%rcx, %r10
 16342                             	        # A[1] * B[1]
 16343                             	        movq	8(%rbx), %rdx
 16344                             	        mulxq	8(%rsi), %rdx, %rcx
 16345                             	        adcxq	%rax, %r11
 16346                             	        adoxq	%rdx, %r10
 16347                             	        # A[3] * B[1]
 16348                             	        movq	8(%rbx), %rdx
 16349                             	        adoxq	%rcx, %r11
 16350                             	        mulxq	24(%rsi), %rcx, %rax
 16351                             	        adcxq	%rcx, %r12
 16352                             	        # A[2] * B[2]
 16353                             	        movq	16(%rbx), %rdx
 16354                             	        mulxq	16(%rsi), %rdx, %rcx
 16355                             	        adcxq	%rax, %r13
 16356                             	        adoxq	%rdx, %r12
 16357                             	        # A[3] * B[3]
 16358                             	        movq	24(%rbx), %rdx
 16359                             	        adoxq	%rcx, %r13
 16360                             	        mulxq	24(%rsi), %rcx, %rax
 16361                             	        adoxq	%r15, %r14
 16362                             	        adcxq	%rcx, %r14
 16363                             	        # A[0] * B[3]
 16364                             	        mulxq	(%rsi), %rdx, %rcx
 16365                             	        adcxq	%rax, %r15
 16366                             	        xorq	%rax, %rax
 16367                             	        adcxq	%rdx, %r11
 16368                             	        # A[3] * B[0]
 16369                             	        movq	(%rbx), %rdx
 16370                             	        adcxq	%rcx, %r12
 16371                             	        mulxq	24(%rsi), %rdx, %rcx
 16372                             	        adoxq	%rdx, %r11
 16373                             	        adoxq	%rcx, %r12
 16374                             	        # A[2] * B[3]
 16375                             	        movq	24(%rbx), %rdx
 16376                             	        mulxq	16(%rsi), %rdx, %rcx
 16377                             	        adcxq	%rdx, %r13
 16378                             	        # A[3] * B[2]
 16379                             	        movq	16(%rbx), %rdx
 16380                             	        adcxq	%rcx, %r14
 16381                             	        mulxq	24(%rsi), %rcx, %rdx
 16382                             	        adcxq	%rax, %r15
 16383                             	        adoxq	%rcx, %r13
 16384                             	        adoxq	%rdx, %r14
 16385                             	        adoxq	%rax, %r15
 16386                             	        # Reduce
 16387                             	        movq	$0x7fffffffffffffff, %rax
 16388                             	        #  Move top half into t4-t7 and remove top bit from t3
 16389                             	        shldq	$0x01, %r14, %r15
 16390                             	        shldq	$0x01, %r13, %r14
 16391                             	        shldq	$0x01, %r12, %r13
 16392                             	        shldq	$0x01, %r11, %r12
 16393                             	        andq	%rax, %r11
 16394                             	        #  Multiply top half by 19
 16395                             	        movq	$19, %rdx
 16396                             	        xorq	%rax, %rax
 16397                             	        mulxq	%r12, %rcx, %r12
 16398                             	        adcxq	%rcx, %r8
 16399                             	        adoxq	%r12, %r9
 16400                             	        mulxq	%r13, %rcx, %r13
 16401                             	        adcxq	%rcx, %r9
 16402                             	        adoxq	%r13, %r10
 16403                             	        mulxq	%r14, %rcx, %r14
 16404                             	        adcxq	%rcx, %r10
 16405                             	        adoxq	%r14, %r11
 16406                             	        mulxq	%r15, %r15, %rdx
 16407                             	        adcxq	%r15, %r11
 16408                             	        adoxq	%rax, %rdx
 16409                             	        adcxq	%rax, %rdx
 16410                             	        #  Overflow
 16411                             	        shldq	$0x01, %r11, %rdx
 16412                             	        movq	$0x7fffffffffffffff, %rax
 16413                             	        imulq	$19, %rdx, %rcx
 16414                             	        andq	%rax, %r11
 16415                             	        addq	%rcx, %r8
 16416                             	        adcq	$0x00, %r9
 16417                             	        adcq	$0x00, %r10
 16418                             	        adcq	$0x00, %r11
 16419                             	        # Reduce if top bit set
 16420                             	        movq	%r11, %rdx
 16421                             	        sarq	$63, %rdx
 16422                             	        andq	$19, %rdx
 16423                             	        andq	%rax, %r11
 16424                             	        addq	%rdx, %r8
 16425                             	        adcq	$0x00, %r9
 16426                             	        adcq	$0x00, %r10
 16427                             	        adcq	$0x00, %r11
 16428                             	        # Store
 16429                             	        movq	%r8, (%rdi)
 16430                             	        movq	%r9, 8(%rdi)
 16431                             	        movq	%r10, 16(%rdi)
 16432                             	        movq	%r11, 24(%rdi)
 16433                             	        leaq	48(%rsp), %rsi
 16434                             	        # Double
 16435                             	        movq	(%rdi), %r8
 16436                             	        movq	8(%rdi), %r9
 16437                             	        addq	%r8, %r8
 16438                             	        movq	16(%rdi), %r10
 16439                             	        adcq	%r9, %r9
 16440                             	        movq	24(%rdi), %rdx
 16441                             	        adcq	%r10, %r10
 16442                             	        movq	$-19, %rcx
 16443                             	        adcq	%rdx, %rdx
 16444                             	        movq	$0x7fffffffffffffff, %rax
 16445                             	        movq	%rdx, %r11
 16446                             	        sarq	$63, %rdx
 16447                             	        #   Mask the modulus
 16448                             	        andq	%rdx, %rcx
 16449                             	        andq	%rdx, %rax
 16450                             	        #   Sub modulus (if overflow)
 16451                             	        subq	%rcx, %r8
 16452                             	        sbbq	%rdx, %r9
 16453                             	        sbbq	%rdx, %r10
 16454                             	        sbbq	%rax, %r11
 16455                             	        movq	%r8, (%rsi)
 16456                             	        movq	%r9, 8(%rsi)
 16457                             	        movq	%r10, 16(%rsi)
 16458                             	        movq	%r11, 24(%rsi)
 16459                             	        movq	8(%rsp), %rbx
 16460                             	        movq	16(%rsp), %rbp
 16461                             	        # Add
 16462                             	        movq	(%rbp), %r8
 16463                             	        movq	8(%rbp), %r9
 16464                             	        movq	16(%rbp), %r10
 16465                             	        movq	24(%rbp), %rdx
 16466                             	        movq	%r8, %r12
 16467                             	        addq	(%rbx), %r8
 16468                             	        movq	%r9, %r13
 16469                             	        adcq	8(%rbx), %r9
 16470                             	        movq	%r10, %r14
 16471                             	        adcq	16(%rbx), %r10
 16472                             	        movq	%rdx, %r15
 16473                             	        adcq	24(%rbx), %rdx
 16474                             	        movq	$-19, %rcx
 16475                             	        movq	%rdx, %r11
 16476                             	        movq	$0x7fffffffffffffff, %rax
 16477                             	        sarq	$63, %rdx
 16478                             	        #   Mask the modulus
 16479                             	        andq	%rdx, %rcx
 16480                             	        andq	%rdx, %rax
 16481                             	        #   Sub modulus (if overflow)
 16482                             	        subq	%rcx, %r8
 16483                             	        sbbq	%rdx, %r9
 16484                             	        sbbq	%rdx, %r10
 16485                             	        sbbq	%rax, %r11
 16486                             	        # Sub
 16487                             	        subq	(%rbx), %r12
 16488                             	        movq	$0x00, %rdx
 16489                             	        sbbq	8(%rbx), %r13
 16490                             	        movq	$-19, %rcx
 16491                             	        sbbq	16(%rbx), %r14
 16492                             	        movq	$0x7fffffffffffffff, %rax
 16493                             	        sbbq	24(%rbx), %r15
 16494                             	        sbbq	$0x00, %rdx
 16495                             	        #   Mask the modulus
 16496                             	        andq	%rdx, %rcx
 16497                             	        andq	%rdx, %rax
 16498                             	        #   Add modulus (if underflow)
 16499                             	        addq	%rcx, %r12
 16500                             	        adcq	%rdx, %r13
 16501                             	        adcq	%rdx, %r14
 16502                             	        adcq	%rax, %r15
 16503                             	        movq	%r8, (%rbx)
 16504                             	        movq	%r9, 8(%rbx)
 16505                             	        movq	%r10, 16(%rbx)
 16506                             	        movq	%r11, 24(%rbx)
 16507                             	        movq	%r12, (%rdi)
 16508                             	        movq	%r13, 8(%rdi)
 16509                             	        movq	%r14, 16(%rdi)
 16510                             	        movq	%r15, 24(%rdi)
 16511                             	        movq	24(%rsp), %rdi
 16512                             	        # Add
 16513                             	        movq	(%rsi), %r8
 16514                             	        movq	8(%rsi), %r9
 16515                             	        movq	16(%rsi), %r10
 16516                             	        movq	24(%rsi), %rdx
 16517                             	        movq	%r8, %r12
 16518                             	        addq	(%rdi), %r8
 16519                             	        movq	%r9, %r13
 16520                             	        adcq	8(%rdi), %r9
 16521                             	        movq	%r10, %r14
 16522                             	        adcq	16(%rdi), %r10
 16523                             	        movq	%rdx, %r15
 16524                             	        adcq	24(%rdi), %rdx
 16525                             	        movq	$-19, %rcx
 16526                             	        movq	%rdx, %r11
 16527                             	        movq	$0x7fffffffffffffff, %rax
 16528                             	        sarq	$63, %rdx
 16529                             	        #   Mask the modulus
 16530                             	        andq	%rdx, %rcx
 16531                             	        andq	%rdx, %rax
 16532                             	        #   Sub modulus (if overflow)
 16533                             	        subq	%rcx, %r8
 16534                             	        sbbq	%rdx, %r9
 16535                             	        sbbq	%rdx, %r10
 16536                             	        sbbq	%rax, %r11
 16537                             	        # Sub
 16538                             	        subq	(%rdi), %r12
 16539                             	        movq	$0x00, %rdx
 16540                             	        sbbq	8(%rdi), %r13
 16541                             	        movq	$-19, %rcx
 16542                             	        sbbq	16(%rdi), %r14
 16543                             	        movq	$0x7fffffffffffffff, %rax
 16544                             	        sbbq	24(%rdi), %r15
 16545                             	        sbbq	$0x00, %rdx
 16546                             	        #   Mask the modulus
 16547                             	        andq	%rdx, %rcx
 16548                             	        andq	%rdx, %rax
 16549                             	        #   Add modulus (if underflow)
 16550                             	        addq	%rcx, %r12
 16551                             	        adcq	%rdx, %r13
 16552                             	        adcq	%rdx, %r14
 16553                             	        adcq	%rax, %r15
 16554                             	        movq	%r8, (%rdi)
 16555                             	        movq	%r9, 8(%rdi)
 16556                             	        movq	%r10, 16(%rdi)
 16557                             	        movq	%r11, 24(%rdi)
 16558                             	        movq	%r12, (%rbp)
 16559                             	        movq	%r13, 8(%rbp)
 16560                             	        movq	%r14, 16(%rbp)
 16561                             	        movq	%r15, 24(%rbp)
 16562                             	        addq	$0x50, %rsp
 16563                             	        popq	%r15
 16564                             	        popq	%r14
 16565                             	        popq	%r13
 16566                             	        popq	%r12
 16567                             	        popq	%rbp
 16568                             	        popq	%rbx
 16569                             	        repz retq
 16570                             	#ifndef __APPLE__
