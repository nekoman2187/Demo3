   1                             	# 1 "../src/wolfcrypt/src/aes_gcm_asm.S"
   1                             	/* aes_gcm_asm
   0                             	
   0                             	
   0                             	
   2                             	 *
   3                             	 * Copyright (C) 2006-2021 wolfSSL Inc.
   4                             	 *
   5                             	 * This file is part of wolfSSL.
   6                             	 *
   7                             	 * wolfSSL is free software; you can redistribute it and/or modify
   8                             	 * it under the terms of the GNU General Public License as published by
   9                             	 * the Free Software Foundation; either version 2 of the License, or
  10                             	 * (at your option) any later version.
  11                             	 *
  12                             	 * wolfSSL is distributed in the hope that it will be useful,
  13                             	 * but WITHOUT ANY WARRANTY; without even the implied warranty of
  14                             	 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  15                             	 * GNU General Public License for more details.
  16                             	 *
  17                             	 * You should have received a copy of the GNU General Public License
  18                             	 * along with this program; if not, write to the Free Software
  19                             	 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1335, USA
  20                             	 */
  21                             	
  22                             	#ifndef HAVE_INTEL_AVX1
  23                             	#define HAVE_INTEL_AVX1
  24                             	#endif /* HAVE_INTEL_AVX1 */
  25                             	#ifndef NO_AVX2_SUPPORT
  26                             	#define HAVE_INTEL_AVX2
  27                             	#endif /* NO_AVX2_SUPPORT */
  28                             	
  29                             	#ifndef __APPLE__
  30                             	.data
  31                             	#else
  32                             	.section	__DATA,__data
  33                             	#endif /* __APPLE__ */
  34                             	#ifndef __APPLE__
  35                             	.align	16
  36                             	#else
  37                             	.p2align	4
  38                             	#endif /* __APPLE__ */
  39                             	L_aes_gcm_one:
  40 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x1
  40      01 00 00 00 00 00 00 00 
  41                             	#ifndef __APPLE__
  42                             	.data
  43                             	#else
  44                             	.section	__DATA,__data
  45                             	#endif /* __APPLE__ */
  46                             	#ifndef __APPLE__
  47                             	.align	16
  48                             	#else
  49                             	.p2align	4
  50                             	#endif /* __APPLE__ */
  51                             	L_aes_gcm_two:
  52 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x2
  52      02 00 00 00 00 00 00 00 
  53                             	#ifndef __APPLE__
  54                             	.data
  55                             	#else
  56                             	.section	__DATA,__data
  57                             	#endif /* __APPLE__ */
  58                             	#ifndef __APPLE__
  59                             	.align	16
  60                             	#else
  61                             	.p2align	4
  62                             	#endif /* __APPLE__ */
  63                             	L_aes_gcm_three:
  64 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x3
  64      03 00 00 00 00 00 00 00 
  65                             	#ifndef __APPLE__
  66                             	.data
  67                             	#else
  68                             	.section	__DATA,__data
  69                             	#endif /* __APPLE__ */
  70                             	#ifndef __APPLE__
  71                             	.align	16
  72                             	#else
  73                             	.p2align	4
  74                             	#endif /* __APPLE__ */
  75                             	L_aes_gcm_four:
  76 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x4
  76      04 00 00 00 00 00 00 00 
  77                             	#ifndef __APPLE__
  78                             	.data
  79                             	#else
  80                             	.section	__DATA,__data
  81                             	#endif /* __APPLE__ */
  82                             	#ifndef __APPLE__
  83                             	.align	16
  84                             	#else
  85                             	.p2align	4
  86                             	#endif /* __APPLE__ */
  87                             	L_aes_gcm_five:
  88 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x5
  88      05 00 00 00 00 00 00 00 
  89                             	#ifndef __APPLE__
  90                             	.data
  91                             	#else
  92                             	.section	__DATA,__data
  93                             	#endif /* __APPLE__ */
  94                             	#ifndef __APPLE__
  95                             	.align	16
  96                             	#else
  97                             	.p2align	4
  98                             	#endif /* __APPLE__ */
  99                             	L_aes_gcm_six:
 100 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x6
 100      06 00 00 00 00 00 00 00 
 101                             	#ifndef __APPLE__
 102                             	.data
 103                             	#else
 104                             	.section	__DATA,__data
 105                             	#endif /* __APPLE__ */
 106                             	#ifndef __APPLE__
 107                             	.align	16
 108                             	#else
 109                             	.p2align	4
 110                             	#endif /* __APPLE__ */
 111                             	L_aes_gcm_seven:
 112 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x7
 112      07 00 00 00 00 00 00 00 
 113                             	#ifndef __APPLE__
 114                             	.data
 115                             	#else
 116                             	.section	__DATA,__data
 117                             	#endif /* __APPLE__ */
 118                             	#ifndef __APPLE__
 119                             	.align	16
 120                             	#else
 121                             	.p2align	4
 122                             	#endif /* __APPLE__ */
 123                             	L_aes_gcm_eight:
 124 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x8
 124      08 00 00 00 00 00 00 00 
 125                             	#ifndef __APPLE__
 126                             	.data
 127                             	#else
 128                             	.section	__DATA,__data
 129                             	#endif /* __APPLE__ */
 130                             	#ifndef __APPLE__
 131                             	.align	16
 132                             	#else
 133                             	.p2align	4
 134                             	#endif /* __APPLE__ */
 135                             	L_aes_gcm_bswap_epi64:
 136 ???? 07 06 05 04 03 02 01 00 	.quad	0x1020304050607, 0x8090a0b0c0d0e0f
 136      0F 0E 0D 0C 0B 0A 09 08 
 137                             	#ifndef __APPLE__
 138                             	.data
 139                             	#else
 140                             	.section	__DATA,__data
 141                             	#endif /* __APPLE__ */
 142                             	#ifndef __APPLE__
 143                             	.align	16
 144                             	#else
 145                             	.p2align	4
 146                             	#endif /* __APPLE__ */
 147                             	L_aes_gcm_bswap_mask:
 148 ???? 0F 0E 0D 0C 0B 0A 09 08 	.quad	0x8090a0b0c0d0e0f, 0x1020304050607
 148      07 06 05 04 03 02 01 00 
 149                             	#ifndef __APPLE__
 150                             	.data
 151                             	#else
 152                             	.section	__DATA,__data
 153                             	#endif /* __APPLE__ */
 154                             	#ifndef __APPLE__
 155                             	.align	16
 156                             	#else
 157                             	.p2align	4
 158                             	#endif /* __APPLE__ */
 159                             	L_aes_gcm_mod2_128:
 160 ???? 01 00 00 00 00 00 00 00 	.quad	0x1, 0xc200000000000000
 160      00 00 00 00 00 00 00 C2 
 161                             	#ifndef __APPLE__
 162                             	.text
 163                             	.globl	AES_GCM_encrypt
 165                             	.align	16
 166                             	AES_GCM_encrypt:
 167                             	#else
 168                             	.section	__TEXT,__text
 169                             	.globl	_AES_GCM_encrypt
 170                             	.p2align	4
 171                             	_AES_GCM_encrypt:
 172                             	#endif /* __APPLE__ */
 173                             	        pushq	%r13
 174                             	        pushq	%r12
 175                             	        pushq	%rbx
 176                             	        pushq	%r14
 177                             	        pushq	%r15
 178                             	        movq	%rdx, %r12
 179                             	        movq	%rcx, %rax
 180                             	        movl	48(%rsp), %r11d
 181                             	        movl	56(%rsp), %ebx
 182                             	        movl	64(%rsp), %r14d
 183                             	        movq	72(%rsp), %r15
 184                             	        movl	80(%rsp), %r10d
 185                             	        subq	$0xa0, %rsp
 186                             	        pxor	%xmm4, %xmm4
 187                             	        pxor	%xmm6, %xmm6
 188                             	        cmpl	$12, %ebx
 189                             	        movl	%ebx, %edx
 190                             	        jne	L_AES_GCM_encrypt_iv_not_12
 191                             	        # # Calculate values when IV is 12 bytes
 192                             	        # Set counter based on IV
 193                             	        movl	$0x1000000, %ecx
 194                             	        pinsrq	$0x00, (%rax), %xmm4
 195                             	        pinsrd	$2, 8(%rax), %xmm4
 196                             	        pinsrd	$3, %ecx, %xmm4
 197                             	        # H = Encrypt X(=0) and T = Encrypt counter
 198                             	        movdqa	%xmm4, %xmm1
 199                             	        movdqa	(%r15), %xmm5
 200                             	        pxor	%xmm5, %xmm1
 201                             	        movdqa	16(%r15), %xmm7
 202                             	        aesenc	%xmm7, %xmm5
 203                             	        aesenc	%xmm7, %xmm1
 204                             	        movdqa	32(%r15), %xmm7
 205                             	        aesenc	%xmm7, %xmm5
 206                             	        aesenc	%xmm7, %xmm1
 207                             	        movdqa	48(%r15), %xmm7
 208                             	        aesenc	%xmm7, %xmm5
 209                             	        aesenc	%xmm7, %xmm1
 210                             	        movdqa	64(%r15), %xmm7
 211                             	        aesenc	%xmm7, %xmm5
 212                             	        aesenc	%xmm7, %xmm1
 213                             	        movdqa	80(%r15), %xmm7
 214                             	        aesenc	%xmm7, %xmm5
 215                             	        aesenc	%xmm7, %xmm1
 216                             	        movdqa	96(%r15), %xmm7
 217                             	        aesenc	%xmm7, %xmm5
 218                             	        aesenc	%xmm7, %xmm1
 219                             	        movdqa	112(%r15), %xmm7
 220                             	        aesenc	%xmm7, %xmm5
 221                             	        aesenc	%xmm7, %xmm1
 222                             	        movdqa	128(%r15), %xmm7
 223                             	        aesenc	%xmm7, %xmm5
 224                             	        aesenc	%xmm7, %xmm1
 225                             	        movdqa	144(%r15), %xmm7
 226                             	        aesenc	%xmm7, %xmm5
 227                             	        aesenc	%xmm7, %xmm1
 228                             	        cmpl	$11, %r10d
 229                             	        movdqa	160(%r15), %xmm7
 230                             	        jl	L_AES_GCM_encrypt_calc_iv_12_last
 231                             	        aesenc	%xmm7, %xmm5
 232                             	        aesenc	%xmm7, %xmm1
 233                             	        movdqa	176(%r15), %xmm7
 234                             	        aesenc	%xmm7, %xmm5
 235                             	        aesenc	%xmm7, %xmm1
 236                             	        cmpl	$13, %r10d
 237                             	        movdqa	192(%r15), %xmm7
 238                             	        jl	L_AES_GCM_encrypt_calc_iv_12_last
 239                             	        aesenc	%xmm7, %xmm5
 240                             	        aesenc	%xmm7, %xmm1
 241                             	        movdqa	208(%r15), %xmm7
 242                             	        aesenc	%xmm7, %xmm5
 243                             	        aesenc	%xmm7, %xmm1
 244                             	        movdqa	224(%r15), %xmm7
 245                             	L_AES_GCM_encrypt_calc_iv_12_last:
 246                             	        aesenclast	%xmm7, %xmm5
 247                             	        aesenclast	%xmm7, %xmm1
 248                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm5
 249                             	        movdqa	%xmm1, 144(%rsp)
 250                             	        jmp	L_AES_GCM_encrypt_iv_done
 251                             	L_AES_GCM_encrypt_iv_not_12:
 252                             	        # Calculate values when IV is not 12 bytes
 253                             	        # H = Encrypt X(=0)
 254                             	        movdqa	(%r15), %xmm5
 255                             	        aesenc	16(%r15), %xmm5
 256                             	        aesenc	32(%r15), %xmm5
 257                             	        aesenc	48(%r15), %xmm5
 258                             	        aesenc	64(%r15), %xmm5
 259                             	        aesenc	80(%r15), %xmm5
 260                             	        aesenc	96(%r15), %xmm5
 261                             	        aesenc	112(%r15), %xmm5
 262                             	        aesenc	128(%r15), %xmm5
 263                             	        aesenc	144(%r15), %xmm5
 264                             	        cmpl	$11, %r10d
 265                             	        movdqa	160(%r15), %xmm9
 266                             	        jl	L_AES_GCM_encrypt_calc_iv_1_aesenc_avx_last
 267                             	        aesenc	%xmm9, %xmm5
 268                             	        aesenc	176(%r15), %xmm5
 269                             	        cmpl	$13, %r10d
 270                             	        movdqa	192(%r15), %xmm9
 271                             	        jl	L_AES_GCM_encrypt_calc_iv_1_aesenc_avx_last
 272                             	        aesenc	%xmm9, %xmm5
 273                             	        aesenc	208(%r15), %xmm5
 274                             	        movdqa	224(%r15), %xmm9
 275                             	L_AES_GCM_encrypt_calc_iv_1_aesenc_avx_last:
 276                             	        aesenclast	%xmm9, %xmm5
 277                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm5
 278                             	        # Calc counter
 279                             	        # Initialization vector
 280                             	        cmpl	$0x00, %edx
 281                             	        movq	$0x00, %rcx
 282                             	        je	L_AES_GCM_encrypt_calc_iv_done
 283                             	        cmpl	$16, %edx
 284                             	        jl	L_AES_GCM_encrypt_calc_iv_lt16
 285                             	        andl	$0xfffffff0, %edx
 286                             	L_AES_GCM_encrypt_calc_iv_16_loop:
 287                             	        movdqu	(%rax,%rcx,1), %xmm8
 288                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 289                             	        pxor	%xmm8, %xmm4
 290                             	        pshufd	$0x4e, %xmm4, %xmm1
 291                             	        pshufd	$0x4e, %xmm5, %xmm2
 292                             	        movdqa	%xmm5, %xmm3
 293                             	        movdqa	%xmm5, %xmm0
 294                             	        pclmulqdq	$0x11, %xmm4, %xmm3
 295                             	        pclmulqdq	$0x00, %xmm4, %xmm0
 296                             	        pxor	%xmm4, %xmm1
 297                             	        pxor	%xmm5, %xmm2
 298                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 299                             	        pxor	%xmm0, %xmm1
 300                             	        pxor	%xmm3, %xmm1
 301                             	        movdqa	%xmm1, %xmm2
 302                             	        movdqa	%xmm0, %xmm7
 303                             	        movdqa	%xmm3, %xmm4
 304                             	        pslldq	$8, %xmm2
 305                             	        psrldq	$8, %xmm1
 306                             	        pxor	%xmm2, %xmm7
 307                             	        pxor	%xmm1, %xmm4
 308                             	        movdqa	%xmm7, %xmm0
 309                             	        movdqa	%xmm4, %xmm1
 310                             	        psrld	$31, %xmm0
 311                             	        psrld	$31, %xmm1
 312                             	        pslld	$0x01, %xmm7
 313                             	        pslld	$0x01, %xmm4
 314                             	        movdqa	%xmm0, %xmm2
 315                             	        pslldq	$4, %xmm0
 316                             	        psrldq	$12, %xmm2
 317                             	        pslldq	$4, %xmm1
 318                             	        por	%xmm2, %xmm4
 319                             	        por	%xmm0, %xmm7
 320                             	        por	%xmm1, %xmm4
 321                             	        movdqa	%xmm7, %xmm0
 322                             	        movdqa	%xmm7, %xmm1
 323                             	        movdqa	%xmm7, %xmm2
 324                             	        pslld	$31, %xmm0
 325                             	        pslld	$30, %xmm1
 326                             	        pslld	$25, %xmm2
 327                             	        pxor	%xmm1, %xmm0
 328                             	        pxor	%xmm2, %xmm0
 329                             	        movdqa	%xmm0, %xmm1
 330                             	        psrldq	$4, %xmm1
 331                             	        pslldq	$12, %xmm0
 332                             	        pxor	%xmm0, %xmm7
 333                             	        movdqa	%xmm7, %xmm2
 334                             	        movdqa	%xmm7, %xmm3
 335                             	        movdqa	%xmm7, %xmm0
 336                             	        psrld	$0x01, %xmm2
 337                             	        psrld	$2, %xmm3
 338                             	        psrld	$7, %xmm0
 339                             	        pxor	%xmm3, %xmm2
 340                             	        pxor	%xmm0, %xmm2
 341                             	        pxor	%xmm1, %xmm2
 342                             	        pxor	%xmm7, %xmm2
 343                             	        pxor	%xmm2, %xmm4
 344                             	        addl	$16, %ecx
 345                             	        cmpl	%edx, %ecx
 346                             	        jl	L_AES_GCM_encrypt_calc_iv_16_loop
 347                             	        movl	%ebx, %edx
 348                             	        cmpl	%edx, %ecx
 349                             	        je	L_AES_GCM_encrypt_calc_iv_done
 350                             	L_AES_GCM_encrypt_calc_iv_lt16:
 351                             	        subq	$16, %rsp
 352                             	        pxor	%xmm8, %xmm8
 353                             	        xorl	%ebx, %ebx
 354                             	        movdqa	%xmm8, (%rsp)
 355                             	L_AES_GCM_encrypt_calc_iv_loop:
 356                             	        movzbl	(%rax,%rcx,1), %r13d
 357                             	        movb	%r13b, (%rsp,%rbx,1)
 358                             	        incl	%ecx
 359                             	        incl	%ebx
 360                             	        cmpl	%edx, %ecx
 361                             	        jl	L_AES_GCM_encrypt_calc_iv_loop
 362                             	        movdqa	(%rsp), %xmm8
 363                             	        addq	$16, %rsp
 364                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 365                             	        pxor	%xmm8, %xmm4
 366                             	        pshufd	$0x4e, %xmm4, %xmm1
 367                             	        pshufd	$0x4e, %xmm5, %xmm2
 368                             	        movdqa	%xmm5, %xmm3
 369                             	        movdqa	%xmm5, %xmm0
 370                             	        pclmulqdq	$0x11, %xmm4, %xmm3
 371                             	        pclmulqdq	$0x00, %xmm4, %xmm0
 372                             	        pxor	%xmm4, %xmm1
 373                             	        pxor	%xmm5, %xmm2
 374                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 375                             	        pxor	%xmm0, %xmm1
 376                             	        pxor	%xmm3, %xmm1
 377                             	        movdqa	%xmm1, %xmm2
 378                             	        movdqa	%xmm0, %xmm7
 379                             	        movdqa	%xmm3, %xmm4
 380                             	        pslldq	$8, %xmm2
 381                             	        psrldq	$8, %xmm1
 382                             	        pxor	%xmm2, %xmm7
 383                             	        pxor	%xmm1, %xmm4
 384                             	        movdqa	%xmm7, %xmm0
 385                             	        movdqa	%xmm4, %xmm1
 386                             	        psrld	$31, %xmm0
 387                             	        psrld	$31, %xmm1
 388                             	        pslld	$0x01, %xmm7
 389                             	        pslld	$0x01, %xmm4
 390                             	        movdqa	%xmm0, %xmm2
 391                             	        pslldq	$4, %xmm0
 392                             	        psrldq	$12, %xmm2
 393                             	        pslldq	$4, %xmm1
 394                             	        por	%xmm2, %xmm4
 395                             	        por	%xmm0, %xmm7
 396                             	        por	%xmm1, %xmm4
 397                             	        movdqa	%xmm7, %xmm0
 398                             	        movdqa	%xmm7, %xmm1
 399                             	        movdqa	%xmm7, %xmm2
 400                             	        pslld	$31, %xmm0
 401                             	        pslld	$30, %xmm1
 402                             	        pslld	$25, %xmm2
 403                             	        pxor	%xmm1, %xmm0
 404                             	        pxor	%xmm2, %xmm0
 405                             	        movdqa	%xmm0, %xmm1
 406                             	        psrldq	$4, %xmm1
 407                             	        pslldq	$12, %xmm0
 408                             	        pxor	%xmm0, %xmm7
 409                             	        movdqa	%xmm7, %xmm2
 410                             	        movdqa	%xmm7, %xmm3
 411                             	        movdqa	%xmm7, %xmm0
 412                             	        psrld	$0x01, %xmm2
 413                             	        psrld	$2, %xmm3
 414                             	        psrld	$7, %xmm0
 415                             	        pxor	%xmm3, %xmm2
 416                             	        pxor	%xmm0, %xmm2
 417                             	        pxor	%xmm1, %xmm2
 418                             	        pxor	%xmm7, %xmm2
 419                             	        pxor	%xmm2, %xmm4
 420                             	L_AES_GCM_encrypt_calc_iv_done:
 421                             	        # T = Encrypt counter
 422                             	        pxor	%xmm0, %xmm0
 423                             	        shll	$3, %edx
 424                             	        pinsrq	$0x00, %rdx, %xmm0
 425                             	        pxor	%xmm0, %xmm4
 426                             	        pshufd	$0x4e, %xmm4, %xmm1
 427                             	        pshufd	$0x4e, %xmm5, %xmm2
 428                             	        movdqa	%xmm5, %xmm3
 429                             	        movdqa	%xmm5, %xmm0
 430                             	        pclmulqdq	$0x11, %xmm4, %xmm3
 431                             	        pclmulqdq	$0x00, %xmm4, %xmm0
 432                             	        pxor	%xmm4, %xmm1
 433                             	        pxor	%xmm5, %xmm2
 434                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 435                             	        pxor	%xmm0, %xmm1
 436                             	        pxor	%xmm3, %xmm1
 437                             	        movdqa	%xmm1, %xmm2
 438                             	        movdqa	%xmm0, %xmm7
 439                             	        movdqa	%xmm3, %xmm4
 440                             	        pslldq	$8, %xmm2
 441                             	        psrldq	$8, %xmm1
 442                             	        pxor	%xmm2, %xmm7
 443                             	        pxor	%xmm1, %xmm4
 444                             	        movdqa	%xmm7, %xmm0
 445                             	        movdqa	%xmm4, %xmm1
 446                             	        psrld	$31, %xmm0
 447                             	        psrld	$31, %xmm1
 448                             	        pslld	$0x01, %xmm7
 449                             	        pslld	$0x01, %xmm4
 450                             	        movdqa	%xmm0, %xmm2
 451                             	        pslldq	$4, %xmm0
 452                             	        psrldq	$12, %xmm2
 453                             	        pslldq	$4, %xmm1
 454                             	        por	%xmm2, %xmm4
 455                             	        por	%xmm0, %xmm7
 456                             	        por	%xmm1, %xmm4
 457                             	        movdqa	%xmm7, %xmm0
 458                             	        movdqa	%xmm7, %xmm1
 459                             	        movdqa	%xmm7, %xmm2
 460                             	        pslld	$31, %xmm0
 461                             	        pslld	$30, %xmm1
 462                             	        pslld	$25, %xmm2
 463                             	        pxor	%xmm1, %xmm0
 464                             	        pxor	%xmm2, %xmm0
 465                             	        movdqa	%xmm0, %xmm1
 466                             	        psrldq	$4, %xmm1
 467                             	        pslldq	$12, %xmm0
 468                             	        pxor	%xmm0, %xmm7
 469                             	        movdqa	%xmm7, %xmm2
 470                             	        movdqa	%xmm7, %xmm3
 471                             	        movdqa	%xmm7, %xmm0
 472                             	        psrld	$0x01, %xmm2
 473                             	        psrld	$2, %xmm3
 474                             	        psrld	$7, %xmm0
 475                             	        pxor	%xmm3, %xmm2
 476                             	        pxor	%xmm0, %xmm2
 477                             	        pxor	%xmm1, %xmm2
 478                             	        pxor	%xmm7, %xmm2
 479                             	        pxor	%xmm2, %xmm4
 480                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm4
 481                             	        #   Encrypt counter
 482                             	        movdqa	(%r15), %xmm8
 483                             	        pxor	%xmm4, %xmm8
 484                             	        aesenc	16(%r15), %xmm8
 485                             	        aesenc	32(%r15), %xmm8
 486                             	        aesenc	48(%r15), %xmm8
 487                             	        aesenc	64(%r15), %xmm8
 488                             	        aesenc	80(%r15), %xmm8
 489                             	        aesenc	96(%r15), %xmm8
 490                             	        aesenc	112(%r15), %xmm8
 491                             	        aesenc	128(%r15), %xmm8
 492                             	        aesenc	144(%r15), %xmm8
 493                             	        cmpl	$11, %r10d
 494                             	        movdqa	160(%r15), %xmm9
 495                             	        jl	L_AES_GCM_encrypt_calc_iv_2_aesenc_avx_last
 496                             	        aesenc	%xmm9, %xmm8
 497                             	        aesenc	176(%r15), %xmm8
 498                             	        cmpl	$13, %r10d
 499                             	        movdqa	192(%r15), %xmm9
 500                             	        jl	L_AES_GCM_encrypt_calc_iv_2_aesenc_avx_last
 501                             	        aesenc	%xmm9, %xmm8
 502                             	        aesenc	208(%r15), %xmm8
 503                             	        movdqa	224(%r15), %xmm9
 504                             	L_AES_GCM_encrypt_calc_iv_2_aesenc_avx_last:
 505                             	        aesenclast	%xmm9, %xmm8
 506                             	        movdqa	%xmm8, 144(%rsp)
 507                             	L_AES_GCM_encrypt_iv_done:
 508                             	        # Additional authentication data
 509                             	        movl	%r11d, %edx
 510                             	        cmpl	$0x00, %edx
 511                             	        je	L_AES_GCM_encrypt_calc_aad_done
 512                             	        xorl	%ecx, %ecx
 513                             	        cmpl	$16, %edx
 514                             	        jl	L_AES_GCM_encrypt_calc_aad_lt16
 515                             	        andl	$0xfffffff0, %edx
 516                             	L_AES_GCM_encrypt_calc_aad_16_loop:
 517                             	        movdqu	(%r12,%rcx,1), %xmm8
 518                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 519                             	        pxor	%xmm8, %xmm6
 520                             	        pshufd	$0x4e, %xmm6, %xmm1
 521                             	        pshufd	$0x4e, %xmm5, %xmm2
 522                             	        movdqa	%xmm5, %xmm3
 523                             	        movdqa	%xmm5, %xmm0
 524                             	        pclmulqdq	$0x11, %xmm6, %xmm3
 525                             	        pclmulqdq	$0x00, %xmm6, %xmm0
 526                             	        pxor	%xmm6, %xmm1
 527                             	        pxor	%xmm5, %xmm2
 528                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 529                             	        pxor	%xmm0, %xmm1
 530                             	        pxor	%xmm3, %xmm1
 531                             	        movdqa	%xmm1, %xmm2
 532                             	        movdqa	%xmm0, %xmm7
 533                             	        movdqa	%xmm3, %xmm6
 534                             	        pslldq	$8, %xmm2
 535                             	        psrldq	$8, %xmm1
 536                             	        pxor	%xmm2, %xmm7
 537                             	        pxor	%xmm1, %xmm6
 538                             	        movdqa	%xmm7, %xmm0
 539                             	        movdqa	%xmm6, %xmm1
 540                             	        psrld	$31, %xmm0
 541                             	        psrld	$31, %xmm1
 542                             	        pslld	$0x01, %xmm7
 543                             	        pslld	$0x01, %xmm6
 544                             	        movdqa	%xmm0, %xmm2
 545                             	        pslldq	$4, %xmm0
 546                             	        psrldq	$12, %xmm2
 547                             	        pslldq	$4, %xmm1
 548                             	        por	%xmm2, %xmm6
 549                             	        por	%xmm0, %xmm7
 550                             	        por	%xmm1, %xmm6
 551                             	        movdqa	%xmm7, %xmm0
 552                             	        movdqa	%xmm7, %xmm1
 553                             	        movdqa	%xmm7, %xmm2
 554                             	        pslld	$31, %xmm0
 555                             	        pslld	$30, %xmm1
 556                             	        pslld	$25, %xmm2
 557                             	        pxor	%xmm1, %xmm0
 558                             	        pxor	%xmm2, %xmm0
 559                             	        movdqa	%xmm0, %xmm1
 560                             	        psrldq	$4, %xmm1
 561                             	        pslldq	$12, %xmm0
 562                             	        pxor	%xmm0, %xmm7
 563                             	        movdqa	%xmm7, %xmm2
 564                             	        movdqa	%xmm7, %xmm3
 565                             	        movdqa	%xmm7, %xmm0
 566                             	        psrld	$0x01, %xmm2
 567                             	        psrld	$2, %xmm3
 568                             	        psrld	$7, %xmm0
 569                             	        pxor	%xmm3, %xmm2
 570                             	        pxor	%xmm0, %xmm2
 571                             	        pxor	%xmm1, %xmm2
 572                             	        pxor	%xmm7, %xmm2
 573                             	        pxor	%xmm2, %xmm6
 574                             	        addl	$16, %ecx
 575                             	        cmpl	%edx, %ecx
 576                             	        jl	L_AES_GCM_encrypt_calc_aad_16_loop
 577                             	        movl	%r11d, %edx
 578                             	        cmpl	%edx, %ecx
 579                             	        je	L_AES_GCM_encrypt_calc_aad_done
 580                             	L_AES_GCM_encrypt_calc_aad_lt16:
 581                             	        subq	$16, %rsp
 582                             	        pxor	%xmm8, %xmm8
 583                             	        xorl	%ebx, %ebx
 584                             	        movdqa	%xmm8, (%rsp)
 585                             	L_AES_GCM_encrypt_calc_aad_loop:
 586                             	        movzbl	(%r12,%rcx,1), %r13d
 587                             	        movb	%r13b, (%rsp,%rbx,1)
 588                             	        incl	%ecx
 589                             	        incl	%ebx
 590                             	        cmpl	%edx, %ecx
 591                             	        jl	L_AES_GCM_encrypt_calc_aad_loop
 592                             	        movdqa	(%rsp), %xmm8
 593                             	        addq	$16, %rsp
 594                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 595                             	        pxor	%xmm8, %xmm6
 596                             	        pshufd	$0x4e, %xmm6, %xmm1
 597                             	        pshufd	$0x4e, %xmm5, %xmm2
 598                             	        movdqa	%xmm5, %xmm3
 599                             	        movdqa	%xmm5, %xmm0
 600                             	        pclmulqdq	$0x11, %xmm6, %xmm3
 601                             	        pclmulqdq	$0x00, %xmm6, %xmm0
 602                             	        pxor	%xmm6, %xmm1
 603                             	        pxor	%xmm5, %xmm2
 604                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 605                             	        pxor	%xmm0, %xmm1
 606                             	        pxor	%xmm3, %xmm1
 607                             	        movdqa	%xmm1, %xmm2
 608                             	        movdqa	%xmm0, %xmm7
 609                             	        movdqa	%xmm3, %xmm6
 610                             	        pslldq	$8, %xmm2
 611                             	        psrldq	$8, %xmm1
 612                             	        pxor	%xmm2, %xmm7
 613                             	        pxor	%xmm1, %xmm6
 614                             	        movdqa	%xmm7, %xmm0
 615                             	        movdqa	%xmm6, %xmm1
 616                             	        psrld	$31, %xmm0
 617                             	        psrld	$31, %xmm1
 618                             	        pslld	$0x01, %xmm7
 619                             	        pslld	$0x01, %xmm6
 620                             	        movdqa	%xmm0, %xmm2
 621                             	        pslldq	$4, %xmm0
 622                             	        psrldq	$12, %xmm2
 623                             	        pslldq	$4, %xmm1
 624                             	        por	%xmm2, %xmm6
 625                             	        por	%xmm0, %xmm7
 626                             	        por	%xmm1, %xmm6
 627                             	        movdqa	%xmm7, %xmm0
 628                             	        movdqa	%xmm7, %xmm1
 629                             	        movdqa	%xmm7, %xmm2
 630                             	        pslld	$31, %xmm0
 631                             	        pslld	$30, %xmm1
 632                             	        pslld	$25, %xmm2
 633                             	        pxor	%xmm1, %xmm0
 634                             	        pxor	%xmm2, %xmm0
 635                             	        movdqa	%xmm0, %xmm1
 636                             	        psrldq	$4, %xmm1
 637                             	        pslldq	$12, %xmm0
 638                             	        pxor	%xmm0, %xmm7
 639                             	        movdqa	%xmm7, %xmm2
 640                             	        movdqa	%xmm7, %xmm3
 641                             	        movdqa	%xmm7, %xmm0
 642                             	        psrld	$0x01, %xmm2
 643                             	        psrld	$2, %xmm3
 644                             	        psrld	$7, %xmm0
 645                             	        pxor	%xmm3, %xmm2
 646                             	        pxor	%xmm0, %xmm2
 647                             	        pxor	%xmm1, %xmm2
 648                             	        pxor	%xmm7, %xmm2
 649                             	        pxor	%xmm2, %xmm6
 650                             	L_AES_GCM_encrypt_calc_aad_done:
 651                             	        # Calculate counter and H
 652                             	        pshufb	L_aes_gcm_bswap_epi64(%rip), %xmm4
 653                             	        movdqa	%xmm5, %xmm9
 654                             	        paddd	L_aes_gcm_one(%rip), %xmm4
 655                             	        movdqa	%xmm5, %xmm8
 656                             	        movdqa	%xmm4, 128(%rsp)
 657                             	        psrlq	$63, %xmm9
 658                             	        psllq	$0x01, %xmm8
 659                             	        pslldq	$8, %xmm9
 660                             	        por	%xmm9, %xmm8
 661                             	        pshufd	$0xff, %xmm5, %xmm5
 662                             	        psrad	$31, %xmm5
 663                             	        pand	L_aes_gcm_mod2_128(%rip), %xmm5
 664                             	        pxor	%xmm8, %xmm5
 665                             	        xorq	%rbx, %rbx
 666                             	        cmpl	$0x80, %r9d
 667                             	        movl	%r9d, %r13d
 668                             	        jl	L_AES_GCM_encrypt_done_128
 669                             	        andl	$0xffffff80, %r13d
 670                             	        movdqa	%xmm6, %xmm2
 671                             	        # H ^ 1
 672                             	        movdqa	%xmm5, (%rsp)
 673                             	        # H ^ 2
 674                             	        pshufd	$0x4e, %xmm5, %xmm9
 675                             	        pshufd	$0x4e, %xmm5, %xmm10
 676                             	        movdqa	%xmm5, %xmm11
 677                             	        movdqa	%xmm5, %xmm8
 678                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 679                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 680                             	        pxor	%xmm5, %xmm9
 681                             	        pxor	%xmm5, %xmm10
 682                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 683                             	        pxor	%xmm8, %xmm9
 684                             	        pxor	%xmm11, %xmm9
 685                             	        movdqa	%xmm9, %xmm10
 686                             	        movdqa	%xmm11, %xmm0
 687                             	        pslldq	$8, %xmm10
 688                             	        psrldq	$8, %xmm9
 689                             	        pxor	%xmm10, %xmm8
 690                             	        pxor	%xmm9, %xmm0
 691                             	        movdqa	%xmm8, %xmm12
 692                             	        movdqa	%xmm8, %xmm13
 693                             	        movdqa	%xmm8, %xmm14
 694                             	        pslld	$31, %xmm12
 695                             	        pslld	$30, %xmm13
 696                             	        pslld	$25, %xmm14
 697                             	        pxor	%xmm13, %xmm12
 698                             	        pxor	%xmm14, %xmm12
 699                             	        movdqa	%xmm12, %xmm13
 700                             	        psrldq	$4, %xmm13
 701                             	        pslldq	$12, %xmm12
 702                             	        pxor	%xmm12, %xmm8
 703                             	        movdqa	%xmm8, %xmm14
 704                             	        movdqa	%xmm8, %xmm10
 705                             	        movdqa	%xmm8, %xmm9
 706                             	        psrld	$0x01, %xmm14
 707                             	        psrld	$2, %xmm10
 708                             	        psrld	$7, %xmm9
 709                             	        pxor	%xmm10, %xmm14
 710                             	        pxor	%xmm9, %xmm14
 711                             	        pxor	%xmm13, %xmm14
 712                             	        pxor	%xmm8, %xmm14
 713                             	        pxor	%xmm14, %xmm0
 714                             	        movdqa	%xmm0, 16(%rsp)
 715                             	        # H ^ 3
 716                             	        pshufd	$0x4e, %xmm5, %xmm9
 717                             	        pshufd	$0x4e, %xmm0, %xmm10
 718                             	        movdqa	%xmm0, %xmm11
 719                             	        movdqa	%xmm0, %xmm8
 720                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 721                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 722                             	        pxor	%xmm5, %xmm9
 723                             	        pxor	%xmm0, %xmm10
 724                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 725                             	        pxor	%xmm8, %xmm9
 726                             	        pxor	%xmm11, %xmm9
 727                             	        movdqa	%xmm9, %xmm10
 728                             	        movdqa	%xmm11, %xmm1
 729                             	        pslldq	$8, %xmm10
 730                             	        psrldq	$8, %xmm9
 731                             	        pxor	%xmm10, %xmm8
 732                             	        pxor	%xmm9, %xmm1
 733                             	        movdqa	%xmm8, %xmm12
 734                             	        movdqa	%xmm8, %xmm13
 735                             	        movdqa	%xmm8, %xmm14
 736                             	        pslld	$31, %xmm12
 737                             	        pslld	$30, %xmm13
 738                             	        pslld	$25, %xmm14
 739                             	        pxor	%xmm13, %xmm12
 740                             	        pxor	%xmm14, %xmm12
 741                             	        movdqa	%xmm12, %xmm13
 742                             	        psrldq	$4, %xmm13
 743                             	        pslldq	$12, %xmm12
 744                             	        pxor	%xmm12, %xmm8
 745                             	        movdqa	%xmm8, %xmm14
 746                             	        movdqa	%xmm8, %xmm10
 747                             	        movdqa	%xmm8, %xmm9
 748                             	        psrld	$0x01, %xmm14
 749                             	        psrld	$2, %xmm10
 750                             	        psrld	$7, %xmm9
 751                             	        pxor	%xmm10, %xmm14
 752                             	        pxor	%xmm9, %xmm14
 753                             	        pxor	%xmm13, %xmm14
 754                             	        pxor	%xmm8, %xmm14
 755                             	        pxor	%xmm14, %xmm1
 756                             	        movdqa	%xmm1, 32(%rsp)
 757                             	        # H ^ 4
 758                             	        pshufd	$0x4e, %xmm0, %xmm9
 759                             	        pshufd	$0x4e, %xmm0, %xmm10
 760                             	        movdqa	%xmm0, %xmm11
 761                             	        movdqa	%xmm0, %xmm8
 762                             	        pclmulqdq	$0x11, %xmm0, %xmm11
 763                             	        pclmulqdq	$0x00, %xmm0, %xmm8
 764                             	        pxor	%xmm0, %xmm9
 765                             	        pxor	%xmm0, %xmm10
 766                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 767                             	        pxor	%xmm8, %xmm9
 768                             	        pxor	%xmm11, %xmm9
 769                             	        movdqa	%xmm9, %xmm10
 770                             	        movdqa	%xmm11, %xmm3
 771                             	        pslldq	$8, %xmm10
 772                             	        psrldq	$8, %xmm9
 773                             	        pxor	%xmm10, %xmm8
 774                             	        pxor	%xmm9, %xmm3
 775                             	        movdqa	%xmm8, %xmm12
 776                             	        movdqa	%xmm8, %xmm13
 777                             	        movdqa	%xmm8, %xmm14
 778                             	        pslld	$31, %xmm12
 779                             	        pslld	$30, %xmm13
 780                             	        pslld	$25, %xmm14
 781                             	        pxor	%xmm13, %xmm12
 782                             	        pxor	%xmm14, %xmm12
 783                             	        movdqa	%xmm12, %xmm13
 784                             	        psrldq	$4, %xmm13
 785                             	        pslldq	$12, %xmm12
 786                             	        pxor	%xmm12, %xmm8
 787                             	        movdqa	%xmm8, %xmm14
 788                             	        movdqa	%xmm8, %xmm10
 789                             	        movdqa	%xmm8, %xmm9
 790                             	        psrld	$0x01, %xmm14
 791                             	        psrld	$2, %xmm10
 792                             	        psrld	$7, %xmm9
 793                             	        pxor	%xmm10, %xmm14
 794                             	        pxor	%xmm9, %xmm14
 795                             	        pxor	%xmm13, %xmm14
 796                             	        pxor	%xmm8, %xmm14
 797                             	        pxor	%xmm14, %xmm3
 798                             	        movdqa	%xmm3, 48(%rsp)
 799                             	        # H ^ 5
 800                             	        pshufd	$0x4e, %xmm0, %xmm9
 801                             	        pshufd	$0x4e, %xmm1, %xmm10
 802                             	        movdqa	%xmm1, %xmm11
 803                             	        movdqa	%xmm1, %xmm8
 804                             	        pclmulqdq	$0x11, %xmm0, %xmm11
 805                             	        pclmulqdq	$0x00, %xmm0, %xmm8
 806                             	        pxor	%xmm0, %xmm9
 807                             	        pxor	%xmm1, %xmm10
 808                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 809                             	        pxor	%xmm8, %xmm9
 810                             	        pxor	%xmm11, %xmm9
 811                             	        movdqa	%xmm9, %xmm10
 812                             	        movdqa	%xmm11, %xmm7
 813                             	        pslldq	$8, %xmm10
 814                             	        psrldq	$8, %xmm9
 815                             	        pxor	%xmm10, %xmm8
 816                             	        pxor	%xmm9, %xmm7
 817                             	        movdqa	%xmm8, %xmm12
 818                             	        movdqa	%xmm8, %xmm13
 819                             	        movdqa	%xmm8, %xmm14
 820                             	        pslld	$31, %xmm12
 821                             	        pslld	$30, %xmm13
 822                             	        pslld	$25, %xmm14
 823                             	        pxor	%xmm13, %xmm12
 824                             	        pxor	%xmm14, %xmm12
 825                             	        movdqa	%xmm12, %xmm13
 826                             	        psrldq	$4, %xmm13
 827                             	        pslldq	$12, %xmm12
 828                             	        pxor	%xmm12, %xmm8
 829                             	        movdqa	%xmm8, %xmm14
 830                             	        movdqa	%xmm8, %xmm10
 831                             	        movdqa	%xmm8, %xmm9
 832                             	        psrld	$0x01, %xmm14
 833                             	        psrld	$2, %xmm10
 834                             	        psrld	$7, %xmm9
 835                             	        pxor	%xmm10, %xmm14
 836                             	        pxor	%xmm9, %xmm14
 837                             	        pxor	%xmm13, %xmm14
 838                             	        pxor	%xmm8, %xmm14
 839                             	        pxor	%xmm14, %xmm7
 840                             	        movdqa	%xmm7, 64(%rsp)
 841                             	        # H ^ 6
 842                             	        pshufd	$0x4e, %xmm1, %xmm9
 843                             	        pshufd	$0x4e, %xmm1, %xmm10
 844                             	        movdqa	%xmm1, %xmm11
 845                             	        movdqa	%xmm1, %xmm8
 846                             	        pclmulqdq	$0x11, %xmm1, %xmm11
 847                             	        pclmulqdq	$0x00, %xmm1, %xmm8
 848                             	        pxor	%xmm1, %xmm9
 849                             	        pxor	%xmm1, %xmm10
 850                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 851                             	        pxor	%xmm8, %xmm9
 852                             	        pxor	%xmm11, %xmm9
 853                             	        movdqa	%xmm9, %xmm10
 854                             	        movdqa	%xmm11, %xmm7
 855                             	        pslldq	$8, %xmm10
 856                             	        psrldq	$8, %xmm9
 857                             	        pxor	%xmm10, %xmm8
 858                             	        pxor	%xmm9, %xmm7
 859                             	        movdqa	%xmm8, %xmm12
 860                             	        movdqa	%xmm8, %xmm13
 861                             	        movdqa	%xmm8, %xmm14
 862                             	        pslld	$31, %xmm12
 863                             	        pslld	$30, %xmm13
 864                             	        pslld	$25, %xmm14
 865                             	        pxor	%xmm13, %xmm12
 866                             	        pxor	%xmm14, %xmm12
 867                             	        movdqa	%xmm12, %xmm13
 868                             	        psrldq	$4, %xmm13
 869                             	        pslldq	$12, %xmm12
 870                             	        pxor	%xmm12, %xmm8
 871                             	        movdqa	%xmm8, %xmm14
 872                             	        movdqa	%xmm8, %xmm10
 873                             	        movdqa	%xmm8, %xmm9
 874                             	        psrld	$0x01, %xmm14
 875                             	        psrld	$2, %xmm10
 876                             	        psrld	$7, %xmm9
 877                             	        pxor	%xmm10, %xmm14
 878                             	        pxor	%xmm9, %xmm14
 879                             	        pxor	%xmm13, %xmm14
 880                             	        pxor	%xmm8, %xmm14
 881                             	        pxor	%xmm14, %xmm7
 882                             	        movdqa	%xmm7, 80(%rsp)
 883                             	        # H ^ 7
 884                             	        pshufd	$0x4e, %xmm1, %xmm9
 885                             	        pshufd	$0x4e, %xmm3, %xmm10
 886                             	        movdqa	%xmm3, %xmm11
 887                             	        movdqa	%xmm3, %xmm8
 888                             	        pclmulqdq	$0x11, %xmm1, %xmm11
 889                             	        pclmulqdq	$0x00, %xmm1, %xmm8
 890                             	        pxor	%xmm1, %xmm9
 891                             	        pxor	%xmm3, %xmm10
 892                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 893                             	        pxor	%xmm8, %xmm9
 894                             	        pxor	%xmm11, %xmm9
 895                             	        movdqa	%xmm9, %xmm10
 896                             	        movdqa	%xmm11, %xmm7
 897                             	        pslldq	$8, %xmm10
 898                             	        psrldq	$8, %xmm9
 899                             	        pxor	%xmm10, %xmm8
 900                             	        pxor	%xmm9, %xmm7
 901                             	        movdqa	%xmm8, %xmm12
 902                             	        movdqa	%xmm8, %xmm13
 903                             	        movdqa	%xmm8, %xmm14
 904                             	        pslld	$31, %xmm12
 905                             	        pslld	$30, %xmm13
 906                             	        pslld	$25, %xmm14
 907                             	        pxor	%xmm13, %xmm12
 908                             	        pxor	%xmm14, %xmm12
 909                             	        movdqa	%xmm12, %xmm13
 910                             	        psrldq	$4, %xmm13
 911                             	        pslldq	$12, %xmm12
 912                             	        pxor	%xmm12, %xmm8
 913                             	        movdqa	%xmm8, %xmm14
 914                             	        movdqa	%xmm8, %xmm10
 915                             	        movdqa	%xmm8, %xmm9
 916                             	        psrld	$0x01, %xmm14
 917                             	        psrld	$2, %xmm10
 918                             	        psrld	$7, %xmm9
 919                             	        pxor	%xmm10, %xmm14
 920                             	        pxor	%xmm9, %xmm14
 921                             	        pxor	%xmm13, %xmm14
 922                             	        pxor	%xmm8, %xmm14
 923                             	        pxor	%xmm14, %xmm7
 924                             	        movdqa	%xmm7, 96(%rsp)
 925                             	        # H ^ 8
 926                             	        pshufd	$0x4e, %xmm3, %xmm9
 927                             	        pshufd	$0x4e, %xmm3, %xmm10
 928                             	        movdqa	%xmm3, %xmm11
 929                             	        movdqa	%xmm3, %xmm8
 930                             	        pclmulqdq	$0x11, %xmm3, %xmm11
 931                             	        pclmulqdq	$0x00, %xmm3, %xmm8
 932                             	        pxor	%xmm3, %xmm9
 933                             	        pxor	%xmm3, %xmm10
 934                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 935                             	        pxor	%xmm8, %xmm9
 936                             	        pxor	%xmm11, %xmm9
 937                             	        movdqa	%xmm9, %xmm10
 938                             	        movdqa	%xmm11, %xmm7
 939                             	        pslldq	$8, %xmm10
 940                             	        psrldq	$8, %xmm9
 941                             	        pxor	%xmm10, %xmm8
 942                             	        pxor	%xmm9, %xmm7
 943                             	        movdqa	%xmm8, %xmm12
 944                             	        movdqa	%xmm8, %xmm13
 945                             	        movdqa	%xmm8, %xmm14
 946                             	        pslld	$31, %xmm12
 947                             	        pslld	$30, %xmm13
 948                             	        pslld	$25, %xmm14
 949                             	        pxor	%xmm13, %xmm12
 950                             	        pxor	%xmm14, %xmm12
 951                             	        movdqa	%xmm12, %xmm13
 952                             	        psrldq	$4, %xmm13
 953                             	        pslldq	$12, %xmm12
 954                             	        pxor	%xmm12, %xmm8
 955                             	        movdqa	%xmm8, %xmm14
 956                             	        movdqa	%xmm8, %xmm10
 957                             	        movdqa	%xmm8, %xmm9
 958                             	        psrld	$0x01, %xmm14
 959                             	        psrld	$2, %xmm10
 960                             	        psrld	$7, %xmm9
 961                             	        pxor	%xmm10, %xmm14
 962                             	        pxor	%xmm9, %xmm14
 963                             	        pxor	%xmm13, %xmm14
 964                             	        pxor	%xmm8, %xmm14
 965                             	        pxor	%xmm14, %xmm7
 966                             	        movdqa	%xmm7, 112(%rsp)
 967                             	        # First 128 bytes of input
 968                             	        movdqa	128(%rsp), %xmm8
 969                             	        movdqa	L_aes_gcm_bswap_epi64(%rip), %xmm1
 970                             	        movdqa	%xmm8, %xmm0
 971                             	        pshufb	%xmm1, %xmm8
 972                             	        movdqa	%xmm0, %xmm9
 973                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 974                             	        pshufb	%xmm1, %xmm9
 975                             	        movdqa	%xmm0, %xmm10
 976                             	        paddd	L_aes_gcm_two(%rip), %xmm10
 977                             	        pshufb	%xmm1, %xmm10
 978                             	        movdqa	%xmm0, %xmm11
 979                             	        paddd	L_aes_gcm_three(%rip), %xmm11
 980                             	        pshufb	%xmm1, %xmm11
 981                             	        movdqa	%xmm0, %xmm12
 982                             	        paddd	L_aes_gcm_four(%rip), %xmm12
 983                             	        pshufb	%xmm1, %xmm12
 984                             	        movdqa	%xmm0, %xmm13
 985                             	        paddd	L_aes_gcm_five(%rip), %xmm13
 986                             	        pshufb	%xmm1, %xmm13
 987                             	        movdqa	%xmm0, %xmm14
 988                             	        paddd	L_aes_gcm_six(%rip), %xmm14
 989                             	        pshufb	%xmm1, %xmm14
 990                             	        movdqa	%xmm0, %xmm15
 991                             	        paddd	L_aes_gcm_seven(%rip), %xmm15
 992                             	        pshufb	%xmm1, %xmm15
 993                             	        paddd	L_aes_gcm_eight(%rip), %xmm0
 994                             	        movdqa	(%r15), %xmm7
 995                             	        movdqa	%xmm0, 128(%rsp)
 996                             	        pxor	%xmm7, %xmm8
 997                             	        pxor	%xmm7, %xmm9
 998                             	        pxor	%xmm7, %xmm10
 999                             	        pxor	%xmm7, %xmm11
 1000                             	        pxor	%xmm7, %xmm12
 1001                             	        pxor	%xmm7, %xmm13
 1002                             	        pxor	%xmm7, %xmm14
 1003                             	        pxor	%xmm7, %xmm15
 1004                             	        movdqa	16(%r15), %xmm7
 1005                             	        aesenc	%xmm7, %xmm8
 1006                             	        aesenc	%xmm7, %xmm9
 1007                             	        aesenc	%xmm7, %xmm10
 1008                             	        aesenc	%xmm7, %xmm11
 1009                             	        aesenc	%xmm7, %xmm12
 1010                             	        aesenc	%xmm7, %xmm13
 1011                             	        aesenc	%xmm7, %xmm14
 1012                             	        aesenc	%xmm7, %xmm15
 1013                             	        movdqa	32(%r15), %xmm7
 1014                             	        aesenc	%xmm7, %xmm8
 1015                             	        aesenc	%xmm7, %xmm9
 1016                             	        aesenc	%xmm7, %xmm10
 1017                             	        aesenc	%xmm7, %xmm11
 1018                             	        aesenc	%xmm7, %xmm12
 1019                             	        aesenc	%xmm7, %xmm13
 1020                             	        aesenc	%xmm7, %xmm14
 1021                             	        aesenc	%xmm7, %xmm15
 1022                             	        movdqa	48(%r15), %xmm7
 1023                             	        aesenc	%xmm7, %xmm8
 1024                             	        aesenc	%xmm7, %xmm9
 1025                             	        aesenc	%xmm7, %xmm10
 1026                             	        aesenc	%xmm7, %xmm11
 1027                             	        aesenc	%xmm7, %xmm12
 1028                             	        aesenc	%xmm7, %xmm13
 1029                             	        aesenc	%xmm7, %xmm14
 1030                             	        aesenc	%xmm7, %xmm15
 1031                             	        movdqa	64(%r15), %xmm7
 1032                             	        aesenc	%xmm7, %xmm8
 1033                             	        aesenc	%xmm7, %xmm9
 1034                             	        aesenc	%xmm7, %xmm10
 1035                             	        aesenc	%xmm7, %xmm11
 1036                             	        aesenc	%xmm7, %xmm12
 1037                             	        aesenc	%xmm7, %xmm13
 1038                             	        aesenc	%xmm7, %xmm14
 1039                             	        aesenc	%xmm7, %xmm15
 1040                             	        movdqa	80(%r15), %xmm7
 1041                             	        aesenc	%xmm7, %xmm8
 1042                             	        aesenc	%xmm7, %xmm9
 1043                             	        aesenc	%xmm7, %xmm10
 1044                             	        aesenc	%xmm7, %xmm11
 1045                             	        aesenc	%xmm7, %xmm12
 1046                             	        aesenc	%xmm7, %xmm13
 1047                             	        aesenc	%xmm7, %xmm14
 1048                             	        aesenc	%xmm7, %xmm15
 1049                             	        movdqa	96(%r15), %xmm7
 1050                             	        aesenc	%xmm7, %xmm8
 1051                             	        aesenc	%xmm7, %xmm9
 1052                             	        aesenc	%xmm7, %xmm10
 1053                             	        aesenc	%xmm7, %xmm11
 1054                             	        aesenc	%xmm7, %xmm12
 1055                             	        aesenc	%xmm7, %xmm13
 1056                             	        aesenc	%xmm7, %xmm14
 1057                             	        aesenc	%xmm7, %xmm15
 1058                             	        movdqa	112(%r15), %xmm7
 1059                             	        aesenc	%xmm7, %xmm8
 1060                             	        aesenc	%xmm7, %xmm9
 1061                             	        aesenc	%xmm7, %xmm10
 1062                             	        aesenc	%xmm7, %xmm11
 1063                             	        aesenc	%xmm7, %xmm12
 1064                             	        aesenc	%xmm7, %xmm13
 1065                             	        aesenc	%xmm7, %xmm14
 1066                             	        aesenc	%xmm7, %xmm15
 1067                             	        movdqa	128(%r15), %xmm7
 1068                             	        aesenc	%xmm7, %xmm8
 1069                             	        aesenc	%xmm7, %xmm9
 1070                             	        aesenc	%xmm7, %xmm10
 1071                             	        aesenc	%xmm7, %xmm11
 1072                             	        aesenc	%xmm7, %xmm12
 1073                             	        aesenc	%xmm7, %xmm13
 1074                             	        aesenc	%xmm7, %xmm14
 1075                             	        aesenc	%xmm7, %xmm15
 1076                             	        movdqa	144(%r15), %xmm7
 1077                             	        aesenc	%xmm7, %xmm8
 1078                             	        aesenc	%xmm7, %xmm9
 1079                             	        aesenc	%xmm7, %xmm10
 1080                             	        aesenc	%xmm7, %xmm11
 1081                             	        aesenc	%xmm7, %xmm12
 1082                             	        aesenc	%xmm7, %xmm13
 1083                             	        aesenc	%xmm7, %xmm14
 1084                             	        aesenc	%xmm7, %xmm15
 1085                             	        cmpl	$11, %r10d
 1086                             	        movdqa	160(%r15), %xmm7
 1087                             	        jl	L_AES_GCM_encrypt_enc_done
 1088                             	        aesenc	%xmm7, %xmm8
 1089                             	        aesenc	%xmm7, %xmm9
 1090                             	        aesenc	%xmm7, %xmm10
 1091                             	        aesenc	%xmm7, %xmm11
 1092                             	        aesenc	%xmm7, %xmm12
 1093                             	        aesenc	%xmm7, %xmm13
 1094                             	        aesenc	%xmm7, %xmm14
 1095                             	        aesenc	%xmm7, %xmm15
 1096                             	        movdqa	176(%r15), %xmm7
 1097                             	        aesenc	%xmm7, %xmm8
 1098                             	        aesenc	%xmm7, %xmm9
 1099                             	        aesenc	%xmm7, %xmm10
 1100                             	        aesenc	%xmm7, %xmm11
 1101                             	        aesenc	%xmm7, %xmm12
 1102                             	        aesenc	%xmm7, %xmm13
 1103                             	        aesenc	%xmm7, %xmm14
 1104                             	        aesenc	%xmm7, %xmm15
 1105                             	        cmpl	$13, %r10d
 1106                             	        movdqa	192(%r15), %xmm7
 1107                             	        jl	L_AES_GCM_encrypt_enc_done
 1108                             	        aesenc	%xmm7, %xmm8
 1109                             	        aesenc	%xmm7, %xmm9
 1110                             	        aesenc	%xmm7, %xmm10
 1111                             	        aesenc	%xmm7, %xmm11
 1112                             	        aesenc	%xmm7, %xmm12
 1113                             	        aesenc	%xmm7, %xmm13
 1114                             	        aesenc	%xmm7, %xmm14
 1115                             	        aesenc	%xmm7, %xmm15
 1116                             	        movdqa	208(%r15), %xmm7
 1117                             	        aesenc	%xmm7, %xmm8
 1118                             	        aesenc	%xmm7, %xmm9
 1119                             	        aesenc	%xmm7, %xmm10
 1120                             	        aesenc	%xmm7, %xmm11
 1121                             	        aesenc	%xmm7, %xmm12
 1122                             	        aesenc	%xmm7, %xmm13
 1123                             	        aesenc	%xmm7, %xmm14
 1124                             	        aesenc	%xmm7, %xmm15
 1125                             	        movdqa	224(%r15), %xmm7
 1126                             	L_AES_GCM_encrypt_enc_done:
 1127                             	        aesenclast	%xmm7, %xmm8
 1128                             	        aesenclast	%xmm7, %xmm9
 1129                             	        movdqu	(%rdi), %xmm0
 1130                             	        movdqu	16(%rdi), %xmm1
 1131                             	        pxor	%xmm0, %xmm8
 1132                             	        pxor	%xmm1, %xmm9
 1133                             	        movdqu	%xmm8, (%rsi)
 1134                             	        movdqu	%xmm9, 16(%rsi)
 1135                             	        aesenclast	%xmm7, %xmm10
 1136                             	        aesenclast	%xmm7, %xmm11
 1137                             	        movdqu	32(%rdi), %xmm0
 1138                             	        movdqu	48(%rdi), %xmm1
 1139                             	        pxor	%xmm0, %xmm10
 1140                             	        pxor	%xmm1, %xmm11
 1141                             	        movdqu	%xmm10, 32(%rsi)
 1142                             	        movdqu	%xmm11, 48(%rsi)
 1143                             	        aesenclast	%xmm7, %xmm12
 1144                             	        aesenclast	%xmm7, %xmm13
 1145                             	        movdqu	64(%rdi), %xmm0
 1146                             	        movdqu	80(%rdi), %xmm1
 1147                             	        pxor	%xmm0, %xmm12
 1148                             	        pxor	%xmm1, %xmm13
 1149                             	        movdqu	%xmm12, 64(%rsi)
 1150                             	        movdqu	%xmm13, 80(%rsi)
 1151                             	        aesenclast	%xmm7, %xmm14
 1152                             	        aesenclast	%xmm7, %xmm15
 1153                             	        movdqu	96(%rdi), %xmm0
 1154                             	        movdqu	112(%rdi), %xmm1
 1155                             	        pxor	%xmm0, %xmm14
 1156                             	        pxor	%xmm1, %xmm15
 1157                             	        movdqu	%xmm14, 96(%rsi)
 1158                             	        movdqu	%xmm15, 112(%rsi)
 1159                             	        cmpl	$0x80, %r13d
 1160                             	        movl	$0x80, %ebx
 1161                             	        jle	L_AES_GCM_encrypt_end_128
 1162                             	        # More 128 bytes of input
 1163                             	L_AES_GCM_encrypt_ghash_128:
 1164                             	        leaq	(%rdi,%rbx,1), %rcx
 1165                             	        leaq	(%rsi,%rbx,1), %rdx
 1166                             	        movdqa	128(%rsp), %xmm8
 1167                             	        movdqa	L_aes_gcm_bswap_epi64(%rip), %xmm1
 1168                             	        movdqa	%xmm8, %xmm0
 1169                             	        pshufb	%xmm1, %xmm8
 1170                             	        movdqa	%xmm0, %xmm9
 1171                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 1172                             	        pshufb	%xmm1, %xmm9
 1173                             	        movdqa	%xmm0, %xmm10
 1174                             	        paddd	L_aes_gcm_two(%rip), %xmm10
 1175                             	        pshufb	%xmm1, %xmm10
 1176                             	        movdqa	%xmm0, %xmm11
 1177                             	        paddd	L_aes_gcm_three(%rip), %xmm11
 1178                             	        pshufb	%xmm1, %xmm11
 1179                             	        movdqa	%xmm0, %xmm12
 1180                             	        paddd	L_aes_gcm_four(%rip), %xmm12
 1181                             	        pshufb	%xmm1, %xmm12
 1182                             	        movdqa	%xmm0, %xmm13
 1183                             	        paddd	L_aes_gcm_five(%rip), %xmm13
 1184                             	        pshufb	%xmm1, %xmm13
 1185                             	        movdqa	%xmm0, %xmm14
 1186                             	        paddd	L_aes_gcm_six(%rip), %xmm14
 1187                             	        pshufb	%xmm1, %xmm14
 1188                             	        movdqa	%xmm0, %xmm15
 1189                             	        paddd	L_aes_gcm_seven(%rip), %xmm15
 1190                             	        pshufb	%xmm1, %xmm15
 1191                             	        paddd	L_aes_gcm_eight(%rip), %xmm0
 1192                             	        movdqa	(%r15), %xmm7
 1193                             	        movdqa	%xmm0, 128(%rsp)
 1194                             	        pxor	%xmm7, %xmm8
 1195                             	        pxor	%xmm7, %xmm9
 1196                             	        pxor	%xmm7, %xmm10
 1197                             	        pxor	%xmm7, %xmm11
 1198                             	        pxor	%xmm7, %xmm12
 1199                             	        pxor	%xmm7, %xmm13
 1200                             	        pxor	%xmm7, %xmm14
 1201                             	        pxor	%xmm7, %xmm15
 1202                             	        movdqa	112(%rsp), %xmm7
 1203                             	        movdqu	-128(%rdx), %xmm0
 1204                             	        aesenc	16(%r15), %xmm8
 1205                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 1206                             	        pxor	%xmm2, %xmm0
 1207                             	        pshufd	$0x4e, %xmm7, %xmm1
 1208                             	        pshufd	$0x4e, %xmm0, %xmm5
 1209                             	        pxor	%xmm7, %xmm1
 1210                             	        pxor	%xmm0, %xmm5
 1211                             	        movdqa	%xmm0, %xmm3
 1212                             	        pclmulqdq	$0x11, %xmm7, %xmm3
 1213                             	        aesenc	16(%r15), %xmm9
 1214                             	        aesenc	16(%r15), %xmm10
 1215                             	        movdqa	%xmm0, %xmm2
 1216                             	        pclmulqdq	$0x00, %xmm7, %xmm2
 1217                             	        aesenc	16(%r15), %xmm11
 1218                             	        aesenc	16(%r15), %xmm12
 1219                             	        pclmulqdq	$0x00, %xmm5, %xmm1
 1220                             	        aesenc	16(%r15), %xmm13
 1221                             	        aesenc	16(%r15), %xmm14
 1222                             	        aesenc	16(%r15), %xmm15
 1223                             	        pxor	%xmm2, %xmm1
 1224                             	        pxor	%xmm3, %xmm1
 1225                             	        movdqa	96(%rsp), %xmm7
 1226                             	        movdqu	-112(%rdx), %xmm0
 1227                             	        pshufd	$0x4e, %xmm7, %xmm4
 1228                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 1229                             	        aesenc	32(%r15), %xmm8
 1230                             	        pxor	%xmm7, %xmm4
 1231                             	        pshufd	$0x4e, %xmm0, %xmm5
 1232                             	        pxor	%xmm0, %xmm5
 1233                             	        movdqa	%xmm0, %xmm6
 1234                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 1235                             	        aesenc	32(%r15), %xmm9
 1236                             	        aesenc	32(%r15), %xmm10
 1237                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 1238                             	        aesenc	32(%r15), %xmm11
 1239                             	        aesenc	32(%r15), %xmm12
 1240                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 1241                             	        aesenc	32(%r15), %xmm13
 1242                             	        aesenc	32(%r15), %xmm14
 1243                             	        aesenc	32(%r15), %xmm15
 1244                             	        pxor	%xmm7, %xmm1
 1245                             	        pxor	%xmm7, %xmm2
 1246                             	        pxor	%xmm6, %xmm1
 1247                             	        pxor	%xmm6, %xmm3
 1248                             	        pxor	%xmm4, %xmm1
 1249                             	        movdqa	80(%rsp), %xmm7
 1250                             	        movdqu	-96(%rdx), %xmm0
 1251                             	        pshufd	$0x4e, %xmm7, %xmm4
 1252                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 1253                             	        aesenc	48(%r15), %xmm8
 1254                             	        pxor	%xmm7, %xmm4
 1255                             	        pshufd	$0x4e, %xmm0, %xmm5
 1256                             	        pxor	%xmm0, %xmm5
 1257                             	        movdqa	%xmm0, %xmm6
 1258                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 1259                             	        aesenc	48(%r15), %xmm9
 1260                             	        aesenc	48(%r15), %xmm10
 1261                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 1262                             	        aesenc	48(%r15), %xmm11
 1263                             	        aesenc	48(%r15), %xmm12
 1264                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 1265                             	        aesenc	48(%r15), %xmm13
 1266                             	        aesenc	48(%r15), %xmm14
 1267                             	        aesenc	48(%r15), %xmm15
 1268                             	        pxor	%xmm7, %xmm1
 1269                             	        pxor	%xmm7, %xmm2
 1270                             	        pxor	%xmm6, %xmm1
 1271                             	        pxor	%xmm6, %xmm3
 1272                             	        pxor	%xmm4, %xmm1
 1273                             	        movdqa	64(%rsp), %xmm7
 1274                             	        movdqu	-80(%rdx), %xmm0
 1275                             	        pshufd	$0x4e, %xmm7, %xmm4
 1276                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 1277                             	        aesenc	64(%r15), %xmm8
 1278                             	        pxor	%xmm7, %xmm4
 1279                             	        pshufd	$0x4e, %xmm0, %xmm5
 1280                             	        pxor	%xmm0, %xmm5
 1281                             	        movdqa	%xmm0, %xmm6
 1282                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 1283                             	        aesenc	64(%r15), %xmm9
 1284                             	        aesenc	64(%r15), %xmm10
 1285                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 1286                             	        aesenc	64(%r15), %xmm11
 1287                             	        aesenc	64(%r15), %xmm12
 1288                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 1289                             	        aesenc	64(%r15), %xmm13
 1290                             	        aesenc	64(%r15), %xmm14
 1291                             	        aesenc	64(%r15), %xmm15
 1292                             	        pxor	%xmm7, %xmm1
 1293                             	        pxor	%xmm7, %xmm2
 1294                             	        pxor	%xmm6, %xmm1
 1295                             	        pxor	%xmm6, %xmm3
 1296                             	        pxor	%xmm4, %xmm1
 1297                             	        movdqa	48(%rsp), %xmm7
 1298                             	        movdqu	-64(%rdx), %xmm0
 1299                             	        pshufd	$0x4e, %xmm7, %xmm4
 1300                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 1301                             	        aesenc	80(%r15), %xmm8
 1302                             	        pxor	%xmm7, %xmm4
 1303                             	        pshufd	$0x4e, %xmm0, %xmm5
 1304                             	        pxor	%xmm0, %xmm5
 1305                             	        movdqa	%xmm0, %xmm6
 1306                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 1307                             	        aesenc	80(%r15), %xmm9
 1308                             	        aesenc	80(%r15), %xmm10
 1309                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 1310                             	        aesenc	80(%r15), %xmm11
 1311                             	        aesenc	80(%r15), %xmm12
 1312                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 1313                             	        aesenc	80(%r15), %xmm13
 1314                             	        aesenc	80(%r15), %xmm14
 1315                             	        aesenc	80(%r15), %xmm15
 1316                             	        pxor	%xmm7, %xmm1
 1317                             	        pxor	%xmm7, %xmm2
 1318                             	        pxor	%xmm6, %xmm1
 1319                             	        pxor	%xmm6, %xmm3
 1320                             	        pxor	%xmm4, %xmm1
 1321                             	        movdqa	32(%rsp), %xmm7
 1322                             	        movdqu	-48(%rdx), %xmm0
 1323                             	        pshufd	$0x4e, %xmm7, %xmm4
 1324                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 1325                             	        aesenc	96(%r15), %xmm8
 1326                             	        pxor	%xmm7, %xmm4
 1327                             	        pshufd	$0x4e, %xmm0, %xmm5
 1328                             	        pxor	%xmm0, %xmm5
 1329                             	        movdqa	%xmm0, %xmm6
 1330                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 1331                             	        aesenc	96(%r15), %xmm9
 1332                             	        aesenc	96(%r15), %xmm10
 1333                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 1334                             	        aesenc	96(%r15), %xmm11
 1335                             	        aesenc	96(%r15), %xmm12
 1336                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 1337                             	        aesenc	96(%r15), %xmm13
 1338                             	        aesenc	96(%r15), %xmm14
 1339                             	        aesenc	96(%r15), %xmm15
 1340                             	        pxor	%xmm7, %xmm1
 1341                             	        pxor	%xmm7, %xmm2
 1342                             	        pxor	%xmm6, %xmm1
 1343                             	        pxor	%xmm6, %xmm3
 1344                             	        pxor	%xmm4, %xmm1
 1345                             	        movdqa	16(%rsp), %xmm7
 1346                             	        movdqu	-32(%rdx), %xmm0
 1347                             	        pshufd	$0x4e, %xmm7, %xmm4
 1348                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 1349                             	        aesenc	112(%r15), %xmm8
 1350                             	        pxor	%xmm7, %xmm4
 1351                             	        pshufd	$0x4e, %xmm0, %xmm5
 1352                             	        pxor	%xmm0, %xmm5
 1353                             	        movdqa	%xmm0, %xmm6
 1354                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 1355                             	        aesenc	112(%r15), %xmm9
 1356                             	        aesenc	112(%r15), %xmm10
 1357                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 1358                             	        aesenc	112(%r15), %xmm11
 1359                             	        aesenc	112(%r15), %xmm12
 1360                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 1361                             	        aesenc	112(%r15), %xmm13
 1362                             	        aesenc	112(%r15), %xmm14
 1363                             	        aesenc	112(%r15), %xmm15
 1364                             	        pxor	%xmm7, %xmm1
 1365                             	        pxor	%xmm7, %xmm2
 1366                             	        pxor	%xmm6, %xmm1
 1367                             	        pxor	%xmm6, %xmm3
 1368                             	        pxor	%xmm4, %xmm1
 1369                             	        movdqa	(%rsp), %xmm7
 1370                             	        movdqu	-16(%rdx), %xmm0
 1371                             	        pshufd	$0x4e, %xmm7, %xmm4
 1372                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 1373                             	        aesenc	128(%r15), %xmm8
 1374                             	        pxor	%xmm7, %xmm4
 1375                             	        pshufd	$0x4e, %xmm0, %xmm5
 1376                             	        pxor	%xmm0, %xmm5
 1377                             	        movdqa	%xmm0, %xmm6
 1378                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 1379                             	        aesenc	128(%r15), %xmm9
 1380                             	        aesenc	128(%r15), %xmm10
 1381                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 1382                             	        aesenc	128(%r15), %xmm11
 1383                             	        aesenc	128(%r15), %xmm12
 1384                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 1385                             	        aesenc	128(%r15), %xmm13
 1386                             	        aesenc	128(%r15), %xmm14
 1387                             	        aesenc	128(%r15), %xmm15
 1388                             	        pxor	%xmm7, %xmm1
 1389                             	        pxor	%xmm7, %xmm2
 1390                             	        pxor	%xmm6, %xmm1
 1391                             	        pxor	%xmm6, %xmm3
 1392                             	        pxor	%xmm4, %xmm1
 1393                             	        movdqa	%xmm1, %xmm5
 1394                             	        psrldq	$8, %xmm1
 1395                             	        pslldq	$8, %xmm5
 1396                             	        aesenc	144(%r15), %xmm8
 1397                             	        pxor	%xmm5, %xmm2
 1398                             	        pxor	%xmm1, %xmm3
 1399                             	        movdqa	%xmm2, %xmm7
 1400                             	        movdqa	%xmm2, %xmm4
 1401                             	        movdqa	%xmm2, %xmm5
 1402                             	        aesenc	144(%r15), %xmm9
 1403                             	        pslld	$31, %xmm7
 1404                             	        pslld	$30, %xmm4
 1405                             	        pslld	$25, %xmm5
 1406                             	        aesenc	144(%r15), %xmm10
 1407                             	        pxor	%xmm4, %xmm7
 1408                             	        pxor	%xmm5, %xmm7
 1409                             	        aesenc	144(%r15), %xmm11
 1410                             	        movdqa	%xmm7, %xmm4
 1411                             	        pslldq	$12, %xmm7
 1412                             	        psrldq	$4, %xmm4
 1413                             	        aesenc	144(%r15), %xmm12
 1414                             	        pxor	%xmm7, %xmm2
 1415                             	        movdqa	%xmm2, %xmm5
 1416                             	        movdqa	%xmm2, %xmm1
 1417                             	        movdqa	%xmm2, %xmm0
 1418                             	        aesenc	144(%r15), %xmm13
 1419                             	        psrld	$0x01, %xmm5
 1420                             	        psrld	$2, %xmm1
 1421                             	        psrld	$7, %xmm0
 1422                             	        aesenc	144(%r15), %xmm14
 1423                             	        pxor	%xmm1, %xmm5
 1424                             	        pxor	%xmm0, %xmm5
 1425                             	        aesenc	144(%r15), %xmm15
 1426                             	        pxor	%xmm4, %xmm5
 1427                             	        pxor	%xmm5, %xmm2
 1428                             	        pxor	%xmm3, %xmm2
 1429                             	        cmpl	$11, %r10d
 1430                             	        movdqa	160(%r15), %xmm7
 1431                             	        jl	L_AES_GCM_encrypt_aesenc_128_ghash_avx_done
 1432                             	        aesenc	%xmm7, %xmm8
 1433                             	        aesenc	%xmm7, %xmm9
 1434                             	        aesenc	%xmm7, %xmm10
 1435                             	        aesenc	%xmm7, %xmm11
 1436                             	        aesenc	%xmm7, %xmm12
 1437                             	        aesenc	%xmm7, %xmm13
 1438                             	        aesenc	%xmm7, %xmm14
 1439                             	        aesenc	%xmm7, %xmm15
 1440                             	        movdqa	176(%r15), %xmm7
 1441                             	        aesenc	%xmm7, %xmm8
 1442                             	        aesenc	%xmm7, %xmm9
 1443                             	        aesenc	%xmm7, %xmm10
 1444                             	        aesenc	%xmm7, %xmm11
 1445                             	        aesenc	%xmm7, %xmm12
 1446                             	        aesenc	%xmm7, %xmm13
 1447                             	        aesenc	%xmm7, %xmm14
 1448                             	        aesenc	%xmm7, %xmm15
 1449                             	        cmpl	$13, %r10d
 1450                             	        movdqa	192(%r15), %xmm7
 1451                             	        jl	L_AES_GCM_encrypt_aesenc_128_ghash_avx_done
 1452                             	        aesenc	%xmm7, %xmm8
 1453                             	        aesenc	%xmm7, %xmm9
 1454                             	        aesenc	%xmm7, %xmm10
 1455                             	        aesenc	%xmm7, %xmm11
 1456                             	        aesenc	%xmm7, %xmm12
 1457                             	        aesenc	%xmm7, %xmm13
 1458                             	        aesenc	%xmm7, %xmm14
 1459                             	        aesenc	%xmm7, %xmm15
 1460                             	        movdqa	208(%r15), %xmm7
 1461                             	        aesenc	%xmm7, %xmm8
 1462                             	        aesenc	%xmm7, %xmm9
 1463                             	        aesenc	%xmm7, %xmm10
 1464                             	        aesenc	%xmm7, %xmm11
 1465                             	        aesenc	%xmm7, %xmm12
 1466                             	        aesenc	%xmm7, %xmm13
 1467                             	        aesenc	%xmm7, %xmm14
 1468                             	        aesenc	%xmm7, %xmm15
 1469                             	        movdqa	224(%r15), %xmm7
 1470                             	L_AES_GCM_encrypt_aesenc_128_ghash_avx_done:
 1471                             	        aesenclast	%xmm7, %xmm8
 1472                             	        aesenclast	%xmm7, %xmm9
 1473                             	        movdqu	(%rcx), %xmm0
 1474                             	        movdqu	16(%rcx), %xmm1
 1475                             	        pxor	%xmm0, %xmm8
 1476                             	        pxor	%xmm1, %xmm9
 1477                             	        movdqu	%xmm8, (%rdx)
 1478                             	        movdqu	%xmm9, 16(%rdx)
 1479                             	        aesenclast	%xmm7, %xmm10
 1480                             	        aesenclast	%xmm7, %xmm11
 1481                             	        movdqu	32(%rcx), %xmm0
 1482                             	        movdqu	48(%rcx), %xmm1
 1483                             	        pxor	%xmm0, %xmm10
 1484                             	        pxor	%xmm1, %xmm11
 1485                             	        movdqu	%xmm10, 32(%rdx)
 1486                             	        movdqu	%xmm11, 48(%rdx)
 1487                             	        aesenclast	%xmm7, %xmm12
 1488                             	        aesenclast	%xmm7, %xmm13
 1489                             	        movdqu	64(%rcx), %xmm0
 1490                             	        movdqu	80(%rcx), %xmm1
 1491                             	        pxor	%xmm0, %xmm12
 1492                             	        pxor	%xmm1, %xmm13
 1493                             	        movdqu	%xmm12, 64(%rdx)
 1494                             	        movdqu	%xmm13, 80(%rdx)
 1495                             	        aesenclast	%xmm7, %xmm14
 1496                             	        aesenclast	%xmm7, %xmm15
 1497                             	        movdqu	96(%rcx), %xmm0
 1498                             	        movdqu	112(%rcx), %xmm1
 1499                             	        pxor	%xmm0, %xmm14
 1500                             	        pxor	%xmm1, %xmm15
 1501                             	        movdqu	%xmm14, 96(%rdx)
 1502                             	        movdqu	%xmm15, 112(%rdx)
 1503                             	        addl	$0x80, %ebx
 1504                             	        cmpl	%r13d, %ebx
 1505                             	        jl	L_AES_GCM_encrypt_ghash_128
 1506                             	L_AES_GCM_encrypt_end_128:
 1507                             	        movdqa	L_aes_gcm_bswap_mask(%rip), %xmm4
 1508                             	        pshufb	%xmm4, %xmm8
 1509                             	        pshufb	%xmm4, %xmm9
 1510                             	        pshufb	%xmm4, %xmm10
 1511                             	        pshufb	%xmm4, %xmm11
 1512                             	        pxor	%xmm2, %xmm8
 1513                             	        pshufb	%xmm4, %xmm12
 1514                             	        pshufb	%xmm4, %xmm13
 1515                             	        pshufb	%xmm4, %xmm14
 1516                             	        pshufb	%xmm4, %xmm15
 1517                             	        movdqa	112(%rsp), %xmm7
 1518                             	        pshufd	$0x4e, %xmm8, %xmm1
 1519                             	        pshufd	$0x4e, %xmm7, %xmm2
 1520                             	        movdqa	%xmm7, %xmm3
 1521                             	        movdqa	%xmm7, %xmm0
 1522                             	        pclmulqdq	$0x11, %xmm8, %xmm3
 1523                             	        pclmulqdq	$0x00, %xmm8, %xmm0
 1524                             	        pxor	%xmm8, %xmm1
 1525                             	        pxor	%xmm7, %xmm2
 1526                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 1527                             	        pxor	%xmm0, %xmm1
 1528                             	        pxor	%xmm3, %xmm1
 1529                             	        movdqa	%xmm1, %xmm2
 1530                             	        movdqa	%xmm0, %xmm4
 1531                             	        movdqa	%xmm3, %xmm6
 1532                             	        pslldq	$8, %xmm2
 1533                             	        psrldq	$8, %xmm1
 1534                             	        pxor	%xmm2, %xmm4
 1535                             	        pxor	%xmm1, %xmm6
 1536                             	        movdqa	96(%rsp), %xmm7
 1537                             	        pshufd	$0x4e, %xmm9, %xmm1
 1538                             	        pshufd	$0x4e, %xmm7, %xmm2
 1539                             	        movdqa	%xmm7, %xmm3
 1540                             	        movdqa	%xmm7, %xmm0
 1541                             	        pclmulqdq	$0x11, %xmm9, %xmm3
 1542                             	        pclmulqdq	$0x00, %xmm9, %xmm0
 1543                             	        pxor	%xmm9, %xmm1
 1544                             	        pxor	%xmm7, %xmm2
 1545                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 1546                             	        pxor	%xmm0, %xmm1
 1547                             	        pxor	%xmm3, %xmm1
 1548                             	        movdqa	%xmm1, %xmm2
 1549                             	        pxor	%xmm0, %xmm4
 1550                             	        pxor	%xmm3, %xmm6
 1551                             	        pslldq	$8, %xmm2
 1552                             	        psrldq	$8, %xmm1
 1553                             	        pxor	%xmm2, %xmm4
 1554                             	        pxor	%xmm1, %xmm6
 1555                             	        movdqa	80(%rsp), %xmm7
 1556                             	        pshufd	$0x4e, %xmm10, %xmm1
 1557                             	        pshufd	$0x4e, %xmm7, %xmm2
 1558                             	        movdqa	%xmm7, %xmm3
 1559                             	        movdqa	%xmm7, %xmm0
 1560                             	        pclmulqdq	$0x11, %xmm10, %xmm3
 1561                             	        pclmulqdq	$0x00, %xmm10, %xmm0
 1562                             	        pxor	%xmm10, %xmm1
 1563                             	        pxor	%xmm7, %xmm2
 1564                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 1565                             	        pxor	%xmm0, %xmm1
 1566                             	        pxor	%xmm3, %xmm1
 1567                             	        movdqa	%xmm1, %xmm2
 1568                             	        pxor	%xmm0, %xmm4
 1569                             	        pxor	%xmm3, %xmm6
 1570                             	        pslldq	$8, %xmm2
 1571                             	        psrldq	$8, %xmm1
 1572                             	        pxor	%xmm2, %xmm4
 1573                             	        pxor	%xmm1, %xmm6
 1574                             	        movdqa	64(%rsp), %xmm7
 1575                             	        pshufd	$0x4e, %xmm11, %xmm1
 1576                             	        pshufd	$0x4e, %xmm7, %xmm2
 1577                             	        movdqa	%xmm7, %xmm3
 1578                             	        movdqa	%xmm7, %xmm0
 1579                             	        pclmulqdq	$0x11, %xmm11, %xmm3
 1580                             	        pclmulqdq	$0x00, %xmm11, %xmm0
 1581                             	        pxor	%xmm11, %xmm1
 1582                             	        pxor	%xmm7, %xmm2
 1583                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 1584                             	        pxor	%xmm0, %xmm1
 1585                             	        pxor	%xmm3, %xmm1
 1586                             	        movdqa	%xmm1, %xmm2
 1587                             	        pxor	%xmm0, %xmm4
 1588                             	        pxor	%xmm3, %xmm6
 1589                             	        pslldq	$8, %xmm2
 1590                             	        psrldq	$8, %xmm1
 1591                             	        pxor	%xmm2, %xmm4
 1592                             	        pxor	%xmm1, %xmm6
 1593                             	        movdqa	48(%rsp), %xmm7
 1594                             	        pshufd	$0x4e, %xmm12, %xmm1
 1595                             	        pshufd	$0x4e, %xmm7, %xmm2
 1596                             	        movdqa	%xmm7, %xmm3
 1597                             	        movdqa	%xmm7, %xmm0
 1598                             	        pclmulqdq	$0x11, %xmm12, %xmm3
 1599                             	        pclmulqdq	$0x00, %xmm12, %xmm0
 1600                             	        pxor	%xmm12, %xmm1
 1601                             	        pxor	%xmm7, %xmm2
 1602                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 1603                             	        pxor	%xmm0, %xmm1
 1604                             	        pxor	%xmm3, %xmm1
 1605                             	        movdqa	%xmm1, %xmm2
 1606                             	        pxor	%xmm0, %xmm4
 1607                             	        pxor	%xmm3, %xmm6
 1608                             	        pslldq	$8, %xmm2
 1609                             	        psrldq	$8, %xmm1
 1610                             	        pxor	%xmm2, %xmm4
 1611                             	        pxor	%xmm1, %xmm6
 1612                             	        movdqa	32(%rsp), %xmm7
 1613                             	        pshufd	$0x4e, %xmm13, %xmm1
 1614                             	        pshufd	$0x4e, %xmm7, %xmm2
 1615                             	        movdqa	%xmm7, %xmm3
 1616                             	        movdqa	%xmm7, %xmm0
 1617                             	        pclmulqdq	$0x11, %xmm13, %xmm3
 1618                             	        pclmulqdq	$0x00, %xmm13, %xmm0
 1619                             	        pxor	%xmm13, %xmm1
 1620                             	        pxor	%xmm7, %xmm2
 1621                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 1622                             	        pxor	%xmm0, %xmm1
 1623                             	        pxor	%xmm3, %xmm1
 1624                             	        movdqa	%xmm1, %xmm2
 1625                             	        pxor	%xmm0, %xmm4
 1626                             	        pxor	%xmm3, %xmm6
 1627                             	        pslldq	$8, %xmm2
 1628                             	        psrldq	$8, %xmm1
 1629                             	        pxor	%xmm2, %xmm4
 1630                             	        pxor	%xmm1, %xmm6
 1631                             	        movdqa	16(%rsp), %xmm7
 1632                             	        pshufd	$0x4e, %xmm14, %xmm1
 1633                             	        pshufd	$0x4e, %xmm7, %xmm2
 1634                             	        movdqa	%xmm7, %xmm3
 1635                             	        movdqa	%xmm7, %xmm0
 1636                             	        pclmulqdq	$0x11, %xmm14, %xmm3
 1637                             	        pclmulqdq	$0x00, %xmm14, %xmm0
 1638                             	        pxor	%xmm14, %xmm1
 1639                             	        pxor	%xmm7, %xmm2
 1640                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 1641                             	        pxor	%xmm0, %xmm1
 1642                             	        pxor	%xmm3, %xmm1
 1643                             	        movdqa	%xmm1, %xmm2
 1644                             	        pxor	%xmm0, %xmm4
 1645                             	        pxor	%xmm3, %xmm6
 1646                             	        pslldq	$8, %xmm2
 1647                             	        psrldq	$8, %xmm1
 1648                             	        pxor	%xmm2, %xmm4
 1649                             	        pxor	%xmm1, %xmm6
 1650                             	        movdqa	(%rsp), %xmm7
 1651                             	        pshufd	$0x4e, %xmm15, %xmm1
 1652                             	        pshufd	$0x4e, %xmm7, %xmm2
 1653                             	        movdqa	%xmm7, %xmm3
 1654                             	        movdqa	%xmm7, %xmm0
 1655                             	        pclmulqdq	$0x11, %xmm15, %xmm3
 1656                             	        pclmulqdq	$0x00, %xmm15, %xmm0
 1657                             	        pxor	%xmm15, %xmm1
 1658                             	        pxor	%xmm7, %xmm2
 1659                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 1660                             	        pxor	%xmm0, %xmm1
 1661                             	        pxor	%xmm3, %xmm1
 1662                             	        movdqa	%xmm1, %xmm2
 1663                             	        pxor	%xmm0, %xmm4
 1664                             	        pxor	%xmm3, %xmm6
 1665                             	        pslldq	$8, %xmm2
 1666                             	        psrldq	$8, %xmm1
 1667                             	        pxor	%xmm2, %xmm4
 1668                             	        pxor	%xmm1, %xmm6
 1669                             	        movdqa	%xmm4, %xmm0
 1670                             	        movdqa	%xmm4, %xmm1
 1671                             	        movdqa	%xmm4, %xmm2
 1672                             	        pslld	$31, %xmm0
 1673                             	        pslld	$30, %xmm1
 1674                             	        pslld	$25, %xmm2
 1675                             	        pxor	%xmm1, %xmm0
 1676                             	        pxor	%xmm2, %xmm0
 1677                             	        movdqa	%xmm0, %xmm1
 1678                             	        psrldq	$4, %xmm1
 1679                             	        pslldq	$12, %xmm0
 1680                             	        pxor	%xmm0, %xmm4
 1681                             	        movdqa	%xmm4, %xmm2
 1682                             	        movdqa	%xmm4, %xmm3
 1683                             	        movdqa	%xmm4, %xmm0
 1684                             	        psrld	$0x01, %xmm2
 1685                             	        psrld	$2, %xmm3
 1686                             	        psrld	$7, %xmm0
 1687                             	        pxor	%xmm3, %xmm2
 1688                             	        pxor	%xmm0, %xmm2
 1689                             	        pxor	%xmm1, %xmm2
 1690                             	        pxor	%xmm4, %xmm2
 1691                             	        pxor	%xmm2, %xmm6
 1692                             	        movdqa	(%rsp), %xmm5
 1693                             	L_AES_GCM_encrypt_done_128:
 1694                             	        movl	%r9d, %edx
 1695                             	        cmpl	%edx, %ebx
 1696                             	        jge	L_AES_GCM_encrypt_done_enc
 1697                             	        movl	%r9d, %r13d
 1698                             	        andl	$0xfffffff0, %r13d
 1699                             	        cmpl	%r13d, %ebx
 1700                             	        jge	L_AES_GCM_encrypt_last_block_done
 1701                             	        leaq	(%rdi,%rbx,1), %rcx
 1702                             	        leaq	(%rsi,%rbx,1), %rdx
 1703                             	        movdqa	128(%rsp), %xmm8
 1704                             	        movdqa	%xmm8, %xmm9
 1705                             	        pshufb	L_aes_gcm_bswap_epi64(%rip), %xmm8
 1706                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 1707                             	        pxor	(%r15), %xmm8
 1708                             	        movdqa	%xmm9, 128(%rsp)
 1709                             	        aesenc	16(%r15), %xmm8
 1710                             	        aesenc	32(%r15), %xmm8
 1711                             	        aesenc	48(%r15), %xmm8
 1712                             	        aesenc	64(%r15), %xmm8
 1713                             	        aesenc	80(%r15), %xmm8
 1714                             	        aesenc	96(%r15), %xmm8
 1715                             	        aesenc	112(%r15), %xmm8
 1716                             	        aesenc	128(%r15), %xmm8
 1717                             	        aesenc	144(%r15), %xmm8
 1718                             	        cmpl	$11, %r10d
 1719                             	        movdqa	160(%r15), %xmm9
 1720                             	        jl	L_AES_GCM_encrypt_aesenc_block_aesenc_avx_last
 1721                             	        aesenc	%xmm9, %xmm8
 1722                             	        aesenc	176(%r15), %xmm8
 1723                             	        cmpl	$13, %r10d
 1724                             	        movdqa	192(%r15), %xmm9
 1725                             	        jl	L_AES_GCM_encrypt_aesenc_block_aesenc_avx_last
 1726                             	        aesenc	%xmm9, %xmm8
 1727                             	        aesenc	208(%r15), %xmm8
 1728                             	        movdqa	224(%r15), %xmm9
 1729                             	L_AES_GCM_encrypt_aesenc_block_aesenc_avx_last:
 1730                             	        aesenclast	%xmm9, %xmm8
 1731                             	        movdqu	(%rcx), %xmm9
 1732                             	        pxor	%xmm9, %xmm8
 1733                             	        movdqu	%xmm8, (%rdx)
 1734                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 1735                             	        pxor	%xmm8, %xmm6
 1736                             	        addl	$16, %ebx
 1737                             	        cmpl	%r13d, %ebx
 1738                             	        jge	L_AES_GCM_encrypt_last_block_ghash
 1739                             	L_AES_GCM_encrypt_last_block_start:
 1740                             	        leaq	(%rdi,%rbx,1), %rcx
 1741                             	        leaq	(%rsi,%rbx,1), %rdx
 1742                             	        movdqa	128(%rsp), %xmm8
 1743                             	        movdqa	%xmm8, %xmm9
 1744                             	        pshufb	L_aes_gcm_bswap_epi64(%rip), %xmm8
 1745                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 1746                             	        pxor	(%r15), %xmm8
 1747                             	        movdqa	%xmm9, 128(%rsp)
 1748                             	        movdqa	%xmm6, %xmm10
 1749                             	        pclmulqdq	$16, %xmm5, %xmm10
 1750                             	        aesenc	16(%r15), %xmm8
 1751                             	        aesenc	32(%r15), %xmm8
 1752                             	        movdqa	%xmm6, %xmm11
 1753                             	        pclmulqdq	$0x01, %xmm5, %xmm11
 1754                             	        aesenc	48(%r15), %xmm8
 1755                             	        aesenc	64(%r15), %xmm8
 1756                             	        movdqa	%xmm6, %xmm12
 1757                             	        pclmulqdq	$0x00, %xmm5, %xmm12
 1758                             	        aesenc	80(%r15), %xmm8
 1759                             	        movdqa	%xmm6, %xmm1
 1760                             	        pclmulqdq	$0x11, %xmm5, %xmm1
 1761                             	        aesenc	96(%r15), %xmm8
 1762                             	        pxor	%xmm11, %xmm10
 1763                             	        movdqa	%xmm10, %xmm2
 1764                             	        psrldq	$8, %xmm10
 1765                             	        pslldq	$8, %xmm2
 1766                             	        aesenc	112(%r15), %xmm8
 1767                             	        movdqa	%xmm1, %xmm3
 1768                             	        pxor	%xmm12, %xmm2
 1769                             	        pxor	%xmm10, %xmm3
 1770                             	        movdqa	L_aes_gcm_mod2_128(%rip), %xmm0
 1771                             	        movdqa	%xmm2, %xmm11
 1772                             	        pclmulqdq	$16, %xmm0, %xmm11
 1773                             	        aesenc	128(%r15), %xmm8
 1774                             	        pshufd	$0x4e, %xmm2, %xmm10
 1775                             	        pxor	%xmm11, %xmm10
 1776                             	        movdqa	%xmm10, %xmm11
 1777                             	        pclmulqdq	$16, %xmm0, %xmm11
 1778                             	        aesenc	144(%r15), %xmm8
 1779                             	        pshufd	$0x4e, %xmm10, %xmm6
 1780                             	        pxor	%xmm11, %xmm6
 1781                             	        pxor	%xmm3, %xmm6
 1782                             	        cmpl	$11, %r10d
 1783                             	        movdqa	160(%r15), %xmm9
 1784                             	        jl	L_AES_GCM_encrypt_aesenc_gfmul_last
 1785                             	        aesenc	%xmm9, %xmm8
 1786                             	        aesenc	176(%r15), %xmm8
 1787                             	        cmpl	$13, %r10d
 1788                             	        movdqa	192(%r15), %xmm9
 1789                             	        jl	L_AES_GCM_encrypt_aesenc_gfmul_last
 1790                             	        aesenc	%xmm9, %xmm8
 1791                             	        aesenc	208(%r15), %xmm8
 1792                             	        movdqa	224(%r15), %xmm9
 1793                             	L_AES_GCM_encrypt_aesenc_gfmul_last:
 1794                             	        aesenclast	%xmm9, %xmm8
 1795                             	        movdqu	(%rcx), %xmm9
 1796                             	        pxor	%xmm9, %xmm8
 1797                             	        movdqu	%xmm8, (%rdx)
 1798                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 1799                             	        pxor	%xmm8, %xmm6
 1800                             	        addl	$16, %ebx
 1801                             	        cmpl	%r13d, %ebx
 1802                             	        jl	L_AES_GCM_encrypt_last_block_start
 1803                             	L_AES_GCM_encrypt_last_block_ghash:
 1804                             	        pshufd	$0x4e, %xmm5, %xmm9
 1805                             	        pshufd	$0x4e, %xmm6, %xmm10
 1806                             	        movdqa	%xmm6, %xmm11
 1807                             	        movdqa	%xmm6, %xmm8
 1808                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 1809                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 1810                             	        pxor	%xmm5, %xmm9
 1811                             	        pxor	%xmm6, %xmm10
 1812                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 1813                             	        pxor	%xmm8, %xmm9
 1814                             	        pxor	%xmm11, %xmm9
 1815                             	        movdqa	%xmm9, %xmm10
 1816                             	        movdqa	%xmm11, %xmm6
 1817                             	        pslldq	$8, %xmm10
 1818                             	        psrldq	$8, %xmm9
 1819                             	        pxor	%xmm10, %xmm8
 1820                             	        pxor	%xmm9, %xmm6
 1821                             	        movdqa	%xmm8, %xmm12
 1822                             	        movdqa	%xmm8, %xmm13
 1823                             	        movdqa	%xmm8, %xmm14
 1824                             	        pslld	$31, %xmm12
 1825                             	        pslld	$30, %xmm13
 1826                             	        pslld	$25, %xmm14
 1827                             	        pxor	%xmm13, %xmm12
 1828                             	        pxor	%xmm14, %xmm12
 1829                             	        movdqa	%xmm12, %xmm13
 1830                             	        psrldq	$4, %xmm13
 1831                             	        pslldq	$12, %xmm12
 1832                             	        pxor	%xmm12, %xmm8
 1833                             	        movdqa	%xmm8, %xmm14
 1834                             	        movdqa	%xmm8, %xmm10
 1835                             	        movdqa	%xmm8, %xmm9
 1836                             	        psrld	$0x01, %xmm14
 1837                             	        psrld	$2, %xmm10
 1838                             	        psrld	$7, %xmm9
 1839                             	        pxor	%xmm10, %xmm14
 1840                             	        pxor	%xmm9, %xmm14
 1841                             	        pxor	%xmm13, %xmm14
 1842                             	        pxor	%xmm8, %xmm14
 1843                             	        pxor	%xmm14, %xmm6
 1844                             	L_AES_GCM_encrypt_last_block_done:
 1845                             	        movl	%r9d, %ecx
 1846                             	        movl	%ecx, %edx
 1847                             	        andl	$15, %ecx
 1848                             	        jz	L_AES_GCM_encrypt_aesenc_last15_enc_avx_done
 1849                             	        movdqa	128(%rsp), %xmm4
 1850                             	        pshufb	L_aes_gcm_bswap_epi64(%rip), %xmm4
 1851                             	        pxor	(%r15), %xmm4
 1852                             	        aesenc	16(%r15), %xmm4
 1853                             	        aesenc	32(%r15), %xmm4
 1854                             	        aesenc	48(%r15), %xmm4
 1855                             	        aesenc	64(%r15), %xmm4
 1856                             	        aesenc	80(%r15), %xmm4
 1857                             	        aesenc	96(%r15), %xmm4
 1858                             	        aesenc	112(%r15), %xmm4
 1859                             	        aesenc	128(%r15), %xmm4
 1860                             	        aesenc	144(%r15), %xmm4
 1861                             	        cmpl	$11, %r10d
 1862                             	        movdqa	160(%r15), %xmm9
 1863                             	        jl	L_AES_GCM_encrypt_aesenc_last15_enc_avx_aesenc_avx_last
 1864                             	        aesenc	%xmm9, %xmm4
 1865                             	        aesenc	176(%r15), %xmm4
 1866                             	        cmpl	$13, %r10d
 1867                             	        movdqa	192(%r15), %xmm9
 1868                             	        jl	L_AES_GCM_encrypt_aesenc_last15_enc_avx_aesenc_avx_last
 1869                             	        aesenc	%xmm9, %xmm4
 1870                             	        aesenc	208(%r15), %xmm4
 1871                             	        movdqa	224(%r15), %xmm9
 1872                             	L_AES_GCM_encrypt_aesenc_last15_enc_avx_aesenc_avx_last:
 1873                             	        aesenclast	%xmm9, %xmm4
 1874                             	        subq	$16, %rsp
 1875                             	        xorl	%ecx, %ecx
 1876                             	        movdqa	%xmm4, (%rsp)
 1877                             	L_AES_GCM_encrypt_aesenc_last15_enc_avx_loop:
 1878                             	        movzbl	(%rdi,%rbx,1), %r13d
 1879                             	        xorb	(%rsp,%rcx,1), %r13b
 1880                             	        movb	%r13b, (%rsi,%rbx,1)
 1881                             	        movb	%r13b, (%rsp,%rcx,1)
 1882                             	        incl	%ebx
 1883                             	        incl	%ecx
 1884                             	        cmpl	%edx, %ebx
 1885                             	        jl	L_AES_GCM_encrypt_aesenc_last15_enc_avx_loop
 1886                             	        xorq	%r13, %r13
 1887                             	        cmpl	$16, %ecx
 1888                             	        je	L_AES_GCM_encrypt_aesenc_last15_enc_avx_finish_enc
 1889                             	L_AES_GCM_encrypt_aesenc_last15_enc_avx_byte_loop:
 1890                             	        movb	%r13b, (%rsp,%rcx,1)
 1891                             	        incl	%ecx
 1892                             	        cmpl	$16, %ecx
 1893                             	        jl	L_AES_GCM_encrypt_aesenc_last15_enc_avx_byte_loop
 1894                             	L_AES_GCM_encrypt_aesenc_last15_enc_avx_finish_enc:
 1895                             	        movdqa	(%rsp), %xmm4
 1896                             	        addq	$16, %rsp
 1897                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm4
 1898                             	        pxor	%xmm4, %xmm6
 1899                             	        pshufd	$0x4e, %xmm5, %xmm9
 1900                             	        pshufd	$0x4e, %xmm6, %xmm10
 1901                             	        movdqa	%xmm6, %xmm11
 1902                             	        movdqa	%xmm6, %xmm8
 1903                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 1904                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 1905                             	        pxor	%xmm5, %xmm9
 1906                             	        pxor	%xmm6, %xmm10
 1907                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 1908                             	        pxor	%xmm8, %xmm9
 1909                             	        pxor	%xmm11, %xmm9
 1910                             	        movdqa	%xmm9, %xmm10
 1911                             	        movdqa	%xmm11, %xmm6
 1912                             	        pslldq	$8, %xmm10
 1913                             	        psrldq	$8, %xmm9
 1914                             	        pxor	%xmm10, %xmm8
 1915                             	        pxor	%xmm9, %xmm6
 1916                             	        movdqa	%xmm8, %xmm12
 1917                             	        movdqa	%xmm8, %xmm13
 1918                             	        movdqa	%xmm8, %xmm14
 1919                             	        pslld	$31, %xmm12
 1920                             	        pslld	$30, %xmm13
 1921                             	        pslld	$25, %xmm14
 1922                             	        pxor	%xmm13, %xmm12
 1923                             	        pxor	%xmm14, %xmm12
 1924                             	        movdqa	%xmm12, %xmm13
 1925                             	        psrldq	$4, %xmm13
 1926                             	        pslldq	$12, %xmm12
 1927                             	        pxor	%xmm12, %xmm8
 1928                             	        movdqa	%xmm8, %xmm14
 1929                             	        movdqa	%xmm8, %xmm10
 1930                             	        movdqa	%xmm8, %xmm9
 1931                             	        psrld	$0x01, %xmm14
 1932                             	        psrld	$2, %xmm10
 1933                             	        psrld	$7, %xmm9
 1934                             	        pxor	%xmm10, %xmm14
 1935                             	        pxor	%xmm9, %xmm14
 1936                             	        pxor	%xmm13, %xmm14
 1937                             	        pxor	%xmm8, %xmm14
 1938                             	        pxor	%xmm14, %xmm6
 1939                             	L_AES_GCM_encrypt_aesenc_last15_enc_avx_done:
 1940                             	L_AES_GCM_encrypt_done_enc:
 1941                             	        movl	%r9d, %edx
 1942                             	        movl	%r11d, %ecx
 1943                             	        shlq	$3, %rdx
 1944                             	        shlq	$3, %rcx
 1945                             	        pinsrq	$0x00, %rdx, %xmm0
 1946                             	        pinsrq	$0x01, %rcx, %xmm0
 1947                             	        pxor	%xmm0, %xmm6
 1948                             	        pshufd	$0x4e, %xmm5, %xmm9
 1949                             	        pshufd	$0x4e, %xmm6, %xmm10
 1950                             	        movdqa	%xmm6, %xmm11
 1951                             	        movdqa	%xmm6, %xmm8
 1952                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 1953                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 1954                             	        pxor	%xmm5, %xmm9
 1955                             	        pxor	%xmm6, %xmm10
 1956                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 1957                             	        pxor	%xmm8, %xmm9
 1958                             	        pxor	%xmm11, %xmm9
 1959                             	        movdqa	%xmm9, %xmm10
 1960                             	        movdqa	%xmm11, %xmm6
 1961                             	        pslldq	$8, %xmm10
 1962                             	        psrldq	$8, %xmm9
 1963                             	        pxor	%xmm10, %xmm8
 1964                             	        pxor	%xmm9, %xmm6
 1965                             	        movdqa	%xmm8, %xmm12
 1966                             	        movdqa	%xmm8, %xmm13
 1967                             	        movdqa	%xmm8, %xmm14
 1968                             	        pslld	$31, %xmm12
 1969                             	        pslld	$30, %xmm13
 1970                             	        pslld	$25, %xmm14
 1971                             	        pxor	%xmm13, %xmm12
 1972                             	        pxor	%xmm14, %xmm12
 1973                             	        movdqa	%xmm12, %xmm13
 1974                             	        psrldq	$4, %xmm13
 1975                             	        pslldq	$12, %xmm12
 1976                             	        pxor	%xmm12, %xmm8
 1977                             	        movdqa	%xmm8, %xmm14
 1978                             	        movdqa	%xmm8, %xmm10
 1979                             	        movdqa	%xmm8, %xmm9
 1980                             	        psrld	$0x01, %xmm14
 1981                             	        psrld	$2, %xmm10
 1982                             	        psrld	$7, %xmm9
 1983                             	        pxor	%xmm10, %xmm14
 1984                             	        pxor	%xmm9, %xmm14
 1985                             	        pxor	%xmm13, %xmm14
 1986                             	        pxor	%xmm8, %xmm14
 1987                             	        pxor	%xmm14, %xmm6
 1988                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm6
 1989                             	        movdqa	144(%rsp), %xmm0
 1990                             	        pxor	%xmm6, %xmm0
 1991                             	        cmpl	$16, %r14d
 1992                             	        je	L_AES_GCM_encrypt_store_tag_16
 1993                             	        xorq	%rcx, %rcx
 1994                             	        movdqa	%xmm0, (%rsp)
 1995                             	L_AES_GCM_encrypt_store_tag_loop:
 1996                             	        movzbl	(%rsp,%rcx,1), %r13d
 1997                             	        movb	%r13b, (%r8,%rcx,1)
 1998                             	        incl	%ecx
 1999                             	        cmpl	%r14d, %ecx
 2000                             	        jne	L_AES_GCM_encrypt_store_tag_loop
 2001                             	        jmp	L_AES_GCM_encrypt_store_tag_done
 2002                             	L_AES_GCM_encrypt_store_tag_16:
 2003                             	        movdqu	%xmm0, (%r8)
 2004                             	L_AES_GCM_encrypt_store_tag_done:
 2005                             	        addq	$0xa0, %rsp
 2006                             	        popq	%r15
 2007                             	        popq	%r14
 2008                             	        popq	%rbx
 2009                             	        popq	%r12
 2010                             	        popq	%r13
 2011                             	        repz retq
 2012                             	#ifndef __APPLE__
 2014                             	#endif /* __APPLE__ */
 2015                             	#ifndef __APPLE__
 2016                             	.text
 2017                             	.globl	AES_GCM_decrypt
 2019                             	.align	16
 2020                             	AES_GCM_decrypt:
 2021                             	#else
 2022                             	.section	__TEXT,__text
 2023                             	.globl	_AES_GCM_decrypt
 2024                             	.p2align	4
 2025                             	_AES_GCM_decrypt:
 2026                             	#endif /* __APPLE__ */
 2027                             	        pushq	%r13
 2028                             	        pushq	%r12
 2029                             	        pushq	%rbx
 2030                             	        pushq	%r14
 2031                             	        pushq	%r15
 2032                             	        pushq	%rbp
 2033                             	        movq	%rdx, %r12
 2034                             	        movq	%rcx, %rax
 2035                             	        movl	56(%rsp), %r11d
 2036                             	        movl	64(%rsp), %ebx
 2037                             	        movl	72(%rsp), %r14d
 2038                             	        movq	80(%rsp), %r15
 2039                             	        movl	88(%rsp), %r10d
 2040                             	        movq	96(%rsp), %rbp
 2041                             	        subq	$0xa8, %rsp
 2042                             	        pxor	%xmm4, %xmm4
 2043                             	        pxor	%xmm6, %xmm6
 2044                             	        cmpl	$12, %ebx
 2045                             	        movl	%ebx, %edx
 2046                             	        jne	L_AES_GCM_decrypt_iv_not_12
 2047                             	        # # Calculate values when IV is 12 bytes
 2048                             	        # Set counter based on IV
 2049                             	        movl	$0x1000000, %ecx
 2050                             	        pinsrq	$0x00, (%rax), %xmm4
 2051                             	        pinsrd	$2, 8(%rax), %xmm4
 2052                             	        pinsrd	$3, %ecx, %xmm4
 2053                             	        # H = Encrypt X(=0) and T = Encrypt counter
 2054                             	        movdqa	%xmm4, %xmm1
 2055                             	        movdqa	(%r15), %xmm5
 2056                             	        pxor	%xmm5, %xmm1
 2057                             	        movdqa	16(%r15), %xmm7
 2058                             	        aesenc	%xmm7, %xmm5
 2059                             	        aesenc	%xmm7, %xmm1
 2060                             	        movdqa	32(%r15), %xmm7
 2061                             	        aesenc	%xmm7, %xmm5
 2062                             	        aesenc	%xmm7, %xmm1
 2063                             	        movdqa	48(%r15), %xmm7
 2064                             	        aesenc	%xmm7, %xmm5
 2065                             	        aesenc	%xmm7, %xmm1
 2066                             	        movdqa	64(%r15), %xmm7
 2067                             	        aesenc	%xmm7, %xmm5
 2068                             	        aesenc	%xmm7, %xmm1
 2069                             	        movdqa	80(%r15), %xmm7
 2070                             	        aesenc	%xmm7, %xmm5
 2071                             	        aesenc	%xmm7, %xmm1
 2072                             	        movdqa	96(%r15), %xmm7
 2073                             	        aesenc	%xmm7, %xmm5
 2074                             	        aesenc	%xmm7, %xmm1
 2075                             	        movdqa	112(%r15), %xmm7
 2076                             	        aesenc	%xmm7, %xmm5
 2077                             	        aesenc	%xmm7, %xmm1
 2078                             	        movdqa	128(%r15), %xmm7
 2079                             	        aesenc	%xmm7, %xmm5
 2080                             	        aesenc	%xmm7, %xmm1
 2081                             	        movdqa	144(%r15), %xmm7
 2082                             	        aesenc	%xmm7, %xmm5
 2083                             	        aesenc	%xmm7, %xmm1
 2084                             	        cmpl	$11, %r10d
 2085                             	        movdqa	160(%r15), %xmm7
 2086                             	        jl	L_AES_GCM_decrypt_calc_iv_12_last
 2087                             	        aesenc	%xmm7, %xmm5
 2088                             	        aesenc	%xmm7, %xmm1
 2089                             	        movdqa	176(%r15), %xmm7
 2090                             	        aesenc	%xmm7, %xmm5
 2091                             	        aesenc	%xmm7, %xmm1
 2092                             	        cmpl	$13, %r10d
 2093                             	        movdqa	192(%r15), %xmm7
 2094                             	        jl	L_AES_GCM_decrypt_calc_iv_12_last
 2095                             	        aesenc	%xmm7, %xmm5
 2096                             	        aesenc	%xmm7, %xmm1
 2097                             	        movdqa	208(%r15), %xmm7
 2098                             	        aesenc	%xmm7, %xmm5
 2099                             	        aesenc	%xmm7, %xmm1
 2100                             	        movdqa	224(%r15), %xmm7
 2101                             	L_AES_GCM_decrypt_calc_iv_12_last:
 2102                             	        aesenclast	%xmm7, %xmm5
 2103                             	        aesenclast	%xmm7, %xmm1
 2104                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm5
 2105                             	        movdqa	%xmm1, 144(%rsp)
 2106                             	        jmp	L_AES_GCM_decrypt_iv_done
 2107                             	L_AES_GCM_decrypt_iv_not_12:
 2108                             	        # Calculate values when IV is not 12 bytes
 2109                             	        # H = Encrypt X(=0)
 2110                             	        movdqa	(%r15), %xmm5
 2111                             	        aesenc	16(%r15), %xmm5
 2112                             	        aesenc	32(%r15), %xmm5
 2113                             	        aesenc	48(%r15), %xmm5
 2114                             	        aesenc	64(%r15), %xmm5
 2115                             	        aesenc	80(%r15), %xmm5
 2116                             	        aesenc	96(%r15), %xmm5
 2117                             	        aesenc	112(%r15), %xmm5
 2118                             	        aesenc	128(%r15), %xmm5
 2119                             	        aesenc	144(%r15), %xmm5
 2120                             	        cmpl	$11, %r10d
 2121                             	        movdqa	160(%r15), %xmm9
 2122                             	        jl	L_AES_GCM_decrypt_calc_iv_1_aesenc_avx_last
 2123                             	        aesenc	%xmm9, %xmm5
 2124                             	        aesenc	176(%r15), %xmm5
 2125                             	        cmpl	$13, %r10d
 2126                             	        movdqa	192(%r15), %xmm9
 2127                             	        jl	L_AES_GCM_decrypt_calc_iv_1_aesenc_avx_last
 2128                             	        aesenc	%xmm9, %xmm5
 2129                             	        aesenc	208(%r15), %xmm5
 2130                             	        movdqa	224(%r15), %xmm9
 2131                             	L_AES_GCM_decrypt_calc_iv_1_aesenc_avx_last:
 2132                             	        aesenclast	%xmm9, %xmm5
 2133                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm5
 2134                             	        # Calc counter
 2135                             	        # Initialization vector
 2136                             	        cmpl	$0x00, %edx
 2137                             	        movq	$0x00, %rcx
 2138                             	        je	L_AES_GCM_decrypt_calc_iv_done
 2139                             	        cmpl	$16, %edx
 2140                             	        jl	L_AES_GCM_decrypt_calc_iv_lt16
 2141                             	        andl	$0xfffffff0, %edx
 2142                             	L_AES_GCM_decrypt_calc_iv_16_loop:
 2143                             	        movdqu	(%rax,%rcx,1), %xmm8
 2144                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 2145                             	        pxor	%xmm8, %xmm4
 2146                             	        pshufd	$0x4e, %xmm4, %xmm1
 2147                             	        pshufd	$0x4e, %xmm5, %xmm2
 2148                             	        movdqa	%xmm5, %xmm3
 2149                             	        movdqa	%xmm5, %xmm0
 2150                             	        pclmulqdq	$0x11, %xmm4, %xmm3
 2151                             	        pclmulqdq	$0x00, %xmm4, %xmm0
 2152                             	        pxor	%xmm4, %xmm1
 2153                             	        pxor	%xmm5, %xmm2
 2154                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 2155                             	        pxor	%xmm0, %xmm1
 2156                             	        pxor	%xmm3, %xmm1
 2157                             	        movdqa	%xmm1, %xmm2
 2158                             	        movdqa	%xmm0, %xmm7
 2159                             	        movdqa	%xmm3, %xmm4
 2160                             	        pslldq	$8, %xmm2
 2161                             	        psrldq	$8, %xmm1
 2162                             	        pxor	%xmm2, %xmm7
 2163                             	        pxor	%xmm1, %xmm4
 2164                             	        movdqa	%xmm7, %xmm0
 2165                             	        movdqa	%xmm4, %xmm1
 2166                             	        psrld	$31, %xmm0
 2167                             	        psrld	$31, %xmm1
 2168                             	        pslld	$0x01, %xmm7
 2169                             	        pslld	$0x01, %xmm4
 2170                             	        movdqa	%xmm0, %xmm2
 2171                             	        pslldq	$4, %xmm0
 2172                             	        psrldq	$12, %xmm2
 2173                             	        pslldq	$4, %xmm1
 2174                             	        por	%xmm2, %xmm4
 2175                             	        por	%xmm0, %xmm7
 2176                             	        por	%xmm1, %xmm4
 2177                             	        movdqa	%xmm7, %xmm0
 2178                             	        movdqa	%xmm7, %xmm1
 2179                             	        movdqa	%xmm7, %xmm2
 2180                             	        pslld	$31, %xmm0
 2181                             	        pslld	$30, %xmm1
 2182                             	        pslld	$25, %xmm2
 2183                             	        pxor	%xmm1, %xmm0
 2184                             	        pxor	%xmm2, %xmm0
 2185                             	        movdqa	%xmm0, %xmm1
 2186                             	        psrldq	$4, %xmm1
 2187                             	        pslldq	$12, %xmm0
 2188                             	        pxor	%xmm0, %xmm7
 2189                             	        movdqa	%xmm7, %xmm2
 2190                             	        movdqa	%xmm7, %xmm3
 2191                             	        movdqa	%xmm7, %xmm0
 2192                             	        psrld	$0x01, %xmm2
 2193                             	        psrld	$2, %xmm3
 2194                             	        psrld	$7, %xmm0
 2195                             	        pxor	%xmm3, %xmm2
 2196                             	        pxor	%xmm0, %xmm2
 2197                             	        pxor	%xmm1, %xmm2
 2198                             	        pxor	%xmm7, %xmm2
 2199                             	        pxor	%xmm2, %xmm4
 2200                             	        addl	$16, %ecx
 2201                             	        cmpl	%edx, %ecx
 2202                             	        jl	L_AES_GCM_decrypt_calc_iv_16_loop
 2203                             	        movl	%ebx, %edx
 2204                             	        cmpl	%edx, %ecx
 2205                             	        je	L_AES_GCM_decrypt_calc_iv_done
 2206                             	L_AES_GCM_decrypt_calc_iv_lt16:
 2207                             	        subq	$16, %rsp
 2208                             	        pxor	%xmm8, %xmm8
 2209                             	        xorl	%ebx, %ebx
 2210                             	        movdqa	%xmm8, (%rsp)
 2211                             	L_AES_GCM_decrypt_calc_iv_loop:
 2212                             	        movzbl	(%rax,%rcx,1), %r13d
 2213                             	        movb	%r13b, (%rsp,%rbx,1)
 2214                             	        incl	%ecx
 2215                             	        incl	%ebx
 2216                             	        cmpl	%edx, %ecx
 2217                             	        jl	L_AES_GCM_decrypt_calc_iv_loop
 2218                             	        movdqa	(%rsp), %xmm8
 2219                             	        addq	$16, %rsp
 2220                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 2221                             	        pxor	%xmm8, %xmm4
 2222                             	        pshufd	$0x4e, %xmm4, %xmm1
 2223                             	        pshufd	$0x4e, %xmm5, %xmm2
 2224                             	        movdqa	%xmm5, %xmm3
 2225                             	        movdqa	%xmm5, %xmm0
 2226                             	        pclmulqdq	$0x11, %xmm4, %xmm3
 2227                             	        pclmulqdq	$0x00, %xmm4, %xmm0
 2228                             	        pxor	%xmm4, %xmm1
 2229                             	        pxor	%xmm5, %xmm2
 2230                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 2231                             	        pxor	%xmm0, %xmm1
 2232                             	        pxor	%xmm3, %xmm1
 2233                             	        movdqa	%xmm1, %xmm2
 2234                             	        movdqa	%xmm0, %xmm7
 2235                             	        movdqa	%xmm3, %xmm4
 2236                             	        pslldq	$8, %xmm2
 2237                             	        psrldq	$8, %xmm1
 2238                             	        pxor	%xmm2, %xmm7
 2239                             	        pxor	%xmm1, %xmm4
 2240                             	        movdqa	%xmm7, %xmm0
 2241                             	        movdqa	%xmm4, %xmm1
 2242                             	        psrld	$31, %xmm0
 2243                             	        psrld	$31, %xmm1
 2244                             	        pslld	$0x01, %xmm7
 2245                             	        pslld	$0x01, %xmm4
 2246                             	        movdqa	%xmm0, %xmm2
 2247                             	        pslldq	$4, %xmm0
 2248                             	        psrldq	$12, %xmm2
 2249                             	        pslldq	$4, %xmm1
 2250                             	        por	%xmm2, %xmm4
 2251                             	        por	%xmm0, %xmm7
 2252                             	        por	%xmm1, %xmm4
 2253                             	        movdqa	%xmm7, %xmm0
 2254                             	        movdqa	%xmm7, %xmm1
 2255                             	        movdqa	%xmm7, %xmm2
 2256                             	        pslld	$31, %xmm0
 2257                             	        pslld	$30, %xmm1
 2258                             	        pslld	$25, %xmm2
 2259                             	        pxor	%xmm1, %xmm0
 2260                             	        pxor	%xmm2, %xmm0
 2261                             	        movdqa	%xmm0, %xmm1
 2262                             	        psrldq	$4, %xmm1
 2263                             	        pslldq	$12, %xmm0
 2264                             	        pxor	%xmm0, %xmm7
 2265                             	        movdqa	%xmm7, %xmm2
 2266                             	        movdqa	%xmm7, %xmm3
 2267                             	        movdqa	%xmm7, %xmm0
 2268                             	        psrld	$0x01, %xmm2
 2269                             	        psrld	$2, %xmm3
 2270                             	        psrld	$7, %xmm0
 2271                             	        pxor	%xmm3, %xmm2
 2272                             	        pxor	%xmm0, %xmm2
 2273                             	        pxor	%xmm1, %xmm2
 2274                             	        pxor	%xmm7, %xmm2
 2275                             	        pxor	%xmm2, %xmm4
 2276                             	L_AES_GCM_decrypt_calc_iv_done:
 2277                             	        # T = Encrypt counter
 2278                             	        pxor	%xmm0, %xmm0
 2279                             	        shll	$3, %edx
 2280                             	        pinsrq	$0x00, %rdx, %xmm0
 2281                             	        pxor	%xmm0, %xmm4
 2282                             	        pshufd	$0x4e, %xmm4, %xmm1
 2283                             	        pshufd	$0x4e, %xmm5, %xmm2
 2284                             	        movdqa	%xmm5, %xmm3
 2285                             	        movdqa	%xmm5, %xmm0
 2286                             	        pclmulqdq	$0x11, %xmm4, %xmm3
 2287                             	        pclmulqdq	$0x00, %xmm4, %xmm0
 2288                             	        pxor	%xmm4, %xmm1
 2289                             	        pxor	%xmm5, %xmm2
 2290                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 2291                             	        pxor	%xmm0, %xmm1
 2292                             	        pxor	%xmm3, %xmm1
 2293                             	        movdqa	%xmm1, %xmm2
 2294                             	        movdqa	%xmm0, %xmm7
 2295                             	        movdqa	%xmm3, %xmm4
 2296                             	        pslldq	$8, %xmm2
 2297                             	        psrldq	$8, %xmm1
 2298                             	        pxor	%xmm2, %xmm7
 2299                             	        pxor	%xmm1, %xmm4
 2300                             	        movdqa	%xmm7, %xmm0
 2301                             	        movdqa	%xmm4, %xmm1
 2302                             	        psrld	$31, %xmm0
 2303                             	        psrld	$31, %xmm1
 2304                             	        pslld	$0x01, %xmm7
 2305                             	        pslld	$0x01, %xmm4
 2306                             	        movdqa	%xmm0, %xmm2
 2307                             	        pslldq	$4, %xmm0
 2308                             	        psrldq	$12, %xmm2
 2309                             	        pslldq	$4, %xmm1
 2310                             	        por	%xmm2, %xmm4
 2311                             	        por	%xmm0, %xmm7
 2312                             	        por	%xmm1, %xmm4
 2313                             	        movdqa	%xmm7, %xmm0
 2314                             	        movdqa	%xmm7, %xmm1
 2315                             	        movdqa	%xmm7, %xmm2
 2316                             	        pslld	$31, %xmm0
 2317                             	        pslld	$30, %xmm1
 2318                             	        pslld	$25, %xmm2
 2319                             	        pxor	%xmm1, %xmm0
 2320                             	        pxor	%xmm2, %xmm0
 2321                             	        movdqa	%xmm0, %xmm1
 2322                             	        psrldq	$4, %xmm1
 2323                             	        pslldq	$12, %xmm0
 2324                             	        pxor	%xmm0, %xmm7
 2325                             	        movdqa	%xmm7, %xmm2
 2326                             	        movdqa	%xmm7, %xmm3
 2327                             	        movdqa	%xmm7, %xmm0
 2328                             	        psrld	$0x01, %xmm2
 2329                             	        psrld	$2, %xmm3
 2330                             	        psrld	$7, %xmm0
 2331                             	        pxor	%xmm3, %xmm2
 2332                             	        pxor	%xmm0, %xmm2
 2333                             	        pxor	%xmm1, %xmm2
 2334                             	        pxor	%xmm7, %xmm2
 2335                             	        pxor	%xmm2, %xmm4
 2336                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm4
 2337                             	        #   Encrypt counter
 2338                             	        movdqa	(%r15), %xmm8
 2339                             	        pxor	%xmm4, %xmm8
 2340                             	        aesenc	16(%r15), %xmm8
 2341                             	        aesenc	32(%r15), %xmm8
 2342                             	        aesenc	48(%r15), %xmm8
 2343                             	        aesenc	64(%r15), %xmm8
 2344                             	        aesenc	80(%r15), %xmm8
 2345                             	        aesenc	96(%r15), %xmm8
 2346                             	        aesenc	112(%r15), %xmm8
 2347                             	        aesenc	128(%r15), %xmm8
 2348                             	        aesenc	144(%r15), %xmm8
 2349                             	        cmpl	$11, %r10d
 2350                             	        movdqa	160(%r15), %xmm9
 2351                             	        jl	L_AES_GCM_decrypt_calc_iv_2_aesenc_avx_last
 2352                             	        aesenc	%xmm9, %xmm8
 2353                             	        aesenc	176(%r15), %xmm8
 2354                             	        cmpl	$13, %r10d
 2355                             	        movdqa	192(%r15), %xmm9
 2356                             	        jl	L_AES_GCM_decrypt_calc_iv_2_aesenc_avx_last
 2357                             	        aesenc	%xmm9, %xmm8
 2358                             	        aesenc	208(%r15), %xmm8
 2359                             	        movdqa	224(%r15), %xmm9
 2360                             	L_AES_GCM_decrypt_calc_iv_2_aesenc_avx_last:
 2361                             	        aesenclast	%xmm9, %xmm8
 2362                             	        movdqa	%xmm8, 144(%rsp)
 2363                             	L_AES_GCM_decrypt_iv_done:
 2364                             	        # Additional authentication data
 2365                             	        movl	%r11d, %edx
 2366                             	        cmpl	$0x00, %edx
 2367                             	        je	L_AES_GCM_decrypt_calc_aad_done
 2368                             	        xorl	%ecx, %ecx
 2369                             	        cmpl	$16, %edx
 2370                             	        jl	L_AES_GCM_decrypt_calc_aad_lt16
 2371                             	        andl	$0xfffffff0, %edx
 2372                             	L_AES_GCM_decrypt_calc_aad_16_loop:
 2373                             	        movdqu	(%r12,%rcx,1), %xmm8
 2374                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 2375                             	        pxor	%xmm8, %xmm6
 2376                             	        pshufd	$0x4e, %xmm6, %xmm1
 2377                             	        pshufd	$0x4e, %xmm5, %xmm2
 2378                             	        movdqa	%xmm5, %xmm3
 2379                             	        movdqa	%xmm5, %xmm0
 2380                             	        pclmulqdq	$0x11, %xmm6, %xmm3
 2381                             	        pclmulqdq	$0x00, %xmm6, %xmm0
 2382                             	        pxor	%xmm6, %xmm1
 2383                             	        pxor	%xmm5, %xmm2
 2384                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 2385                             	        pxor	%xmm0, %xmm1
 2386                             	        pxor	%xmm3, %xmm1
 2387                             	        movdqa	%xmm1, %xmm2
 2388                             	        movdqa	%xmm0, %xmm7
 2389                             	        movdqa	%xmm3, %xmm6
 2390                             	        pslldq	$8, %xmm2
 2391                             	        psrldq	$8, %xmm1
 2392                             	        pxor	%xmm2, %xmm7
 2393                             	        pxor	%xmm1, %xmm6
 2394                             	        movdqa	%xmm7, %xmm0
 2395                             	        movdqa	%xmm6, %xmm1
 2396                             	        psrld	$31, %xmm0
 2397                             	        psrld	$31, %xmm1
 2398                             	        pslld	$0x01, %xmm7
 2399                             	        pslld	$0x01, %xmm6
 2400                             	        movdqa	%xmm0, %xmm2
 2401                             	        pslldq	$4, %xmm0
 2402                             	        psrldq	$12, %xmm2
 2403                             	        pslldq	$4, %xmm1
 2404                             	        por	%xmm2, %xmm6
 2405                             	        por	%xmm0, %xmm7
 2406                             	        por	%xmm1, %xmm6
 2407                             	        movdqa	%xmm7, %xmm0
 2408                             	        movdqa	%xmm7, %xmm1
 2409                             	        movdqa	%xmm7, %xmm2
 2410                             	        pslld	$31, %xmm0
 2411                             	        pslld	$30, %xmm1
 2412                             	        pslld	$25, %xmm2
 2413                             	        pxor	%xmm1, %xmm0
 2414                             	        pxor	%xmm2, %xmm0
 2415                             	        movdqa	%xmm0, %xmm1
 2416                             	        psrldq	$4, %xmm1
 2417                             	        pslldq	$12, %xmm0
 2418                             	        pxor	%xmm0, %xmm7
 2419                             	        movdqa	%xmm7, %xmm2
 2420                             	        movdqa	%xmm7, %xmm3
 2421                             	        movdqa	%xmm7, %xmm0
 2422                             	        psrld	$0x01, %xmm2
 2423                             	        psrld	$2, %xmm3
 2424                             	        psrld	$7, %xmm0
 2425                             	        pxor	%xmm3, %xmm2
 2426                             	        pxor	%xmm0, %xmm2
 2427                             	        pxor	%xmm1, %xmm2
 2428                             	        pxor	%xmm7, %xmm2
 2429                             	        pxor	%xmm2, %xmm6
 2430                             	        addl	$16, %ecx
 2431                             	        cmpl	%edx, %ecx
 2432                             	        jl	L_AES_GCM_decrypt_calc_aad_16_loop
 2433                             	        movl	%r11d, %edx
 2434                             	        cmpl	%edx, %ecx
 2435                             	        je	L_AES_GCM_decrypt_calc_aad_done
 2436                             	L_AES_GCM_decrypt_calc_aad_lt16:
 2437                             	        subq	$16, %rsp
 2438                             	        pxor	%xmm8, %xmm8
 2439                             	        xorl	%ebx, %ebx
 2440                             	        movdqa	%xmm8, (%rsp)
 2441                             	L_AES_GCM_decrypt_calc_aad_loop:
 2442                             	        movzbl	(%r12,%rcx,1), %r13d
 2443                             	        movb	%r13b, (%rsp,%rbx,1)
 2444                             	        incl	%ecx
 2445                             	        incl	%ebx
 2446                             	        cmpl	%edx, %ecx
 2447                             	        jl	L_AES_GCM_decrypt_calc_aad_loop
 2448                             	        movdqa	(%rsp), %xmm8
 2449                             	        addq	$16, %rsp
 2450                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 2451                             	        pxor	%xmm8, %xmm6
 2452                             	        pshufd	$0x4e, %xmm6, %xmm1
 2453                             	        pshufd	$0x4e, %xmm5, %xmm2
 2454                             	        movdqa	%xmm5, %xmm3
 2455                             	        movdqa	%xmm5, %xmm0
 2456                             	        pclmulqdq	$0x11, %xmm6, %xmm3
 2457                             	        pclmulqdq	$0x00, %xmm6, %xmm0
 2458                             	        pxor	%xmm6, %xmm1
 2459                             	        pxor	%xmm5, %xmm2
 2460                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 2461                             	        pxor	%xmm0, %xmm1
 2462                             	        pxor	%xmm3, %xmm1
 2463                             	        movdqa	%xmm1, %xmm2
 2464                             	        movdqa	%xmm0, %xmm7
 2465                             	        movdqa	%xmm3, %xmm6
 2466                             	        pslldq	$8, %xmm2
 2467                             	        psrldq	$8, %xmm1
 2468                             	        pxor	%xmm2, %xmm7
 2469                             	        pxor	%xmm1, %xmm6
 2470                             	        movdqa	%xmm7, %xmm0
 2471                             	        movdqa	%xmm6, %xmm1
 2472                             	        psrld	$31, %xmm0
 2473                             	        psrld	$31, %xmm1
 2474                             	        pslld	$0x01, %xmm7
 2475                             	        pslld	$0x01, %xmm6
 2476                             	        movdqa	%xmm0, %xmm2
 2477                             	        pslldq	$4, %xmm0
 2478                             	        psrldq	$12, %xmm2
 2479                             	        pslldq	$4, %xmm1
 2480                             	        por	%xmm2, %xmm6
 2481                             	        por	%xmm0, %xmm7
 2482                             	        por	%xmm1, %xmm6
 2483                             	        movdqa	%xmm7, %xmm0
 2484                             	        movdqa	%xmm7, %xmm1
 2485                             	        movdqa	%xmm7, %xmm2
 2486                             	        pslld	$31, %xmm0
 2487                             	        pslld	$30, %xmm1
 2488                             	        pslld	$25, %xmm2
 2489                             	        pxor	%xmm1, %xmm0
 2490                             	        pxor	%xmm2, %xmm0
 2491                             	        movdqa	%xmm0, %xmm1
 2492                             	        psrldq	$4, %xmm1
 2493                             	        pslldq	$12, %xmm0
 2494                             	        pxor	%xmm0, %xmm7
 2495                             	        movdqa	%xmm7, %xmm2
 2496                             	        movdqa	%xmm7, %xmm3
 2497                             	        movdqa	%xmm7, %xmm0
 2498                             	        psrld	$0x01, %xmm2
 2499                             	        psrld	$2, %xmm3
 2500                             	        psrld	$7, %xmm0
 2501                             	        pxor	%xmm3, %xmm2
 2502                             	        pxor	%xmm0, %xmm2
 2503                             	        pxor	%xmm1, %xmm2
 2504                             	        pxor	%xmm7, %xmm2
 2505                             	        pxor	%xmm2, %xmm6
 2506                             	L_AES_GCM_decrypt_calc_aad_done:
 2507                             	        # Calculate counter and H
 2508                             	        pshufb	L_aes_gcm_bswap_epi64(%rip), %xmm4
 2509                             	        movdqa	%xmm5, %xmm9
 2510                             	        paddd	L_aes_gcm_one(%rip), %xmm4
 2511                             	        movdqa	%xmm5, %xmm8
 2512                             	        movdqa	%xmm4, 128(%rsp)
 2513                             	        psrlq	$63, %xmm9
 2514                             	        psllq	$0x01, %xmm8
 2515                             	        pslldq	$8, %xmm9
 2516                             	        por	%xmm9, %xmm8
 2517                             	        pshufd	$0xff, %xmm5, %xmm5
 2518                             	        psrad	$31, %xmm5
 2519                             	        pand	L_aes_gcm_mod2_128(%rip), %xmm5
 2520                             	        pxor	%xmm8, %xmm5
 2521                             	        xorl	%ebx, %ebx
 2522                             	        cmpl	$0x80, %r9d
 2523                             	        movl	%r9d, %r13d
 2524                             	        jl	L_AES_GCM_decrypt_done_128
 2525                             	        andl	$0xffffff80, %r13d
 2526                             	        movdqa	%xmm6, %xmm2
 2527                             	        # H ^ 1
 2528                             	        movdqa	%xmm5, (%rsp)
 2529                             	        # H ^ 2
 2530                             	        pshufd	$0x4e, %xmm5, %xmm9
 2531                             	        pshufd	$0x4e, %xmm5, %xmm10
 2532                             	        movdqa	%xmm5, %xmm11
 2533                             	        movdqa	%xmm5, %xmm8
 2534                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 2535                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 2536                             	        pxor	%xmm5, %xmm9
 2537                             	        pxor	%xmm5, %xmm10
 2538                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 2539                             	        pxor	%xmm8, %xmm9
 2540                             	        pxor	%xmm11, %xmm9
 2541                             	        movdqa	%xmm9, %xmm10
 2542                             	        movdqa	%xmm11, %xmm0
 2543                             	        pslldq	$8, %xmm10
 2544                             	        psrldq	$8, %xmm9
 2545                             	        pxor	%xmm10, %xmm8
 2546                             	        pxor	%xmm9, %xmm0
 2547                             	        movdqa	%xmm8, %xmm12
 2548                             	        movdqa	%xmm8, %xmm13
 2549                             	        movdqa	%xmm8, %xmm14
 2550                             	        pslld	$31, %xmm12
 2551                             	        pslld	$30, %xmm13
 2552                             	        pslld	$25, %xmm14
 2553                             	        pxor	%xmm13, %xmm12
 2554                             	        pxor	%xmm14, %xmm12
 2555                             	        movdqa	%xmm12, %xmm13
 2556                             	        psrldq	$4, %xmm13
 2557                             	        pslldq	$12, %xmm12
 2558                             	        pxor	%xmm12, %xmm8
 2559                             	        movdqa	%xmm8, %xmm14
 2560                             	        movdqa	%xmm8, %xmm10
 2561                             	        movdqa	%xmm8, %xmm9
 2562                             	        psrld	$0x01, %xmm14
 2563                             	        psrld	$2, %xmm10
 2564                             	        psrld	$7, %xmm9
 2565                             	        pxor	%xmm10, %xmm14
 2566                             	        pxor	%xmm9, %xmm14
 2567                             	        pxor	%xmm13, %xmm14
 2568                             	        pxor	%xmm8, %xmm14
 2569                             	        pxor	%xmm14, %xmm0
 2570                             	        movdqa	%xmm0, 16(%rsp)
 2571                             	        # H ^ 3
 2572                             	        pshufd	$0x4e, %xmm5, %xmm9
 2573                             	        pshufd	$0x4e, %xmm0, %xmm10
 2574                             	        movdqa	%xmm0, %xmm11
 2575                             	        movdqa	%xmm0, %xmm8
 2576                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 2577                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 2578                             	        pxor	%xmm5, %xmm9
 2579                             	        pxor	%xmm0, %xmm10
 2580                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 2581                             	        pxor	%xmm8, %xmm9
 2582                             	        pxor	%xmm11, %xmm9
 2583                             	        movdqa	%xmm9, %xmm10
 2584                             	        movdqa	%xmm11, %xmm1
 2585                             	        pslldq	$8, %xmm10
 2586                             	        psrldq	$8, %xmm9
 2587                             	        pxor	%xmm10, %xmm8
 2588                             	        pxor	%xmm9, %xmm1
 2589                             	        movdqa	%xmm8, %xmm12
 2590                             	        movdqa	%xmm8, %xmm13
 2591                             	        movdqa	%xmm8, %xmm14
 2592                             	        pslld	$31, %xmm12
 2593                             	        pslld	$30, %xmm13
 2594                             	        pslld	$25, %xmm14
 2595                             	        pxor	%xmm13, %xmm12
 2596                             	        pxor	%xmm14, %xmm12
 2597                             	        movdqa	%xmm12, %xmm13
 2598                             	        psrldq	$4, %xmm13
 2599                             	        pslldq	$12, %xmm12
 2600                             	        pxor	%xmm12, %xmm8
 2601                             	        movdqa	%xmm8, %xmm14
 2602                             	        movdqa	%xmm8, %xmm10
 2603                             	        movdqa	%xmm8, %xmm9
 2604                             	        psrld	$0x01, %xmm14
 2605                             	        psrld	$2, %xmm10
 2606                             	        psrld	$7, %xmm9
 2607                             	        pxor	%xmm10, %xmm14
 2608                             	        pxor	%xmm9, %xmm14
 2609                             	        pxor	%xmm13, %xmm14
 2610                             	        pxor	%xmm8, %xmm14
 2611                             	        pxor	%xmm14, %xmm1
 2612                             	        movdqa	%xmm1, 32(%rsp)
 2613                             	        # H ^ 4
 2614                             	        pshufd	$0x4e, %xmm0, %xmm9
 2615                             	        pshufd	$0x4e, %xmm0, %xmm10
 2616                             	        movdqa	%xmm0, %xmm11
 2617                             	        movdqa	%xmm0, %xmm8
 2618                             	        pclmulqdq	$0x11, %xmm0, %xmm11
 2619                             	        pclmulqdq	$0x00, %xmm0, %xmm8
 2620                             	        pxor	%xmm0, %xmm9
 2621                             	        pxor	%xmm0, %xmm10
 2622                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 2623                             	        pxor	%xmm8, %xmm9
 2624                             	        pxor	%xmm11, %xmm9
 2625                             	        movdqa	%xmm9, %xmm10
 2626                             	        movdqa	%xmm11, %xmm3
 2627                             	        pslldq	$8, %xmm10
 2628                             	        psrldq	$8, %xmm9
 2629                             	        pxor	%xmm10, %xmm8
 2630                             	        pxor	%xmm9, %xmm3
 2631                             	        movdqa	%xmm8, %xmm12
 2632                             	        movdqa	%xmm8, %xmm13
 2633                             	        movdqa	%xmm8, %xmm14
 2634                             	        pslld	$31, %xmm12
 2635                             	        pslld	$30, %xmm13
 2636                             	        pslld	$25, %xmm14
 2637                             	        pxor	%xmm13, %xmm12
 2638                             	        pxor	%xmm14, %xmm12
 2639                             	        movdqa	%xmm12, %xmm13
 2640                             	        psrldq	$4, %xmm13
 2641                             	        pslldq	$12, %xmm12
 2642                             	        pxor	%xmm12, %xmm8
 2643                             	        movdqa	%xmm8, %xmm14
 2644                             	        movdqa	%xmm8, %xmm10
 2645                             	        movdqa	%xmm8, %xmm9
 2646                             	        psrld	$0x01, %xmm14
 2647                             	        psrld	$2, %xmm10
 2648                             	        psrld	$7, %xmm9
 2649                             	        pxor	%xmm10, %xmm14
 2650                             	        pxor	%xmm9, %xmm14
 2651                             	        pxor	%xmm13, %xmm14
 2652                             	        pxor	%xmm8, %xmm14
 2653                             	        pxor	%xmm14, %xmm3
 2654                             	        movdqa	%xmm3, 48(%rsp)
 2655                             	        # H ^ 5
 2656                             	        pshufd	$0x4e, %xmm0, %xmm9
 2657                             	        pshufd	$0x4e, %xmm1, %xmm10
 2658                             	        movdqa	%xmm1, %xmm11
 2659                             	        movdqa	%xmm1, %xmm8
 2660                             	        pclmulqdq	$0x11, %xmm0, %xmm11
 2661                             	        pclmulqdq	$0x00, %xmm0, %xmm8
 2662                             	        pxor	%xmm0, %xmm9
 2663                             	        pxor	%xmm1, %xmm10
 2664                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 2665                             	        pxor	%xmm8, %xmm9
 2666                             	        pxor	%xmm11, %xmm9
 2667                             	        movdqa	%xmm9, %xmm10
 2668                             	        movdqa	%xmm11, %xmm7
 2669                             	        pslldq	$8, %xmm10
 2670                             	        psrldq	$8, %xmm9
 2671                             	        pxor	%xmm10, %xmm8
 2672                             	        pxor	%xmm9, %xmm7
 2673                             	        movdqa	%xmm8, %xmm12
 2674                             	        movdqa	%xmm8, %xmm13
 2675                             	        movdqa	%xmm8, %xmm14
 2676                             	        pslld	$31, %xmm12
 2677                             	        pslld	$30, %xmm13
 2678                             	        pslld	$25, %xmm14
 2679                             	        pxor	%xmm13, %xmm12
 2680                             	        pxor	%xmm14, %xmm12
 2681                             	        movdqa	%xmm12, %xmm13
 2682                             	        psrldq	$4, %xmm13
 2683                             	        pslldq	$12, %xmm12
 2684                             	        pxor	%xmm12, %xmm8
 2685                             	        movdqa	%xmm8, %xmm14
 2686                             	        movdqa	%xmm8, %xmm10
 2687                             	        movdqa	%xmm8, %xmm9
 2688                             	        psrld	$0x01, %xmm14
 2689                             	        psrld	$2, %xmm10
 2690                             	        psrld	$7, %xmm9
 2691                             	        pxor	%xmm10, %xmm14
 2692                             	        pxor	%xmm9, %xmm14
 2693                             	        pxor	%xmm13, %xmm14
 2694                             	        pxor	%xmm8, %xmm14
 2695                             	        pxor	%xmm14, %xmm7
 2696                             	        movdqa	%xmm7, 64(%rsp)
 2697                             	        # H ^ 6
 2698                             	        pshufd	$0x4e, %xmm1, %xmm9
 2699                             	        pshufd	$0x4e, %xmm1, %xmm10
 2700                             	        movdqa	%xmm1, %xmm11
 2701                             	        movdqa	%xmm1, %xmm8
 2702                             	        pclmulqdq	$0x11, %xmm1, %xmm11
 2703                             	        pclmulqdq	$0x00, %xmm1, %xmm8
 2704                             	        pxor	%xmm1, %xmm9
 2705                             	        pxor	%xmm1, %xmm10
 2706                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 2707                             	        pxor	%xmm8, %xmm9
 2708                             	        pxor	%xmm11, %xmm9
 2709                             	        movdqa	%xmm9, %xmm10
 2710                             	        movdqa	%xmm11, %xmm7
 2711                             	        pslldq	$8, %xmm10
 2712                             	        psrldq	$8, %xmm9
 2713                             	        pxor	%xmm10, %xmm8
 2714                             	        pxor	%xmm9, %xmm7
 2715                             	        movdqa	%xmm8, %xmm12
 2716                             	        movdqa	%xmm8, %xmm13
 2717                             	        movdqa	%xmm8, %xmm14
 2718                             	        pslld	$31, %xmm12
 2719                             	        pslld	$30, %xmm13
 2720                             	        pslld	$25, %xmm14
 2721                             	        pxor	%xmm13, %xmm12
 2722                             	        pxor	%xmm14, %xmm12
 2723                             	        movdqa	%xmm12, %xmm13
 2724                             	        psrldq	$4, %xmm13
 2725                             	        pslldq	$12, %xmm12
 2726                             	        pxor	%xmm12, %xmm8
 2727                             	        movdqa	%xmm8, %xmm14
 2728                             	        movdqa	%xmm8, %xmm10
 2729                             	        movdqa	%xmm8, %xmm9
 2730                             	        psrld	$0x01, %xmm14
 2731                             	        psrld	$2, %xmm10
 2732                             	        psrld	$7, %xmm9
 2733                             	        pxor	%xmm10, %xmm14
 2734                             	        pxor	%xmm9, %xmm14
 2735                             	        pxor	%xmm13, %xmm14
 2736                             	        pxor	%xmm8, %xmm14
 2737                             	        pxor	%xmm14, %xmm7
 2738                             	        movdqa	%xmm7, 80(%rsp)
 2739                             	        # H ^ 7
 2740                             	        pshufd	$0x4e, %xmm1, %xmm9
 2741                             	        pshufd	$0x4e, %xmm3, %xmm10
 2742                             	        movdqa	%xmm3, %xmm11
 2743                             	        movdqa	%xmm3, %xmm8
 2744                             	        pclmulqdq	$0x11, %xmm1, %xmm11
 2745                             	        pclmulqdq	$0x00, %xmm1, %xmm8
 2746                             	        pxor	%xmm1, %xmm9
 2747                             	        pxor	%xmm3, %xmm10
 2748                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 2749                             	        pxor	%xmm8, %xmm9
 2750                             	        pxor	%xmm11, %xmm9
 2751                             	        movdqa	%xmm9, %xmm10
 2752                             	        movdqa	%xmm11, %xmm7
 2753                             	        pslldq	$8, %xmm10
 2754                             	        psrldq	$8, %xmm9
 2755                             	        pxor	%xmm10, %xmm8
 2756                             	        pxor	%xmm9, %xmm7
 2757                             	        movdqa	%xmm8, %xmm12
 2758                             	        movdqa	%xmm8, %xmm13
 2759                             	        movdqa	%xmm8, %xmm14
 2760                             	        pslld	$31, %xmm12
 2761                             	        pslld	$30, %xmm13
 2762                             	        pslld	$25, %xmm14
 2763                             	        pxor	%xmm13, %xmm12
 2764                             	        pxor	%xmm14, %xmm12
 2765                             	        movdqa	%xmm12, %xmm13
 2766                             	        psrldq	$4, %xmm13
 2767                             	        pslldq	$12, %xmm12
 2768                             	        pxor	%xmm12, %xmm8
 2769                             	        movdqa	%xmm8, %xmm14
 2770                             	        movdqa	%xmm8, %xmm10
 2771                             	        movdqa	%xmm8, %xmm9
 2772                             	        psrld	$0x01, %xmm14
 2773                             	        psrld	$2, %xmm10
 2774                             	        psrld	$7, %xmm9
 2775                             	        pxor	%xmm10, %xmm14
 2776                             	        pxor	%xmm9, %xmm14
 2777                             	        pxor	%xmm13, %xmm14
 2778                             	        pxor	%xmm8, %xmm14
 2779                             	        pxor	%xmm14, %xmm7
 2780                             	        movdqa	%xmm7, 96(%rsp)
 2781                             	        # H ^ 8
 2782                             	        pshufd	$0x4e, %xmm3, %xmm9
 2783                             	        pshufd	$0x4e, %xmm3, %xmm10
 2784                             	        movdqa	%xmm3, %xmm11
 2785                             	        movdqa	%xmm3, %xmm8
 2786                             	        pclmulqdq	$0x11, %xmm3, %xmm11
 2787                             	        pclmulqdq	$0x00, %xmm3, %xmm8
 2788                             	        pxor	%xmm3, %xmm9
 2789                             	        pxor	%xmm3, %xmm10
 2790                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 2791                             	        pxor	%xmm8, %xmm9
 2792                             	        pxor	%xmm11, %xmm9
 2793                             	        movdqa	%xmm9, %xmm10
 2794                             	        movdqa	%xmm11, %xmm7
 2795                             	        pslldq	$8, %xmm10
 2796                             	        psrldq	$8, %xmm9
 2797                             	        pxor	%xmm10, %xmm8
 2798                             	        pxor	%xmm9, %xmm7
 2799                             	        movdqa	%xmm8, %xmm12
 2800                             	        movdqa	%xmm8, %xmm13
 2801                             	        movdqa	%xmm8, %xmm14
 2802                             	        pslld	$31, %xmm12
 2803                             	        pslld	$30, %xmm13
 2804                             	        pslld	$25, %xmm14
 2805                             	        pxor	%xmm13, %xmm12
 2806                             	        pxor	%xmm14, %xmm12
 2807                             	        movdqa	%xmm12, %xmm13
 2808                             	        psrldq	$4, %xmm13
 2809                             	        pslldq	$12, %xmm12
 2810                             	        pxor	%xmm12, %xmm8
 2811                             	        movdqa	%xmm8, %xmm14
 2812                             	        movdqa	%xmm8, %xmm10
 2813                             	        movdqa	%xmm8, %xmm9
 2814                             	        psrld	$0x01, %xmm14
 2815                             	        psrld	$2, %xmm10
 2816                             	        psrld	$7, %xmm9
 2817                             	        pxor	%xmm10, %xmm14
 2818                             	        pxor	%xmm9, %xmm14
 2819                             	        pxor	%xmm13, %xmm14
 2820                             	        pxor	%xmm8, %xmm14
 2821                             	        pxor	%xmm14, %xmm7
 2822                             	        movdqa	%xmm7, 112(%rsp)
 2823                             	L_AES_GCM_decrypt_ghash_128:
 2824                             	        leaq	(%rdi,%rbx,1), %rcx
 2825                             	        leaq	(%rsi,%rbx,1), %rdx
 2826                             	        movdqa	128(%rsp), %xmm8
 2827                             	        movdqa	L_aes_gcm_bswap_epi64(%rip), %xmm1
 2828                             	        movdqa	%xmm8, %xmm0
 2829                             	        pshufb	%xmm1, %xmm8
 2830                             	        movdqa	%xmm0, %xmm9
 2831                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 2832                             	        pshufb	%xmm1, %xmm9
 2833                             	        movdqa	%xmm0, %xmm10
 2834                             	        paddd	L_aes_gcm_two(%rip), %xmm10
 2835                             	        pshufb	%xmm1, %xmm10
 2836                             	        movdqa	%xmm0, %xmm11
 2837                             	        paddd	L_aes_gcm_three(%rip), %xmm11
 2838                             	        pshufb	%xmm1, %xmm11
 2839                             	        movdqa	%xmm0, %xmm12
 2840                             	        paddd	L_aes_gcm_four(%rip), %xmm12
 2841                             	        pshufb	%xmm1, %xmm12
 2842                             	        movdqa	%xmm0, %xmm13
 2843                             	        paddd	L_aes_gcm_five(%rip), %xmm13
 2844                             	        pshufb	%xmm1, %xmm13
 2845                             	        movdqa	%xmm0, %xmm14
 2846                             	        paddd	L_aes_gcm_six(%rip), %xmm14
 2847                             	        pshufb	%xmm1, %xmm14
 2848                             	        movdqa	%xmm0, %xmm15
 2849                             	        paddd	L_aes_gcm_seven(%rip), %xmm15
 2850                             	        pshufb	%xmm1, %xmm15
 2851                             	        paddd	L_aes_gcm_eight(%rip), %xmm0
 2852                             	        movdqa	(%r15), %xmm7
 2853                             	        movdqa	%xmm0, 128(%rsp)
 2854                             	        pxor	%xmm7, %xmm8
 2855                             	        pxor	%xmm7, %xmm9
 2856                             	        pxor	%xmm7, %xmm10
 2857                             	        pxor	%xmm7, %xmm11
 2858                             	        pxor	%xmm7, %xmm12
 2859                             	        pxor	%xmm7, %xmm13
 2860                             	        pxor	%xmm7, %xmm14
 2861                             	        pxor	%xmm7, %xmm15
 2862                             	        movdqa	112(%rsp), %xmm7
 2863                             	        movdqu	(%rcx), %xmm0
 2864                             	        aesenc	16(%r15), %xmm8
 2865                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 2866                             	        pxor	%xmm2, %xmm0
 2867                             	        pshufd	$0x4e, %xmm7, %xmm1
 2868                             	        pshufd	$0x4e, %xmm0, %xmm5
 2869                             	        pxor	%xmm7, %xmm1
 2870                             	        pxor	%xmm0, %xmm5
 2871                             	        movdqa	%xmm0, %xmm3
 2872                             	        pclmulqdq	$0x11, %xmm7, %xmm3
 2873                             	        aesenc	16(%r15), %xmm9
 2874                             	        aesenc	16(%r15), %xmm10
 2875                             	        movdqa	%xmm0, %xmm2
 2876                             	        pclmulqdq	$0x00, %xmm7, %xmm2
 2877                             	        aesenc	16(%r15), %xmm11
 2878                             	        aesenc	16(%r15), %xmm12
 2879                             	        pclmulqdq	$0x00, %xmm5, %xmm1
 2880                             	        aesenc	16(%r15), %xmm13
 2881                             	        aesenc	16(%r15), %xmm14
 2882                             	        aesenc	16(%r15), %xmm15
 2883                             	        pxor	%xmm2, %xmm1
 2884                             	        pxor	%xmm3, %xmm1
 2885                             	        movdqa	96(%rsp), %xmm7
 2886                             	        movdqu	16(%rcx), %xmm0
 2887                             	        pshufd	$0x4e, %xmm7, %xmm4
 2888                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 2889                             	        aesenc	32(%r15), %xmm8
 2890                             	        pxor	%xmm7, %xmm4
 2891                             	        pshufd	$0x4e, %xmm0, %xmm5
 2892                             	        pxor	%xmm0, %xmm5
 2893                             	        movdqa	%xmm0, %xmm6
 2894                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 2895                             	        aesenc	32(%r15), %xmm9
 2896                             	        aesenc	32(%r15), %xmm10
 2897                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 2898                             	        aesenc	32(%r15), %xmm11
 2899                             	        aesenc	32(%r15), %xmm12
 2900                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 2901                             	        aesenc	32(%r15), %xmm13
 2902                             	        aesenc	32(%r15), %xmm14
 2903                             	        aesenc	32(%r15), %xmm15
 2904                             	        pxor	%xmm7, %xmm1
 2905                             	        pxor	%xmm7, %xmm2
 2906                             	        pxor	%xmm6, %xmm1
 2907                             	        pxor	%xmm6, %xmm3
 2908                             	        pxor	%xmm4, %xmm1
 2909                             	        movdqa	80(%rsp), %xmm7
 2910                             	        movdqu	32(%rcx), %xmm0
 2911                             	        pshufd	$0x4e, %xmm7, %xmm4
 2912                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 2913                             	        aesenc	48(%r15), %xmm8
 2914                             	        pxor	%xmm7, %xmm4
 2915                             	        pshufd	$0x4e, %xmm0, %xmm5
 2916                             	        pxor	%xmm0, %xmm5
 2917                             	        movdqa	%xmm0, %xmm6
 2918                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 2919                             	        aesenc	48(%r15), %xmm9
 2920                             	        aesenc	48(%r15), %xmm10
 2921                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 2922                             	        aesenc	48(%r15), %xmm11
 2923                             	        aesenc	48(%r15), %xmm12
 2924                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 2925                             	        aesenc	48(%r15), %xmm13
 2926                             	        aesenc	48(%r15), %xmm14
 2927                             	        aesenc	48(%r15), %xmm15
 2928                             	        pxor	%xmm7, %xmm1
 2929                             	        pxor	%xmm7, %xmm2
 2930                             	        pxor	%xmm6, %xmm1
 2931                             	        pxor	%xmm6, %xmm3
 2932                             	        pxor	%xmm4, %xmm1
 2933                             	        movdqa	64(%rsp), %xmm7
 2934                             	        movdqu	48(%rcx), %xmm0
 2935                             	        pshufd	$0x4e, %xmm7, %xmm4
 2936                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 2937                             	        aesenc	64(%r15), %xmm8
 2938                             	        pxor	%xmm7, %xmm4
 2939                             	        pshufd	$0x4e, %xmm0, %xmm5
 2940                             	        pxor	%xmm0, %xmm5
 2941                             	        movdqa	%xmm0, %xmm6
 2942                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 2943                             	        aesenc	64(%r15), %xmm9
 2944                             	        aesenc	64(%r15), %xmm10
 2945                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 2946                             	        aesenc	64(%r15), %xmm11
 2947                             	        aesenc	64(%r15), %xmm12
 2948                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 2949                             	        aesenc	64(%r15), %xmm13
 2950                             	        aesenc	64(%r15), %xmm14
 2951                             	        aesenc	64(%r15), %xmm15
 2952                             	        pxor	%xmm7, %xmm1
 2953                             	        pxor	%xmm7, %xmm2
 2954                             	        pxor	%xmm6, %xmm1
 2955                             	        pxor	%xmm6, %xmm3
 2956                             	        pxor	%xmm4, %xmm1
 2957                             	        movdqa	48(%rsp), %xmm7
 2958                             	        movdqu	64(%rcx), %xmm0
 2959                             	        pshufd	$0x4e, %xmm7, %xmm4
 2960                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 2961                             	        aesenc	80(%r15), %xmm8
 2962                             	        pxor	%xmm7, %xmm4
 2963                             	        pshufd	$0x4e, %xmm0, %xmm5
 2964                             	        pxor	%xmm0, %xmm5
 2965                             	        movdqa	%xmm0, %xmm6
 2966                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 2967                             	        aesenc	80(%r15), %xmm9
 2968                             	        aesenc	80(%r15), %xmm10
 2969                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 2970                             	        aesenc	80(%r15), %xmm11
 2971                             	        aesenc	80(%r15), %xmm12
 2972                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 2973                             	        aesenc	80(%r15), %xmm13
 2974                             	        aesenc	80(%r15), %xmm14
 2975                             	        aesenc	80(%r15), %xmm15
 2976                             	        pxor	%xmm7, %xmm1
 2977                             	        pxor	%xmm7, %xmm2
 2978                             	        pxor	%xmm6, %xmm1
 2979                             	        pxor	%xmm6, %xmm3
 2980                             	        pxor	%xmm4, %xmm1
 2981                             	        movdqa	32(%rsp), %xmm7
 2982                             	        movdqu	80(%rcx), %xmm0
 2983                             	        pshufd	$0x4e, %xmm7, %xmm4
 2984                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 2985                             	        aesenc	96(%r15), %xmm8
 2986                             	        pxor	%xmm7, %xmm4
 2987                             	        pshufd	$0x4e, %xmm0, %xmm5
 2988                             	        pxor	%xmm0, %xmm5
 2989                             	        movdqa	%xmm0, %xmm6
 2990                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 2991                             	        aesenc	96(%r15), %xmm9
 2992                             	        aesenc	96(%r15), %xmm10
 2993                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 2994                             	        aesenc	96(%r15), %xmm11
 2995                             	        aesenc	96(%r15), %xmm12
 2996                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 2997                             	        aesenc	96(%r15), %xmm13
 2998                             	        aesenc	96(%r15), %xmm14
 2999                             	        aesenc	96(%r15), %xmm15
 3000                             	        pxor	%xmm7, %xmm1
 3001                             	        pxor	%xmm7, %xmm2
 3002                             	        pxor	%xmm6, %xmm1
 3003                             	        pxor	%xmm6, %xmm3
 3004                             	        pxor	%xmm4, %xmm1
 3005                             	        movdqa	16(%rsp), %xmm7
 3006                             	        movdqu	96(%rcx), %xmm0
 3007                             	        pshufd	$0x4e, %xmm7, %xmm4
 3008                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 3009                             	        aesenc	112(%r15), %xmm8
 3010                             	        pxor	%xmm7, %xmm4
 3011                             	        pshufd	$0x4e, %xmm0, %xmm5
 3012                             	        pxor	%xmm0, %xmm5
 3013                             	        movdqa	%xmm0, %xmm6
 3014                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 3015                             	        aesenc	112(%r15), %xmm9
 3016                             	        aesenc	112(%r15), %xmm10
 3017                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 3018                             	        aesenc	112(%r15), %xmm11
 3019                             	        aesenc	112(%r15), %xmm12
 3020                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 3021                             	        aesenc	112(%r15), %xmm13
 3022                             	        aesenc	112(%r15), %xmm14
 3023                             	        aesenc	112(%r15), %xmm15
 3024                             	        pxor	%xmm7, %xmm1
 3025                             	        pxor	%xmm7, %xmm2
 3026                             	        pxor	%xmm6, %xmm1
 3027                             	        pxor	%xmm6, %xmm3
 3028                             	        pxor	%xmm4, %xmm1
 3029                             	        movdqa	(%rsp), %xmm7
 3030                             	        movdqu	112(%rcx), %xmm0
 3031                             	        pshufd	$0x4e, %xmm7, %xmm4
 3032                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 3033                             	        aesenc	128(%r15), %xmm8
 3034                             	        pxor	%xmm7, %xmm4
 3035                             	        pshufd	$0x4e, %xmm0, %xmm5
 3036                             	        pxor	%xmm0, %xmm5
 3037                             	        movdqa	%xmm0, %xmm6
 3038                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 3039                             	        aesenc	128(%r15), %xmm9
 3040                             	        aesenc	128(%r15), %xmm10
 3041                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 3042                             	        aesenc	128(%r15), %xmm11
 3043                             	        aesenc	128(%r15), %xmm12
 3044                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 3045                             	        aesenc	128(%r15), %xmm13
 3046                             	        aesenc	128(%r15), %xmm14
 3047                             	        aesenc	128(%r15), %xmm15
 3048                             	        pxor	%xmm7, %xmm1
 3049                             	        pxor	%xmm7, %xmm2
 3050                             	        pxor	%xmm6, %xmm1
 3051                             	        pxor	%xmm6, %xmm3
 3052                             	        pxor	%xmm4, %xmm1
 3053                             	        movdqa	%xmm1, %xmm5
 3054                             	        psrldq	$8, %xmm1
 3055                             	        pslldq	$8, %xmm5
 3056                             	        aesenc	144(%r15), %xmm8
 3057                             	        pxor	%xmm5, %xmm2
 3058                             	        pxor	%xmm1, %xmm3
 3059                             	        movdqa	%xmm2, %xmm7
 3060                             	        movdqa	%xmm2, %xmm4
 3061                             	        movdqa	%xmm2, %xmm5
 3062                             	        aesenc	144(%r15), %xmm9
 3063                             	        pslld	$31, %xmm7
 3064                             	        pslld	$30, %xmm4
 3065                             	        pslld	$25, %xmm5
 3066                             	        aesenc	144(%r15), %xmm10
 3067                             	        pxor	%xmm4, %xmm7
 3068                             	        pxor	%xmm5, %xmm7
 3069                             	        aesenc	144(%r15), %xmm11
 3070                             	        movdqa	%xmm7, %xmm4
 3071                             	        pslldq	$12, %xmm7
 3072                             	        psrldq	$4, %xmm4
 3073                             	        aesenc	144(%r15), %xmm12
 3074                             	        pxor	%xmm7, %xmm2
 3075                             	        movdqa	%xmm2, %xmm5
 3076                             	        movdqa	%xmm2, %xmm1
 3077                             	        movdqa	%xmm2, %xmm0
 3078                             	        aesenc	144(%r15), %xmm13
 3079                             	        psrld	$0x01, %xmm5
 3080                             	        psrld	$2, %xmm1
 3081                             	        psrld	$7, %xmm0
 3082                             	        aesenc	144(%r15), %xmm14
 3083                             	        pxor	%xmm1, %xmm5
 3084                             	        pxor	%xmm0, %xmm5
 3085                             	        aesenc	144(%r15), %xmm15
 3086                             	        pxor	%xmm4, %xmm5
 3087                             	        pxor	%xmm5, %xmm2
 3088                             	        pxor	%xmm3, %xmm2
 3089                             	        cmpl	$11, %r10d
 3090                             	        movdqa	160(%r15), %xmm7
 3091                             	        jl	L_AES_GCM_decrypt_aesenc_128_ghash_avx_done
 3092                             	        aesenc	%xmm7, %xmm8
 3093                             	        aesenc	%xmm7, %xmm9
 3094                             	        aesenc	%xmm7, %xmm10
 3095                             	        aesenc	%xmm7, %xmm11
 3096                             	        aesenc	%xmm7, %xmm12
 3097                             	        aesenc	%xmm7, %xmm13
 3098                             	        aesenc	%xmm7, %xmm14
 3099                             	        aesenc	%xmm7, %xmm15
 3100                             	        movdqa	176(%r15), %xmm7
 3101                             	        aesenc	%xmm7, %xmm8
 3102                             	        aesenc	%xmm7, %xmm9
 3103                             	        aesenc	%xmm7, %xmm10
 3104                             	        aesenc	%xmm7, %xmm11
 3105                             	        aesenc	%xmm7, %xmm12
 3106                             	        aesenc	%xmm7, %xmm13
 3107                             	        aesenc	%xmm7, %xmm14
 3108                             	        aesenc	%xmm7, %xmm15
 3109                             	        cmpl	$13, %r10d
 3110                             	        movdqa	192(%r15), %xmm7
 3111                             	        jl	L_AES_GCM_decrypt_aesenc_128_ghash_avx_done
 3112                             	        aesenc	%xmm7, %xmm8
 3113                             	        aesenc	%xmm7, %xmm9
 3114                             	        aesenc	%xmm7, %xmm10
 3115                             	        aesenc	%xmm7, %xmm11
 3116                             	        aesenc	%xmm7, %xmm12
 3117                             	        aesenc	%xmm7, %xmm13
 3118                             	        aesenc	%xmm7, %xmm14
 3119                             	        aesenc	%xmm7, %xmm15
 3120                             	        movdqa	208(%r15), %xmm7
 3121                             	        aesenc	%xmm7, %xmm8
 3122                             	        aesenc	%xmm7, %xmm9
 3123                             	        aesenc	%xmm7, %xmm10
 3124                             	        aesenc	%xmm7, %xmm11
 3125                             	        aesenc	%xmm7, %xmm12
 3126                             	        aesenc	%xmm7, %xmm13
 3127                             	        aesenc	%xmm7, %xmm14
 3128                             	        aesenc	%xmm7, %xmm15
 3129                             	        movdqa	224(%r15), %xmm7
 3130                             	L_AES_GCM_decrypt_aesenc_128_ghash_avx_done:
 3131                             	        aesenclast	%xmm7, %xmm8
 3132                             	        aesenclast	%xmm7, %xmm9
 3133                             	        movdqu	(%rcx), %xmm0
 3134                             	        movdqu	16(%rcx), %xmm1
 3135                             	        pxor	%xmm0, %xmm8
 3136                             	        pxor	%xmm1, %xmm9
 3137                             	        movdqu	%xmm8, (%rdx)
 3138                             	        movdqu	%xmm9, 16(%rdx)
 3139                             	        aesenclast	%xmm7, %xmm10
 3140                             	        aesenclast	%xmm7, %xmm11
 3141                             	        movdqu	32(%rcx), %xmm0
 3142                             	        movdqu	48(%rcx), %xmm1
 3143                             	        pxor	%xmm0, %xmm10
 3144                             	        pxor	%xmm1, %xmm11
 3145                             	        movdqu	%xmm10, 32(%rdx)
 3146                             	        movdqu	%xmm11, 48(%rdx)
 3147                             	        aesenclast	%xmm7, %xmm12
 3148                             	        aesenclast	%xmm7, %xmm13
 3149                             	        movdqu	64(%rcx), %xmm0
 3150                             	        movdqu	80(%rcx), %xmm1
 3151                             	        pxor	%xmm0, %xmm12
 3152                             	        pxor	%xmm1, %xmm13
 3153                             	        movdqu	%xmm12, 64(%rdx)
 3154                             	        movdqu	%xmm13, 80(%rdx)
 3155                             	        aesenclast	%xmm7, %xmm14
 3156                             	        aesenclast	%xmm7, %xmm15
 3157                             	        movdqu	96(%rcx), %xmm0
 3158                             	        movdqu	112(%rcx), %xmm1
 3159                             	        pxor	%xmm0, %xmm14
 3160                             	        pxor	%xmm1, %xmm15
 3161                             	        movdqu	%xmm14, 96(%rdx)
 3162                             	        movdqu	%xmm15, 112(%rdx)
 3163                             	        addl	$0x80, %ebx
 3164                             	        cmpl	%r13d, %ebx
 3165                             	        jl	L_AES_GCM_decrypt_ghash_128
 3166                             	        movdqa	%xmm2, %xmm6
 3167                             	        movdqa	(%rsp), %xmm5
 3168                             	L_AES_GCM_decrypt_done_128:
 3169                             	        movl	%r9d, %edx
 3170                             	        cmpl	%edx, %ebx
 3171                             	        jge	L_AES_GCM_decrypt_done_dec
 3172                             	        movl	%r9d, %r13d
 3173                             	        andl	$0xfffffff0, %r13d
 3174                             	        cmpl	%r13d, %ebx
 3175                             	        jge	L_AES_GCM_decrypt_last_block_done
 3176                             	L_AES_GCM_decrypt_last_block_start:
 3177                             	        leaq	(%rdi,%rbx,1), %rcx
 3178                             	        leaq	(%rsi,%rbx,1), %rdx
 3179                             	        movdqu	(%rcx), %xmm1
 3180                             	        movdqa	%xmm5, %xmm0
 3181                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm1
 3182                             	        pxor	%xmm6, %xmm1
 3183                             	        movdqa	128(%rsp), %xmm8
 3184                             	        movdqa	%xmm8, %xmm9
 3185                             	        pshufb	L_aes_gcm_bswap_epi64(%rip), %xmm8
 3186                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 3187                             	        pxor	(%r15), %xmm8
 3188                             	        movdqa	%xmm9, 128(%rsp)
 3189                             	        movdqa	%xmm1, %xmm10
 3190                             	        pclmulqdq	$16, %xmm0, %xmm10
 3191                             	        aesenc	16(%r15), %xmm8
 3192                             	        aesenc	32(%r15), %xmm8
 3193                             	        movdqa	%xmm1, %xmm11
 3194                             	        pclmulqdq	$0x01, %xmm0, %xmm11
 3195                             	        aesenc	48(%r15), %xmm8
 3196                             	        aesenc	64(%r15), %xmm8
 3197                             	        movdqa	%xmm1, %xmm12
 3198                             	        pclmulqdq	$0x00, %xmm0, %xmm12
 3199                             	        aesenc	80(%r15), %xmm8
 3200                             	        movdqa	%xmm1, %xmm1
 3201                             	        pclmulqdq	$0x11, %xmm0, %xmm1
 3202                             	        aesenc	96(%r15), %xmm8
 3203                             	        pxor	%xmm11, %xmm10
 3204                             	        movdqa	%xmm10, %xmm2
 3205                             	        psrldq	$8, %xmm10
 3206                             	        pslldq	$8, %xmm2
 3207                             	        aesenc	112(%r15), %xmm8
 3208                             	        movdqa	%xmm1, %xmm3
 3209                             	        pxor	%xmm12, %xmm2
 3210                             	        pxor	%xmm10, %xmm3
 3211                             	        movdqa	L_aes_gcm_mod2_128(%rip), %xmm0
 3212                             	        movdqa	%xmm2, %xmm11
 3213                             	        pclmulqdq	$16, %xmm0, %xmm11
 3214                             	        aesenc	128(%r15), %xmm8
 3215                             	        pshufd	$0x4e, %xmm2, %xmm10
 3216                             	        pxor	%xmm11, %xmm10
 3217                             	        movdqa	%xmm10, %xmm11
 3218                             	        pclmulqdq	$16, %xmm0, %xmm11
 3219                             	        aesenc	144(%r15), %xmm8
 3220                             	        pshufd	$0x4e, %xmm10, %xmm6
 3221                             	        pxor	%xmm11, %xmm6
 3222                             	        pxor	%xmm3, %xmm6
 3223                             	        cmpl	$11, %r10d
 3224                             	        movdqa	160(%r15), %xmm9
 3225                             	        jl	L_AES_GCM_decrypt_aesenc_gfmul_last
 3226                             	        aesenc	%xmm9, %xmm8
 3227                             	        aesenc	176(%r15), %xmm8
 3228                             	        cmpl	$13, %r10d
 3229                             	        movdqa	192(%r15), %xmm9
 3230                             	        jl	L_AES_GCM_decrypt_aesenc_gfmul_last
 3231                             	        aesenc	%xmm9, %xmm8
 3232                             	        aesenc	208(%r15), %xmm8
 3233                             	        movdqa	224(%r15), %xmm9
 3234                             	L_AES_GCM_decrypt_aesenc_gfmul_last:
 3235                             	        aesenclast	%xmm9, %xmm8
 3236                             	        movdqu	(%rcx), %xmm9
 3237                             	        pxor	%xmm9, %xmm8
 3238                             	        movdqu	%xmm8, (%rdx)
 3239                             	        addl	$16, %ebx
 3240                             	        cmpl	%r13d, %ebx
 3241                             	        jl	L_AES_GCM_decrypt_last_block_start
 3242                             	L_AES_GCM_decrypt_last_block_done:
 3243                             	        movl	%r9d, %ecx
 3244                             	        movl	%ecx, %edx
 3245                             	        andl	$15, %ecx
 3246                             	        jz	L_AES_GCM_decrypt_aesenc_last15_dec_avx_done
 3247                             	        movdqa	128(%rsp), %xmm4
 3248                             	        pshufb	L_aes_gcm_bswap_epi64(%rip), %xmm4
 3249                             	        pxor	(%r15), %xmm4
 3250                             	        aesenc	16(%r15), %xmm4
 3251                             	        aesenc	32(%r15), %xmm4
 3252                             	        aesenc	48(%r15), %xmm4
 3253                             	        aesenc	64(%r15), %xmm4
 3254                             	        aesenc	80(%r15), %xmm4
 3255                             	        aesenc	96(%r15), %xmm4
 3256                             	        aesenc	112(%r15), %xmm4
 3257                             	        aesenc	128(%r15), %xmm4
 3258                             	        aesenc	144(%r15), %xmm4
 3259                             	        cmpl	$11, %r10d
 3260                             	        movdqa	160(%r15), %xmm9
 3261                             	        jl	L_AES_GCM_decrypt_aesenc_last15_dec_avx_aesenc_avx_last
 3262                             	        aesenc	%xmm9, %xmm4
 3263                             	        aesenc	176(%r15), %xmm4
 3264                             	        cmpl	$13, %r10d
 3265                             	        movdqa	192(%r15), %xmm9
 3266                             	        jl	L_AES_GCM_decrypt_aesenc_last15_dec_avx_aesenc_avx_last
 3267                             	        aesenc	%xmm9, %xmm4
 3268                             	        aesenc	208(%r15), %xmm4
 3269                             	        movdqa	224(%r15), %xmm9
 3270                             	L_AES_GCM_decrypt_aesenc_last15_dec_avx_aesenc_avx_last:
 3271                             	        aesenclast	%xmm9, %xmm4
 3272                             	        subq	$32, %rsp
 3273                             	        xorl	%ecx, %ecx
 3274                             	        movdqa	%xmm4, (%rsp)
 3275                             	        pxor	%xmm0, %xmm0
 3276                             	        movdqa	%xmm0, 16(%rsp)
 3277                             	L_AES_GCM_decrypt_aesenc_last15_dec_avx_loop:
 3278                             	        movzbl	(%rdi,%rbx,1), %r13d
 3279                             	        movb	%r13b, 16(%rsp,%rcx,1)
 3280                             	        xorb	(%rsp,%rcx,1), %r13b
 3281                             	        movb	%r13b, (%rsi,%rbx,1)
 3282                             	        incl	%ebx
 3283                             	        incl	%ecx
 3284                             	        cmpl	%edx, %ebx
 3285                             	        jl	L_AES_GCM_decrypt_aesenc_last15_dec_avx_loop
 3286                             	        movdqa	16(%rsp), %xmm4
 3287                             	        addq	$32, %rsp
 3288                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm4
 3289                             	        pxor	%xmm4, %xmm6
 3290                             	        pshufd	$0x4e, %xmm5, %xmm9
 3291                             	        pshufd	$0x4e, %xmm6, %xmm10
 3292                             	        movdqa	%xmm6, %xmm11
 3293                             	        movdqa	%xmm6, %xmm8
 3294                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 3295                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 3296                             	        pxor	%xmm5, %xmm9
 3297                             	        pxor	%xmm6, %xmm10
 3298                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 3299                             	        pxor	%xmm8, %xmm9
 3300                             	        pxor	%xmm11, %xmm9
 3301                             	        movdqa	%xmm9, %xmm10
 3302                             	        movdqa	%xmm11, %xmm6
 3303                             	        pslldq	$8, %xmm10
 3304                             	        psrldq	$8, %xmm9
 3305                             	        pxor	%xmm10, %xmm8
 3306                             	        pxor	%xmm9, %xmm6
 3307                             	        movdqa	%xmm8, %xmm12
 3308                             	        movdqa	%xmm8, %xmm13
 3309                             	        movdqa	%xmm8, %xmm14
 3310                             	        pslld	$31, %xmm12
 3311                             	        pslld	$30, %xmm13
 3312                             	        pslld	$25, %xmm14
 3313                             	        pxor	%xmm13, %xmm12
 3314                             	        pxor	%xmm14, %xmm12
 3315                             	        movdqa	%xmm12, %xmm13
 3316                             	        psrldq	$4, %xmm13
 3317                             	        pslldq	$12, %xmm12
 3318                             	        pxor	%xmm12, %xmm8
 3319                             	        movdqa	%xmm8, %xmm14
 3320                             	        movdqa	%xmm8, %xmm10
 3321                             	        movdqa	%xmm8, %xmm9
 3322                             	        psrld	$0x01, %xmm14
 3323                             	        psrld	$2, %xmm10
 3324                             	        psrld	$7, %xmm9
 3325                             	        pxor	%xmm10, %xmm14
 3326                             	        pxor	%xmm9, %xmm14
 3327                             	        pxor	%xmm13, %xmm14
 3328                             	        pxor	%xmm8, %xmm14
 3329                             	        pxor	%xmm14, %xmm6
 3330                             	L_AES_GCM_decrypt_aesenc_last15_dec_avx_done:
 3331                             	L_AES_GCM_decrypt_done_dec:
 3332                             	        movl	%r9d, %edx
 3333                             	        movl	%r11d, %ecx
 3334                             	        shlq	$3, %rdx
 3335                             	        shlq	$3, %rcx
 3336                             	        pinsrq	$0x00, %rdx, %xmm0
 3337                             	        pinsrq	$0x01, %rcx, %xmm0
 3338                             	        pxor	%xmm0, %xmm6
 3339                             	        pshufd	$0x4e, %xmm5, %xmm9
 3340                             	        pshufd	$0x4e, %xmm6, %xmm10
 3341                             	        movdqa	%xmm6, %xmm11
 3342                             	        movdqa	%xmm6, %xmm8
 3343                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 3344                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 3345                             	        pxor	%xmm5, %xmm9
 3346                             	        pxor	%xmm6, %xmm10
 3347                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 3348                             	        pxor	%xmm8, %xmm9
 3349                             	        pxor	%xmm11, %xmm9
 3350                             	        movdqa	%xmm9, %xmm10
 3351                             	        movdqa	%xmm11, %xmm6
 3352                             	        pslldq	$8, %xmm10
 3353                             	        psrldq	$8, %xmm9
 3354                             	        pxor	%xmm10, %xmm8
 3355                             	        pxor	%xmm9, %xmm6
 3356                             	        movdqa	%xmm8, %xmm12
 3357                             	        movdqa	%xmm8, %xmm13
 3358                             	        movdqa	%xmm8, %xmm14
 3359                             	        pslld	$31, %xmm12
 3360                             	        pslld	$30, %xmm13
 3361                             	        pslld	$25, %xmm14
 3362                             	        pxor	%xmm13, %xmm12
 3363                             	        pxor	%xmm14, %xmm12
 3364                             	        movdqa	%xmm12, %xmm13
 3365                             	        psrldq	$4, %xmm13
 3366                             	        pslldq	$12, %xmm12
 3367                             	        pxor	%xmm12, %xmm8
 3368                             	        movdqa	%xmm8, %xmm14
 3369                             	        movdqa	%xmm8, %xmm10
 3370                             	        movdqa	%xmm8, %xmm9
 3371                             	        psrld	$0x01, %xmm14
 3372                             	        psrld	$2, %xmm10
 3373                             	        psrld	$7, %xmm9
 3374                             	        pxor	%xmm10, %xmm14
 3375                             	        pxor	%xmm9, %xmm14
 3376                             	        pxor	%xmm13, %xmm14
 3377                             	        pxor	%xmm8, %xmm14
 3378                             	        pxor	%xmm14, %xmm6
 3379                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm6
 3380                             	        movdqa	144(%rsp), %xmm0
 3381                             	        pxor	%xmm6, %xmm0
 3382                             	        cmpl	$16, %r14d
 3383                             	        je	L_AES_GCM_decrypt_cmp_tag_16
 3384                             	        subq	$16, %rsp
 3385                             	        xorq	%rcx, %rcx
 3386                             	        xorq	%rbx, %rbx
 3387                             	        movdqa	%xmm0, (%rsp)
 3388                             	L_AES_GCM_decrypt_cmp_tag_loop:
 3389                             	        movzbl	(%rsp,%rcx,1), %r13d
 3390                             	        xorb	(%r8,%rcx,1), %r13b
 3391                             	        orb	%r13b, %bl
 3392                             	        incl	%ecx
 3393                             	        cmpl	%r14d, %ecx
 3394                             	        jne	L_AES_GCM_decrypt_cmp_tag_loop
 3395                             	        cmpb	$0x00, %bl
 3396                             	        sete	%bl
 3397                             	        addq	$16, %rsp
 3398                             	        xorq	%rcx, %rcx
 3399                             	        jmp	L_AES_GCM_decrypt_cmp_tag_done
 3400                             	L_AES_GCM_decrypt_cmp_tag_16:
 3401                             	        movdqu	(%r8), %xmm1
 3402                             	        pcmpeqb	%xmm1, %xmm0
 3403                             	        pmovmskb	%xmm0, %rdx
 3404                             	        # %%edx == 0xFFFF then return 1 else => return 0
 3405                             	        xorl	%ebx, %ebx
 3406                             	        cmpl	$0xffff, %edx
 3407                             	        sete	%bl
 3408                             	L_AES_GCM_decrypt_cmp_tag_done:
 3409                             	        movl	%ebx, (%rbp)
 3410                             	        addq	$0xa8, %rsp
 3411                             	        popq	%rbp
 3412                             	        popq	%r15
 3413                             	        popq	%r14
 3414                             	        popq	%rbx
 3415                             	        popq	%r12
 3416                             	        popq	%r13
 3417                             	        repz retq
 3418                             	#ifndef __APPLE__
 3420                             	#endif /* __APPLE__ */
 3421                             	#ifdef WOLFSSL_AESGCM_STREAM
 3422                             	#ifndef __APPLE__
 3423                             	.text
 3424                             	.globl	AES_GCM_init_aesni
 3426                             	.align	16
 3427                             	AES_GCM_init_aesni:
 3428                             	#else
 3429                             	.section	__TEXT,__text
 3430                             	.globl	_AES_GCM_init_aesni
 3431                             	.p2align	4
 3432                             	_AES_GCM_init_aesni:
 3433                             	#endif /* __APPLE__ */
 3434                             	        pushq	%r12
 3435                             	        pushq	%r13
 3436                             	        pushq	%r14
 3437                             	        movq	%rdx, %r10
 3438                             	        movl	%ecx, %r11d
 3439                             	        movq	32(%rsp), %rax
 3440                             	        subq	$16, %rsp
 3441                             	        pxor	%xmm4, %xmm4
 3442                             	        movl	%r11d, %edx
 3443                             	        cmpl	$12, %edx
 3444                             	        jne	L_AES_GCM_init_aesni_iv_not_12
 3445                             	        # # Calculate values when IV is 12 bytes
 3446                             	        # Set counter based on IV
 3447                             	        movl	$0x1000000, %ecx
 3448                             	        pinsrq	$0x00, (%r10), %xmm4
 3449                             	        pinsrd	$2, 8(%r10), %xmm4
 3450                             	        pinsrd	$3, %ecx, %xmm4
 3451                             	        # H = Encrypt X(=0) and T = Encrypt counter
 3452                             	        movdqa	%xmm4, %xmm1
 3453                             	        movdqa	(%rdi), %xmm5
 3454                             	        pxor	%xmm5, %xmm1
 3455                             	        movdqa	16(%rdi), %xmm7
 3456                             	        aesenc	%xmm7, %xmm5
 3457                             	        aesenc	%xmm7, %xmm1
 3458                             	        movdqa	32(%rdi), %xmm7
 3459                             	        aesenc	%xmm7, %xmm5
 3460                             	        aesenc	%xmm7, %xmm1
 3461                             	        movdqa	48(%rdi), %xmm7
 3462                             	        aesenc	%xmm7, %xmm5
 3463                             	        aesenc	%xmm7, %xmm1
 3464                             	        movdqa	64(%rdi), %xmm7
 3465                             	        aesenc	%xmm7, %xmm5
 3466                             	        aesenc	%xmm7, %xmm1
 3467                             	        movdqa	80(%rdi), %xmm7
 3468                             	        aesenc	%xmm7, %xmm5
 3469                             	        aesenc	%xmm7, %xmm1
 3470                             	        movdqa	96(%rdi), %xmm7
 3471                             	        aesenc	%xmm7, %xmm5
 3472                             	        aesenc	%xmm7, %xmm1
 3473                             	        movdqa	112(%rdi), %xmm7
 3474                             	        aesenc	%xmm7, %xmm5
 3475                             	        aesenc	%xmm7, %xmm1
 3476                             	        movdqa	128(%rdi), %xmm7
 3477                             	        aesenc	%xmm7, %xmm5
 3478                             	        aesenc	%xmm7, %xmm1
 3479                             	        movdqa	144(%rdi), %xmm7
 3480                             	        aesenc	%xmm7, %xmm5
 3481                             	        aesenc	%xmm7, %xmm1
 3482                             	        cmpl	$11, %esi
 3483                             	        movdqa	160(%rdi), %xmm7
 3484                             	        jl	L_AES_GCM_init_aesni_calc_iv_12_last
 3485                             	        aesenc	%xmm7, %xmm5
 3486                             	        aesenc	%xmm7, %xmm1
 3487                             	        movdqa	176(%rdi), %xmm7
 3488                             	        aesenc	%xmm7, %xmm5
 3489                             	        aesenc	%xmm7, %xmm1
 3490                             	        cmpl	$13, %esi
 3491                             	        movdqa	192(%rdi), %xmm7
 3492                             	        jl	L_AES_GCM_init_aesni_calc_iv_12_last
 3493                             	        aesenc	%xmm7, %xmm5
 3494                             	        aesenc	%xmm7, %xmm1
 3495                             	        movdqa	208(%rdi), %xmm7
 3496                             	        aesenc	%xmm7, %xmm5
 3497                             	        aesenc	%xmm7, %xmm1
 3498                             	        movdqa	224(%rdi), %xmm7
 3499                             	L_AES_GCM_init_aesni_calc_iv_12_last:
 3500                             	        aesenclast	%xmm7, %xmm5
 3501                             	        aesenclast	%xmm7, %xmm1
 3502                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm5
 3503                             	        movdqa	%xmm1, %xmm15
 3504                             	        jmp	L_AES_GCM_init_aesni_iv_done
 3505                             	L_AES_GCM_init_aesni_iv_not_12:
 3506                             	        # Calculate values when IV is not 12 bytes
 3507                             	        # H = Encrypt X(=0)
 3508                             	        movdqa	(%rdi), %xmm5
 3509                             	        aesenc	16(%rdi), %xmm5
 3510                             	        aesenc	32(%rdi), %xmm5
 3511                             	        aesenc	48(%rdi), %xmm5
 3512                             	        aesenc	64(%rdi), %xmm5
 3513                             	        aesenc	80(%rdi), %xmm5
 3514                             	        aesenc	96(%rdi), %xmm5
 3515                             	        aesenc	112(%rdi), %xmm5
 3516                             	        aesenc	128(%rdi), %xmm5
 3517                             	        aesenc	144(%rdi), %xmm5
 3518                             	        cmpl	$11, %esi
 3519                             	        movdqa	160(%rdi), %xmm9
 3520                             	        jl	L_AES_GCM_init_aesni_calc_iv_1_aesenc_avx_last
 3521                             	        aesenc	%xmm9, %xmm5
 3522                             	        aesenc	176(%rdi), %xmm5
 3523                             	        cmpl	$13, %esi
 3524                             	        movdqa	192(%rdi), %xmm9
 3525                             	        jl	L_AES_GCM_init_aesni_calc_iv_1_aesenc_avx_last
 3526                             	        aesenc	%xmm9, %xmm5
 3527                             	        aesenc	208(%rdi), %xmm5
 3528                             	        movdqa	224(%rdi), %xmm9
 3529                             	L_AES_GCM_init_aesni_calc_iv_1_aesenc_avx_last:
 3530                             	        aesenclast	%xmm9, %xmm5
 3531                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm5
 3532                             	        # Calc counter
 3533                             	        # Initialization vector
 3534                             	        cmpl	$0x00, %edx
 3535                             	        movq	$0x00, %rcx
 3536                             	        je	L_AES_GCM_init_aesni_calc_iv_done
 3537                             	        cmpl	$16, %edx
 3538                             	        jl	L_AES_GCM_init_aesni_calc_iv_lt16
 3539                             	        andl	$0xfffffff0, %edx
 3540                             	L_AES_GCM_init_aesni_calc_iv_16_loop:
 3541                             	        movdqu	(%r10,%rcx,1), %xmm8
 3542                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 3543                             	        pxor	%xmm8, %xmm4
 3544                             	        pshufd	$0x4e, %xmm4, %xmm1
 3545                             	        pshufd	$0x4e, %xmm5, %xmm2
 3546                             	        movdqa	%xmm5, %xmm3
 3547                             	        movdqa	%xmm5, %xmm0
 3548                             	        pclmulqdq	$0x11, %xmm4, %xmm3
 3549                             	        pclmulqdq	$0x00, %xmm4, %xmm0
 3550                             	        pxor	%xmm4, %xmm1
 3551                             	        pxor	%xmm5, %xmm2
 3552                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 3553                             	        pxor	%xmm0, %xmm1
 3554                             	        pxor	%xmm3, %xmm1
 3555                             	        movdqa	%xmm1, %xmm2
 3556                             	        movdqa	%xmm0, %xmm7
 3557                             	        movdqa	%xmm3, %xmm4
 3558                             	        pslldq	$8, %xmm2
 3559                             	        psrldq	$8, %xmm1
 3560                             	        pxor	%xmm2, %xmm7
 3561                             	        pxor	%xmm1, %xmm4
 3562                             	        movdqa	%xmm7, %xmm0
 3563                             	        movdqa	%xmm4, %xmm1
 3564                             	        psrld	$31, %xmm0
 3565                             	        psrld	$31, %xmm1
 3566                             	        pslld	$0x01, %xmm7
 3567                             	        pslld	$0x01, %xmm4
 3568                             	        movdqa	%xmm0, %xmm2
 3569                             	        pslldq	$4, %xmm0
 3570                             	        psrldq	$12, %xmm2
 3571                             	        pslldq	$4, %xmm1
 3572                             	        por	%xmm2, %xmm4
 3573                             	        por	%xmm0, %xmm7
 3574                             	        por	%xmm1, %xmm4
 3575                             	        movdqa	%xmm7, %xmm0
 3576                             	        movdqa	%xmm7, %xmm1
 3577                             	        movdqa	%xmm7, %xmm2
 3578                             	        pslld	$31, %xmm0
 3579                             	        pslld	$30, %xmm1
 3580                             	        pslld	$25, %xmm2
 3581                             	        pxor	%xmm1, %xmm0
 3582                             	        pxor	%xmm2, %xmm0
 3583                             	        movdqa	%xmm0, %xmm1
 3584                             	        psrldq	$4, %xmm1
 3585                             	        pslldq	$12, %xmm0
 3586                             	        pxor	%xmm0, %xmm7
 3587                             	        movdqa	%xmm7, %xmm2
 3588                             	        movdqa	%xmm7, %xmm3
 3589                             	        movdqa	%xmm7, %xmm0
 3590                             	        psrld	$0x01, %xmm2
 3591                             	        psrld	$2, %xmm3
 3592                             	        psrld	$7, %xmm0
 3593                             	        pxor	%xmm3, %xmm2
 3594                             	        pxor	%xmm0, %xmm2
 3595                             	        pxor	%xmm1, %xmm2
 3596                             	        pxor	%xmm7, %xmm2
 3597                             	        pxor	%xmm2, %xmm4
 3598                             	        addl	$16, %ecx
 3599                             	        cmpl	%edx, %ecx
 3600                             	        jl	L_AES_GCM_init_aesni_calc_iv_16_loop
 3601                             	        movl	%r11d, %edx
 3602                             	        cmpl	%edx, %ecx
 3603                             	        je	L_AES_GCM_init_aesni_calc_iv_done
 3604                             	L_AES_GCM_init_aesni_calc_iv_lt16:
 3605                             	        subq	$16, %rsp
 3606                             	        pxor	%xmm8, %xmm8
 3607                             	        xorl	%r13d, %r13d
 3608                             	        movdqa	%xmm8, (%rsp)
 3609                             	L_AES_GCM_init_aesni_calc_iv_loop:
 3610                             	        movzbl	(%r10,%rcx,1), %r12d
 3611                             	        movb	%r12b, (%rsp,%r13,1)
 3612                             	        incl	%ecx
 3613                             	        incl	%r13d
 3614                             	        cmpl	%edx, %ecx
 3615                             	        jl	L_AES_GCM_init_aesni_calc_iv_loop
 3616                             	        movdqa	(%rsp), %xmm8
 3617                             	        addq	$16, %rsp
 3618                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 3619                             	        pxor	%xmm8, %xmm4
 3620                             	        pshufd	$0x4e, %xmm4, %xmm1
 3621                             	        pshufd	$0x4e, %xmm5, %xmm2
 3622                             	        movdqa	%xmm5, %xmm3
 3623                             	        movdqa	%xmm5, %xmm0
 3624                             	        pclmulqdq	$0x11, %xmm4, %xmm3
 3625                             	        pclmulqdq	$0x00, %xmm4, %xmm0
 3626                             	        pxor	%xmm4, %xmm1
 3627                             	        pxor	%xmm5, %xmm2
 3628                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 3629                             	        pxor	%xmm0, %xmm1
 3630                             	        pxor	%xmm3, %xmm1
 3631                             	        movdqa	%xmm1, %xmm2
 3632                             	        movdqa	%xmm0, %xmm7
 3633                             	        movdqa	%xmm3, %xmm4
 3634                             	        pslldq	$8, %xmm2
 3635                             	        psrldq	$8, %xmm1
 3636                             	        pxor	%xmm2, %xmm7
 3637                             	        pxor	%xmm1, %xmm4
 3638                             	        movdqa	%xmm7, %xmm0
 3639                             	        movdqa	%xmm4, %xmm1
 3640                             	        psrld	$31, %xmm0
 3641                             	        psrld	$31, %xmm1
 3642                             	        pslld	$0x01, %xmm7
 3643                             	        pslld	$0x01, %xmm4
 3644                             	        movdqa	%xmm0, %xmm2
 3645                             	        pslldq	$4, %xmm0
 3646                             	        psrldq	$12, %xmm2
 3647                             	        pslldq	$4, %xmm1
 3648                             	        por	%xmm2, %xmm4
 3649                             	        por	%xmm0, %xmm7
 3650                             	        por	%xmm1, %xmm4
 3651                             	        movdqa	%xmm7, %xmm0
 3652                             	        movdqa	%xmm7, %xmm1
 3653                             	        movdqa	%xmm7, %xmm2
 3654                             	        pslld	$31, %xmm0
 3655                             	        pslld	$30, %xmm1
 3656                             	        pslld	$25, %xmm2
 3657                             	        pxor	%xmm1, %xmm0
 3658                             	        pxor	%xmm2, %xmm0
 3659                             	        movdqa	%xmm0, %xmm1
 3660                             	        psrldq	$4, %xmm1
 3661                             	        pslldq	$12, %xmm0
 3662                             	        pxor	%xmm0, %xmm7
 3663                             	        movdqa	%xmm7, %xmm2
 3664                             	        movdqa	%xmm7, %xmm3
 3665                             	        movdqa	%xmm7, %xmm0
 3666                             	        psrld	$0x01, %xmm2
 3667                             	        psrld	$2, %xmm3
 3668                             	        psrld	$7, %xmm0
 3669                             	        pxor	%xmm3, %xmm2
 3670                             	        pxor	%xmm0, %xmm2
 3671                             	        pxor	%xmm1, %xmm2
 3672                             	        pxor	%xmm7, %xmm2
 3673                             	        pxor	%xmm2, %xmm4
 3674                             	L_AES_GCM_init_aesni_calc_iv_done:
 3675                             	        # T = Encrypt counter
 3676                             	        pxor	%xmm0, %xmm0
 3677                             	        shll	$3, %edx
 3678                             	        pinsrq	$0x00, %rdx, %xmm0
 3679                             	        pxor	%xmm0, %xmm4
 3680                             	        pshufd	$0x4e, %xmm4, %xmm1
 3681                             	        pshufd	$0x4e, %xmm5, %xmm2
 3682                             	        movdqa	%xmm5, %xmm3
 3683                             	        movdqa	%xmm5, %xmm0
 3684                             	        pclmulqdq	$0x11, %xmm4, %xmm3
 3685                             	        pclmulqdq	$0x00, %xmm4, %xmm0
 3686                             	        pxor	%xmm4, %xmm1
 3687                             	        pxor	%xmm5, %xmm2
 3688                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 3689                             	        pxor	%xmm0, %xmm1
 3690                             	        pxor	%xmm3, %xmm1
 3691                             	        movdqa	%xmm1, %xmm2
 3692                             	        movdqa	%xmm0, %xmm7
 3693                             	        movdqa	%xmm3, %xmm4
 3694                             	        pslldq	$8, %xmm2
 3695                             	        psrldq	$8, %xmm1
 3696                             	        pxor	%xmm2, %xmm7
 3697                             	        pxor	%xmm1, %xmm4
 3698                             	        movdqa	%xmm7, %xmm0
 3699                             	        movdqa	%xmm4, %xmm1
 3700                             	        psrld	$31, %xmm0
 3701                             	        psrld	$31, %xmm1
 3702                             	        pslld	$0x01, %xmm7
 3703                             	        pslld	$0x01, %xmm4
 3704                             	        movdqa	%xmm0, %xmm2
 3705                             	        pslldq	$4, %xmm0
 3706                             	        psrldq	$12, %xmm2
 3707                             	        pslldq	$4, %xmm1
 3708                             	        por	%xmm2, %xmm4
 3709                             	        por	%xmm0, %xmm7
 3710                             	        por	%xmm1, %xmm4
 3711                             	        movdqa	%xmm7, %xmm0
 3712                             	        movdqa	%xmm7, %xmm1
 3713                             	        movdqa	%xmm7, %xmm2
 3714                             	        pslld	$31, %xmm0
 3715                             	        pslld	$30, %xmm1
 3716                             	        pslld	$25, %xmm2
 3717                             	        pxor	%xmm1, %xmm0
 3718                             	        pxor	%xmm2, %xmm0
 3719                             	        movdqa	%xmm0, %xmm1
 3720                             	        psrldq	$4, %xmm1
 3721                             	        pslldq	$12, %xmm0
 3722                             	        pxor	%xmm0, %xmm7
 3723                             	        movdqa	%xmm7, %xmm2
 3724                             	        movdqa	%xmm7, %xmm3
 3725                             	        movdqa	%xmm7, %xmm0
 3726                             	        psrld	$0x01, %xmm2
 3727                             	        psrld	$2, %xmm3
 3728                             	        psrld	$7, %xmm0
 3729                             	        pxor	%xmm3, %xmm2
 3730                             	        pxor	%xmm0, %xmm2
 3731                             	        pxor	%xmm1, %xmm2
 3732                             	        pxor	%xmm7, %xmm2
 3733                             	        pxor	%xmm2, %xmm4
 3734                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm4
 3735                             	        #   Encrypt counter
 3736                             	        movdqa	(%rdi), %xmm8
 3737                             	        pxor	%xmm4, %xmm8
 3738                             	        aesenc	16(%rdi), %xmm8
 3739                             	        aesenc	32(%rdi), %xmm8
 3740                             	        aesenc	48(%rdi), %xmm8
 3741                             	        aesenc	64(%rdi), %xmm8
 3742                             	        aesenc	80(%rdi), %xmm8
 3743                             	        aesenc	96(%rdi), %xmm8
 3744                             	        aesenc	112(%rdi), %xmm8
 3745                             	        aesenc	128(%rdi), %xmm8
 3746                             	        aesenc	144(%rdi), %xmm8
 3747                             	        cmpl	$11, %esi
 3748                             	        movdqa	160(%rdi), %xmm9
 3749                             	        jl	L_AES_GCM_init_aesni_calc_iv_2_aesenc_avx_last
 3750                             	        aesenc	%xmm9, %xmm8
 3751                             	        aesenc	176(%rdi), %xmm8
 3752                             	        cmpl	$13, %esi
 3753                             	        movdqa	192(%rdi), %xmm9
 3754                             	        jl	L_AES_GCM_init_aesni_calc_iv_2_aesenc_avx_last
 3755                             	        aesenc	%xmm9, %xmm8
 3756                             	        aesenc	208(%rdi), %xmm8
 3757                             	        movdqa	224(%rdi), %xmm9
 3758                             	L_AES_GCM_init_aesni_calc_iv_2_aesenc_avx_last:
 3759                             	        aesenclast	%xmm9, %xmm8
 3760                             	        movdqa	%xmm8, %xmm15
 3761                             	L_AES_GCM_init_aesni_iv_done:
 3762                             	        movdqa	%xmm15, (%rax)
 3763                             	        pshufb	L_aes_gcm_bswap_epi64(%rip), %xmm4
 3764                             	        paddd	L_aes_gcm_one(%rip), %xmm4
 3765                             	        movdqa	%xmm5, (%r8)
 3766                             	        movdqa	%xmm4, (%r9)
 3767                             	        addq	$16, %rsp
 3768                             	        popq	%r14
 3769                             	        popq	%r13
 3770                             	        popq	%r12
 3771                             	        repz retq
 3772                             	#ifndef __APPLE__
 3774                             	#endif /* __APPLE__ */
 3775                             	#ifndef __APPLE__
 3776                             	.text
 3777                             	.globl	AES_GCM_aad_update_aesni
 3779                             	.align	16
 3780                             	AES_GCM_aad_update_aesni:
 3781                             	#else
 3782                             	.section	__TEXT,__text
 3783                             	.globl	_AES_GCM_aad_update_aesni
 3784                             	.p2align	4
 3785                             	_AES_GCM_aad_update_aesni:
 3786                             	#endif /* __APPLE__ */
 3787                             	        movq	%rcx, %rax
 3788                             	        movdqa	(%rdx), %xmm5
 3789                             	        movdqa	(%rax), %xmm6
 3790                             	        xorl	%ecx, %ecx
 3791                             	L_AES_GCM_aad_update_aesni_16_loop:
 3792                             	        movdqu	(%rdi,%rcx,1), %xmm8
 3793                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 3794                             	        pxor	%xmm8, %xmm5
 3795                             	        pshufd	$0x4e, %xmm5, %xmm1
 3796                             	        pshufd	$0x4e, %xmm6, %xmm2
 3797                             	        movdqa	%xmm6, %xmm3
 3798                             	        movdqa	%xmm6, %xmm0
 3799                             	        pclmulqdq	$0x11, %xmm5, %xmm3
 3800                             	        pclmulqdq	$0x00, %xmm5, %xmm0
 3801                             	        pxor	%xmm5, %xmm1
 3802                             	        pxor	%xmm6, %xmm2
 3803                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 3804                             	        pxor	%xmm0, %xmm1
 3805                             	        pxor	%xmm3, %xmm1
 3806                             	        movdqa	%xmm1, %xmm2
 3807                             	        movdqa	%xmm0, %xmm4
 3808                             	        movdqa	%xmm3, %xmm5
 3809                             	        pslldq	$8, %xmm2
 3810                             	        psrldq	$8, %xmm1
 3811                             	        pxor	%xmm2, %xmm4
 3812                             	        pxor	%xmm1, %xmm5
 3813                             	        movdqa	%xmm4, %xmm0
 3814                             	        movdqa	%xmm5, %xmm1
 3815                             	        psrld	$31, %xmm0
 3816                             	        psrld	$31, %xmm1
 3817                             	        pslld	$0x01, %xmm4
 3818                             	        pslld	$0x01, %xmm5
 3819                             	        movdqa	%xmm0, %xmm2
 3820                             	        pslldq	$4, %xmm0
 3821                             	        psrldq	$12, %xmm2
 3822                             	        pslldq	$4, %xmm1
 3823                             	        por	%xmm2, %xmm5
 3824                             	        por	%xmm0, %xmm4
 3825                             	        por	%xmm1, %xmm5
 3826                             	        movdqa	%xmm4, %xmm0
 3827                             	        movdqa	%xmm4, %xmm1
 3828                             	        movdqa	%xmm4, %xmm2
 3829                             	        pslld	$31, %xmm0
 3830                             	        pslld	$30, %xmm1
 3831                             	        pslld	$25, %xmm2
 3832                             	        pxor	%xmm1, %xmm0
 3833                             	        pxor	%xmm2, %xmm0
 3834                             	        movdqa	%xmm0, %xmm1
 3835                             	        psrldq	$4, %xmm1
 3836                             	        pslldq	$12, %xmm0
 3837                             	        pxor	%xmm0, %xmm4
 3838                             	        movdqa	%xmm4, %xmm2
 3839                             	        movdqa	%xmm4, %xmm3
 3840                             	        movdqa	%xmm4, %xmm0
 3841                             	        psrld	$0x01, %xmm2
 3842                             	        psrld	$2, %xmm3
 3843                             	        psrld	$7, %xmm0
 3844                             	        pxor	%xmm3, %xmm2
 3845                             	        pxor	%xmm0, %xmm2
 3846                             	        pxor	%xmm1, %xmm2
 3847                             	        pxor	%xmm4, %xmm2
 3848                             	        pxor	%xmm2, %xmm5
 3849                             	        addl	$16, %ecx
 3850                             	        cmpl	%esi, %ecx
 3851                             	        jl	L_AES_GCM_aad_update_aesni_16_loop
 3852                             	        movdqa	%xmm5, (%rdx)
 3853                             	        repz retq
 3854                             	#ifndef __APPLE__
 3856                             	#endif /* __APPLE__ */
 3857                             	#ifndef __APPLE__
 3858                             	.text
 3859                             	.globl	AES_GCM_encrypt_block_aesni
 3861                             	.align	16
 3862                             	AES_GCM_encrypt_block_aesni:
 3863                             	#else
 3864                             	.section	__TEXT,__text
 3865                             	.globl	_AES_GCM_encrypt_block_aesni
 3866                             	.p2align	4
 3867                             	_AES_GCM_encrypt_block_aesni:
 3868                             	#endif /* __APPLE__ */
 3869                             	        movq	%rdx, %r10
 3870                             	        movq	%rcx, %r11
 3871                             	        movdqa	(%r8), %xmm8
 3872                             	        movdqa	%xmm8, %xmm9
 3873                             	        pshufb	L_aes_gcm_bswap_epi64(%rip), %xmm8
 3874                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 3875                             	        pxor	(%rdi), %xmm8
 3876                             	        movdqa	%xmm9, (%r8)
 3877                             	        aesenc	16(%rdi), %xmm8
 3878                             	        aesenc	32(%rdi), %xmm8
 3879                             	        aesenc	48(%rdi), %xmm8
 3880                             	        aesenc	64(%rdi), %xmm8
 3881                             	        aesenc	80(%rdi), %xmm8
 3882                             	        aesenc	96(%rdi), %xmm8
 3883                             	        aesenc	112(%rdi), %xmm8
 3884                             	        aesenc	128(%rdi), %xmm8
 3885                             	        aesenc	144(%rdi), %xmm8
 3886                             	        cmpl	$11, %esi
 3887                             	        movdqa	160(%rdi), %xmm9
 3888                             	        jl	L_AES_GCM_encrypt_block_aesni_aesenc_block_aesenc_avx_last
 3889                             	        aesenc	%xmm9, %xmm8
 3890                             	        aesenc	176(%rdi), %xmm8
 3891                             	        cmpl	$13, %esi
 3892                             	        movdqa	192(%rdi), %xmm9
 3893                             	        jl	L_AES_GCM_encrypt_block_aesni_aesenc_block_aesenc_avx_last
 3894                             	        aesenc	%xmm9, %xmm8
 3895                             	        aesenc	208(%rdi), %xmm8
 3896                             	        movdqa	224(%rdi), %xmm9
 3897                             	L_AES_GCM_encrypt_block_aesni_aesenc_block_aesenc_avx_last:
 3898                             	        aesenclast	%xmm9, %xmm8
 3899                             	        movdqu	(%r11), %xmm9
 3900                             	        pxor	%xmm9, %xmm8
 3901                             	        movdqu	%xmm8, (%r10)
 3902                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 3903                             	        repz retq
 3904                             	#ifndef __APPLE__
 3906                             	#endif /* __APPLE__ */
 3907                             	#ifndef __APPLE__
 3908                             	.text
 3909                             	.globl	AES_GCM_ghash_block_aesni
 3911                             	.align	16
 3912                             	AES_GCM_ghash_block_aesni:
 3913                             	#else
 3914                             	.section	__TEXT,__text
 3915                             	.globl	_AES_GCM_ghash_block_aesni
 3916                             	.p2align	4
 3917                             	_AES_GCM_ghash_block_aesni:
 3918                             	#endif /* __APPLE__ */
 3919                             	        movdqa	(%rsi), %xmm4
 3920                             	        movdqa	(%rdx), %xmm5
 3921                             	        movdqu	(%rdi), %xmm8
 3922                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 3923                             	        pxor	%xmm8, %xmm4
 3924                             	        pshufd	$0x4e, %xmm4, %xmm1
 3925                             	        pshufd	$0x4e, %xmm5, %xmm2
 3926                             	        movdqa	%xmm5, %xmm3
 3927                             	        movdqa	%xmm5, %xmm0
 3928                             	        pclmulqdq	$0x11, %xmm4, %xmm3
 3929                             	        pclmulqdq	$0x00, %xmm4, %xmm0
 3930                             	        pxor	%xmm4, %xmm1
 3931                             	        pxor	%xmm5, %xmm2
 3932                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 3933                             	        pxor	%xmm0, %xmm1
 3934                             	        pxor	%xmm3, %xmm1
 3935                             	        movdqa	%xmm1, %xmm2
 3936                             	        movdqa	%xmm0, %xmm6
 3937                             	        movdqa	%xmm3, %xmm4
 3938                             	        pslldq	$8, %xmm2
 3939                             	        psrldq	$8, %xmm1
 3940                             	        pxor	%xmm2, %xmm6
 3941                             	        pxor	%xmm1, %xmm4
 3942                             	        movdqa	%xmm6, %xmm0
 3943                             	        movdqa	%xmm4, %xmm1
 3944                             	        psrld	$31, %xmm0
 3945                             	        psrld	$31, %xmm1
 3946                             	        pslld	$0x01, %xmm6
 3947                             	        pslld	$0x01, %xmm4
 3948                             	        movdqa	%xmm0, %xmm2
 3949                             	        pslldq	$4, %xmm0
 3950                             	        psrldq	$12, %xmm2
 3951                             	        pslldq	$4, %xmm1
 3952                             	        por	%xmm2, %xmm4
 3953                             	        por	%xmm0, %xmm6
 3954                             	        por	%xmm1, %xmm4
 3955                             	        movdqa	%xmm6, %xmm0
 3956                             	        movdqa	%xmm6, %xmm1
 3957                             	        movdqa	%xmm6, %xmm2
 3958                             	        pslld	$31, %xmm0
 3959                             	        pslld	$30, %xmm1
 3960                             	        pslld	$25, %xmm2
 3961                             	        pxor	%xmm1, %xmm0
 3962                             	        pxor	%xmm2, %xmm0
 3963                             	        movdqa	%xmm0, %xmm1
 3964                             	        psrldq	$4, %xmm1
 3965                             	        pslldq	$12, %xmm0
 3966                             	        pxor	%xmm0, %xmm6
 3967                             	        movdqa	%xmm6, %xmm2
 3968                             	        movdqa	%xmm6, %xmm3
 3969                             	        movdqa	%xmm6, %xmm0
 3970                             	        psrld	$0x01, %xmm2
 3971                             	        psrld	$2, %xmm3
 3972                             	        psrld	$7, %xmm0
 3973                             	        pxor	%xmm3, %xmm2
 3974                             	        pxor	%xmm0, %xmm2
 3975                             	        pxor	%xmm1, %xmm2
 3976                             	        pxor	%xmm6, %xmm2
 3977                             	        pxor	%xmm2, %xmm4
 3978                             	        movdqa	%xmm4, (%rsi)
 3979                             	        repz retq
 3980                             	#ifndef __APPLE__
 3982                             	#endif /* __APPLE__ */
 3983                             	#ifndef __APPLE__
 3984                             	.text
 3985                             	.globl	AES_GCM_encrypt_update_aesni
 3987                             	.align	16
 3988                             	AES_GCM_encrypt_update_aesni:
 3989                             	#else
 3990                             	.section	__TEXT,__text
 3991                             	.globl	_AES_GCM_encrypt_update_aesni
 3992                             	.p2align	4
 3993                             	_AES_GCM_encrypt_update_aesni:
 3994                             	#endif /* __APPLE__ */
 3995                             	        pushq	%r13
 3996                             	        pushq	%r12
 3997                             	        pushq	%r14
 3998                             	        movq	%rdx, %r10
 3999                             	        movq	%rcx, %r11
 4000                             	        movq	32(%rsp), %rax
 4001                             	        movq	40(%rsp), %r12
 4002                             	        subq	$0xa0, %rsp
 4003                             	        movdqa	(%r9), %xmm6
 4004                             	        movdqa	(%rax), %xmm5
 4005                             	        movdqa	%xmm5, %xmm9
 4006                             	        movdqa	%xmm5, %xmm8
 4007                             	        psrlq	$63, %xmm9
 4008                             	        psllq	$0x01, %xmm8
 4009                             	        pslldq	$8, %xmm9
 4010                             	        por	%xmm9, %xmm8
 4011                             	        pshufd	$0xff, %xmm5, %xmm5
 4012                             	        psrad	$31, %xmm5
 4013                             	        pand	L_aes_gcm_mod2_128(%rip), %xmm5
 4014                             	        pxor	%xmm8, %xmm5
 4015                             	        xorq	%r14, %r14
 4016                             	        cmpl	$0x80, %r8d
 4017                             	        movl	%r8d, %r13d
 4018                             	        jl	L_AES_GCM_encrypt_update_aesni_done_128
 4019                             	        andl	$0xffffff80, %r13d
 4020                             	        movdqa	%xmm6, %xmm2
 4021                             	        # H ^ 1
 4022                             	        movdqa	%xmm5, (%rsp)
 4023                             	        # H ^ 2
 4024                             	        pshufd	$0x4e, %xmm5, %xmm9
 4025                             	        pshufd	$0x4e, %xmm5, %xmm10
 4026                             	        movdqa	%xmm5, %xmm11
 4027                             	        movdqa	%xmm5, %xmm8
 4028                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 4029                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 4030                             	        pxor	%xmm5, %xmm9
 4031                             	        pxor	%xmm5, %xmm10
 4032                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 4033                             	        pxor	%xmm8, %xmm9
 4034                             	        pxor	%xmm11, %xmm9
 4035                             	        movdqa	%xmm9, %xmm10
 4036                             	        movdqa	%xmm11, %xmm0
 4037                             	        pslldq	$8, %xmm10
 4038                             	        psrldq	$8, %xmm9
 4039                             	        pxor	%xmm10, %xmm8
 4040                             	        pxor	%xmm9, %xmm0
 4041                             	        movdqa	%xmm8, %xmm12
 4042                             	        movdqa	%xmm8, %xmm13
 4043                             	        movdqa	%xmm8, %xmm14
 4044                             	        pslld	$31, %xmm12
 4045                             	        pslld	$30, %xmm13
 4046                             	        pslld	$25, %xmm14
 4047                             	        pxor	%xmm13, %xmm12
 4048                             	        pxor	%xmm14, %xmm12
 4049                             	        movdqa	%xmm12, %xmm13
 4050                             	        psrldq	$4, %xmm13
 4051                             	        pslldq	$12, %xmm12
 4052                             	        pxor	%xmm12, %xmm8
 4053                             	        movdqa	%xmm8, %xmm14
 4054                             	        movdqa	%xmm8, %xmm10
 4055                             	        movdqa	%xmm8, %xmm9
 4056                             	        psrld	$0x01, %xmm14
 4057                             	        psrld	$2, %xmm10
 4058                             	        psrld	$7, %xmm9
 4059                             	        pxor	%xmm10, %xmm14
 4060                             	        pxor	%xmm9, %xmm14
 4061                             	        pxor	%xmm13, %xmm14
 4062                             	        pxor	%xmm8, %xmm14
 4063                             	        pxor	%xmm14, %xmm0
 4064                             	        movdqa	%xmm0, 16(%rsp)
 4065                             	        # H ^ 3
 4066                             	        pshufd	$0x4e, %xmm5, %xmm9
 4067                             	        pshufd	$0x4e, %xmm0, %xmm10
 4068                             	        movdqa	%xmm0, %xmm11
 4069                             	        movdqa	%xmm0, %xmm8
 4070                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 4071                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 4072                             	        pxor	%xmm5, %xmm9
 4073                             	        pxor	%xmm0, %xmm10
 4074                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 4075                             	        pxor	%xmm8, %xmm9
 4076                             	        pxor	%xmm11, %xmm9
 4077                             	        movdqa	%xmm9, %xmm10
 4078                             	        movdqa	%xmm11, %xmm1
 4079                             	        pslldq	$8, %xmm10
 4080                             	        psrldq	$8, %xmm9
 4081                             	        pxor	%xmm10, %xmm8
 4082                             	        pxor	%xmm9, %xmm1
 4083                             	        movdqa	%xmm8, %xmm12
 4084                             	        movdqa	%xmm8, %xmm13
 4085                             	        movdqa	%xmm8, %xmm14
 4086                             	        pslld	$31, %xmm12
 4087                             	        pslld	$30, %xmm13
 4088                             	        pslld	$25, %xmm14
 4089                             	        pxor	%xmm13, %xmm12
 4090                             	        pxor	%xmm14, %xmm12
 4091                             	        movdqa	%xmm12, %xmm13
 4092                             	        psrldq	$4, %xmm13
 4093                             	        pslldq	$12, %xmm12
 4094                             	        pxor	%xmm12, %xmm8
 4095                             	        movdqa	%xmm8, %xmm14
 4096                             	        movdqa	%xmm8, %xmm10
 4097                             	        movdqa	%xmm8, %xmm9
 4098                             	        psrld	$0x01, %xmm14
 4099                             	        psrld	$2, %xmm10
 4100                             	        psrld	$7, %xmm9
 4101                             	        pxor	%xmm10, %xmm14
 4102                             	        pxor	%xmm9, %xmm14
 4103                             	        pxor	%xmm13, %xmm14
 4104                             	        pxor	%xmm8, %xmm14
 4105                             	        pxor	%xmm14, %xmm1
 4106                             	        movdqa	%xmm1, 32(%rsp)
 4107                             	        # H ^ 4
 4108                             	        pshufd	$0x4e, %xmm0, %xmm9
 4109                             	        pshufd	$0x4e, %xmm0, %xmm10
 4110                             	        movdqa	%xmm0, %xmm11
 4111                             	        movdqa	%xmm0, %xmm8
 4112                             	        pclmulqdq	$0x11, %xmm0, %xmm11
 4113                             	        pclmulqdq	$0x00, %xmm0, %xmm8
 4114                             	        pxor	%xmm0, %xmm9
 4115                             	        pxor	%xmm0, %xmm10
 4116                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 4117                             	        pxor	%xmm8, %xmm9
 4118                             	        pxor	%xmm11, %xmm9
 4119                             	        movdqa	%xmm9, %xmm10
 4120                             	        movdqa	%xmm11, %xmm3
 4121                             	        pslldq	$8, %xmm10
 4122                             	        psrldq	$8, %xmm9
 4123                             	        pxor	%xmm10, %xmm8
 4124                             	        pxor	%xmm9, %xmm3
 4125                             	        movdqa	%xmm8, %xmm12
 4126                             	        movdqa	%xmm8, %xmm13
 4127                             	        movdqa	%xmm8, %xmm14
 4128                             	        pslld	$31, %xmm12
 4129                             	        pslld	$30, %xmm13
 4130                             	        pslld	$25, %xmm14
 4131                             	        pxor	%xmm13, %xmm12
 4132                             	        pxor	%xmm14, %xmm12
 4133                             	        movdqa	%xmm12, %xmm13
 4134                             	        psrldq	$4, %xmm13
 4135                             	        pslldq	$12, %xmm12
 4136                             	        pxor	%xmm12, %xmm8
 4137                             	        movdqa	%xmm8, %xmm14
 4138                             	        movdqa	%xmm8, %xmm10
 4139                             	        movdqa	%xmm8, %xmm9
 4140                             	        psrld	$0x01, %xmm14
 4141                             	        psrld	$2, %xmm10
 4142                             	        psrld	$7, %xmm9
 4143                             	        pxor	%xmm10, %xmm14
 4144                             	        pxor	%xmm9, %xmm14
 4145                             	        pxor	%xmm13, %xmm14
 4146                             	        pxor	%xmm8, %xmm14
 4147                             	        pxor	%xmm14, %xmm3
 4148                             	        movdqa	%xmm3, 48(%rsp)
 4149                             	        # H ^ 5
 4150                             	        pshufd	$0x4e, %xmm0, %xmm9
 4151                             	        pshufd	$0x4e, %xmm1, %xmm10
 4152                             	        movdqa	%xmm1, %xmm11
 4153                             	        movdqa	%xmm1, %xmm8
 4154                             	        pclmulqdq	$0x11, %xmm0, %xmm11
 4155                             	        pclmulqdq	$0x00, %xmm0, %xmm8
 4156                             	        pxor	%xmm0, %xmm9
 4157                             	        pxor	%xmm1, %xmm10
 4158                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 4159                             	        pxor	%xmm8, %xmm9
 4160                             	        pxor	%xmm11, %xmm9
 4161                             	        movdqa	%xmm9, %xmm10
 4162                             	        movdqa	%xmm11, %xmm7
 4163                             	        pslldq	$8, %xmm10
 4164                             	        psrldq	$8, %xmm9
 4165                             	        pxor	%xmm10, %xmm8
 4166                             	        pxor	%xmm9, %xmm7
 4167                             	        movdqa	%xmm8, %xmm12
 4168                             	        movdqa	%xmm8, %xmm13
 4169                             	        movdqa	%xmm8, %xmm14
 4170                             	        pslld	$31, %xmm12
 4171                             	        pslld	$30, %xmm13
 4172                             	        pslld	$25, %xmm14
 4173                             	        pxor	%xmm13, %xmm12
 4174                             	        pxor	%xmm14, %xmm12
 4175                             	        movdqa	%xmm12, %xmm13
 4176                             	        psrldq	$4, %xmm13
 4177                             	        pslldq	$12, %xmm12
 4178                             	        pxor	%xmm12, %xmm8
 4179                             	        movdqa	%xmm8, %xmm14
 4180                             	        movdqa	%xmm8, %xmm10
 4181                             	        movdqa	%xmm8, %xmm9
 4182                             	        psrld	$0x01, %xmm14
 4183                             	        psrld	$2, %xmm10
 4184                             	        psrld	$7, %xmm9
 4185                             	        pxor	%xmm10, %xmm14
 4186                             	        pxor	%xmm9, %xmm14
 4187                             	        pxor	%xmm13, %xmm14
 4188                             	        pxor	%xmm8, %xmm14
 4189                             	        pxor	%xmm14, %xmm7
 4190                             	        movdqa	%xmm7, 64(%rsp)
 4191                             	        # H ^ 6
 4192                             	        pshufd	$0x4e, %xmm1, %xmm9
 4193                             	        pshufd	$0x4e, %xmm1, %xmm10
 4194                             	        movdqa	%xmm1, %xmm11
 4195                             	        movdqa	%xmm1, %xmm8
 4196                             	        pclmulqdq	$0x11, %xmm1, %xmm11
 4197                             	        pclmulqdq	$0x00, %xmm1, %xmm8
 4198                             	        pxor	%xmm1, %xmm9
 4199                             	        pxor	%xmm1, %xmm10
 4200                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 4201                             	        pxor	%xmm8, %xmm9
 4202                             	        pxor	%xmm11, %xmm9
 4203                             	        movdqa	%xmm9, %xmm10
 4204                             	        movdqa	%xmm11, %xmm7
 4205                             	        pslldq	$8, %xmm10
 4206                             	        psrldq	$8, %xmm9
 4207                             	        pxor	%xmm10, %xmm8
 4208                             	        pxor	%xmm9, %xmm7
 4209                             	        movdqa	%xmm8, %xmm12
 4210                             	        movdqa	%xmm8, %xmm13
 4211                             	        movdqa	%xmm8, %xmm14
 4212                             	        pslld	$31, %xmm12
 4213                             	        pslld	$30, %xmm13
 4214                             	        pslld	$25, %xmm14
 4215                             	        pxor	%xmm13, %xmm12
 4216                             	        pxor	%xmm14, %xmm12
 4217                             	        movdqa	%xmm12, %xmm13
 4218                             	        psrldq	$4, %xmm13
 4219                             	        pslldq	$12, %xmm12
 4220                             	        pxor	%xmm12, %xmm8
 4221                             	        movdqa	%xmm8, %xmm14
 4222                             	        movdqa	%xmm8, %xmm10
 4223                             	        movdqa	%xmm8, %xmm9
 4224                             	        psrld	$0x01, %xmm14
 4225                             	        psrld	$2, %xmm10
 4226                             	        psrld	$7, %xmm9
 4227                             	        pxor	%xmm10, %xmm14
 4228                             	        pxor	%xmm9, %xmm14
 4229                             	        pxor	%xmm13, %xmm14
 4230                             	        pxor	%xmm8, %xmm14
 4231                             	        pxor	%xmm14, %xmm7
 4232                             	        movdqa	%xmm7, 80(%rsp)
 4233                             	        # H ^ 7
 4234                             	        pshufd	$0x4e, %xmm1, %xmm9
 4235                             	        pshufd	$0x4e, %xmm3, %xmm10
 4236                             	        movdqa	%xmm3, %xmm11
 4237                             	        movdqa	%xmm3, %xmm8
 4238                             	        pclmulqdq	$0x11, %xmm1, %xmm11
 4239                             	        pclmulqdq	$0x00, %xmm1, %xmm8
 4240                             	        pxor	%xmm1, %xmm9
 4241                             	        pxor	%xmm3, %xmm10
 4242                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 4243                             	        pxor	%xmm8, %xmm9
 4244                             	        pxor	%xmm11, %xmm9
 4245                             	        movdqa	%xmm9, %xmm10
 4246                             	        movdqa	%xmm11, %xmm7
 4247                             	        pslldq	$8, %xmm10
 4248                             	        psrldq	$8, %xmm9
 4249                             	        pxor	%xmm10, %xmm8
 4250                             	        pxor	%xmm9, %xmm7
 4251                             	        movdqa	%xmm8, %xmm12
 4252                             	        movdqa	%xmm8, %xmm13
 4253                             	        movdqa	%xmm8, %xmm14
 4254                             	        pslld	$31, %xmm12
 4255                             	        pslld	$30, %xmm13
 4256                             	        pslld	$25, %xmm14
 4257                             	        pxor	%xmm13, %xmm12
 4258                             	        pxor	%xmm14, %xmm12
 4259                             	        movdqa	%xmm12, %xmm13
 4260                             	        psrldq	$4, %xmm13
 4261                             	        pslldq	$12, %xmm12
 4262                             	        pxor	%xmm12, %xmm8
 4263                             	        movdqa	%xmm8, %xmm14
 4264                             	        movdqa	%xmm8, %xmm10
 4265                             	        movdqa	%xmm8, %xmm9
 4266                             	        psrld	$0x01, %xmm14
 4267                             	        psrld	$2, %xmm10
 4268                             	        psrld	$7, %xmm9
 4269                             	        pxor	%xmm10, %xmm14
 4270                             	        pxor	%xmm9, %xmm14
 4271                             	        pxor	%xmm13, %xmm14
 4272                             	        pxor	%xmm8, %xmm14
 4273                             	        pxor	%xmm14, %xmm7
 4274                             	        movdqa	%xmm7, 96(%rsp)
 4275                             	        # H ^ 8
 4276                             	        pshufd	$0x4e, %xmm3, %xmm9
 4277                             	        pshufd	$0x4e, %xmm3, %xmm10
 4278                             	        movdqa	%xmm3, %xmm11
 4279                             	        movdqa	%xmm3, %xmm8
 4280                             	        pclmulqdq	$0x11, %xmm3, %xmm11
 4281                             	        pclmulqdq	$0x00, %xmm3, %xmm8
 4282                             	        pxor	%xmm3, %xmm9
 4283                             	        pxor	%xmm3, %xmm10
 4284                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 4285                             	        pxor	%xmm8, %xmm9
 4286                             	        pxor	%xmm11, %xmm9
 4287                             	        movdqa	%xmm9, %xmm10
 4288                             	        movdqa	%xmm11, %xmm7
 4289                             	        pslldq	$8, %xmm10
 4290                             	        psrldq	$8, %xmm9
 4291                             	        pxor	%xmm10, %xmm8
 4292                             	        pxor	%xmm9, %xmm7
 4293                             	        movdqa	%xmm8, %xmm12
 4294                             	        movdqa	%xmm8, %xmm13
 4295                             	        movdqa	%xmm8, %xmm14
 4296                             	        pslld	$31, %xmm12
 4297                             	        pslld	$30, %xmm13
 4298                             	        pslld	$25, %xmm14
 4299                             	        pxor	%xmm13, %xmm12
 4300                             	        pxor	%xmm14, %xmm12
 4301                             	        movdqa	%xmm12, %xmm13
 4302                             	        psrldq	$4, %xmm13
 4303                             	        pslldq	$12, %xmm12
 4304                             	        pxor	%xmm12, %xmm8
 4305                             	        movdqa	%xmm8, %xmm14
 4306                             	        movdqa	%xmm8, %xmm10
 4307                             	        movdqa	%xmm8, %xmm9
 4308                             	        psrld	$0x01, %xmm14
 4309                             	        psrld	$2, %xmm10
 4310                             	        psrld	$7, %xmm9
 4311                             	        pxor	%xmm10, %xmm14
 4312                             	        pxor	%xmm9, %xmm14
 4313                             	        pxor	%xmm13, %xmm14
 4314                             	        pxor	%xmm8, %xmm14
 4315                             	        pxor	%xmm14, %xmm7
 4316                             	        movdqa	%xmm7, 112(%rsp)
 4317                             	        # First 128 bytes of input
 4318                             	        movdqa	(%r12), %xmm8
 4319                             	        movdqa	L_aes_gcm_bswap_epi64(%rip), %xmm1
 4320                             	        movdqa	%xmm8, %xmm0
 4321                             	        pshufb	%xmm1, %xmm8
 4322                             	        movdqa	%xmm0, %xmm9
 4323                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 4324                             	        pshufb	%xmm1, %xmm9
 4325                             	        movdqa	%xmm0, %xmm10
 4326                             	        paddd	L_aes_gcm_two(%rip), %xmm10
 4327                             	        pshufb	%xmm1, %xmm10
 4328                             	        movdqa	%xmm0, %xmm11
 4329                             	        paddd	L_aes_gcm_three(%rip), %xmm11
 4330                             	        pshufb	%xmm1, %xmm11
 4331                             	        movdqa	%xmm0, %xmm12
 4332                             	        paddd	L_aes_gcm_four(%rip), %xmm12
 4333                             	        pshufb	%xmm1, %xmm12
 4334                             	        movdqa	%xmm0, %xmm13
 4335                             	        paddd	L_aes_gcm_five(%rip), %xmm13
 4336                             	        pshufb	%xmm1, %xmm13
 4337                             	        movdqa	%xmm0, %xmm14
 4338                             	        paddd	L_aes_gcm_six(%rip), %xmm14
 4339                             	        pshufb	%xmm1, %xmm14
 4340                             	        movdqa	%xmm0, %xmm15
 4341                             	        paddd	L_aes_gcm_seven(%rip), %xmm15
 4342                             	        pshufb	%xmm1, %xmm15
 4343                             	        paddd	L_aes_gcm_eight(%rip), %xmm0
 4344                             	        movdqa	(%rdi), %xmm7
 4345                             	        movdqa	%xmm0, (%r12)
 4346                             	        pxor	%xmm7, %xmm8
 4347                             	        pxor	%xmm7, %xmm9
 4348                             	        pxor	%xmm7, %xmm10
 4349                             	        pxor	%xmm7, %xmm11
 4350                             	        pxor	%xmm7, %xmm12
 4351                             	        pxor	%xmm7, %xmm13
 4352                             	        pxor	%xmm7, %xmm14
 4353                             	        pxor	%xmm7, %xmm15
 4354                             	        movdqa	16(%rdi), %xmm7
 4355                             	        aesenc	%xmm7, %xmm8
 4356                             	        aesenc	%xmm7, %xmm9
 4357                             	        aesenc	%xmm7, %xmm10
 4358                             	        aesenc	%xmm7, %xmm11
 4359                             	        aesenc	%xmm7, %xmm12
 4360                             	        aesenc	%xmm7, %xmm13
 4361                             	        aesenc	%xmm7, %xmm14
 4362                             	        aesenc	%xmm7, %xmm15
 4363                             	        movdqa	32(%rdi), %xmm7
 4364                             	        aesenc	%xmm7, %xmm8
 4365                             	        aesenc	%xmm7, %xmm9
 4366                             	        aesenc	%xmm7, %xmm10
 4367                             	        aesenc	%xmm7, %xmm11
 4368                             	        aesenc	%xmm7, %xmm12
 4369                             	        aesenc	%xmm7, %xmm13
 4370                             	        aesenc	%xmm7, %xmm14
 4371                             	        aesenc	%xmm7, %xmm15
 4372                             	        movdqa	48(%rdi), %xmm7
 4373                             	        aesenc	%xmm7, %xmm8
 4374                             	        aesenc	%xmm7, %xmm9
 4375                             	        aesenc	%xmm7, %xmm10
 4376                             	        aesenc	%xmm7, %xmm11
 4377                             	        aesenc	%xmm7, %xmm12
 4378                             	        aesenc	%xmm7, %xmm13
 4379                             	        aesenc	%xmm7, %xmm14
 4380                             	        aesenc	%xmm7, %xmm15
 4381                             	        movdqa	64(%rdi), %xmm7
 4382                             	        aesenc	%xmm7, %xmm8
 4383                             	        aesenc	%xmm7, %xmm9
 4384                             	        aesenc	%xmm7, %xmm10
 4385                             	        aesenc	%xmm7, %xmm11
 4386                             	        aesenc	%xmm7, %xmm12
 4387                             	        aesenc	%xmm7, %xmm13
 4388                             	        aesenc	%xmm7, %xmm14
 4389                             	        aesenc	%xmm7, %xmm15
 4390                             	        movdqa	80(%rdi), %xmm7
 4391                             	        aesenc	%xmm7, %xmm8
 4392                             	        aesenc	%xmm7, %xmm9
 4393                             	        aesenc	%xmm7, %xmm10
 4394                             	        aesenc	%xmm7, %xmm11
 4395                             	        aesenc	%xmm7, %xmm12
 4396                             	        aesenc	%xmm7, %xmm13
 4397                             	        aesenc	%xmm7, %xmm14
 4398                             	        aesenc	%xmm7, %xmm15
 4399                             	        movdqa	96(%rdi), %xmm7
 4400                             	        aesenc	%xmm7, %xmm8
 4401                             	        aesenc	%xmm7, %xmm9
 4402                             	        aesenc	%xmm7, %xmm10
 4403                             	        aesenc	%xmm7, %xmm11
 4404                             	        aesenc	%xmm7, %xmm12
 4405                             	        aesenc	%xmm7, %xmm13
 4406                             	        aesenc	%xmm7, %xmm14
 4407                             	        aesenc	%xmm7, %xmm15
 4408                             	        movdqa	112(%rdi), %xmm7
 4409                             	        aesenc	%xmm7, %xmm8
 4410                             	        aesenc	%xmm7, %xmm9
 4411                             	        aesenc	%xmm7, %xmm10
 4412                             	        aesenc	%xmm7, %xmm11
 4413                             	        aesenc	%xmm7, %xmm12
 4414                             	        aesenc	%xmm7, %xmm13
 4415                             	        aesenc	%xmm7, %xmm14
 4416                             	        aesenc	%xmm7, %xmm15
 4417                             	        movdqa	128(%rdi), %xmm7
 4418                             	        aesenc	%xmm7, %xmm8
 4419                             	        aesenc	%xmm7, %xmm9
 4420                             	        aesenc	%xmm7, %xmm10
 4421                             	        aesenc	%xmm7, %xmm11
 4422                             	        aesenc	%xmm7, %xmm12
 4423                             	        aesenc	%xmm7, %xmm13
 4424                             	        aesenc	%xmm7, %xmm14
 4425                             	        aesenc	%xmm7, %xmm15
 4426                             	        movdqa	144(%rdi), %xmm7
 4427                             	        aesenc	%xmm7, %xmm8
 4428                             	        aesenc	%xmm7, %xmm9
 4429                             	        aesenc	%xmm7, %xmm10
 4430                             	        aesenc	%xmm7, %xmm11
 4431                             	        aesenc	%xmm7, %xmm12
 4432                             	        aesenc	%xmm7, %xmm13
 4433                             	        aesenc	%xmm7, %xmm14
 4434                             	        aesenc	%xmm7, %xmm15
 4435                             	        cmpl	$11, %esi
 4436                             	        movdqa	160(%rdi), %xmm7
 4437                             	        jl	L_AES_GCM_encrypt_update_aesni_enc_done
 4438                             	        aesenc	%xmm7, %xmm8
 4439                             	        aesenc	%xmm7, %xmm9
 4440                             	        aesenc	%xmm7, %xmm10
 4441                             	        aesenc	%xmm7, %xmm11
 4442                             	        aesenc	%xmm7, %xmm12
 4443                             	        aesenc	%xmm7, %xmm13
 4444                             	        aesenc	%xmm7, %xmm14
 4445                             	        aesenc	%xmm7, %xmm15
 4446                             	        movdqa	176(%rdi), %xmm7
 4447                             	        aesenc	%xmm7, %xmm8
 4448                             	        aesenc	%xmm7, %xmm9
 4449                             	        aesenc	%xmm7, %xmm10
 4450                             	        aesenc	%xmm7, %xmm11
 4451                             	        aesenc	%xmm7, %xmm12
 4452                             	        aesenc	%xmm7, %xmm13
 4453                             	        aesenc	%xmm7, %xmm14
 4454                             	        aesenc	%xmm7, %xmm15
 4455                             	        cmpl	$13, %esi
 4456                             	        movdqa	192(%rdi), %xmm7
 4457                             	        jl	L_AES_GCM_encrypt_update_aesni_enc_done
 4458                             	        aesenc	%xmm7, %xmm8
 4459                             	        aesenc	%xmm7, %xmm9
 4460                             	        aesenc	%xmm7, %xmm10
 4461                             	        aesenc	%xmm7, %xmm11
 4462                             	        aesenc	%xmm7, %xmm12
 4463                             	        aesenc	%xmm7, %xmm13
 4464                             	        aesenc	%xmm7, %xmm14
 4465                             	        aesenc	%xmm7, %xmm15
 4466                             	        movdqa	208(%rdi), %xmm7
 4467                             	        aesenc	%xmm7, %xmm8
 4468                             	        aesenc	%xmm7, %xmm9
 4469                             	        aesenc	%xmm7, %xmm10
 4470                             	        aesenc	%xmm7, %xmm11
 4471                             	        aesenc	%xmm7, %xmm12
 4472                             	        aesenc	%xmm7, %xmm13
 4473                             	        aesenc	%xmm7, %xmm14
 4474                             	        aesenc	%xmm7, %xmm15
 4475                             	        movdqa	224(%rdi), %xmm7
 4476                             	L_AES_GCM_encrypt_update_aesni_enc_done:
 4477                             	        aesenclast	%xmm7, %xmm8
 4478                             	        aesenclast	%xmm7, %xmm9
 4479                             	        movdqu	(%r11), %xmm0
 4480                             	        movdqu	16(%r11), %xmm1
 4481                             	        pxor	%xmm0, %xmm8
 4482                             	        pxor	%xmm1, %xmm9
 4483                             	        movdqu	%xmm8, (%r10)
 4484                             	        movdqu	%xmm9, 16(%r10)
 4485                             	        aesenclast	%xmm7, %xmm10
 4486                             	        aesenclast	%xmm7, %xmm11
 4487                             	        movdqu	32(%r11), %xmm0
 4488                             	        movdqu	48(%r11), %xmm1
 4489                             	        pxor	%xmm0, %xmm10
 4490                             	        pxor	%xmm1, %xmm11
 4491                             	        movdqu	%xmm10, 32(%r10)
 4492                             	        movdqu	%xmm11, 48(%r10)
 4493                             	        aesenclast	%xmm7, %xmm12
 4494                             	        aesenclast	%xmm7, %xmm13
 4495                             	        movdqu	64(%r11), %xmm0
 4496                             	        movdqu	80(%r11), %xmm1
 4497                             	        pxor	%xmm0, %xmm12
 4498                             	        pxor	%xmm1, %xmm13
 4499                             	        movdqu	%xmm12, 64(%r10)
 4500                             	        movdqu	%xmm13, 80(%r10)
 4501                             	        aesenclast	%xmm7, %xmm14
 4502                             	        aesenclast	%xmm7, %xmm15
 4503                             	        movdqu	96(%r11), %xmm0
 4504                             	        movdqu	112(%r11), %xmm1
 4505                             	        pxor	%xmm0, %xmm14
 4506                             	        pxor	%xmm1, %xmm15
 4507                             	        movdqu	%xmm14, 96(%r10)
 4508                             	        movdqu	%xmm15, 112(%r10)
 4509                             	        cmpl	$0x80, %r13d
 4510                             	        movl	$0x80, %r14d
 4511                             	        jle	L_AES_GCM_encrypt_update_aesni_end_128
 4512                             	        # More 128 bytes of input
 4513                             	L_AES_GCM_encrypt_update_aesni_ghash_128:
 4514                             	        leaq	(%r11,%r14,1), %rcx
 4515                             	        leaq	(%r10,%r14,1), %rdx
 4516                             	        movdqa	(%r12), %xmm8
 4517                             	        movdqa	L_aes_gcm_bswap_epi64(%rip), %xmm1
 4518                             	        movdqa	%xmm8, %xmm0
 4519                             	        pshufb	%xmm1, %xmm8
 4520                             	        movdqa	%xmm0, %xmm9
 4521                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 4522                             	        pshufb	%xmm1, %xmm9
 4523                             	        movdqa	%xmm0, %xmm10
 4524                             	        paddd	L_aes_gcm_two(%rip), %xmm10
 4525                             	        pshufb	%xmm1, %xmm10
 4526                             	        movdqa	%xmm0, %xmm11
 4527                             	        paddd	L_aes_gcm_three(%rip), %xmm11
 4528                             	        pshufb	%xmm1, %xmm11
 4529                             	        movdqa	%xmm0, %xmm12
 4530                             	        paddd	L_aes_gcm_four(%rip), %xmm12
 4531                             	        pshufb	%xmm1, %xmm12
 4532                             	        movdqa	%xmm0, %xmm13
 4533                             	        paddd	L_aes_gcm_five(%rip), %xmm13
 4534                             	        pshufb	%xmm1, %xmm13
 4535                             	        movdqa	%xmm0, %xmm14
 4536                             	        paddd	L_aes_gcm_six(%rip), %xmm14
 4537                             	        pshufb	%xmm1, %xmm14
 4538                             	        movdqa	%xmm0, %xmm15
 4539                             	        paddd	L_aes_gcm_seven(%rip), %xmm15
 4540                             	        pshufb	%xmm1, %xmm15
 4541                             	        paddd	L_aes_gcm_eight(%rip), %xmm0
 4542                             	        movdqa	(%rdi), %xmm7
 4543                             	        movdqa	%xmm0, (%r12)
 4544                             	        pxor	%xmm7, %xmm8
 4545                             	        pxor	%xmm7, %xmm9
 4546                             	        pxor	%xmm7, %xmm10
 4547                             	        pxor	%xmm7, %xmm11
 4548                             	        pxor	%xmm7, %xmm12
 4549                             	        pxor	%xmm7, %xmm13
 4550                             	        pxor	%xmm7, %xmm14
 4551                             	        pxor	%xmm7, %xmm15
 4552                             	        movdqa	112(%rsp), %xmm7
 4553                             	        movdqu	-128(%rdx), %xmm0
 4554                             	        aesenc	16(%rdi), %xmm8
 4555                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 4556                             	        pxor	%xmm2, %xmm0
 4557                             	        pshufd	$0x4e, %xmm7, %xmm1
 4558                             	        pshufd	$0x4e, %xmm0, %xmm5
 4559                             	        pxor	%xmm7, %xmm1
 4560                             	        pxor	%xmm0, %xmm5
 4561                             	        movdqa	%xmm0, %xmm3
 4562                             	        pclmulqdq	$0x11, %xmm7, %xmm3
 4563                             	        aesenc	16(%rdi), %xmm9
 4564                             	        aesenc	16(%rdi), %xmm10
 4565                             	        movdqa	%xmm0, %xmm2
 4566                             	        pclmulqdq	$0x00, %xmm7, %xmm2
 4567                             	        aesenc	16(%rdi), %xmm11
 4568                             	        aesenc	16(%rdi), %xmm12
 4569                             	        pclmulqdq	$0x00, %xmm5, %xmm1
 4570                             	        aesenc	16(%rdi), %xmm13
 4571                             	        aesenc	16(%rdi), %xmm14
 4572                             	        aesenc	16(%rdi), %xmm15
 4573                             	        pxor	%xmm2, %xmm1
 4574                             	        pxor	%xmm3, %xmm1
 4575                             	        movdqa	96(%rsp), %xmm7
 4576                             	        movdqu	-112(%rdx), %xmm0
 4577                             	        pshufd	$0x4e, %xmm7, %xmm4
 4578                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 4579                             	        aesenc	32(%rdi), %xmm8
 4580                             	        pxor	%xmm7, %xmm4
 4581                             	        pshufd	$0x4e, %xmm0, %xmm5
 4582                             	        pxor	%xmm0, %xmm5
 4583                             	        movdqa	%xmm0, %xmm6
 4584                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 4585                             	        aesenc	32(%rdi), %xmm9
 4586                             	        aesenc	32(%rdi), %xmm10
 4587                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 4588                             	        aesenc	32(%rdi), %xmm11
 4589                             	        aesenc	32(%rdi), %xmm12
 4590                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 4591                             	        aesenc	32(%rdi), %xmm13
 4592                             	        aesenc	32(%rdi), %xmm14
 4593                             	        aesenc	32(%rdi), %xmm15
 4594                             	        pxor	%xmm7, %xmm1
 4595                             	        pxor	%xmm7, %xmm2
 4596                             	        pxor	%xmm6, %xmm1
 4597                             	        pxor	%xmm6, %xmm3
 4598                             	        pxor	%xmm4, %xmm1
 4599                             	        movdqa	80(%rsp), %xmm7
 4600                             	        movdqu	-96(%rdx), %xmm0
 4601                             	        pshufd	$0x4e, %xmm7, %xmm4
 4602                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 4603                             	        aesenc	48(%rdi), %xmm8
 4604                             	        pxor	%xmm7, %xmm4
 4605                             	        pshufd	$0x4e, %xmm0, %xmm5
 4606                             	        pxor	%xmm0, %xmm5
 4607                             	        movdqa	%xmm0, %xmm6
 4608                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 4609                             	        aesenc	48(%rdi), %xmm9
 4610                             	        aesenc	48(%rdi), %xmm10
 4611                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 4612                             	        aesenc	48(%rdi), %xmm11
 4613                             	        aesenc	48(%rdi), %xmm12
 4614                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 4615                             	        aesenc	48(%rdi), %xmm13
 4616                             	        aesenc	48(%rdi), %xmm14
 4617                             	        aesenc	48(%rdi), %xmm15
 4618                             	        pxor	%xmm7, %xmm1
 4619                             	        pxor	%xmm7, %xmm2
 4620                             	        pxor	%xmm6, %xmm1
 4621                             	        pxor	%xmm6, %xmm3
 4622                             	        pxor	%xmm4, %xmm1
 4623                             	        movdqa	64(%rsp), %xmm7
 4624                             	        movdqu	-80(%rdx), %xmm0
 4625                             	        pshufd	$0x4e, %xmm7, %xmm4
 4626                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 4627                             	        aesenc	64(%rdi), %xmm8
 4628                             	        pxor	%xmm7, %xmm4
 4629                             	        pshufd	$0x4e, %xmm0, %xmm5
 4630                             	        pxor	%xmm0, %xmm5
 4631                             	        movdqa	%xmm0, %xmm6
 4632                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 4633                             	        aesenc	64(%rdi), %xmm9
 4634                             	        aesenc	64(%rdi), %xmm10
 4635                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 4636                             	        aesenc	64(%rdi), %xmm11
 4637                             	        aesenc	64(%rdi), %xmm12
 4638                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 4639                             	        aesenc	64(%rdi), %xmm13
 4640                             	        aesenc	64(%rdi), %xmm14
 4641                             	        aesenc	64(%rdi), %xmm15
 4642                             	        pxor	%xmm7, %xmm1
 4643                             	        pxor	%xmm7, %xmm2
 4644                             	        pxor	%xmm6, %xmm1
 4645                             	        pxor	%xmm6, %xmm3
 4646                             	        pxor	%xmm4, %xmm1
 4647                             	        movdqa	48(%rsp), %xmm7
 4648                             	        movdqu	-64(%rdx), %xmm0
 4649                             	        pshufd	$0x4e, %xmm7, %xmm4
 4650                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 4651                             	        aesenc	80(%rdi), %xmm8
 4652                             	        pxor	%xmm7, %xmm4
 4653                             	        pshufd	$0x4e, %xmm0, %xmm5
 4654                             	        pxor	%xmm0, %xmm5
 4655                             	        movdqa	%xmm0, %xmm6
 4656                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 4657                             	        aesenc	80(%rdi), %xmm9
 4658                             	        aesenc	80(%rdi), %xmm10
 4659                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 4660                             	        aesenc	80(%rdi), %xmm11
 4661                             	        aesenc	80(%rdi), %xmm12
 4662                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 4663                             	        aesenc	80(%rdi), %xmm13
 4664                             	        aesenc	80(%rdi), %xmm14
 4665                             	        aesenc	80(%rdi), %xmm15
 4666                             	        pxor	%xmm7, %xmm1
 4667                             	        pxor	%xmm7, %xmm2
 4668                             	        pxor	%xmm6, %xmm1
 4669                             	        pxor	%xmm6, %xmm3
 4670                             	        pxor	%xmm4, %xmm1
 4671                             	        movdqa	32(%rsp), %xmm7
 4672                             	        movdqu	-48(%rdx), %xmm0
 4673                             	        pshufd	$0x4e, %xmm7, %xmm4
 4674                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 4675                             	        aesenc	96(%rdi), %xmm8
 4676                             	        pxor	%xmm7, %xmm4
 4677                             	        pshufd	$0x4e, %xmm0, %xmm5
 4678                             	        pxor	%xmm0, %xmm5
 4679                             	        movdqa	%xmm0, %xmm6
 4680                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 4681                             	        aesenc	96(%rdi), %xmm9
 4682                             	        aesenc	96(%rdi), %xmm10
 4683                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 4684                             	        aesenc	96(%rdi), %xmm11
 4685                             	        aesenc	96(%rdi), %xmm12
 4686                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 4687                             	        aesenc	96(%rdi), %xmm13
 4688                             	        aesenc	96(%rdi), %xmm14
 4689                             	        aesenc	96(%rdi), %xmm15
 4690                             	        pxor	%xmm7, %xmm1
 4691                             	        pxor	%xmm7, %xmm2
 4692                             	        pxor	%xmm6, %xmm1
 4693                             	        pxor	%xmm6, %xmm3
 4694                             	        pxor	%xmm4, %xmm1
 4695                             	        movdqa	16(%rsp), %xmm7
 4696                             	        movdqu	-32(%rdx), %xmm0
 4697                             	        pshufd	$0x4e, %xmm7, %xmm4
 4698                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 4699                             	        aesenc	112(%rdi), %xmm8
 4700                             	        pxor	%xmm7, %xmm4
 4701                             	        pshufd	$0x4e, %xmm0, %xmm5
 4702                             	        pxor	%xmm0, %xmm5
 4703                             	        movdqa	%xmm0, %xmm6
 4704                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 4705                             	        aesenc	112(%rdi), %xmm9
 4706                             	        aesenc	112(%rdi), %xmm10
 4707                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 4708                             	        aesenc	112(%rdi), %xmm11
 4709                             	        aesenc	112(%rdi), %xmm12
 4710                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 4711                             	        aesenc	112(%rdi), %xmm13
 4712                             	        aesenc	112(%rdi), %xmm14
 4713                             	        aesenc	112(%rdi), %xmm15
 4714                             	        pxor	%xmm7, %xmm1
 4715                             	        pxor	%xmm7, %xmm2
 4716                             	        pxor	%xmm6, %xmm1
 4717                             	        pxor	%xmm6, %xmm3
 4718                             	        pxor	%xmm4, %xmm1
 4719                             	        movdqa	(%rsp), %xmm7
 4720                             	        movdqu	-16(%rdx), %xmm0
 4721                             	        pshufd	$0x4e, %xmm7, %xmm4
 4722                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 4723                             	        aesenc	128(%rdi), %xmm8
 4724                             	        pxor	%xmm7, %xmm4
 4725                             	        pshufd	$0x4e, %xmm0, %xmm5
 4726                             	        pxor	%xmm0, %xmm5
 4727                             	        movdqa	%xmm0, %xmm6
 4728                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 4729                             	        aesenc	128(%rdi), %xmm9
 4730                             	        aesenc	128(%rdi), %xmm10
 4731                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 4732                             	        aesenc	128(%rdi), %xmm11
 4733                             	        aesenc	128(%rdi), %xmm12
 4734                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 4735                             	        aesenc	128(%rdi), %xmm13
 4736                             	        aesenc	128(%rdi), %xmm14
 4737                             	        aesenc	128(%rdi), %xmm15
 4738                             	        pxor	%xmm7, %xmm1
 4739                             	        pxor	%xmm7, %xmm2
 4740                             	        pxor	%xmm6, %xmm1
 4741                             	        pxor	%xmm6, %xmm3
 4742                             	        pxor	%xmm4, %xmm1
 4743                             	        movdqa	%xmm1, %xmm5
 4744                             	        psrldq	$8, %xmm1
 4745                             	        pslldq	$8, %xmm5
 4746                             	        aesenc	144(%rdi), %xmm8
 4747                             	        pxor	%xmm5, %xmm2
 4748                             	        pxor	%xmm1, %xmm3
 4749                             	        movdqa	%xmm2, %xmm7
 4750                             	        movdqa	%xmm2, %xmm4
 4751                             	        movdqa	%xmm2, %xmm5
 4752                             	        aesenc	144(%rdi), %xmm9
 4753                             	        pslld	$31, %xmm7
 4754                             	        pslld	$30, %xmm4
 4755                             	        pslld	$25, %xmm5
 4756                             	        aesenc	144(%rdi), %xmm10
 4757                             	        pxor	%xmm4, %xmm7
 4758                             	        pxor	%xmm5, %xmm7
 4759                             	        aesenc	144(%rdi), %xmm11
 4760                             	        movdqa	%xmm7, %xmm4
 4761                             	        pslldq	$12, %xmm7
 4762                             	        psrldq	$4, %xmm4
 4763                             	        aesenc	144(%rdi), %xmm12
 4764                             	        pxor	%xmm7, %xmm2
 4765                             	        movdqa	%xmm2, %xmm5
 4766                             	        movdqa	%xmm2, %xmm1
 4767                             	        movdqa	%xmm2, %xmm0
 4768                             	        aesenc	144(%rdi), %xmm13
 4769                             	        psrld	$0x01, %xmm5
 4770                             	        psrld	$2, %xmm1
 4771                             	        psrld	$7, %xmm0
 4772                             	        aesenc	144(%rdi), %xmm14
 4773                             	        pxor	%xmm1, %xmm5
 4774                             	        pxor	%xmm0, %xmm5
 4775                             	        aesenc	144(%rdi), %xmm15
 4776                             	        pxor	%xmm4, %xmm5
 4777                             	        pxor	%xmm5, %xmm2
 4778                             	        pxor	%xmm3, %xmm2
 4779                             	        cmpl	$11, %esi
 4780                             	        movdqa	160(%rdi), %xmm7
 4781                             	        jl	L_AES_GCM_encrypt_update_aesni_aesenc_128_ghash_avx_done
 4782                             	        aesenc	%xmm7, %xmm8
 4783                             	        aesenc	%xmm7, %xmm9
 4784                             	        aesenc	%xmm7, %xmm10
 4785                             	        aesenc	%xmm7, %xmm11
 4786                             	        aesenc	%xmm7, %xmm12
 4787                             	        aesenc	%xmm7, %xmm13
 4788                             	        aesenc	%xmm7, %xmm14
 4789                             	        aesenc	%xmm7, %xmm15
 4790                             	        movdqa	176(%rdi), %xmm7
 4791                             	        aesenc	%xmm7, %xmm8
 4792                             	        aesenc	%xmm7, %xmm9
 4793                             	        aesenc	%xmm7, %xmm10
 4794                             	        aesenc	%xmm7, %xmm11
 4795                             	        aesenc	%xmm7, %xmm12
 4796                             	        aesenc	%xmm7, %xmm13
 4797                             	        aesenc	%xmm7, %xmm14
 4798                             	        aesenc	%xmm7, %xmm15
 4799                             	        cmpl	$13, %esi
 4800                             	        movdqa	192(%rdi), %xmm7
 4801                             	        jl	L_AES_GCM_encrypt_update_aesni_aesenc_128_ghash_avx_done
 4802                             	        aesenc	%xmm7, %xmm8
 4803                             	        aesenc	%xmm7, %xmm9
 4804                             	        aesenc	%xmm7, %xmm10
 4805                             	        aesenc	%xmm7, %xmm11
 4806                             	        aesenc	%xmm7, %xmm12
 4807                             	        aesenc	%xmm7, %xmm13
 4808                             	        aesenc	%xmm7, %xmm14
 4809                             	        aesenc	%xmm7, %xmm15
 4810                             	        movdqa	208(%rdi), %xmm7
 4811                             	        aesenc	%xmm7, %xmm8
 4812                             	        aesenc	%xmm7, %xmm9
 4813                             	        aesenc	%xmm7, %xmm10
 4814                             	        aesenc	%xmm7, %xmm11
 4815                             	        aesenc	%xmm7, %xmm12
 4816                             	        aesenc	%xmm7, %xmm13
 4817                             	        aesenc	%xmm7, %xmm14
 4818                             	        aesenc	%xmm7, %xmm15
 4819                             	        movdqa	224(%rdi), %xmm7
 4820                             	L_AES_GCM_encrypt_update_aesni_aesenc_128_ghash_avx_done:
 4821                             	        aesenclast	%xmm7, %xmm8
 4822                             	        aesenclast	%xmm7, %xmm9
 4823                             	        movdqu	(%rcx), %xmm0
 4824                             	        movdqu	16(%rcx), %xmm1
 4825                             	        pxor	%xmm0, %xmm8
 4826                             	        pxor	%xmm1, %xmm9
 4827                             	        movdqu	%xmm8, (%rdx)
 4828                             	        movdqu	%xmm9, 16(%rdx)
 4829                             	        aesenclast	%xmm7, %xmm10
 4830                             	        aesenclast	%xmm7, %xmm11
 4831                             	        movdqu	32(%rcx), %xmm0
 4832                             	        movdqu	48(%rcx), %xmm1
 4833                             	        pxor	%xmm0, %xmm10
 4834                             	        pxor	%xmm1, %xmm11
 4835                             	        movdqu	%xmm10, 32(%rdx)
 4836                             	        movdqu	%xmm11, 48(%rdx)
 4837                             	        aesenclast	%xmm7, %xmm12
 4838                             	        aesenclast	%xmm7, %xmm13
 4839                             	        movdqu	64(%rcx), %xmm0
 4840                             	        movdqu	80(%rcx), %xmm1
 4841                             	        pxor	%xmm0, %xmm12
 4842                             	        pxor	%xmm1, %xmm13
 4843                             	        movdqu	%xmm12, 64(%rdx)
 4844                             	        movdqu	%xmm13, 80(%rdx)
 4845                             	        aesenclast	%xmm7, %xmm14
 4846                             	        aesenclast	%xmm7, %xmm15
 4847                             	        movdqu	96(%rcx), %xmm0
 4848                             	        movdqu	112(%rcx), %xmm1
 4849                             	        pxor	%xmm0, %xmm14
 4850                             	        pxor	%xmm1, %xmm15
 4851                             	        movdqu	%xmm14, 96(%rdx)
 4852                             	        movdqu	%xmm15, 112(%rdx)
 4853                             	        addl	$0x80, %r14d
 4854                             	        cmpl	%r13d, %r14d
 4855                             	        jl	L_AES_GCM_encrypt_update_aesni_ghash_128
 4856                             	L_AES_GCM_encrypt_update_aesni_end_128:
 4857                             	        movdqa	L_aes_gcm_bswap_mask(%rip), %xmm4
 4858                             	        pshufb	%xmm4, %xmm8
 4859                             	        pshufb	%xmm4, %xmm9
 4860                             	        pshufb	%xmm4, %xmm10
 4861                             	        pshufb	%xmm4, %xmm11
 4862                             	        pxor	%xmm2, %xmm8
 4863                             	        pshufb	%xmm4, %xmm12
 4864                             	        pshufb	%xmm4, %xmm13
 4865                             	        pshufb	%xmm4, %xmm14
 4866                             	        pshufb	%xmm4, %xmm15
 4867                             	        movdqa	112(%rsp), %xmm7
 4868                             	        pshufd	$0x4e, %xmm8, %xmm1
 4869                             	        pshufd	$0x4e, %xmm7, %xmm2
 4870                             	        movdqa	%xmm7, %xmm3
 4871                             	        movdqa	%xmm7, %xmm0
 4872                             	        pclmulqdq	$0x11, %xmm8, %xmm3
 4873                             	        pclmulqdq	$0x00, %xmm8, %xmm0
 4874                             	        pxor	%xmm8, %xmm1
 4875                             	        pxor	%xmm7, %xmm2
 4876                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 4877                             	        pxor	%xmm0, %xmm1
 4878                             	        pxor	%xmm3, %xmm1
 4879                             	        movdqa	%xmm1, %xmm2
 4880                             	        movdqa	%xmm0, %xmm4
 4881                             	        movdqa	%xmm3, %xmm6
 4882                             	        pslldq	$8, %xmm2
 4883                             	        psrldq	$8, %xmm1
 4884                             	        pxor	%xmm2, %xmm4
 4885                             	        pxor	%xmm1, %xmm6
 4886                             	        movdqa	96(%rsp), %xmm7
 4887                             	        pshufd	$0x4e, %xmm9, %xmm1
 4888                             	        pshufd	$0x4e, %xmm7, %xmm2
 4889                             	        movdqa	%xmm7, %xmm3
 4890                             	        movdqa	%xmm7, %xmm0
 4891                             	        pclmulqdq	$0x11, %xmm9, %xmm3
 4892                             	        pclmulqdq	$0x00, %xmm9, %xmm0
 4893                             	        pxor	%xmm9, %xmm1
 4894                             	        pxor	%xmm7, %xmm2
 4895                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 4896                             	        pxor	%xmm0, %xmm1
 4897                             	        pxor	%xmm3, %xmm1
 4898                             	        movdqa	%xmm1, %xmm2
 4899                             	        pxor	%xmm0, %xmm4
 4900                             	        pxor	%xmm3, %xmm6
 4901                             	        pslldq	$8, %xmm2
 4902                             	        psrldq	$8, %xmm1
 4903                             	        pxor	%xmm2, %xmm4
 4904                             	        pxor	%xmm1, %xmm6
 4905                             	        movdqa	80(%rsp), %xmm7
 4906                             	        pshufd	$0x4e, %xmm10, %xmm1
 4907                             	        pshufd	$0x4e, %xmm7, %xmm2
 4908                             	        movdqa	%xmm7, %xmm3
 4909                             	        movdqa	%xmm7, %xmm0
 4910                             	        pclmulqdq	$0x11, %xmm10, %xmm3
 4911                             	        pclmulqdq	$0x00, %xmm10, %xmm0
 4912                             	        pxor	%xmm10, %xmm1
 4913                             	        pxor	%xmm7, %xmm2
 4914                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 4915                             	        pxor	%xmm0, %xmm1
 4916                             	        pxor	%xmm3, %xmm1
 4917                             	        movdqa	%xmm1, %xmm2
 4918                             	        pxor	%xmm0, %xmm4
 4919                             	        pxor	%xmm3, %xmm6
 4920                             	        pslldq	$8, %xmm2
 4921                             	        psrldq	$8, %xmm1
 4922                             	        pxor	%xmm2, %xmm4
 4923                             	        pxor	%xmm1, %xmm6
 4924                             	        movdqa	64(%rsp), %xmm7
 4925                             	        pshufd	$0x4e, %xmm11, %xmm1
 4926                             	        pshufd	$0x4e, %xmm7, %xmm2
 4927                             	        movdqa	%xmm7, %xmm3
 4928                             	        movdqa	%xmm7, %xmm0
 4929                             	        pclmulqdq	$0x11, %xmm11, %xmm3
 4930                             	        pclmulqdq	$0x00, %xmm11, %xmm0
 4931                             	        pxor	%xmm11, %xmm1
 4932                             	        pxor	%xmm7, %xmm2
 4933                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 4934                             	        pxor	%xmm0, %xmm1
 4935                             	        pxor	%xmm3, %xmm1
 4936                             	        movdqa	%xmm1, %xmm2
 4937                             	        pxor	%xmm0, %xmm4
 4938                             	        pxor	%xmm3, %xmm6
 4939                             	        pslldq	$8, %xmm2
 4940                             	        psrldq	$8, %xmm1
 4941                             	        pxor	%xmm2, %xmm4
 4942                             	        pxor	%xmm1, %xmm6
 4943                             	        movdqa	48(%rsp), %xmm7
 4944                             	        pshufd	$0x4e, %xmm12, %xmm1
 4945                             	        pshufd	$0x4e, %xmm7, %xmm2
 4946                             	        movdqa	%xmm7, %xmm3
 4947                             	        movdqa	%xmm7, %xmm0
 4948                             	        pclmulqdq	$0x11, %xmm12, %xmm3
 4949                             	        pclmulqdq	$0x00, %xmm12, %xmm0
 4950                             	        pxor	%xmm12, %xmm1
 4951                             	        pxor	%xmm7, %xmm2
 4952                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 4953                             	        pxor	%xmm0, %xmm1
 4954                             	        pxor	%xmm3, %xmm1
 4955                             	        movdqa	%xmm1, %xmm2
 4956                             	        pxor	%xmm0, %xmm4
 4957                             	        pxor	%xmm3, %xmm6
 4958                             	        pslldq	$8, %xmm2
 4959                             	        psrldq	$8, %xmm1
 4960                             	        pxor	%xmm2, %xmm4
 4961                             	        pxor	%xmm1, %xmm6
 4962                             	        movdqa	32(%rsp), %xmm7
 4963                             	        pshufd	$0x4e, %xmm13, %xmm1
 4964                             	        pshufd	$0x4e, %xmm7, %xmm2
 4965                             	        movdqa	%xmm7, %xmm3
 4966                             	        movdqa	%xmm7, %xmm0
 4967                             	        pclmulqdq	$0x11, %xmm13, %xmm3
 4968                             	        pclmulqdq	$0x00, %xmm13, %xmm0
 4969                             	        pxor	%xmm13, %xmm1
 4970                             	        pxor	%xmm7, %xmm2
 4971                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 4972                             	        pxor	%xmm0, %xmm1
 4973                             	        pxor	%xmm3, %xmm1
 4974                             	        movdqa	%xmm1, %xmm2
 4975                             	        pxor	%xmm0, %xmm4
 4976                             	        pxor	%xmm3, %xmm6
 4977                             	        pslldq	$8, %xmm2
 4978                             	        psrldq	$8, %xmm1
 4979                             	        pxor	%xmm2, %xmm4
 4980                             	        pxor	%xmm1, %xmm6
 4981                             	        movdqa	16(%rsp), %xmm7
 4982                             	        pshufd	$0x4e, %xmm14, %xmm1
 4983                             	        pshufd	$0x4e, %xmm7, %xmm2
 4984                             	        movdqa	%xmm7, %xmm3
 4985                             	        movdqa	%xmm7, %xmm0
 4986                             	        pclmulqdq	$0x11, %xmm14, %xmm3
 4987                             	        pclmulqdq	$0x00, %xmm14, %xmm0
 4988                             	        pxor	%xmm14, %xmm1
 4989                             	        pxor	%xmm7, %xmm2
 4990                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 4991                             	        pxor	%xmm0, %xmm1
 4992                             	        pxor	%xmm3, %xmm1
 4993                             	        movdqa	%xmm1, %xmm2
 4994                             	        pxor	%xmm0, %xmm4
 4995                             	        pxor	%xmm3, %xmm6
 4996                             	        pslldq	$8, %xmm2
 4997                             	        psrldq	$8, %xmm1
 4998                             	        pxor	%xmm2, %xmm4
 4999                             	        pxor	%xmm1, %xmm6
 5000                             	        movdqa	(%rsp), %xmm7
 5001                             	        pshufd	$0x4e, %xmm15, %xmm1
 5002                             	        pshufd	$0x4e, %xmm7, %xmm2
 5003                             	        movdqa	%xmm7, %xmm3
 5004                             	        movdqa	%xmm7, %xmm0
 5005                             	        pclmulqdq	$0x11, %xmm15, %xmm3
 5006                             	        pclmulqdq	$0x00, %xmm15, %xmm0
 5007                             	        pxor	%xmm15, %xmm1
 5008                             	        pxor	%xmm7, %xmm2
 5009                             	        pclmulqdq	$0x00, %xmm2, %xmm1
 5010                             	        pxor	%xmm0, %xmm1
 5011                             	        pxor	%xmm3, %xmm1
 5012                             	        movdqa	%xmm1, %xmm2
 5013                             	        pxor	%xmm0, %xmm4
 5014                             	        pxor	%xmm3, %xmm6
 5015                             	        pslldq	$8, %xmm2
 5016                             	        psrldq	$8, %xmm1
 5017                             	        pxor	%xmm2, %xmm4
 5018                             	        pxor	%xmm1, %xmm6
 5019                             	        movdqa	%xmm4, %xmm0
 5020                             	        movdqa	%xmm4, %xmm1
 5021                             	        movdqa	%xmm4, %xmm2
 5022                             	        pslld	$31, %xmm0
 5023                             	        pslld	$30, %xmm1
 5024                             	        pslld	$25, %xmm2
 5025                             	        pxor	%xmm1, %xmm0
 5026                             	        pxor	%xmm2, %xmm0
 5027                             	        movdqa	%xmm0, %xmm1
 5028                             	        psrldq	$4, %xmm1
 5029                             	        pslldq	$12, %xmm0
 5030                             	        pxor	%xmm0, %xmm4
 5031                             	        movdqa	%xmm4, %xmm2
 5032                             	        movdqa	%xmm4, %xmm3
 5033                             	        movdqa	%xmm4, %xmm0
 5034                             	        psrld	$0x01, %xmm2
 5035                             	        psrld	$2, %xmm3
 5036                             	        psrld	$7, %xmm0
 5037                             	        pxor	%xmm3, %xmm2
 5038                             	        pxor	%xmm0, %xmm2
 5039                             	        pxor	%xmm1, %xmm2
 5040                             	        pxor	%xmm4, %xmm2
 5041                             	        pxor	%xmm2, %xmm6
 5042                             	        movdqa	(%rsp), %xmm5
 5043                             	L_AES_GCM_encrypt_update_aesni_done_128:
 5044                             	        movl	%r8d, %edx
 5045                             	        cmpl	%edx, %r14d
 5046                             	        jge	L_AES_GCM_encrypt_update_aesni_done_enc
 5047                             	        movl	%r8d, %r13d
 5048                             	        andl	$0xfffffff0, %r13d
 5049                             	        cmpl	%r13d, %r14d
 5050                             	        jge	L_AES_GCM_encrypt_update_aesni_last_block_done
 5051                             	        leaq	(%r11,%r14,1), %rcx
 5052                             	        leaq	(%r10,%r14,1), %rdx
 5053                             	        movdqa	(%r12), %xmm8
 5054                             	        movdqa	%xmm8, %xmm9
 5055                             	        pshufb	L_aes_gcm_bswap_epi64(%rip), %xmm8
 5056                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 5057                             	        pxor	(%rdi), %xmm8
 5058                             	        movdqa	%xmm9, (%r12)
 5059                             	        aesenc	16(%rdi), %xmm8
 5060                             	        aesenc	32(%rdi), %xmm8
 5061                             	        aesenc	48(%rdi), %xmm8
 5062                             	        aesenc	64(%rdi), %xmm8
 5063                             	        aesenc	80(%rdi), %xmm8
 5064                             	        aesenc	96(%rdi), %xmm8
 5065                             	        aesenc	112(%rdi), %xmm8
 5066                             	        aesenc	128(%rdi), %xmm8
 5067                             	        aesenc	144(%rdi), %xmm8
 5068                             	        cmpl	$11, %esi
 5069                             	        movdqa	160(%rdi), %xmm9
 5070                             	        jl	L_AES_GCM_encrypt_update_aesni_aesenc_block_aesenc_avx_last
 5071                             	        aesenc	%xmm9, %xmm8
 5072                             	        aesenc	176(%rdi), %xmm8
 5073                             	        cmpl	$13, %esi
 5074                             	        movdqa	192(%rdi), %xmm9
 5075                             	        jl	L_AES_GCM_encrypt_update_aesni_aesenc_block_aesenc_avx_last
 5076                             	        aesenc	%xmm9, %xmm8
 5077                             	        aesenc	208(%rdi), %xmm8
 5078                             	        movdqa	224(%rdi), %xmm9
 5079                             	L_AES_GCM_encrypt_update_aesni_aesenc_block_aesenc_avx_last:
 5080                             	        aesenclast	%xmm9, %xmm8
 5081                             	        movdqu	(%rcx), %xmm9
 5082                             	        pxor	%xmm9, %xmm8
 5083                             	        movdqu	%xmm8, (%rdx)
 5084                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 5085                             	        pxor	%xmm8, %xmm6
 5086                             	        addl	$16, %r14d
 5087                             	        cmpl	%r13d, %r14d
 5088                             	        jge	L_AES_GCM_encrypt_update_aesni_last_block_ghash
 5089                             	L_AES_GCM_encrypt_update_aesni_last_block_start:
 5090                             	        leaq	(%r11,%r14,1), %rcx
 5091                             	        leaq	(%r10,%r14,1), %rdx
 5092                             	        movdqa	(%r12), %xmm8
 5093                             	        movdqa	%xmm8, %xmm9
 5094                             	        pshufb	L_aes_gcm_bswap_epi64(%rip), %xmm8
 5095                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 5096                             	        pxor	(%rdi), %xmm8
 5097                             	        movdqa	%xmm9, (%r12)
 5098                             	        movdqa	%xmm6, %xmm10
 5099                             	        pclmulqdq	$16, %xmm5, %xmm10
 5100                             	        aesenc	16(%rdi), %xmm8
 5101                             	        aesenc	32(%rdi), %xmm8
 5102                             	        movdqa	%xmm6, %xmm11
 5103                             	        pclmulqdq	$0x01, %xmm5, %xmm11
 5104                             	        aesenc	48(%rdi), %xmm8
 5105                             	        aesenc	64(%rdi), %xmm8
 5106                             	        movdqa	%xmm6, %xmm12
 5107                             	        pclmulqdq	$0x00, %xmm5, %xmm12
 5108                             	        aesenc	80(%rdi), %xmm8
 5109                             	        movdqa	%xmm6, %xmm1
 5110                             	        pclmulqdq	$0x11, %xmm5, %xmm1
 5111                             	        aesenc	96(%rdi), %xmm8
 5112                             	        pxor	%xmm11, %xmm10
 5113                             	        movdqa	%xmm10, %xmm2
 5114                             	        psrldq	$8, %xmm10
 5115                             	        pslldq	$8, %xmm2
 5116                             	        aesenc	112(%rdi), %xmm8
 5117                             	        movdqa	%xmm1, %xmm3
 5118                             	        pxor	%xmm12, %xmm2
 5119                             	        pxor	%xmm10, %xmm3
 5120                             	        movdqa	L_aes_gcm_mod2_128(%rip), %xmm0
 5121                             	        movdqa	%xmm2, %xmm11
 5122                             	        pclmulqdq	$16, %xmm0, %xmm11
 5123                             	        aesenc	128(%rdi), %xmm8
 5124                             	        pshufd	$0x4e, %xmm2, %xmm10
 5125                             	        pxor	%xmm11, %xmm10
 5126                             	        movdqa	%xmm10, %xmm11
 5127                             	        pclmulqdq	$16, %xmm0, %xmm11
 5128                             	        aesenc	144(%rdi), %xmm8
 5129                             	        pshufd	$0x4e, %xmm10, %xmm6
 5130                             	        pxor	%xmm11, %xmm6
 5131                             	        pxor	%xmm3, %xmm6
 5132                             	        cmpl	$11, %esi
 5133                             	        movdqa	160(%rdi), %xmm9
 5134                             	        jl	L_AES_GCM_encrypt_update_aesni_aesenc_gfmul_last
 5135                             	        aesenc	%xmm9, %xmm8
 5136                             	        aesenc	176(%rdi), %xmm8
 5137                             	        cmpl	$13, %esi
 5138                             	        movdqa	192(%rdi), %xmm9
 5139                             	        jl	L_AES_GCM_encrypt_update_aesni_aesenc_gfmul_last
 5140                             	        aesenc	%xmm9, %xmm8
 5141                             	        aesenc	208(%rdi), %xmm8
 5142                             	        movdqa	224(%rdi), %xmm9
 5143                             	L_AES_GCM_encrypt_update_aesni_aesenc_gfmul_last:
 5144                             	        aesenclast	%xmm9, %xmm8
 5145                             	        movdqu	(%rcx), %xmm9
 5146                             	        pxor	%xmm9, %xmm8
 5147                             	        movdqu	%xmm8, (%rdx)
 5148                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm8
 5149                             	        pxor	%xmm8, %xmm6
 5150                             	        addl	$16, %r14d
 5151                             	        cmpl	%r13d, %r14d
 5152                             	        jl	L_AES_GCM_encrypt_update_aesni_last_block_start
 5153                             	L_AES_GCM_encrypt_update_aesni_last_block_ghash:
 5154                             	        pshufd	$0x4e, %xmm5, %xmm9
 5155                             	        pshufd	$0x4e, %xmm6, %xmm10
 5156                             	        movdqa	%xmm6, %xmm11
 5157                             	        movdqa	%xmm6, %xmm8
 5158                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 5159                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 5160                             	        pxor	%xmm5, %xmm9
 5161                             	        pxor	%xmm6, %xmm10
 5162                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 5163                             	        pxor	%xmm8, %xmm9
 5164                             	        pxor	%xmm11, %xmm9
 5165                             	        movdqa	%xmm9, %xmm10
 5166                             	        movdqa	%xmm11, %xmm6
 5167                             	        pslldq	$8, %xmm10
 5168                             	        psrldq	$8, %xmm9
 5169                             	        pxor	%xmm10, %xmm8
 5170                             	        pxor	%xmm9, %xmm6
 5171                             	        movdqa	%xmm8, %xmm12
 5172                             	        movdqa	%xmm8, %xmm13
 5173                             	        movdqa	%xmm8, %xmm14
 5174                             	        pslld	$31, %xmm12
 5175                             	        pslld	$30, %xmm13
 5176                             	        pslld	$25, %xmm14
 5177                             	        pxor	%xmm13, %xmm12
 5178                             	        pxor	%xmm14, %xmm12
 5179                             	        movdqa	%xmm12, %xmm13
 5180                             	        psrldq	$4, %xmm13
 5181                             	        pslldq	$12, %xmm12
 5182                             	        pxor	%xmm12, %xmm8
 5183                             	        movdqa	%xmm8, %xmm14
 5184                             	        movdqa	%xmm8, %xmm10
 5185                             	        movdqa	%xmm8, %xmm9
 5186                             	        psrld	$0x01, %xmm14
 5187                             	        psrld	$2, %xmm10
 5188                             	        psrld	$7, %xmm9
 5189                             	        pxor	%xmm10, %xmm14
 5190                             	        pxor	%xmm9, %xmm14
 5191                             	        pxor	%xmm13, %xmm14
 5192                             	        pxor	%xmm8, %xmm14
 5193                             	        pxor	%xmm14, %xmm6
 5194                             	L_AES_GCM_encrypt_update_aesni_last_block_done:
 5195                             	L_AES_GCM_encrypt_update_aesni_done_enc:
 5196                             	        movdqa	%xmm6, (%r9)
 5197                             	        addq	$0xa0, %rsp
 5198                             	        popq	%r14
 5199                             	        popq	%r12
 5200                             	        popq	%r13
 5201                             	        repz retq
 5202                             	#ifndef __APPLE__
 5204                             	#endif /* __APPLE__ */
 5205                             	#ifndef __APPLE__
 5206                             	.text
 5207                             	.globl	AES_GCM_encrypt_final_aesni
 5209                             	.align	16
 5210                             	AES_GCM_encrypt_final_aesni:
 5211                             	#else
 5212                             	.section	__TEXT,__text
 5213                             	.globl	_AES_GCM_encrypt_final_aesni
 5214                             	.p2align	4
 5215                             	_AES_GCM_encrypt_final_aesni:
 5216                             	#endif /* __APPLE__ */
 5217                             	        pushq	%r13
 5218                             	        movq	%rdx, %rax
 5219                             	        movl	%ecx, %r10d
 5220                             	        movl	%r8d, %r11d
 5221                             	        movq	16(%rsp), %r8
 5222                             	        subq	$16, %rsp
 5223                             	        movdqa	(%rdi), %xmm4
 5224                             	        movdqa	(%r9), %xmm5
 5225                             	        movdqa	(%r8), %xmm6
 5226                             	        movdqa	%xmm5, %xmm9
 5227                             	        movdqa	%xmm5, %xmm8
 5228                             	        psrlq	$63, %xmm9
 5229                             	        psllq	$0x01, %xmm8
 5230                             	        pslldq	$8, %xmm9
 5231                             	        por	%xmm9, %xmm8
 5232                             	        pshufd	$0xff, %xmm5, %xmm5
 5233                             	        psrad	$31, %xmm5
 5234                             	        pand	L_aes_gcm_mod2_128(%rip), %xmm5
 5235                             	        pxor	%xmm8, %xmm5
 5236                             	        movl	%r10d, %edx
 5237                             	        movl	%r11d, %ecx
 5238                             	        shlq	$3, %rdx
 5239                             	        shlq	$3, %rcx
 5240                             	        pinsrq	$0x00, %rdx, %xmm0
 5241                             	        pinsrq	$0x01, %rcx, %xmm0
 5242                             	        pxor	%xmm0, %xmm4
 5243                             	        pshufd	$0x4e, %xmm5, %xmm9
 5244                             	        pshufd	$0x4e, %xmm4, %xmm10
 5245                             	        movdqa	%xmm4, %xmm11
 5246                             	        movdqa	%xmm4, %xmm8
 5247                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 5248                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 5249                             	        pxor	%xmm5, %xmm9
 5250                             	        pxor	%xmm4, %xmm10
 5251                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 5252                             	        pxor	%xmm8, %xmm9
 5253                             	        pxor	%xmm11, %xmm9
 5254                             	        movdqa	%xmm9, %xmm10
 5255                             	        movdqa	%xmm11, %xmm4
 5256                             	        pslldq	$8, %xmm10
 5257                             	        psrldq	$8, %xmm9
 5258                             	        pxor	%xmm10, %xmm8
 5259                             	        pxor	%xmm9, %xmm4
 5260                             	        movdqa	%xmm8, %xmm12
 5261                             	        movdqa	%xmm8, %xmm13
 5262                             	        movdqa	%xmm8, %xmm14
 5263                             	        pslld	$31, %xmm12
 5264                             	        pslld	$30, %xmm13
 5265                             	        pslld	$25, %xmm14
 5266                             	        pxor	%xmm13, %xmm12
 5267                             	        pxor	%xmm14, %xmm12
 5268                             	        movdqa	%xmm12, %xmm13
 5269                             	        psrldq	$4, %xmm13
 5270                             	        pslldq	$12, %xmm12
 5271                             	        pxor	%xmm12, %xmm8
 5272                             	        movdqa	%xmm8, %xmm14
 5273                             	        movdqa	%xmm8, %xmm10
 5274                             	        movdqa	%xmm8, %xmm9
 5275                             	        psrld	$0x01, %xmm14
 5276                             	        psrld	$2, %xmm10
 5277                             	        psrld	$7, %xmm9
 5278                             	        pxor	%xmm10, %xmm14
 5279                             	        pxor	%xmm9, %xmm14
 5280                             	        pxor	%xmm13, %xmm14
 5281                             	        pxor	%xmm8, %xmm14
 5282                             	        pxor	%xmm14, %xmm4
 5283                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm4
 5284                             	        movdqa	%xmm6, %xmm0
 5285                             	        pxor	%xmm4, %xmm0
 5286                             	        cmpl	$16, %eax
 5287                             	        je	L_AES_GCM_encrypt_final_aesni_store_tag_16
 5288                             	        xorq	%rcx, %rcx
 5289                             	        movdqa	%xmm0, (%rsp)
 5290                             	L_AES_GCM_encrypt_final_aesni_store_tag_loop:
 5291                             	        movzbl	(%rsp,%rcx,1), %r13d
 5292                             	        movb	%r13b, (%rsi,%rcx,1)
 5293                             	        incl	%ecx
 5294                             	        cmpl	%eax, %ecx
 5295                             	        jne	L_AES_GCM_encrypt_final_aesni_store_tag_loop
 5296                             	        jmp	L_AES_GCM_encrypt_final_aesni_store_tag_done
 5297                             	L_AES_GCM_encrypt_final_aesni_store_tag_16:
 5298                             	        movdqu	%xmm0, (%rsi)
 5299                             	L_AES_GCM_encrypt_final_aesni_store_tag_done:
 5300                             	        addq	$16, %rsp
 5301                             	        popq	%r13
 5302                             	        repz retq
 5303                             	#ifndef __APPLE__
 5305                             	#endif /* __APPLE__ */
 5306                             	#ifndef __APPLE__
 5307                             	.text
 5308                             	.globl	AES_GCM_decrypt_update_aesni
 5310                             	.align	16
 5311                             	AES_GCM_decrypt_update_aesni:
 5312                             	#else
 5313                             	.section	__TEXT,__text
 5314                             	.globl	_AES_GCM_decrypt_update_aesni
 5315                             	.p2align	4
 5316                             	_AES_GCM_decrypt_update_aesni:
 5317                             	#endif /* __APPLE__ */
 5318                             	        pushq	%r13
 5319                             	        pushq	%r12
 5320                             	        pushq	%r14
 5321                             	        pushq	%r15
 5322                             	        movq	%rdx, %r10
 5323                             	        movq	%rcx, %r11
 5324                             	        movq	40(%rsp), %rax
 5325                             	        movq	48(%rsp), %r12
 5326                             	        subq	$0xa8, %rsp
 5327                             	        movdqa	(%r9), %xmm6
 5328                             	        movdqa	(%rax), %xmm5
 5329                             	        movdqa	%xmm5, %xmm9
 5330                             	        movdqa	%xmm5, %xmm8
 5331                             	        psrlq	$63, %xmm9
 5332                             	        psllq	$0x01, %xmm8
 5333                             	        pslldq	$8, %xmm9
 5334                             	        por	%xmm9, %xmm8
 5335                             	        pshufd	$0xff, %xmm5, %xmm5
 5336                             	        psrad	$31, %xmm5
 5337                             	        pand	L_aes_gcm_mod2_128(%rip), %xmm5
 5338                             	        pxor	%xmm8, %xmm5
 5339                             	        xorl	%r14d, %r14d
 5340                             	        cmpl	$0x80, %r8d
 5341                             	        movl	%r8d, %r13d
 5342                             	        jl	L_AES_GCM_decrypt_update_aesni_done_128
 5343                             	        andl	$0xffffff80, %r13d
 5344                             	        movdqa	%xmm6, %xmm2
 5345                             	        # H ^ 1
 5346                             	        movdqa	%xmm5, (%rsp)
 5347                             	        # H ^ 2
 5348                             	        pshufd	$0x4e, %xmm5, %xmm9
 5349                             	        pshufd	$0x4e, %xmm5, %xmm10
 5350                             	        movdqa	%xmm5, %xmm11
 5351                             	        movdqa	%xmm5, %xmm8
 5352                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 5353                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 5354                             	        pxor	%xmm5, %xmm9
 5355                             	        pxor	%xmm5, %xmm10
 5356                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 5357                             	        pxor	%xmm8, %xmm9
 5358                             	        pxor	%xmm11, %xmm9
 5359                             	        movdqa	%xmm9, %xmm10
 5360                             	        movdqa	%xmm11, %xmm0
 5361                             	        pslldq	$8, %xmm10
 5362                             	        psrldq	$8, %xmm9
 5363                             	        pxor	%xmm10, %xmm8
 5364                             	        pxor	%xmm9, %xmm0
 5365                             	        movdqa	%xmm8, %xmm12
 5366                             	        movdqa	%xmm8, %xmm13
 5367                             	        movdqa	%xmm8, %xmm14
 5368                             	        pslld	$31, %xmm12
 5369                             	        pslld	$30, %xmm13
 5370                             	        pslld	$25, %xmm14
 5371                             	        pxor	%xmm13, %xmm12
 5372                             	        pxor	%xmm14, %xmm12
 5373                             	        movdqa	%xmm12, %xmm13
 5374                             	        psrldq	$4, %xmm13
 5375                             	        pslldq	$12, %xmm12
 5376                             	        pxor	%xmm12, %xmm8
 5377                             	        movdqa	%xmm8, %xmm14
 5378                             	        movdqa	%xmm8, %xmm10
 5379                             	        movdqa	%xmm8, %xmm9
 5380                             	        psrld	$0x01, %xmm14
 5381                             	        psrld	$2, %xmm10
 5382                             	        psrld	$7, %xmm9
 5383                             	        pxor	%xmm10, %xmm14
 5384                             	        pxor	%xmm9, %xmm14
 5385                             	        pxor	%xmm13, %xmm14
 5386                             	        pxor	%xmm8, %xmm14
 5387                             	        pxor	%xmm14, %xmm0
 5388                             	        movdqa	%xmm0, 16(%rsp)
 5389                             	        # H ^ 3
 5390                             	        pshufd	$0x4e, %xmm5, %xmm9
 5391                             	        pshufd	$0x4e, %xmm0, %xmm10
 5392                             	        movdqa	%xmm0, %xmm11
 5393                             	        movdqa	%xmm0, %xmm8
 5394                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 5395                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 5396                             	        pxor	%xmm5, %xmm9
 5397                             	        pxor	%xmm0, %xmm10
 5398                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 5399                             	        pxor	%xmm8, %xmm9
 5400                             	        pxor	%xmm11, %xmm9
 5401                             	        movdqa	%xmm9, %xmm10
 5402                             	        movdqa	%xmm11, %xmm1
 5403                             	        pslldq	$8, %xmm10
 5404                             	        psrldq	$8, %xmm9
 5405                             	        pxor	%xmm10, %xmm8
 5406                             	        pxor	%xmm9, %xmm1
 5407                             	        movdqa	%xmm8, %xmm12
 5408                             	        movdqa	%xmm8, %xmm13
 5409                             	        movdqa	%xmm8, %xmm14
 5410                             	        pslld	$31, %xmm12
 5411                             	        pslld	$30, %xmm13
 5412                             	        pslld	$25, %xmm14
 5413                             	        pxor	%xmm13, %xmm12
 5414                             	        pxor	%xmm14, %xmm12
 5415                             	        movdqa	%xmm12, %xmm13
 5416                             	        psrldq	$4, %xmm13
 5417                             	        pslldq	$12, %xmm12
 5418                             	        pxor	%xmm12, %xmm8
 5419                             	        movdqa	%xmm8, %xmm14
 5420                             	        movdqa	%xmm8, %xmm10
 5421                             	        movdqa	%xmm8, %xmm9
 5422                             	        psrld	$0x01, %xmm14
 5423                             	        psrld	$2, %xmm10
 5424                             	        psrld	$7, %xmm9
 5425                             	        pxor	%xmm10, %xmm14
 5426                             	        pxor	%xmm9, %xmm14
 5427                             	        pxor	%xmm13, %xmm14
 5428                             	        pxor	%xmm8, %xmm14
 5429                             	        pxor	%xmm14, %xmm1
 5430                             	        movdqa	%xmm1, 32(%rsp)
 5431                             	        # H ^ 4
 5432                             	        pshufd	$0x4e, %xmm0, %xmm9
 5433                             	        pshufd	$0x4e, %xmm0, %xmm10
 5434                             	        movdqa	%xmm0, %xmm11
 5435                             	        movdqa	%xmm0, %xmm8
 5436                             	        pclmulqdq	$0x11, %xmm0, %xmm11
 5437                             	        pclmulqdq	$0x00, %xmm0, %xmm8
 5438                             	        pxor	%xmm0, %xmm9
 5439                             	        pxor	%xmm0, %xmm10
 5440                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 5441                             	        pxor	%xmm8, %xmm9
 5442                             	        pxor	%xmm11, %xmm9
 5443                             	        movdqa	%xmm9, %xmm10
 5444                             	        movdqa	%xmm11, %xmm3
 5445                             	        pslldq	$8, %xmm10
 5446                             	        psrldq	$8, %xmm9
 5447                             	        pxor	%xmm10, %xmm8
 5448                             	        pxor	%xmm9, %xmm3
 5449                             	        movdqa	%xmm8, %xmm12
 5450                             	        movdqa	%xmm8, %xmm13
 5451                             	        movdqa	%xmm8, %xmm14
 5452                             	        pslld	$31, %xmm12
 5453                             	        pslld	$30, %xmm13
 5454                             	        pslld	$25, %xmm14
 5455                             	        pxor	%xmm13, %xmm12
 5456                             	        pxor	%xmm14, %xmm12
 5457                             	        movdqa	%xmm12, %xmm13
 5458                             	        psrldq	$4, %xmm13
 5459                             	        pslldq	$12, %xmm12
 5460                             	        pxor	%xmm12, %xmm8
 5461                             	        movdqa	%xmm8, %xmm14
 5462                             	        movdqa	%xmm8, %xmm10
 5463                             	        movdqa	%xmm8, %xmm9
 5464                             	        psrld	$0x01, %xmm14
 5465                             	        psrld	$2, %xmm10
 5466                             	        psrld	$7, %xmm9
 5467                             	        pxor	%xmm10, %xmm14
 5468                             	        pxor	%xmm9, %xmm14
 5469                             	        pxor	%xmm13, %xmm14
 5470                             	        pxor	%xmm8, %xmm14
 5471                             	        pxor	%xmm14, %xmm3
 5472                             	        movdqa	%xmm3, 48(%rsp)
 5473                             	        # H ^ 5
 5474                             	        pshufd	$0x4e, %xmm0, %xmm9
 5475                             	        pshufd	$0x4e, %xmm1, %xmm10
 5476                             	        movdqa	%xmm1, %xmm11
 5477                             	        movdqa	%xmm1, %xmm8
 5478                             	        pclmulqdq	$0x11, %xmm0, %xmm11
 5479                             	        pclmulqdq	$0x00, %xmm0, %xmm8
 5480                             	        pxor	%xmm0, %xmm9
 5481                             	        pxor	%xmm1, %xmm10
 5482                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 5483                             	        pxor	%xmm8, %xmm9
 5484                             	        pxor	%xmm11, %xmm9
 5485                             	        movdqa	%xmm9, %xmm10
 5486                             	        movdqa	%xmm11, %xmm7
 5487                             	        pslldq	$8, %xmm10
 5488                             	        psrldq	$8, %xmm9
 5489                             	        pxor	%xmm10, %xmm8
 5490                             	        pxor	%xmm9, %xmm7
 5491                             	        movdqa	%xmm8, %xmm12
 5492                             	        movdqa	%xmm8, %xmm13
 5493                             	        movdqa	%xmm8, %xmm14
 5494                             	        pslld	$31, %xmm12
 5495                             	        pslld	$30, %xmm13
 5496                             	        pslld	$25, %xmm14
 5497                             	        pxor	%xmm13, %xmm12
 5498                             	        pxor	%xmm14, %xmm12
 5499                             	        movdqa	%xmm12, %xmm13
 5500                             	        psrldq	$4, %xmm13
 5501                             	        pslldq	$12, %xmm12
 5502                             	        pxor	%xmm12, %xmm8
 5503                             	        movdqa	%xmm8, %xmm14
 5504                             	        movdqa	%xmm8, %xmm10
 5505                             	        movdqa	%xmm8, %xmm9
 5506                             	        psrld	$0x01, %xmm14
 5507                             	        psrld	$2, %xmm10
 5508                             	        psrld	$7, %xmm9
 5509                             	        pxor	%xmm10, %xmm14
 5510                             	        pxor	%xmm9, %xmm14
 5511                             	        pxor	%xmm13, %xmm14
 5512                             	        pxor	%xmm8, %xmm14
 5513                             	        pxor	%xmm14, %xmm7
 5514                             	        movdqa	%xmm7, 64(%rsp)
 5515                             	        # H ^ 6
 5516                             	        pshufd	$0x4e, %xmm1, %xmm9
 5517                             	        pshufd	$0x4e, %xmm1, %xmm10
 5518                             	        movdqa	%xmm1, %xmm11
 5519                             	        movdqa	%xmm1, %xmm8
 5520                             	        pclmulqdq	$0x11, %xmm1, %xmm11
 5521                             	        pclmulqdq	$0x00, %xmm1, %xmm8
 5522                             	        pxor	%xmm1, %xmm9
 5523                             	        pxor	%xmm1, %xmm10
 5524                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 5525                             	        pxor	%xmm8, %xmm9
 5526                             	        pxor	%xmm11, %xmm9
 5527                             	        movdqa	%xmm9, %xmm10
 5528                             	        movdqa	%xmm11, %xmm7
 5529                             	        pslldq	$8, %xmm10
 5530                             	        psrldq	$8, %xmm9
 5531                             	        pxor	%xmm10, %xmm8
 5532                             	        pxor	%xmm9, %xmm7
 5533                             	        movdqa	%xmm8, %xmm12
 5534                             	        movdqa	%xmm8, %xmm13
 5535                             	        movdqa	%xmm8, %xmm14
 5536                             	        pslld	$31, %xmm12
 5537                             	        pslld	$30, %xmm13
 5538                             	        pslld	$25, %xmm14
 5539                             	        pxor	%xmm13, %xmm12
 5540                             	        pxor	%xmm14, %xmm12
 5541                             	        movdqa	%xmm12, %xmm13
 5542                             	        psrldq	$4, %xmm13
 5543                             	        pslldq	$12, %xmm12
 5544                             	        pxor	%xmm12, %xmm8
 5545                             	        movdqa	%xmm8, %xmm14
 5546                             	        movdqa	%xmm8, %xmm10
 5547                             	        movdqa	%xmm8, %xmm9
 5548                             	        psrld	$0x01, %xmm14
 5549                             	        psrld	$2, %xmm10
 5550                             	        psrld	$7, %xmm9
 5551                             	        pxor	%xmm10, %xmm14
 5552                             	        pxor	%xmm9, %xmm14
 5553                             	        pxor	%xmm13, %xmm14
 5554                             	        pxor	%xmm8, %xmm14
 5555                             	        pxor	%xmm14, %xmm7
 5556                             	        movdqa	%xmm7, 80(%rsp)
 5557                             	        # H ^ 7
 5558                             	        pshufd	$0x4e, %xmm1, %xmm9
 5559                             	        pshufd	$0x4e, %xmm3, %xmm10
 5560                             	        movdqa	%xmm3, %xmm11
 5561                             	        movdqa	%xmm3, %xmm8
 5562                             	        pclmulqdq	$0x11, %xmm1, %xmm11
 5563                             	        pclmulqdq	$0x00, %xmm1, %xmm8
 5564                             	        pxor	%xmm1, %xmm9
 5565                             	        pxor	%xmm3, %xmm10
 5566                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 5567                             	        pxor	%xmm8, %xmm9
 5568                             	        pxor	%xmm11, %xmm9
 5569                             	        movdqa	%xmm9, %xmm10
 5570                             	        movdqa	%xmm11, %xmm7
 5571                             	        pslldq	$8, %xmm10
 5572                             	        psrldq	$8, %xmm9
 5573                             	        pxor	%xmm10, %xmm8
 5574                             	        pxor	%xmm9, %xmm7
 5575                             	        movdqa	%xmm8, %xmm12
 5576                             	        movdqa	%xmm8, %xmm13
 5577                             	        movdqa	%xmm8, %xmm14
 5578                             	        pslld	$31, %xmm12
 5579                             	        pslld	$30, %xmm13
 5580                             	        pslld	$25, %xmm14
 5581                             	        pxor	%xmm13, %xmm12
 5582                             	        pxor	%xmm14, %xmm12
 5583                             	        movdqa	%xmm12, %xmm13
 5584                             	        psrldq	$4, %xmm13
 5585                             	        pslldq	$12, %xmm12
 5586                             	        pxor	%xmm12, %xmm8
 5587                             	        movdqa	%xmm8, %xmm14
 5588                             	        movdqa	%xmm8, %xmm10
 5589                             	        movdqa	%xmm8, %xmm9
 5590                             	        psrld	$0x01, %xmm14
 5591                             	        psrld	$2, %xmm10
 5592                             	        psrld	$7, %xmm9
 5593                             	        pxor	%xmm10, %xmm14
 5594                             	        pxor	%xmm9, %xmm14
 5595                             	        pxor	%xmm13, %xmm14
 5596                             	        pxor	%xmm8, %xmm14
 5597                             	        pxor	%xmm14, %xmm7
 5598                             	        movdqa	%xmm7, 96(%rsp)
 5599                             	        # H ^ 8
 5600                             	        pshufd	$0x4e, %xmm3, %xmm9
 5601                             	        pshufd	$0x4e, %xmm3, %xmm10
 5602                             	        movdqa	%xmm3, %xmm11
 5603                             	        movdqa	%xmm3, %xmm8
 5604                             	        pclmulqdq	$0x11, %xmm3, %xmm11
 5605                             	        pclmulqdq	$0x00, %xmm3, %xmm8
 5606                             	        pxor	%xmm3, %xmm9
 5607                             	        pxor	%xmm3, %xmm10
 5608                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 5609                             	        pxor	%xmm8, %xmm9
 5610                             	        pxor	%xmm11, %xmm9
 5611                             	        movdqa	%xmm9, %xmm10
 5612                             	        movdqa	%xmm11, %xmm7
 5613                             	        pslldq	$8, %xmm10
 5614                             	        psrldq	$8, %xmm9
 5615                             	        pxor	%xmm10, %xmm8
 5616                             	        pxor	%xmm9, %xmm7
 5617                             	        movdqa	%xmm8, %xmm12
 5618                             	        movdqa	%xmm8, %xmm13
 5619                             	        movdqa	%xmm8, %xmm14
 5620                             	        pslld	$31, %xmm12
 5621                             	        pslld	$30, %xmm13
 5622                             	        pslld	$25, %xmm14
 5623                             	        pxor	%xmm13, %xmm12
 5624                             	        pxor	%xmm14, %xmm12
 5625                             	        movdqa	%xmm12, %xmm13
 5626                             	        psrldq	$4, %xmm13
 5627                             	        pslldq	$12, %xmm12
 5628                             	        pxor	%xmm12, %xmm8
 5629                             	        movdqa	%xmm8, %xmm14
 5630                             	        movdqa	%xmm8, %xmm10
 5631                             	        movdqa	%xmm8, %xmm9
 5632                             	        psrld	$0x01, %xmm14
 5633                             	        psrld	$2, %xmm10
 5634                             	        psrld	$7, %xmm9
 5635                             	        pxor	%xmm10, %xmm14
 5636                             	        pxor	%xmm9, %xmm14
 5637                             	        pxor	%xmm13, %xmm14
 5638                             	        pxor	%xmm8, %xmm14
 5639                             	        pxor	%xmm14, %xmm7
 5640                             	        movdqa	%xmm7, 112(%rsp)
 5641                             	L_AES_GCM_decrypt_update_aesni_ghash_128:
 5642                             	        leaq	(%r11,%r14,1), %rcx
 5643                             	        leaq	(%r10,%r14,1), %rdx
 5644                             	        movdqa	(%r12), %xmm8
 5645                             	        movdqa	L_aes_gcm_bswap_epi64(%rip), %xmm1
 5646                             	        movdqa	%xmm8, %xmm0
 5647                             	        pshufb	%xmm1, %xmm8
 5648                             	        movdqa	%xmm0, %xmm9
 5649                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 5650                             	        pshufb	%xmm1, %xmm9
 5651                             	        movdqa	%xmm0, %xmm10
 5652                             	        paddd	L_aes_gcm_two(%rip), %xmm10
 5653                             	        pshufb	%xmm1, %xmm10
 5654                             	        movdqa	%xmm0, %xmm11
 5655                             	        paddd	L_aes_gcm_three(%rip), %xmm11
 5656                             	        pshufb	%xmm1, %xmm11
 5657                             	        movdqa	%xmm0, %xmm12
 5658                             	        paddd	L_aes_gcm_four(%rip), %xmm12
 5659                             	        pshufb	%xmm1, %xmm12
 5660                             	        movdqa	%xmm0, %xmm13
 5661                             	        paddd	L_aes_gcm_five(%rip), %xmm13
 5662                             	        pshufb	%xmm1, %xmm13
 5663                             	        movdqa	%xmm0, %xmm14
 5664                             	        paddd	L_aes_gcm_six(%rip), %xmm14
 5665                             	        pshufb	%xmm1, %xmm14
 5666                             	        movdqa	%xmm0, %xmm15
 5667                             	        paddd	L_aes_gcm_seven(%rip), %xmm15
 5668                             	        pshufb	%xmm1, %xmm15
 5669                             	        paddd	L_aes_gcm_eight(%rip), %xmm0
 5670                             	        movdqa	(%rdi), %xmm7
 5671                             	        movdqa	%xmm0, (%r12)
 5672                             	        pxor	%xmm7, %xmm8
 5673                             	        pxor	%xmm7, %xmm9
 5674                             	        pxor	%xmm7, %xmm10
 5675                             	        pxor	%xmm7, %xmm11
 5676                             	        pxor	%xmm7, %xmm12
 5677                             	        pxor	%xmm7, %xmm13
 5678                             	        pxor	%xmm7, %xmm14
 5679                             	        pxor	%xmm7, %xmm15
 5680                             	        movdqa	112(%rsp), %xmm7
 5681                             	        movdqu	(%rcx), %xmm0
 5682                             	        aesenc	16(%rdi), %xmm8
 5683                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 5684                             	        pxor	%xmm2, %xmm0
 5685                             	        pshufd	$0x4e, %xmm7, %xmm1
 5686                             	        pshufd	$0x4e, %xmm0, %xmm5
 5687                             	        pxor	%xmm7, %xmm1
 5688                             	        pxor	%xmm0, %xmm5
 5689                             	        movdqa	%xmm0, %xmm3
 5690                             	        pclmulqdq	$0x11, %xmm7, %xmm3
 5691                             	        aesenc	16(%rdi), %xmm9
 5692                             	        aesenc	16(%rdi), %xmm10
 5693                             	        movdqa	%xmm0, %xmm2
 5694                             	        pclmulqdq	$0x00, %xmm7, %xmm2
 5695                             	        aesenc	16(%rdi), %xmm11
 5696                             	        aesenc	16(%rdi), %xmm12
 5697                             	        pclmulqdq	$0x00, %xmm5, %xmm1
 5698                             	        aesenc	16(%rdi), %xmm13
 5699                             	        aesenc	16(%rdi), %xmm14
 5700                             	        aesenc	16(%rdi), %xmm15
 5701                             	        pxor	%xmm2, %xmm1
 5702                             	        pxor	%xmm3, %xmm1
 5703                             	        movdqa	96(%rsp), %xmm7
 5704                             	        movdqu	16(%rcx), %xmm0
 5705                             	        pshufd	$0x4e, %xmm7, %xmm4
 5706                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 5707                             	        aesenc	32(%rdi), %xmm8
 5708                             	        pxor	%xmm7, %xmm4
 5709                             	        pshufd	$0x4e, %xmm0, %xmm5
 5710                             	        pxor	%xmm0, %xmm5
 5711                             	        movdqa	%xmm0, %xmm6
 5712                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 5713                             	        aesenc	32(%rdi), %xmm9
 5714                             	        aesenc	32(%rdi), %xmm10
 5715                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 5716                             	        aesenc	32(%rdi), %xmm11
 5717                             	        aesenc	32(%rdi), %xmm12
 5718                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 5719                             	        aesenc	32(%rdi), %xmm13
 5720                             	        aesenc	32(%rdi), %xmm14
 5721                             	        aesenc	32(%rdi), %xmm15
 5722                             	        pxor	%xmm7, %xmm1
 5723                             	        pxor	%xmm7, %xmm2
 5724                             	        pxor	%xmm6, %xmm1
 5725                             	        pxor	%xmm6, %xmm3
 5726                             	        pxor	%xmm4, %xmm1
 5727                             	        movdqa	80(%rsp), %xmm7
 5728                             	        movdqu	32(%rcx), %xmm0
 5729                             	        pshufd	$0x4e, %xmm7, %xmm4
 5730                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 5731                             	        aesenc	48(%rdi), %xmm8
 5732                             	        pxor	%xmm7, %xmm4
 5733                             	        pshufd	$0x4e, %xmm0, %xmm5
 5734                             	        pxor	%xmm0, %xmm5
 5735                             	        movdqa	%xmm0, %xmm6
 5736                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 5737                             	        aesenc	48(%rdi), %xmm9
 5738                             	        aesenc	48(%rdi), %xmm10
 5739                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 5740                             	        aesenc	48(%rdi), %xmm11
 5741                             	        aesenc	48(%rdi), %xmm12
 5742                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 5743                             	        aesenc	48(%rdi), %xmm13
 5744                             	        aesenc	48(%rdi), %xmm14
 5745                             	        aesenc	48(%rdi), %xmm15
 5746                             	        pxor	%xmm7, %xmm1
 5747                             	        pxor	%xmm7, %xmm2
 5748                             	        pxor	%xmm6, %xmm1
 5749                             	        pxor	%xmm6, %xmm3
 5750                             	        pxor	%xmm4, %xmm1
 5751                             	        movdqa	64(%rsp), %xmm7
 5752                             	        movdqu	48(%rcx), %xmm0
 5753                             	        pshufd	$0x4e, %xmm7, %xmm4
 5754                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 5755                             	        aesenc	64(%rdi), %xmm8
 5756                             	        pxor	%xmm7, %xmm4
 5757                             	        pshufd	$0x4e, %xmm0, %xmm5
 5758                             	        pxor	%xmm0, %xmm5
 5759                             	        movdqa	%xmm0, %xmm6
 5760                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 5761                             	        aesenc	64(%rdi), %xmm9
 5762                             	        aesenc	64(%rdi), %xmm10
 5763                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 5764                             	        aesenc	64(%rdi), %xmm11
 5765                             	        aesenc	64(%rdi), %xmm12
 5766                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 5767                             	        aesenc	64(%rdi), %xmm13
 5768                             	        aesenc	64(%rdi), %xmm14
 5769                             	        aesenc	64(%rdi), %xmm15
 5770                             	        pxor	%xmm7, %xmm1
 5771                             	        pxor	%xmm7, %xmm2
 5772                             	        pxor	%xmm6, %xmm1
 5773                             	        pxor	%xmm6, %xmm3
 5774                             	        pxor	%xmm4, %xmm1
 5775                             	        movdqa	48(%rsp), %xmm7
 5776                             	        movdqu	64(%rcx), %xmm0
 5777                             	        pshufd	$0x4e, %xmm7, %xmm4
 5778                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 5779                             	        aesenc	80(%rdi), %xmm8
 5780                             	        pxor	%xmm7, %xmm4
 5781                             	        pshufd	$0x4e, %xmm0, %xmm5
 5782                             	        pxor	%xmm0, %xmm5
 5783                             	        movdqa	%xmm0, %xmm6
 5784                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 5785                             	        aesenc	80(%rdi), %xmm9
 5786                             	        aesenc	80(%rdi), %xmm10
 5787                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 5788                             	        aesenc	80(%rdi), %xmm11
 5789                             	        aesenc	80(%rdi), %xmm12
 5790                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 5791                             	        aesenc	80(%rdi), %xmm13
 5792                             	        aesenc	80(%rdi), %xmm14
 5793                             	        aesenc	80(%rdi), %xmm15
 5794                             	        pxor	%xmm7, %xmm1
 5795                             	        pxor	%xmm7, %xmm2
 5796                             	        pxor	%xmm6, %xmm1
 5797                             	        pxor	%xmm6, %xmm3
 5798                             	        pxor	%xmm4, %xmm1
 5799                             	        movdqa	32(%rsp), %xmm7
 5800                             	        movdqu	80(%rcx), %xmm0
 5801                             	        pshufd	$0x4e, %xmm7, %xmm4
 5802                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 5803                             	        aesenc	96(%rdi), %xmm8
 5804                             	        pxor	%xmm7, %xmm4
 5805                             	        pshufd	$0x4e, %xmm0, %xmm5
 5806                             	        pxor	%xmm0, %xmm5
 5807                             	        movdqa	%xmm0, %xmm6
 5808                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 5809                             	        aesenc	96(%rdi), %xmm9
 5810                             	        aesenc	96(%rdi), %xmm10
 5811                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 5812                             	        aesenc	96(%rdi), %xmm11
 5813                             	        aesenc	96(%rdi), %xmm12
 5814                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 5815                             	        aesenc	96(%rdi), %xmm13
 5816                             	        aesenc	96(%rdi), %xmm14
 5817                             	        aesenc	96(%rdi), %xmm15
 5818                             	        pxor	%xmm7, %xmm1
 5819                             	        pxor	%xmm7, %xmm2
 5820                             	        pxor	%xmm6, %xmm1
 5821                             	        pxor	%xmm6, %xmm3
 5822                             	        pxor	%xmm4, %xmm1
 5823                             	        movdqa	16(%rsp), %xmm7
 5824                             	        movdqu	96(%rcx), %xmm0
 5825                             	        pshufd	$0x4e, %xmm7, %xmm4
 5826                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 5827                             	        aesenc	112(%rdi), %xmm8
 5828                             	        pxor	%xmm7, %xmm4
 5829                             	        pshufd	$0x4e, %xmm0, %xmm5
 5830                             	        pxor	%xmm0, %xmm5
 5831                             	        movdqa	%xmm0, %xmm6
 5832                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 5833                             	        aesenc	112(%rdi), %xmm9
 5834                             	        aesenc	112(%rdi), %xmm10
 5835                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 5836                             	        aesenc	112(%rdi), %xmm11
 5837                             	        aesenc	112(%rdi), %xmm12
 5838                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 5839                             	        aesenc	112(%rdi), %xmm13
 5840                             	        aesenc	112(%rdi), %xmm14
 5841                             	        aesenc	112(%rdi), %xmm15
 5842                             	        pxor	%xmm7, %xmm1
 5843                             	        pxor	%xmm7, %xmm2
 5844                             	        pxor	%xmm6, %xmm1
 5845                             	        pxor	%xmm6, %xmm3
 5846                             	        pxor	%xmm4, %xmm1
 5847                             	        movdqa	(%rsp), %xmm7
 5848                             	        movdqu	112(%rcx), %xmm0
 5849                             	        pshufd	$0x4e, %xmm7, %xmm4
 5850                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm0
 5851                             	        aesenc	128(%rdi), %xmm8
 5852                             	        pxor	%xmm7, %xmm4
 5853                             	        pshufd	$0x4e, %xmm0, %xmm5
 5854                             	        pxor	%xmm0, %xmm5
 5855                             	        movdqa	%xmm0, %xmm6
 5856                             	        pclmulqdq	$0x11, %xmm7, %xmm6
 5857                             	        aesenc	128(%rdi), %xmm9
 5858                             	        aesenc	128(%rdi), %xmm10
 5859                             	        pclmulqdq	$0x00, %xmm0, %xmm7
 5860                             	        aesenc	128(%rdi), %xmm11
 5861                             	        aesenc	128(%rdi), %xmm12
 5862                             	        pclmulqdq	$0x00, %xmm5, %xmm4
 5863                             	        aesenc	128(%rdi), %xmm13
 5864                             	        aesenc	128(%rdi), %xmm14
 5865                             	        aesenc	128(%rdi), %xmm15
 5866                             	        pxor	%xmm7, %xmm1
 5867                             	        pxor	%xmm7, %xmm2
 5868                             	        pxor	%xmm6, %xmm1
 5869                             	        pxor	%xmm6, %xmm3
 5870                             	        pxor	%xmm4, %xmm1
 5871                             	        movdqa	%xmm1, %xmm5
 5872                             	        psrldq	$8, %xmm1
 5873                             	        pslldq	$8, %xmm5
 5874                             	        aesenc	144(%rdi), %xmm8
 5875                             	        pxor	%xmm5, %xmm2
 5876                             	        pxor	%xmm1, %xmm3
 5877                             	        movdqa	%xmm2, %xmm7
 5878                             	        movdqa	%xmm2, %xmm4
 5879                             	        movdqa	%xmm2, %xmm5
 5880                             	        aesenc	144(%rdi), %xmm9
 5881                             	        pslld	$31, %xmm7
 5882                             	        pslld	$30, %xmm4
 5883                             	        pslld	$25, %xmm5
 5884                             	        aesenc	144(%rdi), %xmm10
 5885                             	        pxor	%xmm4, %xmm7
 5886                             	        pxor	%xmm5, %xmm7
 5887                             	        aesenc	144(%rdi), %xmm11
 5888                             	        movdqa	%xmm7, %xmm4
 5889                             	        pslldq	$12, %xmm7
 5890                             	        psrldq	$4, %xmm4
 5891                             	        aesenc	144(%rdi), %xmm12
 5892                             	        pxor	%xmm7, %xmm2
 5893                             	        movdqa	%xmm2, %xmm5
 5894                             	        movdqa	%xmm2, %xmm1
 5895                             	        movdqa	%xmm2, %xmm0
 5896                             	        aesenc	144(%rdi), %xmm13
 5897                             	        psrld	$0x01, %xmm5
 5898                             	        psrld	$2, %xmm1
 5899                             	        psrld	$7, %xmm0
 5900                             	        aesenc	144(%rdi), %xmm14
 5901                             	        pxor	%xmm1, %xmm5
 5902                             	        pxor	%xmm0, %xmm5
 5903                             	        aesenc	144(%rdi), %xmm15
 5904                             	        pxor	%xmm4, %xmm5
 5905                             	        pxor	%xmm5, %xmm2
 5906                             	        pxor	%xmm3, %xmm2
 5907                             	        cmpl	$11, %esi
 5908                             	        movdqa	160(%rdi), %xmm7
 5909                             	        jl	L_AES_GCM_decrypt_update_aesni_aesenc_128_ghash_avx_done
 5910                             	        aesenc	%xmm7, %xmm8
 5911                             	        aesenc	%xmm7, %xmm9
 5912                             	        aesenc	%xmm7, %xmm10
 5913                             	        aesenc	%xmm7, %xmm11
 5914                             	        aesenc	%xmm7, %xmm12
 5915                             	        aesenc	%xmm7, %xmm13
 5916                             	        aesenc	%xmm7, %xmm14
 5917                             	        aesenc	%xmm7, %xmm15
 5918                             	        movdqa	176(%rdi), %xmm7
 5919                             	        aesenc	%xmm7, %xmm8
 5920                             	        aesenc	%xmm7, %xmm9
 5921                             	        aesenc	%xmm7, %xmm10
 5922                             	        aesenc	%xmm7, %xmm11
 5923                             	        aesenc	%xmm7, %xmm12
 5924                             	        aesenc	%xmm7, %xmm13
 5925                             	        aesenc	%xmm7, %xmm14
 5926                             	        aesenc	%xmm7, %xmm15
 5927                             	        cmpl	$13, %esi
 5928                             	        movdqa	192(%rdi), %xmm7
 5929                             	        jl	L_AES_GCM_decrypt_update_aesni_aesenc_128_ghash_avx_done
 5930                             	        aesenc	%xmm7, %xmm8
 5931                             	        aesenc	%xmm7, %xmm9
 5932                             	        aesenc	%xmm7, %xmm10
 5933                             	        aesenc	%xmm7, %xmm11
 5934                             	        aesenc	%xmm7, %xmm12
 5935                             	        aesenc	%xmm7, %xmm13
 5936                             	        aesenc	%xmm7, %xmm14
 5937                             	        aesenc	%xmm7, %xmm15
 5938                             	        movdqa	208(%rdi), %xmm7
 5939                             	        aesenc	%xmm7, %xmm8
 5940                             	        aesenc	%xmm7, %xmm9
 5941                             	        aesenc	%xmm7, %xmm10
 5942                             	        aesenc	%xmm7, %xmm11
 5943                             	        aesenc	%xmm7, %xmm12
 5944                             	        aesenc	%xmm7, %xmm13
 5945                             	        aesenc	%xmm7, %xmm14
 5946                             	        aesenc	%xmm7, %xmm15
 5947                             	        movdqa	224(%rdi), %xmm7
 5948                             	L_AES_GCM_decrypt_update_aesni_aesenc_128_ghash_avx_done:
 5949                             	        aesenclast	%xmm7, %xmm8
 5950                             	        aesenclast	%xmm7, %xmm9
 5951                             	        movdqu	(%rcx), %xmm0
 5952                             	        movdqu	16(%rcx), %xmm1
 5953                             	        pxor	%xmm0, %xmm8
 5954                             	        pxor	%xmm1, %xmm9
 5955                             	        movdqu	%xmm8, (%rdx)
 5956                             	        movdqu	%xmm9, 16(%rdx)
 5957                             	        aesenclast	%xmm7, %xmm10
 5958                             	        aesenclast	%xmm7, %xmm11
 5959                             	        movdqu	32(%rcx), %xmm0
 5960                             	        movdqu	48(%rcx), %xmm1
 5961                             	        pxor	%xmm0, %xmm10
 5962                             	        pxor	%xmm1, %xmm11
 5963                             	        movdqu	%xmm10, 32(%rdx)
 5964                             	        movdqu	%xmm11, 48(%rdx)
 5965                             	        aesenclast	%xmm7, %xmm12
 5966                             	        aesenclast	%xmm7, %xmm13
 5967                             	        movdqu	64(%rcx), %xmm0
 5968                             	        movdqu	80(%rcx), %xmm1
 5969                             	        pxor	%xmm0, %xmm12
 5970                             	        pxor	%xmm1, %xmm13
 5971                             	        movdqu	%xmm12, 64(%rdx)
 5972                             	        movdqu	%xmm13, 80(%rdx)
 5973                             	        aesenclast	%xmm7, %xmm14
 5974                             	        aesenclast	%xmm7, %xmm15
 5975                             	        movdqu	96(%rcx), %xmm0
 5976                             	        movdqu	112(%rcx), %xmm1
 5977                             	        pxor	%xmm0, %xmm14
 5978                             	        pxor	%xmm1, %xmm15
 5979                             	        movdqu	%xmm14, 96(%rdx)
 5980                             	        movdqu	%xmm15, 112(%rdx)
 5981                             	        addl	$0x80, %r14d
 5982                             	        cmpl	%r13d, %r14d
 5983                             	        jl	L_AES_GCM_decrypt_update_aesni_ghash_128
 5984                             	        movdqa	%xmm2, %xmm6
 5985                             	        movdqa	(%rsp), %xmm5
 5986                             	L_AES_GCM_decrypt_update_aesni_done_128:
 5987                             	        movl	%r8d, %edx
 5988                             	        cmpl	%edx, %r14d
 5989                             	        jge	L_AES_GCM_decrypt_update_aesni_done_dec
 5990                             	        movl	%r8d, %r13d
 5991                             	        andl	$0xfffffff0, %r13d
 5992                             	        cmpl	%r13d, %r14d
 5993                             	        jge	L_AES_GCM_decrypt_update_aesni_last_block_done
 5994                             	L_AES_GCM_decrypt_update_aesni_last_block_start:
 5995                             	        leaq	(%r11,%r14,1), %rcx
 5996                             	        leaq	(%r10,%r14,1), %rdx
 5997                             	        movdqu	(%rcx), %xmm1
 5998                             	        movdqa	%xmm5, %xmm0
 5999                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm1
 6000                             	        pxor	%xmm6, %xmm1
 6001                             	        movdqa	(%r12), %xmm8
 6002                             	        movdqa	%xmm8, %xmm9
 6003                             	        pshufb	L_aes_gcm_bswap_epi64(%rip), %xmm8
 6004                             	        paddd	L_aes_gcm_one(%rip), %xmm9
 6005                             	        pxor	(%rdi), %xmm8
 6006                             	        movdqa	%xmm9, (%r12)
 6007                             	        movdqa	%xmm1, %xmm10
 6008                             	        pclmulqdq	$16, %xmm0, %xmm10
 6009                             	        aesenc	16(%rdi), %xmm8
 6010                             	        aesenc	32(%rdi), %xmm8
 6011                             	        movdqa	%xmm1, %xmm11
 6012                             	        pclmulqdq	$0x01, %xmm0, %xmm11
 6013                             	        aesenc	48(%rdi), %xmm8
 6014                             	        aesenc	64(%rdi), %xmm8
 6015                             	        movdqa	%xmm1, %xmm12
 6016                             	        pclmulqdq	$0x00, %xmm0, %xmm12
 6017                             	        aesenc	80(%rdi), %xmm8
 6018                             	        movdqa	%xmm1, %xmm1
 6019                             	        pclmulqdq	$0x11, %xmm0, %xmm1
 6020                             	        aesenc	96(%rdi), %xmm8
 6021                             	        pxor	%xmm11, %xmm10
 6022                             	        movdqa	%xmm10, %xmm2
 6023                             	        psrldq	$8, %xmm10
 6024                             	        pslldq	$8, %xmm2
 6025                             	        aesenc	112(%rdi), %xmm8
 6026                             	        movdqa	%xmm1, %xmm3
 6027                             	        pxor	%xmm12, %xmm2
 6028                             	        pxor	%xmm10, %xmm3
 6029                             	        movdqa	L_aes_gcm_mod2_128(%rip), %xmm0
 6030                             	        movdqa	%xmm2, %xmm11
 6031                             	        pclmulqdq	$16, %xmm0, %xmm11
 6032                             	        aesenc	128(%rdi), %xmm8
 6033                             	        pshufd	$0x4e, %xmm2, %xmm10
 6034                             	        pxor	%xmm11, %xmm10
 6035                             	        movdqa	%xmm10, %xmm11
 6036                             	        pclmulqdq	$16, %xmm0, %xmm11
 6037                             	        aesenc	144(%rdi), %xmm8
 6038                             	        pshufd	$0x4e, %xmm10, %xmm6
 6039                             	        pxor	%xmm11, %xmm6
 6040                             	        pxor	%xmm3, %xmm6
 6041                             	        cmpl	$11, %esi
 6042                             	        movdqa	160(%rdi), %xmm9
 6043                             	        jl	L_AES_GCM_decrypt_update_aesni_aesenc_gfmul_last
 6044                             	        aesenc	%xmm9, %xmm8
 6045                             	        aesenc	176(%rdi), %xmm8
 6046                             	        cmpl	$13, %esi
 6047                             	        movdqa	192(%rdi), %xmm9
 6048                             	        jl	L_AES_GCM_decrypt_update_aesni_aesenc_gfmul_last
 6049                             	        aesenc	%xmm9, %xmm8
 6050                             	        aesenc	208(%rdi), %xmm8
 6051                             	        movdqa	224(%rdi), %xmm9
 6052                             	L_AES_GCM_decrypt_update_aesni_aesenc_gfmul_last:
 6053                             	        aesenclast	%xmm9, %xmm8
 6054                             	        movdqu	(%rcx), %xmm9
 6055                             	        pxor	%xmm9, %xmm8
 6056                             	        movdqu	%xmm8, (%rdx)
 6057                             	        addl	$16, %r14d
 6058                             	        cmpl	%r13d, %r14d
 6059                             	        jl	L_AES_GCM_decrypt_update_aesni_last_block_start
 6060                             	L_AES_GCM_decrypt_update_aesni_last_block_done:
 6061                             	L_AES_GCM_decrypt_update_aesni_done_dec:
 6062                             	        movdqa	%xmm6, (%r9)
 6063                             	        addq	$0xa8, %rsp
 6064                             	        popq	%r15
 6065                             	        popq	%r14
 6066                             	        popq	%r12
 6067                             	        popq	%r13
 6068                             	        repz retq
 6069                             	#ifndef __APPLE__
 6071                             	#endif /* __APPLE__ */
 6072                             	#ifndef __APPLE__
 6073                             	.text
 6074                             	.globl	AES_GCM_decrypt_final_aesni
 6076                             	.align	16
 6077                             	AES_GCM_decrypt_final_aesni:
 6078                             	#else
 6079                             	.section	__TEXT,__text
 6080                             	.globl	_AES_GCM_decrypt_final_aesni
 6081                             	.p2align	4
 6082                             	_AES_GCM_decrypt_final_aesni:
 6083                             	#endif /* __APPLE__ */
 6084                             	        pushq	%r13
 6085                             	        pushq	%rbp
 6086                             	        pushq	%r12
 6087                             	        movq	%rdx, %rax
 6088                             	        movl	%ecx, %r10d
 6089                             	        movl	%r8d, %r11d
 6090                             	        movq	32(%rsp), %r8
 6091                             	        movq	40(%rsp), %rbp
 6092                             	        subq	$16, %rsp
 6093                             	        movdqa	(%rdi), %xmm6
 6094                             	        movdqa	(%r9), %xmm5
 6095                             	        movdqa	(%r8), %xmm15
 6096                             	        movdqa	%xmm5, %xmm9
 6097                             	        movdqa	%xmm5, %xmm8
 6098                             	        psrlq	$63, %xmm9
 6099                             	        psllq	$0x01, %xmm8
 6100                             	        pslldq	$8, %xmm9
 6101                             	        por	%xmm9, %xmm8
 6102                             	        pshufd	$0xff, %xmm5, %xmm5
 6103                             	        psrad	$31, %xmm5
 6104                             	        pand	L_aes_gcm_mod2_128(%rip), %xmm5
 6105                             	        pxor	%xmm8, %xmm5
 6106                             	        movl	%r10d, %edx
 6107                             	        movl	%r11d, %ecx
 6108                             	        shlq	$3, %rdx
 6109                             	        shlq	$3, %rcx
 6110                             	        pinsrq	$0x00, %rdx, %xmm0
 6111                             	        pinsrq	$0x01, %rcx, %xmm0
 6112                             	        pxor	%xmm0, %xmm6
 6113                             	        pshufd	$0x4e, %xmm5, %xmm9
 6114                             	        pshufd	$0x4e, %xmm6, %xmm10
 6115                             	        movdqa	%xmm6, %xmm11
 6116                             	        movdqa	%xmm6, %xmm8
 6117                             	        pclmulqdq	$0x11, %xmm5, %xmm11
 6118                             	        pclmulqdq	$0x00, %xmm5, %xmm8
 6119                             	        pxor	%xmm5, %xmm9
 6120                             	        pxor	%xmm6, %xmm10
 6121                             	        pclmulqdq	$0x00, %xmm10, %xmm9
 6122                             	        pxor	%xmm8, %xmm9
 6123                             	        pxor	%xmm11, %xmm9
 6124                             	        movdqa	%xmm9, %xmm10
 6125                             	        movdqa	%xmm11, %xmm6
 6126                             	        pslldq	$8, %xmm10
 6127                             	        psrldq	$8, %xmm9
 6128                             	        pxor	%xmm10, %xmm8
 6129                             	        pxor	%xmm9, %xmm6
 6130                             	        movdqa	%xmm8, %xmm12
 6131                             	        movdqa	%xmm8, %xmm13
 6132                             	        movdqa	%xmm8, %xmm14
 6133                             	        pslld	$31, %xmm12
 6134                             	        pslld	$30, %xmm13
 6135                             	        pslld	$25, %xmm14
 6136                             	        pxor	%xmm13, %xmm12
 6137                             	        pxor	%xmm14, %xmm12
 6138                             	        movdqa	%xmm12, %xmm13
 6139                             	        psrldq	$4, %xmm13
 6140                             	        pslldq	$12, %xmm12
 6141                             	        pxor	%xmm12, %xmm8
 6142                             	        movdqa	%xmm8, %xmm14
 6143                             	        movdqa	%xmm8, %xmm10
 6144                             	        movdqa	%xmm8, %xmm9
 6145                             	        psrld	$0x01, %xmm14
 6146                             	        psrld	$2, %xmm10
 6147                             	        psrld	$7, %xmm9
 6148                             	        pxor	%xmm10, %xmm14
 6149                             	        pxor	%xmm9, %xmm14
 6150                             	        pxor	%xmm13, %xmm14
 6151                             	        pxor	%xmm8, %xmm14
 6152                             	        pxor	%xmm14, %xmm6
 6153                             	        pshufb	L_aes_gcm_bswap_mask(%rip), %xmm6
 6154                             	        movdqa	%xmm15, %xmm0
 6155                             	        pxor	%xmm6, %xmm0
 6156                             	        cmpl	$16, %eax
 6157                             	        je	L_AES_GCM_decrypt_final_aesni_cmp_tag_16
 6158                             	        subq	$16, %rsp
 6159                             	        xorq	%rcx, %rcx
 6160                             	        xorq	%r12, %r12
 6161                             	        movdqa	%xmm0, (%rsp)
 6162                             	L_AES_GCM_decrypt_final_aesni_cmp_tag_loop:
 6163                             	        movzbl	(%rsp,%rcx,1), %r13d
 6164                             	        xorb	(%rsi,%rcx,1), %r13b
 6165                             	        orb	%r13b, %r12b
 6166                             	        incl	%ecx
 6167                             	        cmpl	%eax, %ecx
 6168                             	        jne	L_AES_GCM_decrypt_final_aesni_cmp_tag_loop
 6169                             	        cmpb	$0x00, %r12b
 6170                             	        sete	%r12b
 6171                             	        addq	$16, %rsp
 6172                             	        xorq	%rcx, %rcx
 6173                             	        jmp	L_AES_GCM_decrypt_final_aesni_cmp_tag_done
 6174                             	L_AES_GCM_decrypt_final_aesni_cmp_tag_16:
 6175                             	        movdqu	(%rsi), %xmm1
 6176                             	        pcmpeqb	%xmm1, %xmm0
 6177                             	        pmovmskb	%xmm0, %rdx
 6178                             	        # %%edx == 0xFFFF then return 1 else => return 0
 6179                             	        xorl	%r12d, %r12d
 6180                             	        cmpl	$0xffff, %edx
 6181                             	        sete	%r12b
 6182                             	L_AES_GCM_decrypt_final_aesni_cmp_tag_done:
 6183                             	        movl	%r12d, (%rbp)
 6184                             	        addq	$16, %rsp
 6185                             	        popq	%r12
 6186                             	        popq	%rbp
 6187                             	        popq	%r13
 6188                             	        repz retq
 6189                             	#ifndef __APPLE__
 6191                             	#endif /* __APPLE__ */
 6192                             	#endif /* WOLFSSL_AESGCM_STREAM */
 6193                             	#ifdef HAVE_INTEL_AVX1
 6194                             	#ifndef __APPLE__
 6195                             	.data
 6196                             	#else
 6197                             	.section	__DATA,__data
 6198                             	#endif /* __APPLE__ */
 6199                             	#ifndef __APPLE__
 6200                             	.align	16
 6201                             	#else
 6202                             	.p2align	4
 6203                             	#endif /* __APPLE__ */
 6204                             	L_avx1_aes_gcm_one:
 6205 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x1
 6205      01 00 00 00 00 00 00 00 
 6206                             	#ifndef __APPLE__
 6207                             	.data
 6208                             	#else
 6209                             	.section	__DATA,__data
 6210                             	#endif /* __APPLE__ */
 6211                             	#ifndef __APPLE__
 6212                             	.align	16
 6213                             	#else
 6214                             	.p2align	4
 6215                             	#endif /* __APPLE__ */
 6216                             	L_avx1_aes_gcm_two:
 6217 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x2
 6217      02 00 00 00 00 00 00 00 
 6218                             	#ifndef __APPLE__
 6219                             	.data
 6220                             	#else
 6221                             	.section	__DATA,__data
 6222                             	#endif /* __APPLE__ */
 6223                             	#ifndef __APPLE__
 6224                             	.align	16
 6225                             	#else
 6226                             	.p2align	4
 6227                             	#endif /* __APPLE__ */
 6228                             	L_avx1_aes_gcm_three:
 6229 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x3
 6229      03 00 00 00 00 00 00 00 
 6230                             	#ifndef __APPLE__
 6231                             	.data
 6232                             	#else
 6233                             	.section	__DATA,__data
 6234                             	#endif /* __APPLE__ */
 6235                             	#ifndef __APPLE__
 6236                             	.align	16
 6237                             	#else
 6238                             	.p2align	4
 6239                             	#endif /* __APPLE__ */
 6240                             	L_avx1_aes_gcm_four:
 6241 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x4
 6241      04 00 00 00 00 00 00 00 
 6242                             	#ifndef __APPLE__
 6243                             	.data
 6244                             	#else
 6245                             	.section	__DATA,__data
 6246                             	#endif /* __APPLE__ */
 6247                             	#ifndef __APPLE__
 6248                             	.align	16
 6249                             	#else
 6250                             	.p2align	4
 6251                             	#endif /* __APPLE__ */
 6252                             	L_avx1_aes_gcm_five:
 6253 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x5
 6253      05 00 00 00 00 00 00 00 
 6254                             	#ifndef __APPLE__
 6255                             	.data
 6256                             	#else
 6257                             	.section	__DATA,__data
 6258                             	#endif /* __APPLE__ */
 6259                             	#ifndef __APPLE__
 6260                             	.align	16
 6261                             	#else
 6262                             	.p2align	4
 6263                             	#endif /* __APPLE__ */
 6264                             	L_avx1_aes_gcm_six:
 6265 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x6
 6265      06 00 00 00 00 00 00 00 
 6266                             	#ifndef __APPLE__
 6267                             	.data
 6268                             	#else
 6269                             	.section	__DATA,__data
 6270                             	#endif /* __APPLE__ */
 6271                             	#ifndef __APPLE__
 6272                             	.align	16
 6273                             	#else
 6274                             	.p2align	4
 6275                             	#endif /* __APPLE__ */
 6276                             	L_avx1_aes_gcm_seven:
 6277 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x7
 6277      07 00 00 00 00 00 00 00 
 6278                             	#ifndef __APPLE__
 6279                             	.data
 6280                             	#else
 6281                             	.section	__DATA,__data
 6282                             	#endif /* __APPLE__ */
 6283                             	#ifndef __APPLE__
 6284                             	.align	16
 6285                             	#else
 6286                             	.p2align	4
 6287                             	#endif /* __APPLE__ */
 6288                             	L_avx1_aes_gcm_eight:
 6289 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x8
 6289      08 00 00 00 00 00 00 00 
 6290                             	#ifndef __APPLE__
 6291                             	.data
 6292                             	#else
 6293                             	.section	__DATA,__data
 6294                             	#endif /* __APPLE__ */
 6295                             	#ifndef __APPLE__
 6296                             	.align	16
 6297                             	#else
 6298                             	.p2align	4
 6299                             	#endif /* __APPLE__ */
 6300                             	L_avx1_aes_gcm_bswap_epi64:
 6301 ???? 07 06 05 04 03 02 01 00 	.quad	0x1020304050607, 0x8090a0b0c0d0e0f
 6301      0F 0E 0D 0C 0B 0A 09 08 
 6302                             	#ifndef __APPLE__
 6303                             	.data
 6304                             	#else
 6305                             	.section	__DATA,__data
 6306                             	#endif /* __APPLE__ */
 6307                             	#ifndef __APPLE__
 6308                             	.align	16
 6309                             	#else
 6310                             	.p2align	4
 6311                             	#endif /* __APPLE__ */
 6312                             	L_avx1_aes_gcm_bswap_mask:
 6313 ???? 0F 0E 0D 0C 0B 0A 09 08 	.quad	0x8090a0b0c0d0e0f, 0x1020304050607
 6313      07 06 05 04 03 02 01 00 
 6314                             	#ifndef __APPLE__
 6315                             	.data
 6316                             	#else
 6317                             	.section	__DATA,__data
 6318                             	#endif /* __APPLE__ */
 6319                             	#ifndef __APPLE__
 6320                             	.align	16
 6321                             	#else
 6322                             	.p2align	4
 6323                             	#endif /* __APPLE__ */
 6324                             	L_avx1_aes_gcm_mod2_128:
 6325 ???? 01 00 00 00 00 00 00 00 	.quad	0x1, 0xc200000000000000
 6325      00 00 00 00 00 00 00 C2 
 6326                             	#ifndef __APPLE__
 6327                             	.text
 6328                             	.globl	AES_GCM_encrypt_avx1
 6330                             	.align	16
 6331                             	AES_GCM_encrypt_avx1:
 6332                             	#else
 6333                             	.section	__TEXT,__text
 6334                             	.globl	_AES_GCM_encrypt_avx1
 6335                             	.p2align	4
 6336                             	_AES_GCM_encrypt_avx1:
 6337                             	#endif /* __APPLE__ */
 6338                             	        pushq	%r13
 6339                             	        pushq	%r12
 6340                             	        pushq	%rbx
 6341                             	        pushq	%r14
 6342                             	        pushq	%r15
 6343                             	        movq	%rdx, %r12
 6344                             	        movq	%rcx, %rax
 6345                             	        movl	48(%rsp), %r11d
 6346                             	        movl	56(%rsp), %ebx
 6347                             	        movl	64(%rsp), %r14d
 6348                             	        movq	72(%rsp), %r15
 6349                             	        movl	80(%rsp), %r10d
 6350                             	        subq	$0xa0, %rsp
 6351                             	        vpxor	%xmm4, %xmm4, %xmm4
 6352                             	        vpxor	%xmm6, %xmm6, %xmm6
 6353                             	        movl	%ebx, %edx
 6354                             	        cmpl	$12, %edx
 6355                             	        jne	L_AES_GCM_encrypt_avx1_iv_not_12
 6356                             	        # # Calculate values when IV is 12 bytes
 6357                             	        # Set counter based on IV
 6358                             	        movl	$0x1000000, %ecx
 6359                             	        vpinsrq	$0x00, (%rax), %xmm4, %xmm4
 6360                             	        vpinsrd	$2, 8(%rax), %xmm4, %xmm4
 6361                             	        vpinsrd	$3, %ecx, %xmm4, %xmm4
 6362                             	        # H = Encrypt X(=0) and T = Encrypt counter
 6363                             	        vmovdqa	(%r15), %xmm5
 6364                             	        vpxor	%xmm5, %xmm4, %xmm1
 6365                             	        vmovdqa	16(%r15), %xmm7
 6366                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6367                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6368                             	        vmovdqa	32(%r15), %xmm7
 6369                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6370                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6371                             	        vmovdqa	48(%r15), %xmm7
 6372                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6373                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6374                             	        vmovdqa	64(%r15), %xmm7
 6375                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6376                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6377                             	        vmovdqa	80(%r15), %xmm7
 6378                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6379                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6380                             	        vmovdqa	96(%r15), %xmm7
 6381                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6382                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6383                             	        vmovdqa	112(%r15), %xmm7
 6384                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6385                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6386                             	        vmovdqa	128(%r15), %xmm7
 6387                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6388                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6389                             	        vmovdqa	144(%r15), %xmm7
 6390                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6391                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6392                             	        cmpl	$11, %r10d
 6393                             	        vmovdqa	160(%r15), %xmm7
 6394                             	        jl	L_AES_GCM_encrypt_avx1_calc_iv_12_last
 6395                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6396                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6397                             	        vmovdqa	176(%r15), %xmm7
 6398                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6399                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6400                             	        cmpl	$13, %r10d
 6401                             	        vmovdqa	192(%r15), %xmm7
 6402                             	        jl	L_AES_GCM_encrypt_avx1_calc_iv_12_last
 6403                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6404                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6405                             	        vmovdqa	208(%r15), %xmm7
 6406                             	        vaesenc	%xmm7, %xmm5, %xmm5
 6407                             	        vaesenc	%xmm7, %xmm1, %xmm1
 6408                             	        vmovdqa	224(%r15), %xmm7
 6409                             	L_AES_GCM_encrypt_avx1_calc_iv_12_last:
 6410                             	        vaesenclast	%xmm7, %xmm5, %xmm5
 6411                             	        vaesenclast	%xmm7, %xmm1, %xmm1
 6412                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm5, %xmm5
 6413                             	        vmovdqa	%xmm1, 144(%rsp)
 6414                             	        jmp	L_AES_GCM_encrypt_avx1_iv_done
 6415                             	L_AES_GCM_encrypt_avx1_iv_not_12:
 6416                             	        # Calculate values when IV is not 12 bytes
 6417                             	        # H = Encrypt X(=0)
 6418                             	        vmovdqa	(%r15), %xmm5
 6419                             	        vaesenc	16(%r15), %xmm5, %xmm5
 6420                             	        vaesenc	32(%r15), %xmm5, %xmm5
 6421                             	        vaesenc	48(%r15), %xmm5, %xmm5
 6422                             	        vaesenc	64(%r15), %xmm5, %xmm5
 6423                             	        vaesenc	80(%r15), %xmm5, %xmm5
 6424                             	        vaesenc	96(%r15), %xmm5, %xmm5
 6425                             	        vaesenc	112(%r15), %xmm5, %xmm5
 6426                             	        vaesenc	128(%r15), %xmm5, %xmm5
 6427                             	        vaesenc	144(%r15), %xmm5, %xmm5
 6428                             	        cmpl	$11, %r10d
 6429                             	        vmovdqa	160(%r15), %xmm9
 6430                             	        jl	L_AES_GCM_encrypt_avx1_calc_iv_1_aesenc_avx_last
 6431                             	        vaesenc	%xmm9, %xmm5, %xmm5
 6432                             	        vaesenc	176(%r15), %xmm5, %xmm5
 6433                             	        cmpl	$13, %r10d
 6434                             	        vmovdqa	192(%r15), %xmm9
 6435                             	        jl	L_AES_GCM_encrypt_avx1_calc_iv_1_aesenc_avx_last
 6436                             	        vaesenc	%xmm9, %xmm5, %xmm5
 6437                             	        vaesenc	208(%r15), %xmm5, %xmm5
 6438                             	        vmovdqa	224(%r15), %xmm9
 6439                             	L_AES_GCM_encrypt_avx1_calc_iv_1_aesenc_avx_last:
 6440                             	        vaesenclast	%xmm9, %xmm5, %xmm5
 6441                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm5, %xmm5
 6442                             	        # Calc counter
 6443                             	        # Initialization vector
 6444                             	        cmpl	$0x00, %edx
 6445                             	        movq	$0x00, %rcx
 6446                             	        je	L_AES_GCM_encrypt_avx1_calc_iv_done
 6447                             	        cmpl	$16, %edx
 6448                             	        jl	L_AES_GCM_encrypt_avx1_calc_iv_lt16
 6449                             	        andl	$0xfffffff0, %edx
 6450                             	L_AES_GCM_encrypt_avx1_calc_iv_16_loop:
 6451                             	        vmovdqu	(%rax,%rcx,1), %xmm8
 6452                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 6453                             	        vpxor	%xmm8, %xmm4, %xmm4
 6454                             	        # ghash_gfmul_avx
 6455                             	        vpshufd	$0x4e, %xmm4, %xmm1
 6456                             	        vpshufd	$0x4e, %xmm5, %xmm2
 6457                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 6458                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 6459                             	        vpxor	%xmm4, %xmm1, %xmm1
 6460                             	        vpxor	%xmm5, %xmm2, %xmm2
 6461                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 6462                             	        vpxor	%xmm0, %xmm1, %xmm1
 6463                             	        vpxor	%xmm3, %xmm1, %xmm1
 6464                             	        vmovdqa	%xmm0, %xmm7
 6465                             	        vmovdqa	%xmm3, %xmm4
 6466                             	        vpslldq	$8, %xmm1, %xmm2
 6467                             	        vpsrldq	$8, %xmm1, %xmm1
 6468                             	        vpxor	%xmm2, %xmm7, %xmm7
 6469                             	        vpxor	%xmm1, %xmm4, %xmm4
 6470                             	        vpsrld	$31, %xmm7, %xmm0
 6471                             	        vpsrld	$31, %xmm4, %xmm1
 6472                             	        vpslld	$0x01, %xmm7, %xmm7
 6473                             	        vpslld	$0x01, %xmm4, %xmm4
 6474                             	        vpsrldq	$12, %xmm0, %xmm2
 6475                             	        vpslldq	$4, %xmm0, %xmm0
 6476                             	        vpslldq	$4, %xmm1, %xmm1
 6477                             	        vpor	%xmm2, %xmm4, %xmm4
 6478                             	        vpor	%xmm0, %xmm7, %xmm7
 6479                             	        vpor	%xmm1, %xmm4, %xmm4
 6480                             	        vpslld	$31, %xmm7, %xmm0
 6481                             	        vpslld	$30, %xmm7, %xmm1
 6482                             	        vpslld	$25, %xmm7, %xmm2
 6483                             	        vpxor	%xmm1, %xmm0, %xmm0
 6484                             	        vpxor	%xmm2, %xmm0, %xmm0
 6485                             	        vmovdqa	%xmm0, %xmm1
 6486                             	        vpsrldq	$4, %xmm1, %xmm1
 6487                             	        vpslldq	$12, %xmm0, %xmm0
 6488                             	        vpxor	%xmm0, %xmm7, %xmm7
 6489                             	        vpsrld	$0x01, %xmm7, %xmm2
 6490                             	        vpsrld	$2, %xmm7, %xmm3
 6491                             	        vpsrld	$7, %xmm7, %xmm0
 6492                             	        vpxor	%xmm3, %xmm2, %xmm2
 6493                             	        vpxor	%xmm0, %xmm2, %xmm2
 6494                             	        vpxor	%xmm1, %xmm2, %xmm2
 6495                             	        vpxor	%xmm7, %xmm2, %xmm2
 6496                             	        vpxor	%xmm2, %xmm4, %xmm4
 6497                             	        addl	$16, %ecx
 6498                             	        cmpl	%edx, %ecx
 6499                             	        jl	L_AES_GCM_encrypt_avx1_calc_iv_16_loop
 6500                             	        movl	%ebx, %edx
 6501                             	        cmpl	%edx, %ecx
 6502                             	        je	L_AES_GCM_encrypt_avx1_calc_iv_done
 6503                             	L_AES_GCM_encrypt_avx1_calc_iv_lt16:
 6504                             	        subq	$16, %rsp
 6505                             	        vpxor	%xmm8, %xmm8, %xmm8
 6506                             	        xorl	%ebx, %ebx
 6507                             	        vmovdqu	%xmm8, (%rsp)
 6508                             	L_AES_GCM_encrypt_avx1_calc_iv_loop:
 6509                             	        movzbl	(%rax,%rcx,1), %r13d
 6510                             	        movb	%r13b, (%rsp,%rbx,1)
 6511                             	        incl	%ecx
 6512                             	        incl	%ebx
 6513                             	        cmpl	%edx, %ecx
 6514                             	        jl	L_AES_GCM_encrypt_avx1_calc_iv_loop
 6515                             	        vmovdqu	(%rsp), %xmm8
 6516                             	        addq	$16, %rsp
 6517                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 6518                             	        vpxor	%xmm8, %xmm4, %xmm4
 6519                             	        # ghash_gfmul_avx
 6520                             	        vpshufd	$0x4e, %xmm4, %xmm1
 6521                             	        vpshufd	$0x4e, %xmm5, %xmm2
 6522                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 6523                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 6524                             	        vpxor	%xmm4, %xmm1, %xmm1
 6525                             	        vpxor	%xmm5, %xmm2, %xmm2
 6526                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 6527                             	        vpxor	%xmm0, %xmm1, %xmm1
 6528                             	        vpxor	%xmm3, %xmm1, %xmm1
 6529                             	        vmovdqa	%xmm0, %xmm7
 6530                             	        vmovdqa	%xmm3, %xmm4
 6531                             	        vpslldq	$8, %xmm1, %xmm2
 6532                             	        vpsrldq	$8, %xmm1, %xmm1
 6533                             	        vpxor	%xmm2, %xmm7, %xmm7
 6534                             	        vpxor	%xmm1, %xmm4, %xmm4
 6535                             	        vpsrld	$31, %xmm7, %xmm0
 6536                             	        vpsrld	$31, %xmm4, %xmm1
 6537                             	        vpslld	$0x01, %xmm7, %xmm7
 6538                             	        vpslld	$0x01, %xmm4, %xmm4
 6539                             	        vpsrldq	$12, %xmm0, %xmm2
 6540                             	        vpslldq	$4, %xmm0, %xmm0
 6541                             	        vpslldq	$4, %xmm1, %xmm1
 6542                             	        vpor	%xmm2, %xmm4, %xmm4
 6543                             	        vpor	%xmm0, %xmm7, %xmm7
 6544                             	        vpor	%xmm1, %xmm4, %xmm4
 6545                             	        vpslld	$31, %xmm7, %xmm0
 6546                             	        vpslld	$30, %xmm7, %xmm1
 6547                             	        vpslld	$25, %xmm7, %xmm2
 6548                             	        vpxor	%xmm1, %xmm0, %xmm0
 6549                             	        vpxor	%xmm2, %xmm0, %xmm0
 6550                             	        vmovdqa	%xmm0, %xmm1
 6551                             	        vpsrldq	$4, %xmm1, %xmm1
 6552                             	        vpslldq	$12, %xmm0, %xmm0
 6553                             	        vpxor	%xmm0, %xmm7, %xmm7
 6554                             	        vpsrld	$0x01, %xmm7, %xmm2
 6555                             	        vpsrld	$2, %xmm7, %xmm3
 6556                             	        vpsrld	$7, %xmm7, %xmm0
 6557                             	        vpxor	%xmm3, %xmm2, %xmm2
 6558                             	        vpxor	%xmm0, %xmm2, %xmm2
 6559                             	        vpxor	%xmm1, %xmm2, %xmm2
 6560                             	        vpxor	%xmm7, %xmm2, %xmm2
 6561                             	        vpxor	%xmm2, %xmm4, %xmm4
 6562                             	L_AES_GCM_encrypt_avx1_calc_iv_done:
 6563                             	        # T = Encrypt counter
 6564                             	        vpxor	%xmm0, %xmm0, %xmm0
 6565                             	        shll	$3, %edx
 6566                             	        vpinsrq	$0x00, %rdx, %xmm0, %xmm0
 6567                             	        vpxor	%xmm0, %xmm4, %xmm4
 6568                             	        # ghash_gfmul_avx
 6569                             	        vpshufd	$0x4e, %xmm4, %xmm1
 6570                             	        vpshufd	$0x4e, %xmm5, %xmm2
 6571                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 6572                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 6573                             	        vpxor	%xmm4, %xmm1, %xmm1
 6574                             	        vpxor	%xmm5, %xmm2, %xmm2
 6575                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 6576                             	        vpxor	%xmm0, %xmm1, %xmm1
 6577                             	        vpxor	%xmm3, %xmm1, %xmm1
 6578                             	        vmovdqa	%xmm0, %xmm7
 6579                             	        vmovdqa	%xmm3, %xmm4
 6580                             	        vpslldq	$8, %xmm1, %xmm2
 6581                             	        vpsrldq	$8, %xmm1, %xmm1
 6582                             	        vpxor	%xmm2, %xmm7, %xmm7
 6583                             	        vpxor	%xmm1, %xmm4, %xmm4
 6584                             	        vpsrld	$31, %xmm7, %xmm0
 6585                             	        vpsrld	$31, %xmm4, %xmm1
 6586                             	        vpslld	$0x01, %xmm7, %xmm7
 6587                             	        vpslld	$0x01, %xmm4, %xmm4
 6588                             	        vpsrldq	$12, %xmm0, %xmm2
 6589                             	        vpslldq	$4, %xmm0, %xmm0
 6590                             	        vpslldq	$4, %xmm1, %xmm1
 6591                             	        vpor	%xmm2, %xmm4, %xmm4
 6592                             	        vpor	%xmm0, %xmm7, %xmm7
 6593                             	        vpor	%xmm1, %xmm4, %xmm4
 6594                             	        vpslld	$31, %xmm7, %xmm0
 6595                             	        vpslld	$30, %xmm7, %xmm1
 6596                             	        vpslld	$25, %xmm7, %xmm2
 6597                             	        vpxor	%xmm1, %xmm0, %xmm0
 6598                             	        vpxor	%xmm2, %xmm0, %xmm0
 6599                             	        vmovdqa	%xmm0, %xmm1
 6600                             	        vpsrldq	$4, %xmm1, %xmm1
 6601                             	        vpslldq	$12, %xmm0, %xmm0
 6602                             	        vpxor	%xmm0, %xmm7, %xmm7
 6603                             	        vpsrld	$0x01, %xmm7, %xmm2
 6604                             	        vpsrld	$2, %xmm7, %xmm3
 6605                             	        vpsrld	$7, %xmm7, %xmm0
 6606                             	        vpxor	%xmm3, %xmm2, %xmm2
 6607                             	        vpxor	%xmm0, %xmm2, %xmm2
 6608                             	        vpxor	%xmm1, %xmm2, %xmm2
 6609                             	        vpxor	%xmm7, %xmm2, %xmm2
 6610                             	        vpxor	%xmm2, %xmm4, %xmm4
 6611                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm4, %xmm4
 6612                             	        #   Encrypt counter
 6613                             	        vmovdqa	(%r15), %xmm8
 6614                             	        vpxor	%xmm4, %xmm8, %xmm8
 6615                             	        vaesenc	16(%r15), %xmm8, %xmm8
 6616                             	        vaesenc	32(%r15), %xmm8, %xmm8
 6617                             	        vaesenc	48(%r15), %xmm8, %xmm8
 6618                             	        vaesenc	64(%r15), %xmm8, %xmm8
 6619                             	        vaesenc	80(%r15), %xmm8, %xmm8
 6620                             	        vaesenc	96(%r15), %xmm8, %xmm8
 6621                             	        vaesenc	112(%r15), %xmm8, %xmm8
 6622                             	        vaesenc	128(%r15), %xmm8, %xmm8
 6623                             	        vaesenc	144(%r15), %xmm8, %xmm8
 6624                             	        cmpl	$11, %r10d
 6625                             	        vmovdqa	160(%r15), %xmm9
 6626                             	        jl	L_AES_GCM_encrypt_avx1_calc_iv_2_aesenc_avx_last
 6627                             	        vaesenc	%xmm9, %xmm8, %xmm8
 6628                             	        vaesenc	176(%r15), %xmm8, %xmm8
 6629                             	        cmpl	$13, %r10d
 6630                             	        vmovdqa	192(%r15), %xmm9
 6631                             	        jl	L_AES_GCM_encrypt_avx1_calc_iv_2_aesenc_avx_last
 6632                             	        vaesenc	%xmm9, %xmm8, %xmm8
 6633                             	        vaesenc	208(%r15), %xmm8, %xmm8
 6634                             	        vmovdqa	224(%r15), %xmm9
 6635                             	L_AES_GCM_encrypt_avx1_calc_iv_2_aesenc_avx_last:
 6636                             	        vaesenclast	%xmm9, %xmm8, %xmm8
 6637                             	        vmovdqa	%xmm8, 144(%rsp)
 6638                             	L_AES_GCM_encrypt_avx1_iv_done:
 6639                             	        # Additional authentication data
 6640                             	        movl	%r11d, %edx
 6641                             	        cmpl	$0x00, %edx
 6642                             	        je	L_AES_GCM_encrypt_avx1_calc_aad_done
 6643                             	        xorl	%ecx, %ecx
 6644                             	        cmpl	$16, %edx
 6645                             	        jl	L_AES_GCM_encrypt_avx1_calc_aad_lt16
 6646                             	        andl	$0xfffffff0, %edx
 6647                             	L_AES_GCM_encrypt_avx1_calc_aad_16_loop:
 6648                             	        vmovdqu	(%r12,%rcx,1), %xmm8
 6649                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 6650                             	        vpxor	%xmm8, %xmm6, %xmm6
 6651                             	        # ghash_gfmul_avx
 6652                             	        vpshufd	$0x4e, %xmm6, %xmm1
 6653                             	        vpshufd	$0x4e, %xmm5, %xmm2
 6654                             	        vpclmulqdq	$0x11, %xmm6, %xmm5, %xmm3
 6655                             	        vpclmulqdq	$0x00, %xmm6, %xmm5, %xmm0
 6656                             	        vpxor	%xmm6, %xmm1, %xmm1
 6657                             	        vpxor	%xmm5, %xmm2, %xmm2
 6658                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 6659                             	        vpxor	%xmm0, %xmm1, %xmm1
 6660                             	        vpxor	%xmm3, %xmm1, %xmm1
 6661                             	        vmovdqa	%xmm0, %xmm7
 6662                             	        vmovdqa	%xmm3, %xmm6
 6663                             	        vpslldq	$8, %xmm1, %xmm2
 6664                             	        vpsrldq	$8, %xmm1, %xmm1
 6665                             	        vpxor	%xmm2, %xmm7, %xmm7
 6666                             	        vpxor	%xmm1, %xmm6, %xmm6
 6667                             	        vpsrld	$31, %xmm7, %xmm0
 6668                             	        vpsrld	$31, %xmm6, %xmm1
 6669                             	        vpslld	$0x01, %xmm7, %xmm7
 6670                             	        vpslld	$0x01, %xmm6, %xmm6
 6671                             	        vpsrldq	$12, %xmm0, %xmm2
 6672                             	        vpslldq	$4, %xmm0, %xmm0
 6673                             	        vpslldq	$4, %xmm1, %xmm1
 6674                             	        vpor	%xmm2, %xmm6, %xmm6
 6675                             	        vpor	%xmm0, %xmm7, %xmm7
 6676                             	        vpor	%xmm1, %xmm6, %xmm6
 6677                             	        vpslld	$31, %xmm7, %xmm0
 6678                             	        vpslld	$30, %xmm7, %xmm1
 6679                             	        vpslld	$25, %xmm7, %xmm2
 6680                             	        vpxor	%xmm1, %xmm0, %xmm0
 6681                             	        vpxor	%xmm2, %xmm0, %xmm0
 6682                             	        vmovdqa	%xmm0, %xmm1
 6683                             	        vpsrldq	$4, %xmm1, %xmm1
 6684                             	        vpslldq	$12, %xmm0, %xmm0
 6685                             	        vpxor	%xmm0, %xmm7, %xmm7
 6686                             	        vpsrld	$0x01, %xmm7, %xmm2
 6687                             	        vpsrld	$2, %xmm7, %xmm3
 6688                             	        vpsrld	$7, %xmm7, %xmm0
 6689                             	        vpxor	%xmm3, %xmm2, %xmm2
 6690                             	        vpxor	%xmm0, %xmm2, %xmm2
 6691                             	        vpxor	%xmm1, %xmm2, %xmm2
 6692                             	        vpxor	%xmm7, %xmm2, %xmm2
 6693                             	        vpxor	%xmm2, %xmm6, %xmm6
 6694                             	        addl	$16, %ecx
 6695                             	        cmpl	%edx, %ecx
 6696                             	        jl	L_AES_GCM_encrypt_avx1_calc_aad_16_loop
 6697                             	        movl	%r11d, %edx
 6698                             	        cmpl	%edx, %ecx
 6699                             	        je	L_AES_GCM_encrypt_avx1_calc_aad_done
 6700                             	L_AES_GCM_encrypt_avx1_calc_aad_lt16:
 6701                             	        subq	$16, %rsp
 6702                             	        vpxor	%xmm8, %xmm8, %xmm8
 6703                             	        xorl	%ebx, %ebx
 6704                             	        vmovdqu	%xmm8, (%rsp)
 6705                             	L_AES_GCM_encrypt_avx1_calc_aad_loop:
 6706                             	        movzbl	(%r12,%rcx,1), %r13d
 6707                             	        movb	%r13b, (%rsp,%rbx,1)
 6708                             	        incl	%ecx
 6709                             	        incl	%ebx
 6710                             	        cmpl	%edx, %ecx
 6711                             	        jl	L_AES_GCM_encrypt_avx1_calc_aad_loop
 6712                             	        vmovdqu	(%rsp), %xmm8
 6713                             	        addq	$16, %rsp
 6714                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 6715                             	        vpxor	%xmm8, %xmm6, %xmm6
 6716                             	        # ghash_gfmul_avx
 6717                             	        vpshufd	$0x4e, %xmm6, %xmm1
 6718                             	        vpshufd	$0x4e, %xmm5, %xmm2
 6719                             	        vpclmulqdq	$0x11, %xmm6, %xmm5, %xmm3
 6720                             	        vpclmulqdq	$0x00, %xmm6, %xmm5, %xmm0
 6721                             	        vpxor	%xmm6, %xmm1, %xmm1
 6722                             	        vpxor	%xmm5, %xmm2, %xmm2
 6723                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 6724                             	        vpxor	%xmm0, %xmm1, %xmm1
 6725                             	        vpxor	%xmm3, %xmm1, %xmm1
 6726                             	        vmovdqa	%xmm0, %xmm7
 6727                             	        vmovdqa	%xmm3, %xmm6
 6728                             	        vpslldq	$8, %xmm1, %xmm2
 6729                             	        vpsrldq	$8, %xmm1, %xmm1
 6730                             	        vpxor	%xmm2, %xmm7, %xmm7
 6731                             	        vpxor	%xmm1, %xmm6, %xmm6
 6732                             	        vpsrld	$31, %xmm7, %xmm0
 6733                             	        vpsrld	$31, %xmm6, %xmm1
 6734                             	        vpslld	$0x01, %xmm7, %xmm7
 6735                             	        vpslld	$0x01, %xmm6, %xmm6
 6736                             	        vpsrldq	$12, %xmm0, %xmm2
 6737                             	        vpslldq	$4, %xmm0, %xmm0
 6738                             	        vpslldq	$4, %xmm1, %xmm1
 6739                             	        vpor	%xmm2, %xmm6, %xmm6
 6740                             	        vpor	%xmm0, %xmm7, %xmm7
 6741                             	        vpor	%xmm1, %xmm6, %xmm6
 6742                             	        vpslld	$31, %xmm7, %xmm0
 6743                             	        vpslld	$30, %xmm7, %xmm1
 6744                             	        vpslld	$25, %xmm7, %xmm2
 6745                             	        vpxor	%xmm1, %xmm0, %xmm0
 6746                             	        vpxor	%xmm2, %xmm0, %xmm0
 6747                             	        vmovdqa	%xmm0, %xmm1
 6748                             	        vpsrldq	$4, %xmm1, %xmm1
 6749                             	        vpslldq	$12, %xmm0, %xmm0
 6750                             	        vpxor	%xmm0, %xmm7, %xmm7
 6751                             	        vpsrld	$0x01, %xmm7, %xmm2
 6752                             	        vpsrld	$2, %xmm7, %xmm3
 6753                             	        vpsrld	$7, %xmm7, %xmm0
 6754                             	        vpxor	%xmm3, %xmm2, %xmm2
 6755                             	        vpxor	%xmm0, %xmm2, %xmm2
 6756                             	        vpxor	%xmm1, %xmm2, %xmm2
 6757                             	        vpxor	%xmm7, %xmm2, %xmm2
 6758                             	        vpxor	%xmm2, %xmm6, %xmm6
 6759                             	L_AES_GCM_encrypt_avx1_calc_aad_done:
 6760                             	        # Calculate counter and H
 6761                             	        vpsrlq	$63, %xmm5, %xmm9
 6762                             	        vpsllq	$0x01, %xmm5, %xmm8
 6763                             	        vpslldq	$8, %xmm9, %xmm9
 6764                             	        vpor	%xmm9, %xmm8, %xmm8
 6765                             	        vpshufd	$0xff, %xmm5, %xmm5
 6766                             	        vpsrad	$31, %xmm5, %xmm5
 6767                             	        vpshufb	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm4, %xmm4
 6768                             	        vpand	L_avx1_aes_gcm_mod2_128(%rip), %xmm5, %xmm5
 6769                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm4, %xmm4
 6770                             	        vpxor	%xmm8, %xmm5, %xmm5
 6771                             	        vmovdqa	%xmm4, 128(%rsp)
 6772                             	        xorl	%ebx, %ebx
 6773                             	        cmpl	$0x80, %r9d
 6774                             	        movl	%r9d, %r13d
 6775                             	        jl	L_AES_GCM_encrypt_avx1_done_128
 6776                             	        andl	$0xffffff80, %r13d
 6777                             	        vmovdqa	%xmm6, %xmm2
 6778                             	        # H ^ 1
 6779                             	        vmovdqa	%xmm5, (%rsp)
 6780                             	        # H ^ 2
 6781                             	        vpclmulqdq	$0x00, %xmm5, %xmm5, %xmm8
 6782                             	        vpclmulqdq	$0x11, %xmm5, %xmm5, %xmm0
 6783                             	        vpslld	$31, %xmm8, %xmm12
 6784                             	        vpslld	$30, %xmm8, %xmm13
 6785                             	        vpslld	$25, %xmm8, %xmm14
 6786                             	        vpxor	%xmm13, %xmm12, %xmm12
 6787                             	        vpxor	%xmm14, %xmm12, %xmm12
 6788                             	        vpsrldq	$4, %xmm12, %xmm13
 6789                             	        vpslldq	$12, %xmm12, %xmm12
 6790                             	        vpxor	%xmm12, %xmm8, %xmm8
 6791                             	        vpsrld	$0x01, %xmm8, %xmm14
 6792                             	        vpsrld	$2, %xmm8, %xmm10
 6793                             	        vpsrld	$7, %xmm8, %xmm9
 6794                             	        vpxor	%xmm10, %xmm14, %xmm14
 6795                             	        vpxor	%xmm9, %xmm14, %xmm14
 6796                             	        vpxor	%xmm13, %xmm14, %xmm14
 6797                             	        vpxor	%xmm8, %xmm14, %xmm14
 6798                             	        vpxor	%xmm14, %xmm0, %xmm0
 6799                             	        vmovdqa	%xmm0, 16(%rsp)
 6800                             	        # H ^ 3
 6801                             	        # ghash_gfmul_red_avx
 6802                             	        vpshufd	$0x4e, %xmm5, %xmm9
 6803                             	        vpshufd	$0x4e, %xmm0, %xmm10
 6804                             	        vpclmulqdq	$0x11, %xmm5, %xmm0, %xmm11
 6805                             	        vpclmulqdq	$0x00, %xmm5, %xmm0, %xmm8
 6806                             	        vpxor	%xmm5, %xmm9, %xmm9
 6807                             	        vpxor	%xmm0, %xmm10, %xmm10
 6808                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 6809                             	        vpxor	%xmm8, %xmm9, %xmm9
 6810                             	        vpxor	%xmm11, %xmm9, %xmm9
 6811                             	        vpslldq	$8, %xmm9, %xmm10
 6812                             	        vpsrldq	$8, %xmm9, %xmm9
 6813                             	        vpxor	%xmm10, %xmm8, %xmm8
 6814                             	        vpxor	%xmm9, %xmm11, %xmm1
 6815                             	        vpslld	$31, %xmm8, %xmm12
 6816                             	        vpslld	$30, %xmm8, %xmm13
 6817                             	        vpslld	$25, %xmm8, %xmm14
 6818                             	        vpxor	%xmm13, %xmm12, %xmm12
 6819                             	        vpxor	%xmm14, %xmm12, %xmm12
 6820                             	        vpsrldq	$4, %xmm12, %xmm13
 6821                             	        vpslldq	$12, %xmm12, %xmm12
 6822                             	        vpxor	%xmm12, %xmm8, %xmm8
 6823                             	        vpsrld	$0x01, %xmm8, %xmm14
 6824                             	        vpsrld	$2, %xmm8, %xmm10
 6825                             	        vpsrld	$7, %xmm8, %xmm9
 6826                             	        vpxor	%xmm10, %xmm14, %xmm14
 6827                             	        vpxor	%xmm9, %xmm14, %xmm14
 6828                             	        vpxor	%xmm13, %xmm14, %xmm14
 6829                             	        vpxor	%xmm8, %xmm14, %xmm14
 6830                             	        vpxor	%xmm14, %xmm1, %xmm1
 6831                             	        vmovdqa	%xmm1, 32(%rsp)
 6832                             	        # H ^ 4
 6833                             	        vpclmulqdq	$0x00, %xmm0, %xmm0, %xmm8
 6834                             	        vpclmulqdq	$0x11, %xmm0, %xmm0, %xmm3
 6835                             	        vpslld	$31, %xmm8, %xmm12
 6836                             	        vpslld	$30, %xmm8, %xmm13
 6837                             	        vpslld	$25, %xmm8, %xmm14
 6838                             	        vpxor	%xmm13, %xmm12, %xmm12
 6839                             	        vpxor	%xmm14, %xmm12, %xmm12
 6840                             	        vpsrldq	$4, %xmm12, %xmm13
 6841                             	        vpslldq	$12, %xmm12, %xmm12
 6842                             	        vpxor	%xmm12, %xmm8, %xmm8
 6843                             	        vpsrld	$0x01, %xmm8, %xmm14
 6844                             	        vpsrld	$2, %xmm8, %xmm10
 6845                             	        vpsrld	$7, %xmm8, %xmm9
 6846                             	        vpxor	%xmm10, %xmm14, %xmm14
 6847                             	        vpxor	%xmm9, %xmm14, %xmm14
 6848                             	        vpxor	%xmm13, %xmm14, %xmm14
 6849                             	        vpxor	%xmm8, %xmm14, %xmm14
 6850                             	        vpxor	%xmm14, %xmm3, %xmm3
 6851                             	        vmovdqa	%xmm3, 48(%rsp)
 6852                             	        # H ^ 5
 6853                             	        # ghash_gfmul_red_avx
 6854                             	        vpshufd	$0x4e, %xmm0, %xmm9
 6855                             	        vpshufd	$0x4e, %xmm1, %xmm10
 6856                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm11
 6857                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm8
 6858                             	        vpxor	%xmm0, %xmm9, %xmm9
 6859                             	        vpxor	%xmm1, %xmm10, %xmm10
 6860                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 6861                             	        vpxor	%xmm8, %xmm9, %xmm9
 6862                             	        vpxor	%xmm11, %xmm9, %xmm9
 6863                             	        vpslldq	$8, %xmm9, %xmm10
 6864                             	        vpsrldq	$8, %xmm9, %xmm9
 6865                             	        vpxor	%xmm10, %xmm8, %xmm8
 6866                             	        vpxor	%xmm9, %xmm11, %xmm7
 6867                             	        vpslld	$31, %xmm8, %xmm12
 6868                             	        vpslld	$30, %xmm8, %xmm13
 6869                             	        vpslld	$25, %xmm8, %xmm14
 6870                             	        vpxor	%xmm13, %xmm12, %xmm12
 6871                             	        vpxor	%xmm14, %xmm12, %xmm12
 6872                             	        vpsrldq	$4, %xmm12, %xmm13
 6873                             	        vpslldq	$12, %xmm12, %xmm12
 6874                             	        vpxor	%xmm12, %xmm8, %xmm8
 6875                             	        vpsrld	$0x01, %xmm8, %xmm14
 6876                             	        vpsrld	$2, %xmm8, %xmm10
 6877                             	        vpsrld	$7, %xmm8, %xmm9
 6878                             	        vpxor	%xmm10, %xmm14, %xmm14
 6879                             	        vpxor	%xmm9, %xmm14, %xmm14
 6880                             	        vpxor	%xmm13, %xmm14, %xmm14
 6881                             	        vpxor	%xmm8, %xmm14, %xmm14
 6882                             	        vpxor	%xmm14, %xmm7, %xmm7
 6883                             	        vmovdqa	%xmm7, 64(%rsp)
 6884                             	        # H ^ 6
 6885                             	        vpclmulqdq	$0x00, %xmm1, %xmm1, %xmm8
 6886                             	        vpclmulqdq	$0x11, %xmm1, %xmm1, %xmm7
 6887                             	        vpslld	$31, %xmm8, %xmm12
 6888                             	        vpslld	$30, %xmm8, %xmm13
 6889                             	        vpslld	$25, %xmm8, %xmm14
 6890                             	        vpxor	%xmm13, %xmm12, %xmm12
 6891                             	        vpxor	%xmm14, %xmm12, %xmm12
 6892                             	        vpsrldq	$4, %xmm12, %xmm13
 6893                             	        vpslldq	$12, %xmm12, %xmm12
 6894                             	        vpxor	%xmm12, %xmm8, %xmm8
 6895                             	        vpsrld	$0x01, %xmm8, %xmm14
 6896                             	        vpsrld	$2, %xmm8, %xmm10
 6897                             	        vpsrld	$7, %xmm8, %xmm9
 6898                             	        vpxor	%xmm10, %xmm14, %xmm14
 6899                             	        vpxor	%xmm9, %xmm14, %xmm14
 6900                             	        vpxor	%xmm13, %xmm14, %xmm14
 6901                             	        vpxor	%xmm8, %xmm14, %xmm14
 6902                             	        vpxor	%xmm14, %xmm7, %xmm7
 6903                             	        vmovdqa	%xmm7, 80(%rsp)
 6904                             	        # H ^ 7
 6905                             	        # ghash_gfmul_red_avx
 6906                             	        vpshufd	$0x4e, %xmm1, %xmm9
 6907                             	        vpshufd	$0x4e, %xmm3, %xmm10
 6908                             	        vpclmulqdq	$0x11, %xmm1, %xmm3, %xmm11
 6909                             	        vpclmulqdq	$0x00, %xmm1, %xmm3, %xmm8
 6910                             	        vpxor	%xmm1, %xmm9, %xmm9
 6911                             	        vpxor	%xmm3, %xmm10, %xmm10
 6912                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 6913                             	        vpxor	%xmm8, %xmm9, %xmm9
 6914                             	        vpxor	%xmm11, %xmm9, %xmm9
 6915                             	        vpslldq	$8, %xmm9, %xmm10
 6916                             	        vpsrldq	$8, %xmm9, %xmm9
 6917                             	        vpxor	%xmm10, %xmm8, %xmm8
 6918                             	        vpxor	%xmm9, %xmm11, %xmm7
 6919                             	        vpslld	$31, %xmm8, %xmm12
 6920                             	        vpslld	$30, %xmm8, %xmm13
 6921                             	        vpslld	$25, %xmm8, %xmm14
 6922                             	        vpxor	%xmm13, %xmm12, %xmm12
 6923                             	        vpxor	%xmm14, %xmm12, %xmm12
 6924                             	        vpsrldq	$4, %xmm12, %xmm13
 6925                             	        vpslldq	$12, %xmm12, %xmm12
 6926                             	        vpxor	%xmm12, %xmm8, %xmm8
 6927                             	        vpsrld	$0x01, %xmm8, %xmm14
 6928                             	        vpsrld	$2, %xmm8, %xmm10
 6929                             	        vpsrld	$7, %xmm8, %xmm9
 6930                             	        vpxor	%xmm10, %xmm14, %xmm14
 6931                             	        vpxor	%xmm9, %xmm14, %xmm14
 6932                             	        vpxor	%xmm13, %xmm14, %xmm14
 6933                             	        vpxor	%xmm8, %xmm14, %xmm14
 6934                             	        vpxor	%xmm14, %xmm7, %xmm7
 6935                             	        vmovdqa	%xmm7, 96(%rsp)
 6936                             	        # H ^ 8
 6937                             	        vpclmulqdq	$0x00, %xmm3, %xmm3, %xmm8
 6938                             	        vpclmulqdq	$0x11, %xmm3, %xmm3, %xmm7
 6939                             	        vpslld	$31, %xmm8, %xmm12
 6940                             	        vpslld	$30, %xmm8, %xmm13
 6941                             	        vpslld	$25, %xmm8, %xmm14
 6942                             	        vpxor	%xmm13, %xmm12, %xmm12
 6943                             	        vpxor	%xmm14, %xmm12, %xmm12
 6944                             	        vpsrldq	$4, %xmm12, %xmm13
 6945                             	        vpslldq	$12, %xmm12, %xmm12
 6946                             	        vpxor	%xmm12, %xmm8, %xmm8
 6947                             	        vpsrld	$0x01, %xmm8, %xmm14
 6948                             	        vpsrld	$2, %xmm8, %xmm10
 6949                             	        vpsrld	$7, %xmm8, %xmm9
 6950                             	        vpxor	%xmm10, %xmm14, %xmm14
 6951                             	        vpxor	%xmm9, %xmm14, %xmm14
 6952                             	        vpxor	%xmm13, %xmm14, %xmm14
 6953                             	        vpxor	%xmm8, %xmm14, %xmm14
 6954                             	        vpxor	%xmm14, %xmm7, %xmm7
 6955                             	        vmovdqa	%xmm7, 112(%rsp)
 6956                             	        # First 128 bytes of input
 6957                             	        vmovdqa	128(%rsp), %xmm0
 6958                             	        vmovdqa	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm1
 6959                             	        vpshufb	%xmm1, %xmm0, %xmm8
 6960                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm0, %xmm9
 6961                             	        vpshufb	%xmm1, %xmm9, %xmm9
 6962                             	        vpaddd	L_avx1_aes_gcm_two(%rip), %xmm0, %xmm10
 6963                             	        vpshufb	%xmm1, %xmm10, %xmm10
 6964                             	        vpaddd	L_avx1_aes_gcm_three(%rip), %xmm0, %xmm11
 6965                             	        vpshufb	%xmm1, %xmm11, %xmm11
 6966                             	        vpaddd	L_avx1_aes_gcm_four(%rip), %xmm0, %xmm12
 6967                             	        vpshufb	%xmm1, %xmm12, %xmm12
 6968                             	        vpaddd	L_avx1_aes_gcm_five(%rip), %xmm0, %xmm13
 6969                             	        vpshufb	%xmm1, %xmm13, %xmm13
 6970                             	        vpaddd	L_avx1_aes_gcm_six(%rip), %xmm0, %xmm14
 6971                             	        vpshufb	%xmm1, %xmm14, %xmm14
 6972                             	        vpaddd	L_avx1_aes_gcm_seven(%rip), %xmm0, %xmm15
 6973                             	        vpshufb	%xmm1, %xmm15, %xmm15
 6974                             	        vpaddd	L_avx1_aes_gcm_eight(%rip), %xmm0, %xmm0
 6975                             	        vmovdqa	(%r15), %xmm7
 6976                             	        vmovdqa	%xmm0, 128(%rsp)
 6977                             	        vpxor	%xmm7, %xmm8, %xmm8
 6978                             	        vpxor	%xmm7, %xmm9, %xmm9
 6979                             	        vpxor	%xmm7, %xmm10, %xmm10
 6980                             	        vpxor	%xmm7, %xmm11, %xmm11
 6981                             	        vpxor	%xmm7, %xmm12, %xmm12
 6982                             	        vpxor	%xmm7, %xmm13, %xmm13
 6983                             	        vpxor	%xmm7, %xmm14, %xmm14
 6984                             	        vpxor	%xmm7, %xmm15, %xmm15
 6985                             	        vmovdqa	16(%r15), %xmm7
 6986                             	        vaesenc	%xmm7, %xmm8, %xmm8
 6987                             	        vaesenc	%xmm7, %xmm9, %xmm9
 6988                             	        vaesenc	%xmm7, %xmm10, %xmm10
 6989                             	        vaesenc	%xmm7, %xmm11, %xmm11
 6990                             	        vaesenc	%xmm7, %xmm12, %xmm12
 6991                             	        vaesenc	%xmm7, %xmm13, %xmm13
 6992                             	        vaesenc	%xmm7, %xmm14, %xmm14
 6993                             	        vaesenc	%xmm7, %xmm15, %xmm15
 6994                             	        vmovdqa	32(%r15), %xmm7
 6995                             	        vaesenc	%xmm7, %xmm8, %xmm8
 6996                             	        vaesenc	%xmm7, %xmm9, %xmm9
 6997                             	        vaesenc	%xmm7, %xmm10, %xmm10
 6998                             	        vaesenc	%xmm7, %xmm11, %xmm11
 6999                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7000                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7001                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7002                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7003                             	        vmovdqa	48(%r15), %xmm7
 7004                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7005                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7006                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7007                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7008                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7009                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7010                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7011                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7012                             	        vmovdqa	64(%r15), %xmm7
 7013                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7014                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7015                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7016                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7017                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7018                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7019                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7020                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7021                             	        vmovdqa	80(%r15), %xmm7
 7022                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7023                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7024                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7025                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7026                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7027                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7028                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7029                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7030                             	        vmovdqa	96(%r15), %xmm7
 7031                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7032                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7033                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7034                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7035                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7036                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7037                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7038                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7039                             	        vmovdqa	112(%r15), %xmm7
 7040                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7041                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7042                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7043                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7044                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7045                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7046                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7047                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7048                             	        vmovdqa	128(%r15), %xmm7
 7049                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7050                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7051                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7052                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7053                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7054                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7055                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7056                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7057                             	        vmovdqa	144(%r15), %xmm7
 7058                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7059                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7060                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7061                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7062                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7063                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7064                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7065                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7066                             	        cmpl	$11, %r10d
 7067                             	        vmovdqa	160(%r15), %xmm7
 7068                             	        jl	L_AES_GCM_encrypt_avx1_aesenc_128_enc_done
 7069                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7070                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7071                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7072                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7073                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7074                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7075                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7076                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7077                             	        vmovdqa	176(%r15), %xmm7
 7078                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7079                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7080                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7081                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7082                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7083                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7084                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7085                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7086                             	        cmpl	$13, %r10d
 7087                             	        vmovdqa	192(%r15), %xmm7
 7088                             	        jl	L_AES_GCM_encrypt_avx1_aesenc_128_enc_done
 7089                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7090                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7091                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7092                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7093                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7094                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7095                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7096                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7097                             	        vmovdqa	208(%r15), %xmm7
 7098                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7099                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7100                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7101                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7102                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7103                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7104                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7105                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7106                             	        vmovdqa	224(%r15), %xmm7
 7107                             	L_AES_GCM_encrypt_avx1_aesenc_128_enc_done:
 7108                             	        vaesenclast	%xmm7, %xmm8, %xmm8
 7109                             	        vaesenclast	%xmm7, %xmm9, %xmm9
 7110                             	        vmovdqu	(%rdi), %xmm0
 7111                             	        vmovdqu	16(%rdi), %xmm1
 7112                             	        vpxor	%xmm0, %xmm8, %xmm8
 7113                             	        vpxor	%xmm1, %xmm9, %xmm9
 7114                             	        vmovdqu	%xmm8, (%rsi)
 7115                             	        vmovdqu	%xmm9, 16(%rsi)
 7116                             	        vaesenclast	%xmm7, %xmm10, %xmm10
 7117                             	        vaesenclast	%xmm7, %xmm11, %xmm11
 7118                             	        vmovdqu	32(%rdi), %xmm0
 7119                             	        vmovdqu	48(%rdi), %xmm1
 7120                             	        vpxor	%xmm0, %xmm10, %xmm10
 7121                             	        vpxor	%xmm1, %xmm11, %xmm11
 7122                             	        vmovdqu	%xmm10, 32(%rsi)
 7123                             	        vmovdqu	%xmm11, 48(%rsi)
 7124                             	        vaesenclast	%xmm7, %xmm12, %xmm12
 7125                             	        vaesenclast	%xmm7, %xmm13, %xmm13
 7126                             	        vmovdqu	64(%rdi), %xmm0
 7127                             	        vmovdqu	80(%rdi), %xmm1
 7128                             	        vpxor	%xmm0, %xmm12, %xmm12
 7129                             	        vpxor	%xmm1, %xmm13, %xmm13
 7130                             	        vmovdqu	%xmm12, 64(%rsi)
 7131                             	        vmovdqu	%xmm13, 80(%rsi)
 7132                             	        vaesenclast	%xmm7, %xmm14, %xmm14
 7133                             	        vaesenclast	%xmm7, %xmm15, %xmm15
 7134                             	        vmovdqu	96(%rdi), %xmm0
 7135                             	        vmovdqu	112(%rdi), %xmm1
 7136                             	        vpxor	%xmm0, %xmm14, %xmm14
 7137                             	        vpxor	%xmm1, %xmm15, %xmm15
 7138                             	        vmovdqu	%xmm14, 96(%rsi)
 7139                             	        vmovdqu	%xmm15, 112(%rsi)
 7140                             	        cmpl	$0x80, %r13d
 7141                             	        movl	$0x80, %ebx
 7142                             	        jle	L_AES_GCM_encrypt_avx1_end_128
 7143                             	        # More 128 bytes of input
 7144                             	L_AES_GCM_encrypt_avx1_ghash_128:
 7145                             	        leaq	(%rdi,%rbx,1), %rcx
 7146                             	        leaq	(%rsi,%rbx,1), %rdx
 7147                             	        vmovdqa	128(%rsp), %xmm0
 7148                             	        vmovdqa	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm1
 7149                             	        vpshufb	%xmm1, %xmm0, %xmm8
 7150                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm0, %xmm9
 7151                             	        vpshufb	%xmm1, %xmm9, %xmm9
 7152                             	        vpaddd	L_avx1_aes_gcm_two(%rip), %xmm0, %xmm10
 7153                             	        vpshufb	%xmm1, %xmm10, %xmm10
 7154                             	        vpaddd	L_avx1_aes_gcm_three(%rip), %xmm0, %xmm11
 7155                             	        vpshufb	%xmm1, %xmm11, %xmm11
 7156                             	        vpaddd	L_avx1_aes_gcm_four(%rip), %xmm0, %xmm12
 7157                             	        vpshufb	%xmm1, %xmm12, %xmm12
 7158                             	        vpaddd	L_avx1_aes_gcm_five(%rip), %xmm0, %xmm13
 7159                             	        vpshufb	%xmm1, %xmm13, %xmm13
 7160                             	        vpaddd	L_avx1_aes_gcm_six(%rip), %xmm0, %xmm14
 7161                             	        vpshufb	%xmm1, %xmm14, %xmm14
 7162                             	        vpaddd	L_avx1_aes_gcm_seven(%rip), %xmm0, %xmm15
 7163                             	        vpshufb	%xmm1, %xmm15, %xmm15
 7164                             	        vpaddd	L_avx1_aes_gcm_eight(%rip), %xmm0, %xmm0
 7165                             	        vmovdqa	(%r15), %xmm7
 7166                             	        vmovdqa	%xmm0, 128(%rsp)
 7167                             	        vpxor	%xmm7, %xmm8, %xmm8
 7168                             	        vpxor	%xmm7, %xmm9, %xmm9
 7169                             	        vpxor	%xmm7, %xmm10, %xmm10
 7170                             	        vpxor	%xmm7, %xmm11, %xmm11
 7171                             	        vpxor	%xmm7, %xmm12, %xmm12
 7172                             	        vpxor	%xmm7, %xmm13, %xmm13
 7173                             	        vpxor	%xmm7, %xmm14, %xmm14
 7174                             	        vpxor	%xmm7, %xmm15, %xmm15
 7175                             	        vmovdqa	112(%rsp), %xmm7
 7176                             	        vmovdqu	-128(%rdx), %xmm0
 7177                             	        vaesenc	16(%r15), %xmm8, %xmm8
 7178                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 7179                             	        vpxor	%xmm2, %xmm0, %xmm0
 7180                             	        vpshufd	$0x4e, %xmm7, %xmm1
 7181                             	        vpshufd	$0x4e, %xmm0, %xmm5
 7182                             	        vpxor	%xmm7, %xmm1, %xmm1
 7183                             	        vpxor	%xmm0, %xmm5, %xmm5
 7184                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm3
 7185                             	        vaesenc	16(%r15), %xmm9, %xmm9
 7186                             	        vaesenc	16(%r15), %xmm10, %xmm10
 7187                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm2
 7188                             	        vaesenc	16(%r15), %xmm11, %xmm11
 7189                             	        vaesenc	16(%r15), %xmm12, %xmm12
 7190                             	        vpclmulqdq	$0x00, %xmm5, %xmm1, %xmm1
 7191                             	        vaesenc	16(%r15), %xmm13, %xmm13
 7192                             	        vaesenc	16(%r15), %xmm14, %xmm14
 7193                             	        vaesenc	16(%r15), %xmm15, %xmm15
 7194                             	        vpxor	%xmm2, %xmm1, %xmm1
 7195                             	        vpxor	%xmm3, %xmm1, %xmm1
 7196                             	        vmovdqa	96(%rsp), %xmm7
 7197                             	        vmovdqu	-112(%rdx), %xmm0
 7198                             	        vpshufd	$0x4e, %xmm7, %xmm4
 7199                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 7200                             	        vaesenc	32(%r15), %xmm8, %xmm8
 7201                             	        vpxor	%xmm7, %xmm4, %xmm4
 7202                             	        vpshufd	$0x4e, %xmm0, %xmm5
 7203                             	        vpxor	%xmm0, %xmm5, %xmm5
 7204                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 7205                             	        vaesenc	32(%r15), %xmm9, %xmm9
 7206                             	        vaesenc	32(%r15), %xmm10, %xmm10
 7207                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 7208                             	        vaesenc	32(%r15), %xmm11, %xmm11
 7209                             	        vaesenc	32(%r15), %xmm12, %xmm12
 7210                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 7211                             	        vaesenc	32(%r15), %xmm13, %xmm13
 7212                             	        vaesenc	32(%r15), %xmm14, %xmm14
 7213                             	        vaesenc	32(%r15), %xmm15, %xmm15
 7214                             	        vpxor	%xmm7, %xmm1, %xmm1
 7215                             	        vpxor	%xmm7, %xmm2, %xmm2
 7216                             	        vpxor	%xmm6, %xmm1, %xmm1
 7217                             	        vpxor	%xmm6, %xmm3, %xmm3
 7218                             	        vpxor	%xmm4, %xmm1, %xmm1
 7219                             	        vmovdqa	80(%rsp), %xmm7
 7220                             	        vmovdqu	-96(%rdx), %xmm0
 7221                             	        vpshufd	$0x4e, %xmm7, %xmm4
 7222                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 7223                             	        vaesenc	48(%r15), %xmm8, %xmm8
 7224                             	        vpxor	%xmm7, %xmm4, %xmm4
 7225                             	        vpshufd	$0x4e, %xmm0, %xmm5
 7226                             	        vpxor	%xmm0, %xmm5, %xmm5
 7227                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 7228                             	        vaesenc	48(%r15), %xmm9, %xmm9
 7229                             	        vaesenc	48(%r15), %xmm10, %xmm10
 7230                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 7231                             	        vaesenc	48(%r15), %xmm11, %xmm11
 7232                             	        vaesenc	48(%r15), %xmm12, %xmm12
 7233                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 7234                             	        vaesenc	48(%r15), %xmm13, %xmm13
 7235                             	        vaesenc	48(%r15), %xmm14, %xmm14
 7236                             	        vaesenc	48(%r15), %xmm15, %xmm15
 7237                             	        vpxor	%xmm7, %xmm1, %xmm1
 7238                             	        vpxor	%xmm7, %xmm2, %xmm2
 7239                             	        vpxor	%xmm6, %xmm1, %xmm1
 7240                             	        vpxor	%xmm6, %xmm3, %xmm3
 7241                             	        vpxor	%xmm4, %xmm1, %xmm1
 7242                             	        vmovdqa	64(%rsp), %xmm7
 7243                             	        vmovdqu	-80(%rdx), %xmm0
 7244                             	        vpshufd	$0x4e, %xmm7, %xmm4
 7245                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 7246                             	        vaesenc	64(%r15), %xmm8, %xmm8
 7247                             	        vpxor	%xmm7, %xmm4, %xmm4
 7248                             	        vpshufd	$0x4e, %xmm0, %xmm5
 7249                             	        vpxor	%xmm0, %xmm5, %xmm5
 7250                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 7251                             	        vaesenc	64(%r15), %xmm9, %xmm9
 7252                             	        vaesenc	64(%r15), %xmm10, %xmm10
 7253                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 7254                             	        vaesenc	64(%r15), %xmm11, %xmm11
 7255                             	        vaesenc	64(%r15), %xmm12, %xmm12
 7256                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 7257                             	        vaesenc	64(%r15), %xmm13, %xmm13
 7258                             	        vaesenc	64(%r15), %xmm14, %xmm14
 7259                             	        vaesenc	64(%r15), %xmm15, %xmm15
 7260                             	        vpxor	%xmm7, %xmm1, %xmm1
 7261                             	        vpxor	%xmm7, %xmm2, %xmm2
 7262                             	        vpxor	%xmm6, %xmm1, %xmm1
 7263                             	        vpxor	%xmm6, %xmm3, %xmm3
 7264                             	        vpxor	%xmm4, %xmm1, %xmm1
 7265                             	        vmovdqa	48(%rsp), %xmm7
 7266                             	        vmovdqu	-64(%rdx), %xmm0
 7267                             	        vpshufd	$0x4e, %xmm7, %xmm4
 7268                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 7269                             	        vaesenc	80(%r15), %xmm8, %xmm8
 7270                             	        vpxor	%xmm7, %xmm4, %xmm4
 7271                             	        vpshufd	$0x4e, %xmm0, %xmm5
 7272                             	        vpxor	%xmm0, %xmm5, %xmm5
 7273                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 7274                             	        vaesenc	80(%r15), %xmm9, %xmm9
 7275                             	        vaesenc	80(%r15), %xmm10, %xmm10
 7276                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 7277                             	        vaesenc	80(%r15), %xmm11, %xmm11
 7278                             	        vaesenc	80(%r15), %xmm12, %xmm12
 7279                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 7280                             	        vaesenc	80(%r15), %xmm13, %xmm13
 7281                             	        vaesenc	80(%r15), %xmm14, %xmm14
 7282                             	        vaesenc	80(%r15), %xmm15, %xmm15
 7283                             	        vpxor	%xmm7, %xmm1, %xmm1
 7284                             	        vpxor	%xmm7, %xmm2, %xmm2
 7285                             	        vpxor	%xmm6, %xmm1, %xmm1
 7286                             	        vpxor	%xmm6, %xmm3, %xmm3
 7287                             	        vpxor	%xmm4, %xmm1, %xmm1
 7288                             	        vmovdqa	32(%rsp), %xmm7
 7289                             	        vmovdqu	-48(%rdx), %xmm0
 7290                             	        vpshufd	$0x4e, %xmm7, %xmm4
 7291                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 7292                             	        vaesenc	96(%r15), %xmm8, %xmm8
 7293                             	        vpxor	%xmm7, %xmm4, %xmm4
 7294                             	        vpshufd	$0x4e, %xmm0, %xmm5
 7295                             	        vpxor	%xmm0, %xmm5, %xmm5
 7296                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 7297                             	        vaesenc	96(%r15), %xmm9, %xmm9
 7298                             	        vaesenc	96(%r15), %xmm10, %xmm10
 7299                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 7300                             	        vaesenc	96(%r15), %xmm11, %xmm11
 7301                             	        vaesenc	96(%r15), %xmm12, %xmm12
 7302                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 7303                             	        vaesenc	96(%r15), %xmm13, %xmm13
 7304                             	        vaesenc	96(%r15), %xmm14, %xmm14
 7305                             	        vaesenc	96(%r15), %xmm15, %xmm15
 7306                             	        vpxor	%xmm7, %xmm1, %xmm1
 7307                             	        vpxor	%xmm7, %xmm2, %xmm2
 7308                             	        vpxor	%xmm6, %xmm1, %xmm1
 7309                             	        vpxor	%xmm6, %xmm3, %xmm3
 7310                             	        vpxor	%xmm4, %xmm1, %xmm1
 7311                             	        vmovdqa	16(%rsp), %xmm7
 7312                             	        vmovdqu	-32(%rdx), %xmm0
 7313                             	        vpshufd	$0x4e, %xmm7, %xmm4
 7314                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 7315                             	        vaesenc	112(%r15), %xmm8, %xmm8
 7316                             	        vpxor	%xmm7, %xmm4, %xmm4
 7317                             	        vpshufd	$0x4e, %xmm0, %xmm5
 7318                             	        vpxor	%xmm0, %xmm5, %xmm5
 7319                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 7320                             	        vaesenc	112(%r15), %xmm9, %xmm9
 7321                             	        vaesenc	112(%r15), %xmm10, %xmm10
 7322                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 7323                             	        vaesenc	112(%r15), %xmm11, %xmm11
 7324                             	        vaesenc	112(%r15), %xmm12, %xmm12
 7325                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 7326                             	        vaesenc	112(%r15), %xmm13, %xmm13
 7327                             	        vaesenc	112(%r15), %xmm14, %xmm14
 7328                             	        vaesenc	112(%r15), %xmm15, %xmm15
 7329                             	        vpxor	%xmm7, %xmm1, %xmm1
 7330                             	        vpxor	%xmm7, %xmm2, %xmm2
 7331                             	        vpxor	%xmm6, %xmm1, %xmm1
 7332                             	        vpxor	%xmm6, %xmm3, %xmm3
 7333                             	        vpxor	%xmm4, %xmm1, %xmm1
 7334                             	        vmovdqa	(%rsp), %xmm7
 7335                             	        vmovdqu	-16(%rdx), %xmm0
 7336                             	        vpshufd	$0x4e, %xmm7, %xmm4
 7337                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 7338                             	        vaesenc	128(%r15), %xmm8, %xmm8
 7339                             	        vpxor	%xmm7, %xmm4, %xmm4
 7340                             	        vpshufd	$0x4e, %xmm0, %xmm5
 7341                             	        vpxor	%xmm0, %xmm5, %xmm5
 7342                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 7343                             	        vaesenc	128(%r15), %xmm9, %xmm9
 7344                             	        vaesenc	128(%r15), %xmm10, %xmm10
 7345                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 7346                             	        vaesenc	128(%r15), %xmm11, %xmm11
 7347                             	        vaesenc	128(%r15), %xmm12, %xmm12
 7348                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 7349                             	        vaesenc	128(%r15), %xmm13, %xmm13
 7350                             	        vaesenc	128(%r15), %xmm14, %xmm14
 7351                             	        vaesenc	128(%r15), %xmm15, %xmm15
 7352                             	        vpxor	%xmm7, %xmm1, %xmm1
 7353                             	        vpxor	%xmm7, %xmm2, %xmm2
 7354                             	        vpxor	%xmm6, %xmm1, %xmm1
 7355                             	        vpxor	%xmm6, %xmm3, %xmm3
 7356                             	        vpxor	%xmm4, %xmm1, %xmm1
 7357                             	        vpslldq	$8, %xmm1, %xmm5
 7358                             	        vpsrldq	$8, %xmm1, %xmm1
 7359                             	        vaesenc	144(%r15), %xmm8, %xmm8
 7360                             	        vpxor	%xmm5, %xmm2, %xmm2
 7361                             	        vpxor	%xmm1, %xmm3, %xmm3
 7362                             	        vaesenc	144(%r15), %xmm9, %xmm9
 7363                             	        vpslld	$31, %xmm2, %xmm7
 7364                             	        vpslld	$30, %xmm2, %xmm4
 7365                             	        vpslld	$25, %xmm2, %xmm5
 7366                             	        vaesenc	144(%r15), %xmm10, %xmm10
 7367                             	        vpxor	%xmm4, %xmm7, %xmm7
 7368                             	        vpxor	%xmm5, %xmm7, %xmm7
 7369                             	        vaesenc	144(%r15), %xmm11, %xmm11
 7370                             	        vpsrldq	$4, %xmm7, %xmm4
 7371                             	        vpslldq	$12, %xmm7, %xmm7
 7372                             	        vaesenc	144(%r15), %xmm12, %xmm12
 7373                             	        vpxor	%xmm7, %xmm2, %xmm2
 7374                             	        vpsrld	$0x01, %xmm2, %xmm5
 7375                             	        vaesenc	144(%r15), %xmm13, %xmm13
 7376                             	        vpsrld	$2, %xmm2, %xmm1
 7377                             	        vpsrld	$7, %xmm2, %xmm0
 7378                             	        vaesenc	144(%r15), %xmm14, %xmm14
 7379                             	        vpxor	%xmm1, %xmm5, %xmm5
 7380                             	        vpxor	%xmm0, %xmm5, %xmm5
 7381                             	        vaesenc	144(%r15), %xmm15, %xmm15
 7382                             	        vpxor	%xmm4, %xmm5, %xmm5
 7383                             	        vpxor	%xmm5, %xmm2, %xmm2
 7384                             	        vpxor	%xmm3, %xmm2, %xmm2
 7385                             	        cmpl	$11, %r10d
 7386                             	        vmovdqa	160(%r15), %xmm7
 7387                             	        jl	L_AES_GCM_encrypt_avx1_aesenc_128_ghash_avx_done
 7388                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7389                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7390                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7391                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7392                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7393                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7394                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7395                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7396                             	        vmovdqa	176(%r15), %xmm7
 7397                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7398                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7399                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7400                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7401                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7402                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7403                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7404                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7405                             	        cmpl	$13, %r10d
 7406                             	        vmovdqa	192(%r15), %xmm7
 7407                             	        jl	L_AES_GCM_encrypt_avx1_aesenc_128_ghash_avx_done
 7408                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7409                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7410                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7411                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7412                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7413                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7414                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7415                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7416                             	        vmovdqa	208(%r15), %xmm7
 7417                             	        vaesenc	%xmm7, %xmm8, %xmm8
 7418                             	        vaesenc	%xmm7, %xmm9, %xmm9
 7419                             	        vaesenc	%xmm7, %xmm10, %xmm10
 7420                             	        vaesenc	%xmm7, %xmm11, %xmm11
 7421                             	        vaesenc	%xmm7, %xmm12, %xmm12
 7422                             	        vaesenc	%xmm7, %xmm13, %xmm13
 7423                             	        vaesenc	%xmm7, %xmm14, %xmm14
 7424                             	        vaesenc	%xmm7, %xmm15, %xmm15
 7425                             	        vmovdqa	224(%r15), %xmm7
 7426                             	L_AES_GCM_encrypt_avx1_aesenc_128_ghash_avx_done:
 7427                             	        vaesenclast	%xmm7, %xmm8, %xmm8
 7428                             	        vaesenclast	%xmm7, %xmm9, %xmm9
 7429                             	        vmovdqu	(%rcx), %xmm0
 7430                             	        vmovdqu	16(%rcx), %xmm1
 7431                             	        vpxor	%xmm0, %xmm8, %xmm8
 7432                             	        vpxor	%xmm1, %xmm9, %xmm9
 7433                             	        vmovdqu	%xmm8, (%rdx)
 7434                             	        vmovdqu	%xmm9, 16(%rdx)
 7435                             	        vaesenclast	%xmm7, %xmm10, %xmm10
 7436                             	        vaesenclast	%xmm7, %xmm11, %xmm11
 7437                             	        vmovdqu	32(%rcx), %xmm0
 7438                             	        vmovdqu	48(%rcx), %xmm1
 7439                             	        vpxor	%xmm0, %xmm10, %xmm10
 7440                             	        vpxor	%xmm1, %xmm11, %xmm11
 7441                             	        vmovdqu	%xmm10, 32(%rdx)
 7442                             	        vmovdqu	%xmm11, 48(%rdx)
 7443                             	        vaesenclast	%xmm7, %xmm12, %xmm12
 7444                             	        vaesenclast	%xmm7, %xmm13, %xmm13
 7445                             	        vmovdqu	64(%rcx), %xmm0
 7446                             	        vmovdqu	80(%rcx), %xmm1
 7447                             	        vpxor	%xmm0, %xmm12, %xmm12
 7448                             	        vpxor	%xmm1, %xmm13, %xmm13
 7449                             	        vmovdqu	%xmm12, 64(%rdx)
 7450                             	        vmovdqu	%xmm13, 80(%rdx)
 7451                             	        vaesenclast	%xmm7, %xmm14, %xmm14
 7452                             	        vaesenclast	%xmm7, %xmm15, %xmm15
 7453                             	        vmovdqu	96(%rcx), %xmm0
 7454                             	        vmovdqu	112(%rcx), %xmm1
 7455                             	        vpxor	%xmm0, %xmm14, %xmm14
 7456                             	        vpxor	%xmm1, %xmm15, %xmm15
 7457                             	        vmovdqu	%xmm14, 96(%rdx)
 7458                             	        vmovdqu	%xmm15, 112(%rdx)
 7459                             	        addl	$0x80, %ebx
 7460                             	        cmpl	%r13d, %ebx
 7461                             	        jl	L_AES_GCM_encrypt_avx1_ghash_128
 7462                             	L_AES_GCM_encrypt_avx1_end_128:
 7463                             	        vmovdqa	L_avx1_aes_gcm_bswap_mask(%rip), %xmm4
 7464                             	        vpshufb	%xmm4, %xmm8, %xmm8
 7465                             	        vpshufb	%xmm4, %xmm9, %xmm9
 7466                             	        vpshufb	%xmm4, %xmm10, %xmm10
 7467                             	        vpshufb	%xmm4, %xmm11, %xmm11
 7468                             	        vpxor	%xmm2, %xmm8, %xmm8
 7469                             	        vpshufb	%xmm4, %xmm12, %xmm12
 7470                             	        vpshufb	%xmm4, %xmm13, %xmm13
 7471                             	        vpshufb	%xmm4, %xmm14, %xmm14
 7472                             	        vpshufb	%xmm4, %xmm15, %xmm15
 7473                             	        vmovdqa	(%rsp), %xmm7
 7474                             	        vmovdqa	16(%rsp), %xmm5
 7475                             	        # ghash_gfmul_avx
 7476                             	        vpshufd	$0x4e, %xmm15, %xmm1
 7477                             	        vpshufd	$0x4e, %xmm7, %xmm2
 7478                             	        vpclmulqdq	$0x11, %xmm15, %xmm7, %xmm3
 7479                             	        vpclmulqdq	$0x00, %xmm15, %xmm7, %xmm0
 7480                             	        vpxor	%xmm15, %xmm1, %xmm1
 7481                             	        vpxor	%xmm7, %xmm2, %xmm2
 7482                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 7483                             	        vpxor	%xmm0, %xmm1, %xmm1
 7484                             	        vpxor	%xmm3, %xmm1, %xmm1
 7485                             	        vmovdqa	%xmm0, %xmm4
 7486                             	        vmovdqa	%xmm3, %xmm6
 7487                             	        vpslldq	$8, %xmm1, %xmm2
 7488                             	        vpsrldq	$8, %xmm1, %xmm1
 7489                             	        vpxor	%xmm2, %xmm4, %xmm4
 7490                             	        vpxor	%xmm1, %xmm6, %xmm6
 7491                             	        # ghash_gfmul_xor_avx
 7492                             	        vpshufd	$0x4e, %xmm14, %xmm1
 7493                             	        vpshufd	$0x4e, %xmm5, %xmm2
 7494                             	        vpclmulqdq	$0x11, %xmm14, %xmm5, %xmm3
 7495                             	        vpclmulqdq	$0x00, %xmm14, %xmm5, %xmm0
 7496                             	        vpxor	%xmm14, %xmm1, %xmm1
 7497                             	        vpxor	%xmm5, %xmm2, %xmm2
 7498                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 7499                             	        vpxor	%xmm0, %xmm1, %xmm1
 7500                             	        vpxor	%xmm3, %xmm1, %xmm1
 7501                             	        vpxor	%xmm0, %xmm4, %xmm4
 7502                             	        vpxor	%xmm3, %xmm6, %xmm6
 7503                             	        vpslldq	$8, %xmm1, %xmm2
 7504                             	        vpsrldq	$8, %xmm1, %xmm1
 7505                             	        vpxor	%xmm2, %xmm4, %xmm4
 7506                             	        vpxor	%xmm1, %xmm6, %xmm6
 7507                             	        vmovdqa	32(%rsp), %xmm7
 7508                             	        vmovdqa	48(%rsp), %xmm5
 7509                             	        # ghash_gfmul_xor_avx
 7510                             	        vpshufd	$0x4e, %xmm13, %xmm1
 7511                             	        vpshufd	$0x4e, %xmm7, %xmm2
 7512                             	        vpclmulqdq	$0x11, %xmm13, %xmm7, %xmm3
 7513                             	        vpclmulqdq	$0x00, %xmm13, %xmm7, %xmm0
 7514                             	        vpxor	%xmm13, %xmm1, %xmm1
 7515                             	        vpxor	%xmm7, %xmm2, %xmm2
 7516                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 7517                             	        vpxor	%xmm0, %xmm1, %xmm1
 7518                             	        vpxor	%xmm3, %xmm1, %xmm1
 7519                             	        vpxor	%xmm0, %xmm4, %xmm4
 7520                             	        vpxor	%xmm3, %xmm6, %xmm6
 7521                             	        vpslldq	$8, %xmm1, %xmm2
 7522                             	        vpsrldq	$8, %xmm1, %xmm1
 7523                             	        vpxor	%xmm2, %xmm4, %xmm4
 7524                             	        vpxor	%xmm1, %xmm6, %xmm6
 7525                             	        # ghash_gfmul_xor_avx
 7526                             	        vpshufd	$0x4e, %xmm12, %xmm1
 7527                             	        vpshufd	$0x4e, %xmm5, %xmm2
 7528                             	        vpclmulqdq	$0x11, %xmm12, %xmm5, %xmm3
 7529                             	        vpclmulqdq	$0x00, %xmm12, %xmm5, %xmm0
 7530                             	        vpxor	%xmm12, %xmm1, %xmm1
 7531                             	        vpxor	%xmm5, %xmm2, %xmm2
 7532                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 7533                             	        vpxor	%xmm0, %xmm1, %xmm1
 7534                             	        vpxor	%xmm3, %xmm1, %xmm1
 7535                             	        vpxor	%xmm0, %xmm4, %xmm4
 7536                             	        vpxor	%xmm3, %xmm6, %xmm6
 7537                             	        vpslldq	$8, %xmm1, %xmm2
 7538                             	        vpsrldq	$8, %xmm1, %xmm1
 7539                             	        vpxor	%xmm2, %xmm4, %xmm4
 7540                             	        vpxor	%xmm1, %xmm6, %xmm6
 7541                             	        vmovdqa	64(%rsp), %xmm7
 7542                             	        vmovdqa	80(%rsp), %xmm5
 7543                             	        # ghash_gfmul_xor_avx
 7544                             	        vpshufd	$0x4e, %xmm11, %xmm1
 7545                             	        vpshufd	$0x4e, %xmm7, %xmm2
 7546                             	        vpclmulqdq	$0x11, %xmm11, %xmm7, %xmm3
 7547                             	        vpclmulqdq	$0x00, %xmm11, %xmm7, %xmm0
 7548                             	        vpxor	%xmm11, %xmm1, %xmm1
 7549                             	        vpxor	%xmm7, %xmm2, %xmm2
 7550                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 7551                             	        vpxor	%xmm0, %xmm1, %xmm1
 7552                             	        vpxor	%xmm3, %xmm1, %xmm1
 7553                             	        vpxor	%xmm0, %xmm4, %xmm4
 7554                             	        vpxor	%xmm3, %xmm6, %xmm6
 7555                             	        vpslldq	$8, %xmm1, %xmm2
 7556                             	        vpsrldq	$8, %xmm1, %xmm1
 7557                             	        vpxor	%xmm2, %xmm4, %xmm4
 7558                             	        vpxor	%xmm1, %xmm6, %xmm6
 7559                             	        # ghash_gfmul_xor_avx
 7560                             	        vpshufd	$0x4e, %xmm10, %xmm1
 7561                             	        vpshufd	$0x4e, %xmm5, %xmm2
 7562                             	        vpclmulqdq	$0x11, %xmm10, %xmm5, %xmm3
 7563                             	        vpclmulqdq	$0x00, %xmm10, %xmm5, %xmm0
 7564                             	        vpxor	%xmm10, %xmm1, %xmm1
 7565                             	        vpxor	%xmm5, %xmm2, %xmm2
 7566                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 7567                             	        vpxor	%xmm0, %xmm1, %xmm1
 7568                             	        vpxor	%xmm3, %xmm1, %xmm1
 7569                             	        vpxor	%xmm0, %xmm4, %xmm4
 7570                             	        vpxor	%xmm3, %xmm6, %xmm6
 7571                             	        vpslldq	$8, %xmm1, %xmm2
 7572                             	        vpsrldq	$8, %xmm1, %xmm1
 7573                             	        vpxor	%xmm2, %xmm4, %xmm4
 7574                             	        vpxor	%xmm1, %xmm6, %xmm6
 7575                             	        vmovdqa	96(%rsp), %xmm7
 7576                             	        vmovdqa	112(%rsp), %xmm5
 7577                             	        # ghash_gfmul_xor_avx
 7578                             	        vpshufd	$0x4e, %xmm9, %xmm1
 7579                             	        vpshufd	$0x4e, %xmm7, %xmm2
 7580                             	        vpclmulqdq	$0x11, %xmm9, %xmm7, %xmm3
 7581                             	        vpclmulqdq	$0x00, %xmm9, %xmm7, %xmm0
 7582                             	        vpxor	%xmm9, %xmm1, %xmm1
 7583                             	        vpxor	%xmm7, %xmm2, %xmm2
 7584                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 7585                             	        vpxor	%xmm0, %xmm1, %xmm1
 7586                             	        vpxor	%xmm3, %xmm1, %xmm1
 7587                             	        vpxor	%xmm0, %xmm4, %xmm4
 7588                             	        vpxor	%xmm3, %xmm6, %xmm6
 7589                             	        vpslldq	$8, %xmm1, %xmm2
 7590                             	        vpsrldq	$8, %xmm1, %xmm1
 7591                             	        vpxor	%xmm2, %xmm4, %xmm4
 7592                             	        vpxor	%xmm1, %xmm6, %xmm6
 7593                             	        # ghash_gfmul_xor_avx
 7594                             	        vpshufd	$0x4e, %xmm8, %xmm1
 7595                             	        vpshufd	$0x4e, %xmm5, %xmm2
 7596                             	        vpclmulqdq	$0x11, %xmm8, %xmm5, %xmm3
 7597                             	        vpclmulqdq	$0x00, %xmm8, %xmm5, %xmm0
 7598                             	        vpxor	%xmm8, %xmm1, %xmm1
 7599                             	        vpxor	%xmm5, %xmm2, %xmm2
 7600                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 7601                             	        vpxor	%xmm0, %xmm1, %xmm1
 7602                             	        vpxor	%xmm3, %xmm1, %xmm1
 7603                             	        vpxor	%xmm0, %xmm4, %xmm4
 7604                             	        vpxor	%xmm3, %xmm6, %xmm6
 7605                             	        vpslldq	$8, %xmm1, %xmm2
 7606                             	        vpsrldq	$8, %xmm1, %xmm1
 7607                             	        vpxor	%xmm2, %xmm4, %xmm4
 7608                             	        vpxor	%xmm1, %xmm6, %xmm6
 7609                             	        vpslld	$31, %xmm4, %xmm0
 7610                             	        vpslld	$30, %xmm4, %xmm1
 7611                             	        vpslld	$25, %xmm4, %xmm2
 7612                             	        vpxor	%xmm1, %xmm0, %xmm0
 7613                             	        vpxor	%xmm2, %xmm0, %xmm0
 7614                             	        vmovdqa	%xmm0, %xmm1
 7615                             	        vpsrldq	$4, %xmm1, %xmm1
 7616                             	        vpslldq	$12, %xmm0, %xmm0
 7617                             	        vpxor	%xmm0, %xmm4, %xmm4
 7618                             	        vpsrld	$0x01, %xmm4, %xmm2
 7619                             	        vpsrld	$2, %xmm4, %xmm3
 7620                             	        vpsrld	$7, %xmm4, %xmm0
 7621                             	        vpxor	%xmm3, %xmm2, %xmm2
 7622                             	        vpxor	%xmm0, %xmm2, %xmm2
 7623                             	        vpxor	%xmm1, %xmm2, %xmm2
 7624                             	        vpxor	%xmm4, %xmm2, %xmm2
 7625                             	        vpxor	%xmm2, %xmm6, %xmm6
 7626                             	        vmovdqa	(%rsp), %xmm5
 7627                             	L_AES_GCM_encrypt_avx1_done_128:
 7628                             	        movl	%r9d, %edx
 7629                             	        cmpl	%edx, %ebx
 7630                             	        jge	L_AES_GCM_encrypt_avx1_done_enc
 7631                             	        movl	%r9d, %r13d
 7632                             	        andl	$0xfffffff0, %r13d
 7633                             	        cmpl	%r13d, %ebx
 7634                             	        jge	L_AES_GCM_encrypt_avx1_last_block_done
 7635                             	        vmovdqa	128(%rsp), %xmm9
 7636                             	        vpshufb	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm9, %xmm8
 7637                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm9, %xmm9
 7638                             	        vmovdqa	%xmm9, 128(%rsp)
 7639                             	        vpxor	(%r15), %xmm8, %xmm8
 7640                             	        vaesenc	16(%r15), %xmm8, %xmm8
 7641                             	        vaesenc	32(%r15), %xmm8, %xmm8
 7642                             	        vaesenc	48(%r15), %xmm8, %xmm8
 7643                             	        vaesenc	64(%r15), %xmm8, %xmm8
 7644                             	        vaesenc	80(%r15), %xmm8, %xmm8
 7645                             	        vaesenc	96(%r15), %xmm8, %xmm8
 7646                             	        vaesenc	112(%r15), %xmm8, %xmm8
 7647                             	        vaesenc	128(%r15), %xmm8, %xmm8
 7648                             	        vaesenc	144(%r15), %xmm8, %xmm8
 7649                             	        cmpl	$11, %r10d
 7650                             	        vmovdqa	160(%r15), %xmm9
 7651                             	        jl	L_AES_GCM_encrypt_avx1_aesenc_block_last
 7652                             	        vaesenc	%xmm9, %xmm8, %xmm8
 7653                             	        vaesenc	176(%r15), %xmm8, %xmm8
 7654                             	        cmpl	$13, %r10d
 7655                             	        vmovdqa	192(%r15), %xmm9
 7656                             	        jl	L_AES_GCM_encrypt_avx1_aesenc_block_last
 7657                             	        vaesenc	%xmm9, %xmm8, %xmm8
 7658                             	        vaesenc	208(%r15), %xmm8, %xmm8
 7659                             	        vmovdqa	224(%r15), %xmm9
 7660                             	L_AES_GCM_encrypt_avx1_aesenc_block_last:
 7661                             	        vaesenclast	%xmm9, %xmm8, %xmm8
 7662                             	        vmovdqu	(%rdi,%rbx,1), %xmm9
 7663                             	        vpxor	%xmm9, %xmm8, %xmm8
 7664                             	        vmovdqu	%xmm8, (%rsi,%rbx,1)
 7665                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 7666                             	        vpxor	%xmm8, %xmm6, %xmm6
 7667                             	        addl	$16, %ebx
 7668                             	        cmpl	%r13d, %ebx
 7669                             	        jge	L_AES_GCM_encrypt_avx1_last_block_ghash
 7670                             	L_AES_GCM_encrypt_avx1_last_block_start:
 7671                             	        vmovdqu	(%rdi,%rbx,1), %xmm13
 7672                             	        vmovdqa	128(%rsp), %xmm9
 7673                             	        vpshufb	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm9, %xmm8
 7674                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm9, %xmm9
 7675                             	        vmovdqa	%xmm9, 128(%rsp)
 7676                             	        vpxor	(%r15), %xmm8, %xmm8
 7677                             	        vpclmulqdq	$16, %xmm5, %xmm6, %xmm10
 7678                             	        vaesenc	16(%r15), %xmm8, %xmm8
 7679                             	        vaesenc	32(%r15), %xmm8, %xmm8
 7680                             	        vpclmulqdq	$0x01, %xmm5, %xmm6, %xmm11
 7681                             	        vaesenc	48(%r15), %xmm8, %xmm8
 7682                             	        vaesenc	64(%r15), %xmm8, %xmm8
 7683                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm12
 7684                             	        vaesenc	80(%r15), %xmm8, %xmm8
 7685                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm1
 7686                             	        vaesenc	96(%r15), %xmm8, %xmm8
 7687                             	        vpxor	%xmm11, %xmm10, %xmm10
 7688                             	        vpslldq	$8, %xmm10, %xmm2
 7689                             	        vpsrldq	$8, %xmm10, %xmm10
 7690                             	        vaesenc	112(%r15), %xmm8, %xmm8
 7691                             	        vpxor	%xmm12, %xmm2, %xmm2
 7692                             	        vpxor	%xmm10, %xmm1, %xmm3
 7693                             	        vmovdqa	L_avx1_aes_gcm_mod2_128(%rip), %xmm0
 7694                             	        vpclmulqdq	$16, %xmm0, %xmm2, %xmm11
 7695                             	        vaesenc	128(%r15), %xmm8, %xmm8
 7696                             	        vpshufd	$0x4e, %xmm2, %xmm10
 7697                             	        vpxor	%xmm11, %xmm10, %xmm10
 7698                             	        vpclmulqdq	$16, %xmm0, %xmm10, %xmm11
 7699                             	        vaesenc	144(%r15), %xmm8, %xmm8
 7700                             	        vpshufd	$0x4e, %xmm10, %xmm10
 7701                             	        vpxor	%xmm11, %xmm10, %xmm10
 7702                             	        vpxor	%xmm3, %xmm10, %xmm6
 7703                             	        cmpl	$11, %r10d
 7704                             	        vmovdqa	160(%r15), %xmm9
 7705                             	        jl	L_AES_GCM_encrypt_avx1_aesenc_gfmul_last
 7706                             	        vaesenc	%xmm9, %xmm8, %xmm8
 7707                             	        vaesenc	176(%r15), %xmm8, %xmm8
 7708                             	        cmpl	$13, %r10d
 7709                             	        vmovdqa	192(%r15), %xmm9
 7710                             	        jl	L_AES_GCM_encrypt_avx1_aesenc_gfmul_last
 7711                             	        vaesenc	%xmm9, %xmm8, %xmm8
 7712                             	        vaesenc	208(%r15), %xmm8, %xmm8
 7713                             	        vmovdqa	224(%r15), %xmm9
 7714                             	L_AES_GCM_encrypt_avx1_aesenc_gfmul_last:
 7715                             	        vaesenclast	%xmm9, %xmm8, %xmm8
 7716                             	        vmovdqa	%xmm13, %xmm0
 7717                             	        vpxor	%xmm0, %xmm8, %xmm8
 7718                             	        vmovdqu	%xmm8, (%rsi,%rbx,1)
 7719                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 7720                             	        addl	$16, %ebx
 7721                             	        vpxor	%xmm8, %xmm6, %xmm6
 7722                             	        cmpl	%r13d, %ebx
 7723                             	        jl	L_AES_GCM_encrypt_avx1_last_block_start
 7724                             	L_AES_GCM_encrypt_avx1_last_block_ghash:
 7725                             	        # ghash_gfmul_red_avx
 7726                             	        vpshufd	$0x4e, %xmm5, %xmm9
 7727                             	        vpshufd	$0x4e, %xmm6, %xmm10
 7728                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm11
 7729                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm8
 7730                             	        vpxor	%xmm5, %xmm9, %xmm9
 7731                             	        vpxor	%xmm6, %xmm10, %xmm10
 7732                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 7733                             	        vpxor	%xmm8, %xmm9, %xmm9
 7734                             	        vpxor	%xmm11, %xmm9, %xmm9
 7735                             	        vpslldq	$8, %xmm9, %xmm10
 7736                             	        vpsrldq	$8, %xmm9, %xmm9
 7737                             	        vpxor	%xmm10, %xmm8, %xmm8
 7738                             	        vpxor	%xmm9, %xmm11, %xmm6
 7739                             	        vpslld	$31, %xmm8, %xmm12
 7740                             	        vpslld	$30, %xmm8, %xmm13
 7741                             	        vpslld	$25, %xmm8, %xmm14
 7742                             	        vpxor	%xmm13, %xmm12, %xmm12
 7743                             	        vpxor	%xmm14, %xmm12, %xmm12
 7744                             	        vpsrldq	$4, %xmm12, %xmm13
 7745                             	        vpslldq	$12, %xmm12, %xmm12
 7746                             	        vpxor	%xmm12, %xmm8, %xmm8
 7747                             	        vpsrld	$0x01, %xmm8, %xmm14
 7748                             	        vpsrld	$2, %xmm8, %xmm10
 7749                             	        vpsrld	$7, %xmm8, %xmm9
 7750                             	        vpxor	%xmm10, %xmm14, %xmm14
 7751                             	        vpxor	%xmm9, %xmm14, %xmm14
 7752                             	        vpxor	%xmm13, %xmm14, %xmm14
 7753                             	        vpxor	%xmm8, %xmm14, %xmm14
 7754                             	        vpxor	%xmm14, %xmm6, %xmm6
 7755                             	L_AES_GCM_encrypt_avx1_last_block_done:
 7756                             	        movl	%r9d, %ecx
 7757                             	        movl	%ecx, %edx
 7758                             	        andl	$15, %ecx
 7759                             	        jz	L_AES_GCM_encrypt_avx1_aesenc_last15_enc_avx_done
 7760                             	        vmovdqa	128(%rsp), %xmm4
 7761                             	        vpshufb	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm4, %xmm4
 7762                             	        vpxor	(%r15), %xmm4, %xmm4
 7763                             	        vaesenc	16(%r15), %xmm4, %xmm4
 7764                             	        vaesenc	32(%r15), %xmm4, %xmm4
 7765                             	        vaesenc	48(%r15), %xmm4, %xmm4
 7766                             	        vaesenc	64(%r15), %xmm4, %xmm4
 7767                             	        vaesenc	80(%r15), %xmm4, %xmm4
 7768                             	        vaesenc	96(%r15), %xmm4, %xmm4
 7769                             	        vaesenc	112(%r15), %xmm4, %xmm4
 7770                             	        vaesenc	128(%r15), %xmm4, %xmm4
 7771                             	        vaesenc	144(%r15), %xmm4, %xmm4
 7772                             	        cmpl	$11, %r10d
 7773                             	        vmovdqa	160(%r15), %xmm9
 7774                             	        jl	L_AES_GCM_encrypt_avx1_aesenc_last15_enc_avx_aesenc_avx_last
 7775                             	        vaesenc	%xmm9, %xmm4, %xmm4
 7776                             	        vaesenc	176(%r15), %xmm4, %xmm4
 7777                             	        cmpl	$13, %r10d
 7778                             	        vmovdqa	192(%r15), %xmm9
 7779                             	        jl	L_AES_GCM_encrypt_avx1_aesenc_last15_enc_avx_aesenc_avx_last
 7780                             	        vaesenc	%xmm9, %xmm4, %xmm4
 7781                             	        vaesenc	208(%r15), %xmm4, %xmm4
 7782                             	        vmovdqa	224(%r15), %xmm9
 7783                             	L_AES_GCM_encrypt_avx1_aesenc_last15_enc_avx_aesenc_avx_last:
 7784                             	        vaesenclast	%xmm9, %xmm4, %xmm4
 7785                             	        subq	$16, %rsp
 7786                             	        xorl	%ecx, %ecx
 7787                             	        vmovdqu	%xmm4, (%rsp)
 7788                             	L_AES_GCM_encrypt_avx1_aesenc_last15_enc_avx_loop:
 7789                             	        movzbl	(%rdi,%rbx,1), %r13d
 7790                             	        xorb	(%rsp,%rcx,1), %r13b
 7791                             	        movb	%r13b, (%rsi,%rbx,1)
 7792                             	        movb	%r13b, (%rsp,%rcx,1)
 7793                             	        incl	%ebx
 7794                             	        incl	%ecx
 7795                             	        cmpl	%edx, %ebx
 7796                             	        jl	L_AES_GCM_encrypt_avx1_aesenc_last15_enc_avx_loop
 7797                             	        xorq	%r13, %r13
 7798                             	        cmpl	$16, %ecx
 7799                             	        je	L_AES_GCM_encrypt_avx1_aesenc_last15_enc_avx_finish_enc
 7800                             	L_AES_GCM_encrypt_avx1_aesenc_last15_enc_avx_byte_loop:
 7801                             	        movb	%r13b, (%rsp,%rcx,1)
 7802                             	        incl	%ecx
 7803                             	        cmpl	$16, %ecx
 7804                             	        jl	L_AES_GCM_encrypt_avx1_aesenc_last15_enc_avx_byte_loop
 7805                             	L_AES_GCM_encrypt_avx1_aesenc_last15_enc_avx_finish_enc:
 7806                             	        vmovdqu	(%rsp), %xmm4
 7807                             	        addq	$16, %rsp
 7808                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm4, %xmm4
 7809                             	        vpxor	%xmm4, %xmm6, %xmm6
 7810                             	        # ghash_gfmul_red_avx
 7811                             	        vpshufd	$0x4e, %xmm5, %xmm9
 7812                             	        vpshufd	$0x4e, %xmm6, %xmm10
 7813                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm11
 7814                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm8
 7815                             	        vpxor	%xmm5, %xmm9, %xmm9
 7816                             	        vpxor	%xmm6, %xmm10, %xmm10
 7817                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 7818                             	        vpxor	%xmm8, %xmm9, %xmm9
 7819                             	        vpxor	%xmm11, %xmm9, %xmm9
 7820                             	        vpslldq	$8, %xmm9, %xmm10
 7821                             	        vpsrldq	$8, %xmm9, %xmm9
 7822                             	        vpxor	%xmm10, %xmm8, %xmm8
 7823                             	        vpxor	%xmm9, %xmm11, %xmm6
 7824                             	        vpslld	$31, %xmm8, %xmm12
 7825                             	        vpslld	$30, %xmm8, %xmm13
 7826                             	        vpslld	$25, %xmm8, %xmm14
 7827                             	        vpxor	%xmm13, %xmm12, %xmm12
 7828                             	        vpxor	%xmm14, %xmm12, %xmm12
 7829                             	        vpsrldq	$4, %xmm12, %xmm13
 7830                             	        vpslldq	$12, %xmm12, %xmm12
 7831                             	        vpxor	%xmm12, %xmm8, %xmm8
 7832                             	        vpsrld	$0x01, %xmm8, %xmm14
 7833                             	        vpsrld	$2, %xmm8, %xmm10
 7834                             	        vpsrld	$7, %xmm8, %xmm9
 7835                             	        vpxor	%xmm10, %xmm14, %xmm14
 7836                             	        vpxor	%xmm9, %xmm14, %xmm14
 7837                             	        vpxor	%xmm13, %xmm14, %xmm14
 7838                             	        vpxor	%xmm8, %xmm14, %xmm14
 7839                             	        vpxor	%xmm14, %xmm6, %xmm6
 7840                             	L_AES_GCM_encrypt_avx1_aesenc_last15_enc_avx_done:
 7841                             	L_AES_GCM_encrypt_avx1_done_enc:
 7842                             	        movl	%r9d, %edx
 7843                             	        movl	%r11d, %ecx
 7844                             	        shlq	$3, %rdx
 7845                             	        shlq	$3, %rcx
 7846                             	        vpinsrq	$0x00, %rdx, %xmm0, %xmm0
 7847                             	        vpinsrq	$0x01, %rcx, %xmm0, %xmm0
 7848                             	        vpxor	%xmm0, %xmm6, %xmm6
 7849                             	        # ghash_gfmul_red_avx
 7850                             	        vpshufd	$0x4e, %xmm5, %xmm9
 7851                             	        vpshufd	$0x4e, %xmm6, %xmm10
 7852                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm11
 7853                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm8
 7854                             	        vpxor	%xmm5, %xmm9, %xmm9
 7855                             	        vpxor	%xmm6, %xmm10, %xmm10
 7856                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 7857                             	        vpxor	%xmm8, %xmm9, %xmm9
 7858                             	        vpxor	%xmm11, %xmm9, %xmm9
 7859                             	        vpslldq	$8, %xmm9, %xmm10
 7860                             	        vpsrldq	$8, %xmm9, %xmm9
 7861                             	        vpxor	%xmm10, %xmm8, %xmm8
 7862                             	        vpxor	%xmm9, %xmm11, %xmm6
 7863                             	        vpslld	$31, %xmm8, %xmm12
 7864                             	        vpslld	$30, %xmm8, %xmm13
 7865                             	        vpslld	$25, %xmm8, %xmm14
 7866                             	        vpxor	%xmm13, %xmm12, %xmm12
 7867                             	        vpxor	%xmm14, %xmm12, %xmm12
 7868                             	        vpsrldq	$4, %xmm12, %xmm13
 7869                             	        vpslldq	$12, %xmm12, %xmm12
 7870                             	        vpxor	%xmm12, %xmm8, %xmm8
 7871                             	        vpsrld	$0x01, %xmm8, %xmm14
 7872                             	        vpsrld	$2, %xmm8, %xmm10
 7873                             	        vpsrld	$7, %xmm8, %xmm9
 7874                             	        vpxor	%xmm10, %xmm14, %xmm14
 7875                             	        vpxor	%xmm9, %xmm14, %xmm14
 7876                             	        vpxor	%xmm13, %xmm14, %xmm14
 7877                             	        vpxor	%xmm8, %xmm14, %xmm14
 7878                             	        vpxor	%xmm14, %xmm6, %xmm6
 7879                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm6, %xmm6
 7880                             	        vpxor	144(%rsp), %xmm6, %xmm0
 7881                             	        cmpl	$16, %r14d
 7882                             	        je	L_AES_GCM_encrypt_avx1_store_tag_16
 7883                             	        xorq	%rcx, %rcx
 7884                             	        vmovdqu	%xmm0, (%rsp)
 7885                             	L_AES_GCM_encrypt_avx1_store_tag_loop:
 7886                             	        movzbl	(%rsp,%rcx,1), %r13d
 7887                             	        movb	%r13b, (%r8,%rcx,1)
 7888                             	        incl	%ecx
 7889                             	        cmpl	%r14d, %ecx
 7890                             	        jne	L_AES_GCM_encrypt_avx1_store_tag_loop
 7891                             	        jmp	L_AES_GCM_encrypt_avx1_store_tag_done
 7892                             	L_AES_GCM_encrypt_avx1_store_tag_16:
 7893                             	        vmovdqu	%xmm0, (%r8)
 7894                             	L_AES_GCM_encrypt_avx1_store_tag_done:
 7895                             	        vzeroupper
 7896                             	        addq	$0xa0, %rsp
 7897                             	        popq	%r15
 7898                             	        popq	%r14
 7899                             	        popq	%rbx
 7900                             	        popq	%r12
 7901                             	        popq	%r13
 7902                             	        repz retq
 7903                             	#ifndef __APPLE__
 7905                             	#endif /* __APPLE__ */
 7906                             	#ifndef __APPLE__
 7907                             	.text
 7908                             	.globl	AES_GCM_decrypt_avx1
 7910                             	.align	16
 7911                             	AES_GCM_decrypt_avx1:
 7912                             	#else
 7913                             	.section	__TEXT,__text
 7914                             	.globl	_AES_GCM_decrypt_avx1
 7915                             	.p2align	4
 7916                             	_AES_GCM_decrypt_avx1:
 7917                             	#endif /* __APPLE__ */
 7918                             	        pushq	%r13
 7919                             	        pushq	%r12
 7920                             	        pushq	%rbx
 7921                             	        pushq	%r14
 7922                             	        pushq	%r15
 7923                             	        pushq	%rbp
 7924                             	        movq	%rdx, %r12
 7925                             	        movq	%rcx, %rax
 7926                             	        movl	56(%rsp), %r11d
 7927                             	        movl	64(%rsp), %ebx
 7928                             	        movl	72(%rsp), %r14d
 7929                             	        movq	80(%rsp), %r15
 7930                             	        movl	88(%rsp), %r10d
 7931                             	        movq	96(%rsp), %rbp
 7932                             	        subq	$0xa8, %rsp
 7933                             	        vpxor	%xmm4, %xmm4, %xmm4
 7934                             	        vpxor	%xmm6, %xmm6, %xmm6
 7935                             	        cmpl	$12, %ebx
 7936                             	        movl	%ebx, %edx
 7937                             	        jne	L_AES_GCM_decrypt_avx1_iv_not_12
 7938                             	        # # Calculate values when IV is 12 bytes
 7939                             	        # Set counter based on IV
 7940                             	        movl	$0x1000000, %ecx
 7941                             	        vpinsrq	$0x00, (%rax), %xmm4, %xmm4
 7942                             	        vpinsrd	$2, 8(%rax), %xmm4, %xmm4
 7943                             	        vpinsrd	$3, %ecx, %xmm4, %xmm4
 7944                             	        # H = Encrypt X(=0) and T = Encrypt counter
 7945                             	        vmovdqa	(%r15), %xmm5
 7946                             	        vpxor	%xmm5, %xmm4, %xmm1
 7947                             	        vmovdqa	16(%r15), %xmm7
 7948                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7949                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7950                             	        vmovdqa	32(%r15), %xmm7
 7951                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7952                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7953                             	        vmovdqa	48(%r15), %xmm7
 7954                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7955                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7956                             	        vmovdqa	64(%r15), %xmm7
 7957                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7958                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7959                             	        vmovdqa	80(%r15), %xmm7
 7960                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7961                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7962                             	        vmovdqa	96(%r15), %xmm7
 7963                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7964                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7965                             	        vmovdqa	112(%r15), %xmm7
 7966                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7967                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7968                             	        vmovdqa	128(%r15), %xmm7
 7969                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7970                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7971                             	        vmovdqa	144(%r15), %xmm7
 7972                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7973                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7974                             	        cmpl	$11, %r10d
 7975                             	        vmovdqa	160(%r15), %xmm7
 7976                             	        jl	L_AES_GCM_decrypt_avx1_calc_iv_12_last
 7977                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7978                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7979                             	        vmovdqa	176(%r15), %xmm7
 7980                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7981                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7982                             	        cmpl	$13, %r10d
 7983                             	        vmovdqa	192(%r15), %xmm7
 7984                             	        jl	L_AES_GCM_decrypt_avx1_calc_iv_12_last
 7985                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7986                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7987                             	        vmovdqa	208(%r15), %xmm7
 7988                             	        vaesenc	%xmm7, %xmm5, %xmm5
 7989                             	        vaesenc	%xmm7, %xmm1, %xmm1
 7990                             	        vmovdqa	224(%r15), %xmm7
 7991                             	L_AES_GCM_decrypt_avx1_calc_iv_12_last:
 7992                             	        vaesenclast	%xmm7, %xmm5, %xmm5
 7993                             	        vaesenclast	%xmm7, %xmm1, %xmm1
 7994                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm5, %xmm5
 7995                             	        vmovdqa	%xmm1, 144(%rsp)
 7996                             	        jmp	L_AES_GCM_decrypt_avx1_iv_done
 7997                             	L_AES_GCM_decrypt_avx1_iv_not_12:
 7998                             	        # Calculate values when IV is not 12 bytes
 7999                             	        # H = Encrypt X(=0)
 8000                             	        vmovdqa	(%r15), %xmm5
 8001                             	        vaesenc	16(%r15), %xmm5, %xmm5
 8002                             	        vaesenc	32(%r15), %xmm5, %xmm5
 8003                             	        vaesenc	48(%r15), %xmm5, %xmm5
 8004                             	        vaesenc	64(%r15), %xmm5, %xmm5
 8005                             	        vaesenc	80(%r15), %xmm5, %xmm5
 8006                             	        vaesenc	96(%r15), %xmm5, %xmm5
 8007                             	        vaesenc	112(%r15), %xmm5, %xmm5
 8008                             	        vaesenc	128(%r15), %xmm5, %xmm5
 8009                             	        vaesenc	144(%r15), %xmm5, %xmm5
 8010                             	        cmpl	$11, %r10d
 8011                             	        vmovdqa	160(%r15), %xmm9
 8012                             	        jl	L_AES_GCM_decrypt_avx1_calc_iv_1_aesenc_avx_last
 8013                             	        vaesenc	%xmm9, %xmm5, %xmm5
 8014                             	        vaesenc	176(%r15), %xmm5, %xmm5
 8015                             	        cmpl	$13, %r10d
 8016                             	        vmovdqa	192(%r15), %xmm9
 8017                             	        jl	L_AES_GCM_decrypt_avx1_calc_iv_1_aesenc_avx_last
 8018                             	        vaesenc	%xmm9, %xmm5, %xmm5
 8019                             	        vaesenc	208(%r15), %xmm5, %xmm5
 8020                             	        vmovdqa	224(%r15), %xmm9
 8021                             	L_AES_GCM_decrypt_avx1_calc_iv_1_aesenc_avx_last:
 8022                             	        vaesenclast	%xmm9, %xmm5, %xmm5
 8023                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm5, %xmm5
 8024                             	        # Calc counter
 8025                             	        # Initialization vector
 8026                             	        cmpl	$0x00, %edx
 8027                             	        movq	$0x00, %rcx
 8028                             	        je	L_AES_GCM_decrypt_avx1_calc_iv_done
 8029                             	        cmpl	$16, %edx
 8030                             	        jl	L_AES_GCM_decrypt_avx1_calc_iv_lt16
 8031                             	        andl	$0xfffffff0, %edx
 8032                             	L_AES_GCM_decrypt_avx1_calc_iv_16_loop:
 8033                             	        vmovdqu	(%rax,%rcx,1), %xmm8
 8034                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 8035                             	        vpxor	%xmm8, %xmm4, %xmm4
 8036                             	        # ghash_gfmul_avx
 8037                             	        vpshufd	$0x4e, %xmm4, %xmm1
 8038                             	        vpshufd	$0x4e, %xmm5, %xmm2
 8039                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 8040                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 8041                             	        vpxor	%xmm4, %xmm1, %xmm1
 8042                             	        vpxor	%xmm5, %xmm2, %xmm2
 8043                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 8044                             	        vpxor	%xmm0, %xmm1, %xmm1
 8045                             	        vpxor	%xmm3, %xmm1, %xmm1
 8046                             	        vmovdqa	%xmm0, %xmm7
 8047                             	        vmovdqa	%xmm3, %xmm4
 8048                             	        vpslldq	$8, %xmm1, %xmm2
 8049                             	        vpsrldq	$8, %xmm1, %xmm1
 8050                             	        vpxor	%xmm2, %xmm7, %xmm7
 8051                             	        vpxor	%xmm1, %xmm4, %xmm4
 8052                             	        vpsrld	$31, %xmm7, %xmm0
 8053                             	        vpsrld	$31, %xmm4, %xmm1
 8054                             	        vpslld	$0x01, %xmm7, %xmm7
 8055                             	        vpslld	$0x01, %xmm4, %xmm4
 8056                             	        vpsrldq	$12, %xmm0, %xmm2
 8057                             	        vpslldq	$4, %xmm0, %xmm0
 8058                             	        vpslldq	$4, %xmm1, %xmm1
 8059                             	        vpor	%xmm2, %xmm4, %xmm4
 8060                             	        vpor	%xmm0, %xmm7, %xmm7
 8061                             	        vpor	%xmm1, %xmm4, %xmm4
 8062                             	        vpslld	$31, %xmm7, %xmm0
 8063                             	        vpslld	$30, %xmm7, %xmm1
 8064                             	        vpslld	$25, %xmm7, %xmm2
 8065                             	        vpxor	%xmm1, %xmm0, %xmm0
 8066                             	        vpxor	%xmm2, %xmm0, %xmm0
 8067                             	        vmovdqa	%xmm0, %xmm1
 8068                             	        vpsrldq	$4, %xmm1, %xmm1
 8069                             	        vpslldq	$12, %xmm0, %xmm0
 8070                             	        vpxor	%xmm0, %xmm7, %xmm7
 8071                             	        vpsrld	$0x01, %xmm7, %xmm2
 8072                             	        vpsrld	$2, %xmm7, %xmm3
 8073                             	        vpsrld	$7, %xmm7, %xmm0
 8074                             	        vpxor	%xmm3, %xmm2, %xmm2
 8075                             	        vpxor	%xmm0, %xmm2, %xmm2
 8076                             	        vpxor	%xmm1, %xmm2, %xmm2
 8077                             	        vpxor	%xmm7, %xmm2, %xmm2
 8078                             	        vpxor	%xmm2, %xmm4, %xmm4
 8079                             	        addl	$16, %ecx
 8080                             	        cmpl	%edx, %ecx
 8081                             	        jl	L_AES_GCM_decrypt_avx1_calc_iv_16_loop
 8082                             	        movl	%ebx, %edx
 8083                             	        cmpl	%edx, %ecx
 8084                             	        je	L_AES_GCM_decrypt_avx1_calc_iv_done
 8085                             	L_AES_GCM_decrypt_avx1_calc_iv_lt16:
 8086                             	        subq	$16, %rsp
 8087                             	        vpxor	%xmm8, %xmm8, %xmm8
 8088                             	        xorl	%ebx, %ebx
 8089                             	        vmovdqu	%xmm8, (%rsp)
 8090                             	L_AES_GCM_decrypt_avx1_calc_iv_loop:
 8091                             	        movzbl	(%rax,%rcx,1), %r13d
 8092                             	        movb	%r13b, (%rsp,%rbx,1)
 8093                             	        incl	%ecx
 8094                             	        incl	%ebx
 8095                             	        cmpl	%edx, %ecx
 8096                             	        jl	L_AES_GCM_decrypt_avx1_calc_iv_loop
 8097                             	        vmovdqu	(%rsp), %xmm8
 8098                             	        addq	$16, %rsp
 8099                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 8100                             	        vpxor	%xmm8, %xmm4, %xmm4
 8101                             	        # ghash_gfmul_avx
 8102                             	        vpshufd	$0x4e, %xmm4, %xmm1
 8103                             	        vpshufd	$0x4e, %xmm5, %xmm2
 8104                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 8105                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 8106                             	        vpxor	%xmm4, %xmm1, %xmm1
 8107                             	        vpxor	%xmm5, %xmm2, %xmm2
 8108                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 8109                             	        vpxor	%xmm0, %xmm1, %xmm1
 8110                             	        vpxor	%xmm3, %xmm1, %xmm1
 8111                             	        vmovdqa	%xmm0, %xmm7
 8112                             	        vmovdqa	%xmm3, %xmm4
 8113                             	        vpslldq	$8, %xmm1, %xmm2
 8114                             	        vpsrldq	$8, %xmm1, %xmm1
 8115                             	        vpxor	%xmm2, %xmm7, %xmm7
 8116                             	        vpxor	%xmm1, %xmm4, %xmm4
 8117                             	        vpsrld	$31, %xmm7, %xmm0
 8118                             	        vpsrld	$31, %xmm4, %xmm1
 8119                             	        vpslld	$0x01, %xmm7, %xmm7
 8120                             	        vpslld	$0x01, %xmm4, %xmm4
 8121                             	        vpsrldq	$12, %xmm0, %xmm2
 8122                             	        vpslldq	$4, %xmm0, %xmm0
 8123                             	        vpslldq	$4, %xmm1, %xmm1
 8124                             	        vpor	%xmm2, %xmm4, %xmm4
 8125                             	        vpor	%xmm0, %xmm7, %xmm7
 8126                             	        vpor	%xmm1, %xmm4, %xmm4
 8127                             	        vpslld	$31, %xmm7, %xmm0
 8128                             	        vpslld	$30, %xmm7, %xmm1
 8129                             	        vpslld	$25, %xmm7, %xmm2
 8130                             	        vpxor	%xmm1, %xmm0, %xmm0
 8131                             	        vpxor	%xmm2, %xmm0, %xmm0
 8132                             	        vmovdqa	%xmm0, %xmm1
 8133                             	        vpsrldq	$4, %xmm1, %xmm1
 8134                             	        vpslldq	$12, %xmm0, %xmm0
 8135                             	        vpxor	%xmm0, %xmm7, %xmm7
 8136                             	        vpsrld	$0x01, %xmm7, %xmm2
 8137                             	        vpsrld	$2, %xmm7, %xmm3
 8138                             	        vpsrld	$7, %xmm7, %xmm0
 8139                             	        vpxor	%xmm3, %xmm2, %xmm2
 8140                             	        vpxor	%xmm0, %xmm2, %xmm2
 8141                             	        vpxor	%xmm1, %xmm2, %xmm2
 8142                             	        vpxor	%xmm7, %xmm2, %xmm2
 8143                             	        vpxor	%xmm2, %xmm4, %xmm4
 8144                             	L_AES_GCM_decrypt_avx1_calc_iv_done:
 8145                             	        # T = Encrypt counter
 8146                             	        vpxor	%xmm0, %xmm0, %xmm0
 8147                             	        shll	$3, %edx
 8148                             	        vpinsrq	$0x00, %rdx, %xmm0, %xmm0
 8149                             	        vpxor	%xmm0, %xmm4, %xmm4
 8150                             	        # ghash_gfmul_avx
 8151                             	        vpshufd	$0x4e, %xmm4, %xmm1
 8152                             	        vpshufd	$0x4e, %xmm5, %xmm2
 8153                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 8154                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 8155                             	        vpxor	%xmm4, %xmm1, %xmm1
 8156                             	        vpxor	%xmm5, %xmm2, %xmm2
 8157                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 8158                             	        vpxor	%xmm0, %xmm1, %xmm1
 8159                             	        vpxor	%xmm3, %xmm1, %xmm1
 8160                             	        vmovdqa	%xmm0, %xmm7
 8161                             	        vmovdqa	%xmm3, %xmm4
 8162                             	        vpslldq	$8, %xmm1, %xmm2
 8163                             	        vpsrldq	$8, %xmm1, %xmm1
 8164                             	        vpxor	%xmm2, %xmm7, %xmm7
 8165                             	        vpxor	%xmm1, %xmm4, %xmm4
 8166                             	        vpsrld	$31, %xmm7, %xmm0
 8167                             	        vpsrld	$31, %xmm4, %xmm1
 8168                             	        vpslld	$0x01, %xmm7, %xmm7
 8169                             	        vpslld	$0x01, %xmm4, %xmm4
 8170                             	        vpsrldq	$12, %xmm0, %xmm2
 8171                             	        vpslldq	$4, %xmm0, %xmm0
 8172                             	        vpslldq	$4, %xmm1, %xmm1
 8173                             	        vpor	%xmm2, %xmm4, %xmm4
 8174                             	        vpor	%xmm0, %xmm7, %xmm7
 8175                             	        vpor	%xmm1, %xmm4, %xmm4
 8176                             	        vpslld	$31, %xmm7, %xmm0
 8177                             	        vpslld	$30, %xmm7, %xmm1
 8178                             	        vpslld	$25, %xmm7, %xmm2
 8179                             	        vpxor	%xmm1, %xmm0, %xmm0
 8180                             	        vpxor	%xmm2, %xmm0, %xmm0
 8181                             	        vmovdqa	%xmm0, %xmm1
 8182                             	        vpsrldq	$4, %xmm1, %xmm1
 8183                             	        vpslldq	$12, %xmm0, %xmm0
 8184                             	        vpxor	%xmm0, %xmm7, %xmm7
 8185                             	        vpsrld	$0x01, %xmm7, %xmm2
 8186                             	        vpsrld	$2, %xmm7, %xmm3
 8187                             	        vpsrld	$7, %xmm7, %xmm0
 8188                             	        vpxor	%xmm3, %xmm2, %xmm2
 8189                             	        vpxor	%xmm0, %xmm2, %xmm2
 8190                             	        vpxor	%xmm1, %xmm2, %xmm2
 8191                             	        vpxor	%xmm7, %xmm2, %xmm2
 8192                             	        vpxor	%xmm2, %xmm4, %xmm4
 8193                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm4, %xmm4
 8194                             	        #   Encrypt counter
 8195                             	        vmovdqa	(%r15), %xmm8
 8196                             	        vpxor	%xmm4, %xmm8, %xmm8
 8197                             	        vaesenc	16(%r15), %xmm8, %xmm8
 8198                             	        vaesenc	32(%r15), %xmm8, %xmm8
 8199                             	        vaesenc	48(%r15), %xmm8, %xmm8
 8200                             	        vaesenc	64(%r15), %xmm8, %xmm8
 8201                             	        vaesenc	80(%r15), %xmm8, %xmm8
 8202                             	        vaesenc	96(%r15), %xmm8, %xmm8
 8203                             	        vaesenc	112(%r15), %xmm8, %xmm8
 8204                             	        vaesenc	128(%r15), %xmm8, %xmm8
 8205                             	        vaesenc	144(%r15), %xmm8, %xmm8
 8206                             	        cmpl	$11, %r10d
 8207                             	        vmovdqa	160(%r15), %xmm9
 8208                             	        jl	L_AES_GCM_decrypt_avx1_calc_iv_2_aesenc_avx_last
 8209                             	        vaesenc	%xmm9, %xmm8, %xmm8
 8210                             	        vaesenc	176(%r15), %xmm8, %xmm8
 8211                             	        cmpl	$13, %r10d
 8212                             	        vmovdqa	192(%r15), %xmm9
 8213                             	        jl	L_AES_GCM_decrypt_avx1_calc_iv_2_aesenc_avx_last
 8214                             	        vaesenc	%xmm9, %xmm8, %xmm8
 8215                             	        vaesenc	208(%r15), %xmm8, %xmm8
 8216                             	        vmovdqa	224(%r15), %xmm9
 8217                             	L_AES_GCM_decrypt_avx1_calc_iv_2_aesenc_avx_last:
 8218                             	        vaesenclast	%xmm9, %xmm8, %xmm8
 8219                             	        vmovdqa	%xmm8, 144(%rsp)
 8220                             	L_AES_GCM_decrypt_avx1_iv_done:
 8221                             	        # Additional authentication data
 8222                             	        movl	%r11d, %edx
 8223                             	        cmpl	$0x00, %edx
 8224                             	        je	L_AES_GCM_decrypt_avx1_calc_aad_done
 8225                             	        xorl	%ecx, %ecx
 8226                             	        cmpl	$16, %edx
 8227                             	        jl	L_AES_GCM_decrypt_avx1_calc_aad_lt16
 8228                             	        andl	$0xfffffff0, %edx
 8229                             	L_AES_GCM_decrypt_avx1_calc_aad_16_loop:
 8230                             	        vmovdqu	(%r12,%rcx,1), %xmm8
 8231                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 8232                             	        vpxor	%xmm8, %xmm6, %xmm6
 8233                             	        # ghash_gfmul_avx
 8234                             	        vpshufd	$0x4e, %xmm6, %xmm1
 8235                             	        vpshufd	$0x4e, %xmm5, %xmm2
 8236                             	        vpclmulqdq	$0x11, %xmm6, %xmm5, %xmm3
 8237                             	        vpclmulqdq	$0x00, %xmm6, %xmm5, %xmm0
 8238                             	        vpxor	%xmm6, %xmm1, %xmm1
 8239                             	        vpxor	%xmm5, %xmm2, %xmm2
 8240                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 8241                             	        vpxor	%xmm0, %xmm1, %xmm1
 8242                             	        vpxor	%xmm3, %xmm1, %xmm1
 8243                             	        vmovdqa	%xmm0, %xmm7
 8244                             	        vmovdqa	%xmm3, %xmm6
 8245                             	        vpslldq	$8, %xmm1, %xmm2
 8246                             	        vpsrldq	$8, %xmm1, %xmm1
 8247                             	        vpxor	%xmm2, %xmm7, %xmm7
 8248                             	        vpxor	%xmm1, %xmm6, %xmm6
 8249                             	        vpsrld	$31, %xmm7, %xmm0
 8250                             	        vpsrld	$31, %xmm6, %xmm1
 8251                             	        vpslld	$0x01, %xmm7, %xmm7
 8252                             	        vpslld	$0x01, %xmm6, %xmm6
 8253                             	        vpsrldq	$12, %xmm0, %xmm2
 8254                             	        vpslldq	$4, %xmm0, %xmm0
 8255                             	        vpslldq	$4, %xmm1, %xmm1
 8256                             	        vpor	%xmm2, %xmm6, %xmm6
 8257                             	        vpor	%xmm0, %xmm7, %xmm7
 8258                             	        vpor	%xmm1, %xmm6, %xmm6
 8259                             	        vpslld	$31, %xmm7, %xmm0
 8260                             	        vpslld	$30, %xmm7, %xmm1
 8261                             	        vpslld	$25, %xmm7, %xmm2
 8262                             	        vpxor	%xmm1, %xmm0, %xmm0
 8263                             	        vpxor	%xmm2, %xmm0, %xmm0
 8264                             	        vmovdqa	%xmm0, %xmm1
 8265                             	        vpsrldq	$4, %xmm1, %xmm1
 8266                             	        vpslldq	$12, %xmm0, %xmm0
 8267                             	        vpxor	%xmm0, %xmm7, %xmm7
 8268                             	        vpsrld	$0x01, %xmm7, %xmm2
 8269                             	        vpsrld	$2, %xmm7, %xmm3
 8270                             	        vpsrld	$7, %xmm7, %xmm0
 8271                             	        vpxor	%xmm3, %xmm2, %xmm2
 8272                             	        vpxor	%xmm0, %xmm2, %xmm2
 8273                             	        vpxor	%xmm1, %xmm2, %xmm2
 8274                             	        vpxor	%xmm7, %xmm2, %xmm2
 8275                             	        vpxor	%xmm2, %xmm6, %xmm6
 8276                             	        addl	$16, %ecx
 8277                             	        cmpl	%edx, %ecx
 8278                             	        jl	L_AES_GCM_decrypt_avx1_calc_aad_16_loop
 8279                             	        movl	%r11d, %edx
 8280                             	        cmpl	%edx, %ecx
 8281                             	        je	L_AES_GCM_decrypt_avx1_calc_aad_done
 8282                             	L_AES_GCM_decrypt_avx1_calc_aad_lt16:
 8283                             	        subq	$16, %rsp
 8284                             	        vpxor	%xmm8, %xmm8, %xmm8
 8285                             	        xorl	%ebx, %ebx
 8286                             	        vmovdqu	%xmm8, (%rsp)
 8287                             	L_AES_GCM_decrypt_avx1_calc_aad_loop:
 8288                             	        movzbl	(%r12,%rcx,1), %r13d
 8289                             	        movb	%r13b, (%rsp,%rbx,1)
 8290                             	        incl	%ecx
 8291                             	        incl	%ebx
 8292                             	        cmpl	%edx, %ecx
 8293                             	        jl	L_AES_GCM_decrypt_avx1_calc_aad_loop
 8294                             	        vmovdqu	(%rsp), %xmm8
 8295                             	        addq	$16, %rsp
 8296                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 8297                             	        vpxor	%xmm8, %xmm6, %xmm6
 8298                             	        # ghash_gfmul_avx
 8299                             	        vpshufd	$0x4e, %xmm6, %xmm1
 8300                             	        vpshufd	$0x4e, %xmm5, %xmm2
 8301                             	        vpclmulqdq	$0x11, %xmm6, %xmm5, %xmm3
 8302                             	        vpclmulqdq	$0x00, %xmm6, %xmm5, %xmm0
 8303                             	        vpxor	%xmm6, %xmm1, %xmm1
 8304                             	        vpxor	%xmm5, %xmm2, %xmm2
 8305                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 8306                             	        vpxor	%xmm0, %xmm1, %xmm1
 8307                             	        vpxor	%xmm3, %xmm1, %xmm1
 8308                             	        vmovdqa	%xmm0, %xmm7
 8309                             	        vmovdqa	%xmm3, %xmm6
 8310                             	        vpslldq	$8, %xmm1, %xmm2
 8311                             	        vpsrldq	$8, %xmm1, %xmm1
 8312                             	        vpxor	%xmm2, %xmm7, %xmm7
 8313                             	        vpxor	%xmm1, %xmm6, %xmm6
 8314                             	        vpsrld	$31, %xmm7, %xmm0
 8315                             	        vpsrld	$31, %xmm6, %xmm1
 8316                             	        vpslld	$0x01, %xmm7, %xmm7
 8317                             	        vpslld	$0x01, %xmm6, %xmm6
 8318                             	        vpsrldq	$12, %xmm0, %xmm2
 8319                             	        vpslldq	$4, %xmm0, %xmm0
 8320                             	        vpslldq	$4, %xmm1, %xmm1
 8321                             	        vpor	%xmm2, %xmm6, %xmm6
 8322                             	        vpor	%xmm0, %xmm7, %xmm7
 8323                             	        vpor	%xmm1, %xmm6, %xmm6
 8324                             	        vpslld	$31, %xmm7, %xmm0
 8325                             	        vpslld	$30, %xmm7, %xmm1
 8326                             	        vpslld	$25, %xmm7, %xmm2
 8327                             	        vpxor	%xmm1, %xmm0, %xmm0
 8328                             	        vpxor	%xmm2, %xmm0, %xmm0
 8329                             	        vmovdqa	%xmm0, %xmm1
 8330                             	        vpsrldq	$4, %xmm1, %xmm1
 8331                             	        vpslldq	$12, %xmm0, %xmm0
 8332                             	        vpxor	%xmm0, %xmm7, %xmm7
 8333                             	        vpsrld	$0x01, %xmm7, %xmm2
 8334                             	        vpsrld	$2, %xmm7, %xmm3
 8335                             	        vpsrld	$7, %xmm7, %xmm0
 8336                             	        vpxor	%xmm3, %xmm2, %xmm2
 8337                             	        vpxor	%xmm0, %xmm2, %xmm2
 8338                             	        vpxor	%xmm1, %xmm2, %xmm2
 8339                             	        vpxor	%xmm7, %xmm2, %xmm2
 8340                             	        vpxor	%xmm2, %xmm6, %xmm6
 8341                             	L_AES_GCM_decrypt_avx1_calc_aad_done:
 8342                             	        # Calculate counter and H
 8343                             	        vpsrlq	$63, %xmm5, %xmm9
 8344                             	        vpsllq	$0x01, %xmm5, %xmm8
 8345                             	        vpslldq	$8, %xmm9, %xmm9
 8346                             	        vpor	%xmm9, %xmm8, %xmm8
 8347                             	        vpshufd	$0xff, %xmm5, %xmm5
 8348                             	        vpsrad	$31, %xmm5, %xmm5
 8349                             	        vpshufb	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm4, %xmm4
 8350                             	        vpand	L_avx1_aes_gcm_mod2_128(%rip), %xmm5, %xmm5
 8351                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm4, %xmm4
 8352                             	        vpxor	%xmm8, %xmm5, %xmm5
 8353                             	        vmovdqa	%xmm4, 128(%rsp)
 8354                             	        xorl	%ebx, %ebx
 8355                             	        cmpl	$0x80, %r9d
 8356                             	        movl	%r9d, %r13d
 8357                             	        jl	L_AES_GCM_decrypt_avx1_done_128
 8358                             	        andl	$0xffffff80, %r13d
 8359                             	        vmovdqa	%xmm6, %xmm2
 8360                             	        # H ^ 1
 8361                             	        vmovdqa	%xmm5, (%rsp)
 8362                             	        # H ^ 2
 8363                             	        vpclmulqdq	$0x00, %xmm5, %xmm5, %xmm8
 8364                             	        vpclmulqdq	$0x11, %xmm5, %xmm5, %xmm0
 8365                             	        vpslld	$31, %xmm8, %xmm12
 8366                             	        vpslld	$30, %xmm8, %xmm13
 8367                             	        vpslld	$25, %xmm8, %xmm14
 8368                             	        vpxor	%xmm13, %xmm12, %xmm12
 8369                             	        vpxor	%xmm14, %xmm12, %xmm12
 8370                             	        vpsrldq	$4, %xmm12, %xmm13
 8371                             	        vpslldq	$12, %xmm12, %xmm12
 8372                             	        vpxor	%xmm12, %xmm8, %xmm8
 8373                             	        vpsrld	$0x01, %xmm8, %xmm14
 8374                             	        vpsrld	$2, %xmm8, %xmm10
 8375                             	        vpsrld	$7, %xmm8, %xmm9
 8376                             	        vpxor	%xmm10, %xmm14, %xmm14
 8377                             	        vpxor	%xmm9, %xmm14, %xmm14
 8378                             	        vpxor	%xmm13, %xmm14, %xmm14
 8379                             	        vpxor	%xmm8, %xmm14, %xmm14
 8380                             	        vpxor	%xmm14, %xmm0, %xmm0
 8381                             	        vmovdqa	%xmm0, 16(%rsp)
 8382                             	        # H ^ 3
 8383                             	        # ghash_gfmul_red_avx
 8384                             	        vpshufd	$0x4e, %xmm5, %xmm9
 8385                             	        vpshufd	$0x4e, %xmm0, %xmm10
 8386                             	        vpclmulqdq	$0x11, %xmm5, %xmm0, %xmm11
 8387                             	        vpclmulqdq	$0x00, %xmm5, %xmm0, %xmm8
 8388                             	        vpxor	%xmm5, %xmm9, %xmm9
 8389                             	        vpxor	%xmm0, %xmm10, %xmm10
 8390                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 8391                             	        vpxor	%xmm8, %xmm9, %xmm9
 8392                             	        vpxor	%xmm11, %xmm9, %xmm9
 8393                             	        vpslldq	$8, %xmm9, %xmm10
 8394                             	        vpsrldq	$8, %xmm9, %xmm9
 8395                             	        vpxor	%xmm10, %xmm8, %xmm8
 8396                             	        vpxor	%xmm9, %xmm11, %xmm1
 8397                             	        vpslld	$31, %xmm8, %xmm12
 8398                             	        vpslld	$30, %xmm8, %xmm13
 8399                             	        vpslld	$25, %xmm8, %xmm14
 8400                             	        vpxor	%xmm13, %xmm12, %xmm12
 8401                             	        vpxor	%xmm14, %xmm12, %xmm12
 8402                             	        vpsrldq	$4, %xmm12, %xmm13
 8403                             	        vpslldq	$12, %xmm12, %xmm12
 8404                             	        vpxor	%xmm12, %xmm8, %xmm8
 8405                             	        vpsrld	$0x01, %xmm8, %xmm14
 8406                             	        vpsrld	$2, %xmm8, %xmm10
 8407                             	        vpsrld	$7, %xmm8, %xmm9
 8408                             	        vpxor	%xmm10, %xmm14, %xmm14
 8409                             	        vpxor	%xmm9, %xmm14, %xmm14
 8410                             	        vpxor	%xmm13, %xmm14, %xmm14
 8411                             	        vpxor	%xmm8, %xmm14, %xmm14
 8412                             	        vpxor	%xmm14, %xmm1, %xmm1
 8413                             	        vmovdqa	%xmm1, 32(%rsp)
 8414                             	        # H ^ 4
 8415                             	        vpclmulqdq	$0x00, %xmm0, %xmm0, %xmm8
 8416                             	        vpclmulqdq	$0x11, %xmm0, %xmm0, %xmm3
 8417                             	        vpslld	$31, %xmm8, %xmm12
 8418                             	        vpslld	$30, %xmm8, %xmm13
 8419                             	        vpslld	$25, %xmm8, %xmm14
 8420                             	        vpxor	%xmm13, %xmm12, %xmm12
 8421                             	        vpxor	%xmm14, %xmm12, %xmm12
 8422                             	        vpsrldq	$4, %xmm12, %xmm13
 8423                             	        vpslldq	$12, %xmm12, %xmm12
 8424                             	        vpxor	%xmm12, %xmm8, %xmm8
 8425                             	        vpsrld	$0x01, %xmm8, %xmm14
 8426                             	        vpsrld	$2, %xmm8, %xmm10
 8427                             	        vpsrld	$7, %xmm8, %xmm9
 8428                             	        vpxor	%xmm10, %xmm14, %xmm14
 8429                             	        vpxor	%xmm9, %xmm14, %xmm14
 8430                             	        vpxor	%xmm13, %xmm14, %xmm14
 8431                             	        vpxor	%xmm8, %xmm14, %xmm14
 8432                             	        vpxor	%xmm14, %xmm3, %xmm3
 8433                             	        vmovdqa	%xmm3, 48(%rsp)
 8434                             	        # H ^ 5
 8435                             	        # ghash_gfmul_red_avx
 8436                             	        vpshufd	$0x4e, %xmm0, %xmm9
 8437                             	        vpshufd	$0x4e, %xmm1, %xmm10
 8438                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm11
 8439                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm8
 8440                             	        vpxor	%xmm0, %xmm9, %xmm9
 8441                             	        vpxor	%xmm1, %xmm10, %xmm10
 8442                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 8443                             	        vpxor	%xmm8, %xmm9, %xmm9
 8444                             	        vpxor	%xmm11, %xmm9, %xmm9
 8445                             	        vpslldq	$8, %xmm9, %xmm10
 8446                             	        vpsrldq	$8, %xmm9, %xmm9
 8447                             	        vpxor	%xmm10, %xmm8, %xmm8
 8448                             	        vpxor	%xmm9, %xmm11, %xmm7
 8449                             	        vpslld	$31, %xmm8, %xmm12
 8450                             	        vpslld	$30, %xmm8, %xmm13
 8451                             	        vpslld	$25, %xmm8, %xmm14
 8452                             	        vpxor	%xmm13, %xmm12, %xmm12
 8453                             	        vpxor	%xmm14, %xmm12, %xmm12
 8454                             	        vpsrldq	$4, %xmm12, %xmm13
 8455                             	        vpslldq	$12, %xmm12, %xmm12
 8456                             	        vpxor	%xmm12, %xmm8, %xmm8
 8457                             	        vpsrld	$0x01, %xmm8, %xmm14
 8458                             	        vpsrld	$2, %xmm8, %xmm10
 8459                             	        vpsrld	$7, %xmm8, %xmm9
 8460                             	        vpxor	%xmm10, %xmm14, %xmm14
 8461                             	        vpxor	%xmm9, %xmm14, %xmm14
 8462                             	        vpxor	%xmm13, %xmm14, %xmm14
 8463                             	        vpxor	%xmm8, %xmm14, %xmm14
 8464                             	        vpxor	%xmm14, %xmm7, %xmm7
 8465                             	        vmovdqa	%xmm7, 64(%rsp)
 8466                             	        # H ^ 6
 8467                             	        vpclmulqdq	$0x00, %xmm1, %xmm1, %xmm8
 8468                             	        vpclmulqdq	$0x11, %xmm1, %xmm1, %xmm7
 8469                             	        vpslld	$31, %xmm8, %xmm12
 8470                             	        vpslld	$30, %xmm8, %xmm13
 8471                             	        vpslld	$25, %xmm8, %xmm14
 8472                             	        vpxor	%xmm13, %xmm12, %xmm12
 8473                             	        vpxor	%xmm14, %xmm12, %xmm12
 8474                             	        vpsrldq	$4, %xmm12, %xmm13
 8475                             	        vpslldq	$12, %xmm12, %xmm12
 8476                             	        vpxor	%xmm12, %xmm8, %xmm8
 8477                             	        vpsrld	$0x01, %xmm8, %xmm14
 8478                             	        vpsrld	$2, %xmm8, %xmm10
 8479                             	        vpsrld	$7, %xmm8, %xmm9
 8480                             	        vpxor	%xmm10, %xmm14, %xmm14
 8481                             	        vpxor	%xmm9, %xmm14, %xmm14
 8482                             	        vpxor	%xmm13, %xmm14, %xmm14
 8483                             	        vpxor	%xmm8, %xmm14, %xmm14
 8484                             	        vpxor	%xmm14, %xmm7, %xmm7
 8485                             	        vmovdqa	%xmm7, 80(%rsp)
 8486                             	        # H ^ 7
 8487                             	        # ghash_gfmul_red_avx
 8488                             	        vpshufd	$0x4e, %xmm1, %xmm9
 8489                             	        vpshufd	$0x4e, %xmm3, %xmm10
 8490                             	        vpclmulqdq	$0x11, %xmm1, %xmm3, %xmm11
 8491                             	        vpclmulqdq	$0x00, %xmm1, %xmm3, %xmm8
 8492                             	        vpxor	%xmm1, %xmm9, %xmm9
 8493                             	        vpxor	%xmm3, %xmm10, %xmm10
 8494                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 8495                             	        vpxor	%xmm8, %xmm9, %xmm9
 8496                             	        vpxor	%xmm11, %xmm9, %xmm9
 8497                             	        vpslldq	$8, %xmm9, %xmm10
 8498                             	        vpsrldq	$8, %xmm9, %xmm9
 8499                             	        vpxor	%xmm10, %xmm8, %xmm8
 8500                             	        vpxor	%xmm9, %xmm11, %xmm7
 8501                             	        vpslld	$31, %xmm8, %xmm12
 8502                             	        vpslld	$30, %xmm8, %xmm13
 8503                             	        vpslld	$25, %xmm8, %xmm14
 8504                             	        vpxor	%xmm13, %xmm12, %xmm12
 8505                             	        vpxor	%xmm14, %xmm12, %xmm12
 8506                             	        vpsrldq	$4, %xmm12, %xmm13
 8507                             	        vpslldq	$12, %xmm12, %xmm12
 8508                             	        vpxor	%xmm12, %xmm8, %xmm8
 8509                             	        vpsrld	$0x01, %xmm8, %xmm14
 8510                             	        vpsrld	$2, %xmm8, %xmm10
 8511                             	        vpsrld	$7, %xmm8, %xmm9
 8512                             	        vpxor	%xmm10, %xmm14, %xmm14
 8513                             	        vpxor	%xmm9, %xmm14, %xmm14
 8514                             	        vpxor	%xmm13, %xmm14, %xmm14
 8515                             	        vpxor	%xmm8, %xmm14, %xmm14
 8516                             	        vpxor	%xmm14, %xmm7, %xmm7
 8517                             	        vmovdqa	%xmm7, 96(%rsp)
 8518                             	        # H ^ 8
 8519                             	        vpclmulqdq	$0x00, %xmm3, %xmm3, %xmm8
 8520                             	        vpclmulqdq	$0x11, %xmm3, %xmm3, %xmm7
 8521                             	        vpslld	$31, %xmm8, %xmm12
 8522                             	        vpslld	$30, %xmm8, %xmm13
 8523                             	        vpslld	$25, %xmm8, %xmm14
 8524                             	        vpxor	%xmm13, %xmm12, %xmm12
 8525                             	        vpxor	%xmm14, %xmm12, %xmm12
 8526                             	        vpsrldq	$4, %xmm12, %xmm13
 8527                             	        vpslldq	$12, %xmm12, %xmm12
 8528                             	        vpxor	%xmm12, %xmm8, %xmm8
 8529                             	        vpsrld	$0x01, %xmm8, %xmm14
 8530                             	        vpsrld	$2, %xmm8, %xmm10
 8531                             	        vpsrld	$7, %xmm8, %xmm9
 8532                             	        vpxor	%xmm10, %xmm14, %xmm14
 8533                             	        vpxor	%xmm9, %xmm14, %xmm14
 8534                             	        vpxor	%xmm13, %xmm14, %xmm14
 8535                             	        vpxor	%xmm8, %xmm14, %xmm14
 8536                             	        vpxor	%xmm14, %xmm7, %xmm7
 8537                             	        vmovdqa	%xmm7, 112(%rsp)
 8538                             	L_AES_GCM_decrypt_avx1_ghash_128:
 8539                             	        leaq	(%rdi,%rbx,1), %rcx
 8540                             	        leaq	(%rsi,%rbx,1), %rdx
 8541                             	        vmovdqa	128(%rsp), %xmm0
 8542                             	        vmovdqa	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm1
 8543                             	        vpshufb	%xmm1, %xmm0, %xmm8
 8544                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm0, %xmm9
 8545                             	        vpshufb	%xmm1, %xmm9, %xmm9
 8546                             	        vpaddd	L_avx1_aes_gcm_two(%rip), %xmm0, %xmm10
 8547                             	        vpshufb	%xmm1, %xmm10, %xmm10
 8548                             	        vpaddd	L_avx1_aes_gcm_three(%rip), %xmm0, %xmm11
 8549                             	        vpshufb	%xmm1, %xmm11, %xmm11
 8550                             	        vpaddd	L_avx1_aes_gcm_four(%rip), %xmm0, %xmm12
 8551                             	        vpshufb	%xmm1, %xmm12, %xmm12
 8552                             	        vpaddd	L_avx1_aes_gcm_five(%rip), %xmm0, %xmm13
 8553                             	        vpshufb	%xmm1, %xmm13, %xmm13
 8554                             	        vpaddd	L_avx1_aes_gcm_six(%rip), %xmm0, %xmm14
 8555                             	        vpshufb	%xmm1, %xmm14, %xmm14
 8556                             	        vpaddd	L_avx1_aes_gcm_seven(%rip), %xmm0, %xmm15
 8557                             	        vpshufb	%xmm1, %xmm15, %xmm15
 8558                             	        vpaddd	L_avx1_aes_gcm_eight(%rip), %xmm0, %xmm0
 8559                             	        vmovdqa	(%r15), %xmm7
 8560                             	        vmovdqa	%xmm0, 128(%rsp)
 8561                             	        vpxor	%xmm7, %xmm8, %xmm8
 8562                             	        vpxor	%xmm7, %xmm9, %xmm9
 8563                             	        vpxor	%xmm7, %xmm10, %xmm10
 8564                             	        vpxor	%xmm7, %xmm11, %xmm11
 8565                             	        vpxor	%xmm7, %xmm12, %xmm12
 8566                             	        vpxor	%xmm7, %xmm13, %xmm13
 8567                             	        vpxor	%xmm7, %xmm14, %xmm14
 8568                             	        vpxor	%xmm7, %xmm15, %xmm15
 8569                             	        vmovdqa	112(%rsp), %xmm7
 8570                             	        vmovdqu	(%rcx), %xmm0
 8571                             	        vaesenc	16(%r15), %xmm8, %xmm8
 8572                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 8573                             	        vpxor	%xmm2, %xmm0, %xmm0
 8574                             	        vpshufd	$0x4e, %xmm7, %xmm1
 8575                             	        vpshufd	$0x4e, %xmm0, %xmm5
 8576                             	        vpxor	%xmm7, %xmm1, %xmm1
 8577                             	        vpxor	%xmm0, %xmm5, %xmm5
 8578                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm3
 8579                             	        vaesenc	16(%r15), %xmm9, %xmm9
 8580                             	        vaesenc	16(%r15), %xmm10, %xmm10
 8581                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm2
 8582                             	        vaesenc	16(%r15), %xmm11, %xmm11
 8583                             	        vaesenc	16(%r15), %xmm12, %xmm12
 8584                             	        vpclmulqdq	$0x00, %xmm5, %xmm1, %xmm1
 8585                             	        vaesenc	16(%r15), %xmm13, %xmm13
 8586                             	        vaesenc	16(%r15), %xmm14, %xmm14
 8587                             	        vaesenc	16(%r15), %xmm15, %xmm15
 8588                             	        vpxor	%xmm2, %xmm1, %xmm1
 8589                             	        vpxor	%xmm3, %xmm1, %xmm1
 8590                             	        vmovdqa	96(%rsp), %xmm7
 8591                             	        vmovdqu	16(%rcx), %xmm0
 8592                             	        vpshufd	$0x4e, %xmm7, %xmm4
 8593                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 8594                             	        vaesenc	32(%r15), %xmm8, %xmm8
 8595                             	        vpxor	%xmm7, %xmm4, %xmm4
 8596                             	        vpshufd	$0x4e, %xmm0, %xmm5
 8597                             	        vpxor	%xmm0, %xmm5, %xmm5
 8598                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 8599                             	        vaesenc	32(%r15), %xmm9, %xmm9
 8600                             	        vaesenc	32(%r15), %xmm10, %xmm10
 8601                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 8602                             	        vaesenc	32(%r15), %xmm11, %xmm11
 8603                             	        vaesenc	32(%r15), %xmm12, %xmm12
 8604                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 8605                             	        vaesenc	32(%r15), %xmm13, %xmm13
 8606                             	        vaesenc	32(%r15), %xmm14, %xmm14
 8607                             	        vaesenc	32(%r15), %xmm15, %xmm15
 8608                             	        vpxor	%xmm7, %xmm1, %xmm1
 8609                             	        vpxor	%xmm7, %xmm2, %xmm2
 8610                             	        vpxor	%xmm6, %xmm1, %xmm1
 8611                             	        vpxor	%xmm6, %xmm3, %xmm3
 8612                             	        vpxor	%xmm4, %xmm1, %xmm1
 8613                             	        vmovdqa	80(%rsp), %xmm7
 8614                             	        vmovdqu	32(%rcx), %xmm0
 8615                             	        vpshufd	$0x4e, %xmm7, %xmm4
 8616                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 8617                             	        vaesenc	48(%r15), %xmm8, %xmm8
 8618                             	        vpxor	%xmm7, %xmm4, %xmm4
 8619                             	        vpshufd	$0x4e, %xmm0, %xmm5
 8620                             	        vpxor	%xmm0, %xmm5, %xmm5
 8621                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 8622                             	        vaesenc	48(%r15), %xmm9, %xmm9
 8623                             	        vaesenc	48(%r15), %xmm10, %xmm10
 8624                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 8625                             	        vaesenc	48(%r15), %xmm11, %xmm11
 8626                             	        vaesenc	48(%r15), %xmm12, %xmm12
 8627                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 8628                             	        vaesenc	48(%r15), %xmm13, %xmm13
 8629                             	        vaesenc	48(%r15), %xmm14, %xmm14
 8630                             	        vaesenc	48(%r15), %xmm15, %xmm15
 8631                             	        vpxor	%xmm7, %xmm1, %xmm1
 8632                             	        vpxor	%xmm7, %xmm2, %xmm2
 8633                             	        vpxor	%xmm6, %xmm1, %xmm1
 8634                             	        vpxor	%xmm6, %xmm3, %xmm3
 8635                             	        vpxor	%xmm4, %xmm1, %xmm1
 8636                             	        vmovdqa	64(%rsp), %xmm7
 8637                             	        vmovdqu	48(%rcx), %xmm0
 8638                             	        vpshufd	$0x4e, %xmm7, %xmm4
 8639                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 8640                             	        vaesenc	64(%r15), %xmm8, %xmm8
 8641                             	        vpxor	%xmm7, %xmm4, %xmm4
 8642                             	        vpshufd	$0x4e, %xmm0, %xmm5
 8643                             	        vpxor	%xmm0, %xmm5, %xmm5
 8644                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 8645                             	        vaesenc	64(%r15), %xmm9, %xmm9
 8646                             	        vaesenc	64(%r15), %xmm10, %xmm10
 8647                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 8648                             	        vaesenc	64(%r15), %xmm11, %xmm11
 8649                             	        vaesenc	64(%r15), %xmm12, %xmm12
 8650                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 8651                             	        vaesenc	64(%r15), %xmm13, %xmm13
 8652                             	        vaesenc	64(%r15), %xmm14, %xmm14
 8653                             	        vaesenc	64(%r15), %xmm15, %xmm15
 8654                             	        vpxor	%xmm7, %xmm1, %xmm1
 8655                             	        vpxor	%xmm7, %xmm2, %xmm2
 8656                             	        vpxor	%xmm6, %xmm1, %xmm1
 8657                             	        vpxor	%xmm6, %xmm3, %xmm3
 8658                             	        vpxor	%xmm4, %xmm1, %xmm1
 8659                             	        vmovdqa	48(%rsp), %xmm7
 8660                             	        vmovdqu	64(%rcx), %xmm0
 8661                             	        vpshufd	$0x4e, %xmm7, %xmm4
 8662                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 8663                             	        vaesenc	80(%r15), %xmm8, %xmm8
 8664                             	        vpxor	%xmm7, %xmm4, %xmm4
 8665                             	        vpshufd	$0x4e, %xmm0, %xmm5
 8666                             	        vpxor	%xmm0, %xmm5, %xmm5
 8667                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 8668                             	        vaesenc	80(%r15), %xmm9, %xmm9
 8669                             	        vaesenc	80(%r15), %xmm10, %xmm10
 8670                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 8671                             	        vaesenc	80(%r15), %xmm11, %xmm11
 8672                             	        vaesenc	80(%r15), %xmm12, %xmm12
 8673                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 8674                             	        vaesenc	80(%r15), %xmm13, %xmm13
 8675                             	        vaesenc	80(%r15), %xmm14, %xmm14
 8676                             	        vaesenc	80(%r15), %xmm15, %xmm15
 8677                             	        vpxor	%xmm7, %xmm1, %xmm1
 8678                             	        vpxor	%xmm7, %xmm2, %xmm2
 8679                             	        vpxor	%xmm6, %xmm1, %xmm1
 8680                             	        vpxor	%xmm6, %xmm3, %xmm3
 8681                             	        vpxor	%xmm4, %xmm1, %xmm1
 8682                             	        vmovdqa	32(%rsp), %xmm7
 8683                             	        vmovdqu	80(%rcx), %xmm0
 8684                             	        vpshufd	$0x4e, %xmm7, %xmm4
 8685                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 8686                             	        vaesenc	96(%r15), %xmm8, %xmm8
 8687                             	        vpxor	%xmm7, %xmm4, %xmm4
 8688                             	        vpshufd	$0x4e, %xmm0, %xmm5
 8689                             	        vpxor	%xmm0, %xmm5, %xmm5
 8690                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 8691                             	        vaesenc	96(%r15), %xmm9, %xmm9
 8692                             	        vaesenc	96(%r15), %xmm10, %xmm10
 8693                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 8694                             	        vaesenc	96(%r15), %xmm11, %xmm11
 8695                             	        vaesenc	96(%r15), %xmm12, %xmm12
 8696                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 8697                             	        vaesenc	96(%r15), %xmm13, %xmm13
 8698                             	        vaesenc	96(%r15), %xmm14, %xmm14
 8699                             	        vaesenc	96(%r15), %xmm15, %xmm15
 8700                             	        vpxor	%xmm7, %xmm1, %xmm1
 8701                             	        vpxor	%xmm7, %xmm2, %xmm2
 8702                             	        vpxor	%xmm6, %xmm1, %xmm1
 8703                             	        vpxor	%xmm6, %xmm3, %xmm3
 8704                             	        vpxor	%xmm4, %xmm1, %xmm1
 8705                             	        vmovdqa	16(%rsp), %xmm7
 8706                             	        vmovdqu	96(%rcx), %xmm0
 8707                             	        vpshufd	$0x4e, %xmm7, %xmm4
 8708                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 8709                             	        vaesenc	112(%r15), %xmm8, %xmm8
 8710                             	        vpxor	%xmm7, %xmm4, %xmm4
 8711                             	        vpshufd	$0x4e, %xmm0, %xmm5
 8712                             	        vpxor	%xmm0, %xmm5, %xmm5
 8713                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 8714                             	        vaesenc	112(%r15), %xmm9, %xmm9
 8715                             	        vaesenc	112(%r15), %xmm10, %xmm10
 8716                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 8717                             	        vaesenc	112(%r15), %xmm11, %xmm11
 8718                             	        vaesenc	112(%r15), %xmm12, %xmm12
 8719                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 8720                             	        vaesenc	112(%r15), %xmm13, %xmm13
 8721                             	        vaesenc	112(%r15), %xmm14, %xmm14
 8722                             	        vaesenc	112(%r15), %xmm15, %xmm15
 8723                             	        vpxor	%xmm7, %xmm1, %xmm1
 8724                             	        vpxor	%xmm7, %xmm2, %xmm2
 8725                             	        vpxor	%xmm6, %xmm1, %xmm1
 8726                             	        vpxor	%xmm6, %xmm3, %xmm3
 8727                             	        vpxor	%xmm4, %xmm1, %xmm1
 8728                             	        vmovdqa	(%rsp), %xmm7
 8729                             	        vmovdqu	112(%rcx), %xmm0
 8730                             	        vpshufd	$0x4e, %xmm7, %xmm4
 8731                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 8732                             	        vaesenc	128(%r15), %xmm8, %xmm8
 8733                             	        vpxor	%xmm7, %xmm4, %xmm4
 8734                             	        vpshufd	$0x4e, %xmm0, %xmm5
 8735                             	        vpxor	%xmm0, %xmm5, %xmm5
 8736                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 8737                             	        vaesenc	128(%r15), %xmm9, %xmm9
 8738                             	        vaesenc	128(%r15), %xmm10, %xmm10
 8739                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 8740                             	        vaesenc	128(%r15), %xmm11, %xmm11
 8741                             	        vaesenc	128(%r15), %xmm12, %xmm12
 8742                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 8743                             	        vaesenc	128(%r15), %xmm13, %xmm13
 8744                             	        vaesenc	128(%r15), %xmm14, %xmm14
 8745                             	        vaesenc	128(%r15), %xmm15, %xmm15
 8746                             	        vpxor	%xmm7, %xmm1, %xmm1
 8747                             	        vpxor	%xmm7, %xmm2, %xmm2
 8748                             	        vpxor	%xmm6, %xmm1, %xmm1
 8749                             	        vpxor	%xmm6, %xmm3, %xmm3
 8750                             	        vpxor	%xmm4, %xmm1, %xmm1
 8751                             	        vpslldq	$8, %xmm1, %xmm5
 8752                             	        vpsrldq	$8, %xmm1, %xmm1
 8753                             	        vaesenc	144(%r15), %xmm8, %xmm8
 8754                             	        vpxor	%xmm5, %xmm2, %xmm2
 8755                             	        vpxor	%xmm1, %xmm3, %xmm3
 8756                             	        vaesenc	144(%r15), %xmm9, %xmm9
 8757                             	        vpslld	$31, %xmm2, %xmm7
 8758                             	        vpslld	$30, %xmm2, %xmm4
 8759                             	        vpslld	$25, %xmm2, %xmm5
 8760                             	        vaesenc	144(%r15), %xmm10, %xmm10
 8761                             	        vpxor	%xmm4, %xmm7, %xmm7
 8762                             	        vpxor	%xmm5, %xmm7, %xmm7
 8763                             	        vaesenc	144(%r15), %xmm11, %xmm11
 8764                             	        vpsrldq	$4, %xmm7, %xmm4
 8765                             	        vpslldq	$12, %xmm7, %xmm7
 8766                             	        vaesenc	144(%r15), %xmm12, %xmm12
 8767                             	        vpxor	%xmm7, %xmm2, %xmm2
 8768                             	        vpsrld	$0x01, %xmm2, %xmm5
 8769                             	        vaesenc	144(%r15), %xmm13, %xmm13
 8770                             	        vpsrld	$2, %xmm2, %xmm1
 8771                             	        vpsrld	$7, %xmm2, %xmm0
 8772                             	        vaesenc	144(%r15), %xmm14, %xmm14
 8773                             	        vpxor	%xmm1, %xmm5, %xmm5
 8774                             	        vpxor	%xmm0, %xmm5, %xmm5
 8775                             	        vaesenc	144(%r15), %xmm15, %xmm15
 8776                             	        vpxor	%xmm4, %xmm5, %xmm5
 8777                             	        vpxor	%xmm5, %xmm2, %xmm2
 8778                             	        vpxor	%xmm3, %xmm2, %xmm2
 8779                             	        cmpl	$11, %r10d
 8780                             	        vmovdqa	160(%r15), %xmm7
 8781                             	        jl	L_AES_GCM_decrypt_avx1_aesenc_128_ghash_avx_done
 8782                             	        vaesenc	%xmm7, %xmm8, %xmm8
 8783                             	        vaesenc	%xmm7, %xmm9, %xmm9
 8784                             	        vaesenc	%xmm7, %xmm10, %xmm10
 8785                             	        vaesenc	%xmm7, %xmm11, %xmm11
 8786                             	        vaesenc	%xmm7, %xmm12, %xmm12
 8787                             	        vaesenc	%xmm7, %xmm13, %xmm13
 8788                             	        vaesenc	%xmm7, %xmm14, %xmm14
 8789                             	        vaesenc	%xmm7, %xmm15, %xmm15
 8790                             	        vmovdqa	176(%r15), %xmm7
 8791                             	        vaesenc	%xmm7, %xmm8, %xmm8
 8792                             	        vaesenc	%xmm7, %xmm9, %xmm9
 8793                             	        vaesenc	%xmm7, %xmm10, %xmm10
 8794                             	        vaesenc	%xmm7, %xmm11, %xmm11
 8795                             	        vaesenc	%xmm7, %xmm12, %xmm12
 8796                             	        vaesenc	%xmm7, %xmm13, %xmm13
 8797                             	        vaesenc	%xmm7, %xmm14, %xmm14
 8798                             	        vaesenc	%xmm7, %xmm15, %xmm15
 8799                             	        cmpl	$13, %r10d
 8800                             	        vmovdqa	192(%r15), %xmm7
 8801                             	        jl	L_AES_GCM_decrypt_avx1_aesenc_128_ghash_avx_done
 8802                             	        vaesenc	%xmm7, %xmm8, %xmm8
 8803                             	        vaesenc	%xmm7, %xmm9, %xmm9
 8804                             	        vaesenc	%xmm7, %xmm10, %xmm10
 8805                             	        vaesenc	%xmm7, %xmm11, %xmm11
 8806                             	        vaesenc	%xmm7, %xmm12, %xmm12
 8807                             	        vaesenc	%xmm7, %xmm13, %xmm13
 8808                             	        vaesenc	%xmm7, %xmm14, %xmm14
 8809                             	        vaesenc	%xmm7, %xmm15, %xmm15
 8810                             	        vmovdqa	208(%r15), %xmm7
 8811                             	        vaesenc	%xmm7, %xmm8, %xmm8
 8812                             	        vaesenc	%xmm7, %xmm9, %xmm9
 8813                             	        vaesenc	%xmm7, %xmm10, %xmm10
 8814                             	        vaesenc	%xmm7, %xmm11, %xmm11
 8815                             	        vaesenc	%xmm7, %xmm12, %xmm12
 8816                             	        vaesenc	%xmm7, %xmm13, %xmm13
 8817                             	        vaesenc	%xmm7, %xmm14, %xmm14
 8818                             	        vaesenc	%xmm7, %xmm15, %xmm15
 8819                             	        vmovdqa	224(%r15), %xmm7
 8820                             	L_AES_GCM_decrypt_avx1_aesenc_128_ghash_avx_done:
 8821                             	        vaesenclast	%xmm7, %xmm8, %xmm8
 8822                             	        vaesenclast	%xmm7, %xmm9, %xmm9
 8823                             	        vmovdqu	(%rcx), %xmm0
 8824                             	        vmovdqu	16(%rcx), %xmm1
 8825                             	        vpxor	%xmm0, %xmm8, %xmm8
 8826                             	        vpxor	%xmm1, %xmm9, %xmm9
 8827                             	        vmovdqu	%xmm8, (%rdx)
 8828                             	        vmovdqu	%xmm9, 16(%rdx)
 8829                             	        vaesenclast	%xmm7, %xmm10, %xmm10
 8830                             	        vaesenclast	%xmm7, %xmm11, %xmm11
 8831                             	        vmovdqu	32(%rcx), %xmm0
 8832                             	        vmovdqu	48(%rcx), %xmm1
 8833                             	        vpxor	%xmm0, %xmm10, %xmm10
 8834                             	        vpxor	%xmm1, %xmm11, %xmm11
 8835                             	        vmovdqu	%xmm10, 32(%rdx)
 8836                             	        vmovdqu	%xmm11, 48(%rdx)
 8837                             	        vaesenclast	%xmm7, %xmm12, %xmm12
 8838                             	        vaesenclast	%xmm7, %xmm13, %xmm13
 8839                             	        vmovdqu	64(%rcx), %xmm0
 8840                             	        vmovdqu	80(%rcx), %xmm1
 8841                             	        vpxor	%xmm0, %xmm12, %xmm12
 8842                             	        vpxor	%xmm1, %xmm13, %xmm13
 8843                             	        vmovdqu	%xmm12, 64(%rdx)
 8844                             	        vmovdqu	%xmm13, 80(%rdx)
 8845                             	        vaesenclast	%xmm7, %xmm14, %xmm14
 8846                             	        vaesenclast	%xmm7, %xmm15, %xmm15
 8847                             	        vmovdqu	96(%rcx), %xmm0
 8848                             	        vmovdqu	112(%rcx), %xmm1
 8849                             	        vpxor	%xmm0, %xmm14, %xmm14
 8850                             	        vpxor	%xmm1, %xmm15, %xmm15
 8851                             	        vmovdqu	%xmm14, 96(%rdx)
 8852                             	        vmovdqu	%xmm15, 112(%rdx)
 8853                             	        addl	$0x80, %ebx
 8854                             	        cmpl	%r13d, %ebx
 8855                             	        jl	L_AES_GCM_decrypt_avx1_ghash_128
 8856                             	        vmovdqa	%xmm2, %xmm6
 8857                             	        vmovdqa	(%rsp), %xmm5
 8858                             	L_AES_GCM_decrypt_avx1_done_128:
 8859                             	        movl	%r9d, %edx
 8860                             	        cmpl	%edx, %ebx
 8861                             	        jge	L_AES_GCM_decrypt_avx1_done_dec
 8862                             	        movl	%r9d, %r13d
 8863                             	        andl	$0xfffffff0, %r13d
 8864                             	        cmpl	%r13d, %ebx
 8865                             	        jge	L_AES_GCM_decrypt_avx1_last_block_done
 8866                             	L_AES_GCM_decrypt_avx1_last_block_start:
 8867                             	        vmovdqu	(%rdi,%rbx,1), %xmm13
 8868                             	        vmovdqa	%xmm5, %xmm0
 8869                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm13, %xmm1
 8870                             	        vpxor	%xmm6, %xmm1, %xmm1
 8871                             	        vmovdqa	128(%rsp), %xmm9
 8872                             	        vpshufb	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm9, %xmm8
 8873                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm9, %xmm9
 8874                             	        vmovdqa	%xmm9, 128(%rsp)
 8875                             	        vpxor	(%r15), %xmm8, %xmm8
 8876                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm10
 8877                             	        vaesenc	16(%r15), %xmm8, %xmm8
 8878                             	        vaesenc	32(%r15), %xmm8, %xmm8
 8879                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm11
 8880                             	        vaesenc	48(%r15), %xmm8, %xmm8
 8881                             	        vaesenc	64(%r15), %xmm8, %xmm8
 8882                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm12
 8883                             	        vaesenc	80(%r15), %xmm8, %xmm8
 8884                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 8885                             	        vaesenc	96(%r15), %xmm8, %xmm8
 8886                             	        vpxor	%xmm11, %xmm10, %xmm10
 8887                             	        vpslldq	$8, %xmm10, %xmm2
 8888                             	        vpsrldq	$8, %xmm10, %xmm10
 8889                             	        vaesenc	112(%r15), %xmm8, %xmm8
 8890                             	        vpxor	%xmm12, %xmm2, %xmm2
 8891                             	        vpxor	%xmm10, %xmm1, %xmm3
 8892                             	        vmovdqa	L_avx1_aes_gcm_mod2_128(%rip), %xmm0
 8893                             	        vpclmulqdq	$16, %xmm0, %xmm2, %xmm11
 8894                             	        vaesenc	128(%r15), %xmm8, %xmm8
 8895                             	        vpshufd	$0x4e, %xmm2, %xmm10
 8896                             	        vpxor	%xmm11, %xmm10, %xmm10
 8897                             	        vpclmulqdq	$16, %xmm0, %xmm10, %xmm11
 8898                             	        vaesenc	144(%r15), %xmm8, %xmm8
 8899                             	        vpshufd	$0x4e, %xmm10, %xmm10
 8900                             	        vpxor	%xmm11, %xmm10, %xmm10
 8901                             	        vpxor	%xmm3, %xmm10, %xmm6
 8902                             	        cmpl	$11, %r10d
 8903                             	        vmovdqa	160(%r15), %xmm9
 8904                             	        jl	L_AES_GCM_decrypt_avx1_aesenc_gfmul_last
 8905                             	        vaesenc	%xmm9, %xmm8, %xmm8
 8906                             	        vaesenc	176(%r15), %xmm8, %xmm8
 8907                             	        cmpl	$13, %r10d
 8908                             	        vmovdqa	192(%r15), %xmm9
 8909                             	        jl	L_AES_GCM_decrypt_avx1_aesenc_gfmul_last
 8910                             	        vaesenc	%xmm9, %xmm8, %xmm8
 8911                             	        vaesenc	208(%r15), %xmm8, %xmm8
 8912                             	        vmovdqa	224(%r15), %xmm9
 8913                             	L_AES_GCM_decrypt_avx1_aesenc_gfmul_last:
 8914                             	        vaesenclast	%xmm9, %xmm8, %xmm8
 8915                             	        vmovdqa	%xmm13, %xmm0
 8916                             	        vpxor	%xmm0, %xmm8, %xmm8
 8917                             	        vmovdqu	%xmm8, (%rsi,%rbx,1)
 8918                             	        addl	$16, %ebx
 8919                             	        cmpl	%r13d, %ebx
 8920                             	        jl	L_AES_GCM_decrypt_avx1_last_block_start
 8921                             	L_AES_GCM_decrypt_avx1_last_block_done:
 8922                             	        movl	%r9d, %ecx
 8923                             	        movl	%ecx, %edx
 8924                             	        andl	$15, %ecx
 8925                             	        jz	L_AES_GCM_decrypt_avx1_aesenc_last15_dec_avx_done
 8926                             	        vmovdqa	128(%rsp), %xmm4
 8927                             	        vpshufb	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm4, %xmm4
 8928                             	        vpxor	(%r15), %xmm4, %xmm4
 8929                             	        vaesenc	16(%r15), %xmm4, %xmm4
 8930                             	        vaesenc	32(%r15), %xmm4, %xmm4
 8931                             	        vaesenc	48(%r15), %xmm4, %xmm4
 8932                             	        vaesenc	64(%r15), %xmm4, %xmm4
 8933                             	        vaesenc	80(%r15), %xmm4, %xmm4
 8934                             	        vaesenc	96(%r15), %xmm4, %xmm4
 8935                             	        vaesenc	112(%r15), %xmm4, %xmm4
 8936                             	        vaesenc	128(%r15), %xmm4, %xmm4
 8937                             	        vaesenc	144(%r15), %xmm4, %xmm4
 8938                             	        cmpl	$11, %r10d
 8939                             	        vmovdqa	160(%r15), %xmm9
 8940                             	        jl	L_AES_GCM_decrypt_avx1_aesenc_last15_dec_avx_aesenc_avx_last
 8941                             	        vaesenc	%xmm9, %xmm4, %xmm4
 8942                             	        vaesenc	176(%r15), %xmm4, %xmm4
 8943                             	        cmpl	$13, %r10d
 8944                             	        vmovdqa	192(%r15), %xmm9
 8945                             	        jl	L_AES_GCM_decrypt_avx1_aesenc_last15_dec_avx_aesenc_avx_last
 8946                             	        vaesenc	%xmm9, %xmm4, %xmm4
 8947                             	        vaesenc	208(%r15), %xmm4, %xmm4
 8948                             	        vmovdqa	224(%r15), %xmm9
 8949                             	L_AES_GCM_decrypt_avx1_aesenc_last15_dec_avx_aesenc_avx_last:
 8950                             	        vaesenclast	%xmm9, %xmm4, %xmm4
 8951                             	        subq	$32, %rsp
 8952                             	        xorl	%ecx, %ecx
 8953                             	        vmovdqu	%xmm4, (%rsp)
 8954                             	        vpxor	%xmm0, %xmm0, %xmm0
 8955                             	        vmovdqu	%xmm0, 16(%rsp)
 8956                             	L_AES_GCM_decrypt_avx1_aesenc_last15_dec_avx_loop:
 8957                             	        movzbl	(%rdi,%rbx,1), %r13d
 8958                             	        movb	%r13b, 16(%rsp,%rcx,1)
 8959                             	        xorb	(%rsp,%rcx,1), %r13b
 8960                             	        movb	%r13b, (%rsi,%rbx,1)
 8961                             	        incl	%ebx
 8962                             	        incl	%ecx
 8963                             	        cmpl	%edx, %ebx
 8964                             	        jl	L_AES_GCM_decrypt_avx1_aesenc_last15_dec_avx_loop
 8965                             	        vmovdqu	16(%rsp), %xmm4
 8966                             	        addq	$32, %rsp
 8967                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm4, %xmm4
 8968                             	        vpxor	%xmm4, %xmm6, %xmm6
 8969                             	        # ghash_gfmul_red_avx
 8970                             	        vpshufd	$0x4e, %xmm5, %xmm9
 8971                             	        vpshufd	$0x4e, %xmm6, %xmm10
 8972                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm11
 8973                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm8
 8974                             	        vpxor	%xmm5, %xmm9, %xmm9
 8975                             	        vpxor	%xmm6, %xmm10, %xmm10
 8976                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 8977                             	        vpxor	%xmm8, %xmm9, %xmm9
 8978                             	        vpxor	%xmm11, %xmm9, %xmm9
 8979                             	        vpslldq	$8, %xmm9, %xmm10
 8980                             	        vpsrldq	$8, %xmm9, %xmm9
 8981                             	        vpxor	%xmm10, %xmm8, %xmm8
 8982                             	        vpxor	%xmm9, %xmm11, %xmm6
 8983                             	        vpslld	$31, %xmm8, %xmm12
 8984                             	        vpslld	$30, %xmm8, %xmm13
 8985                             	        vpslld	$25, %xmm8, %xmm14
 8986                             	        vpxor	%xmm13, %xmm12, %xmm12
 8987                             	        vpxor	%xmm14, %xmm12, %xmm12
 8988                             	        vpsrldq	$4, %xmm12, %xmm13
 8989                             	        vpslldq	$12, %xmm12, %xmm12
 8990                             	        vpxor	%xmm12, %xmm8, %xmm8
 8991                             	        vpsrld	$0x01, %xmm8, %xmm14
 8992                             	        vpsrld	$2, %xmm8, %xmm10
 8993                             	        vpsrld	$7, %xmm8, %xmm9
 8994                             	        vpxor	%xmm10, %xmm14, %xmm14
 8995                             	        vpxor	%xmm9, %xmm14, %xmm14
 8996                             	        vpxor	%xmm13, %xmm14, %xmm14
 8997                             	        vpxor	%xmm8, %xmm14, %xmm14
 8998                             	        vpxor	%xmm14, %xmm6, %xmm6
 8999                             	L_AES_GCM_decrypt_avx1_aesenc_last15_dec_avx_done:
 9000                             	L_AES_GCM_decrypt_avx1_done_dec:
 9001                             	        movl	%r9d, %edx
 9002                             	        movl	%r11d, %ecx
 9003                             	        shlq	$3, %rdx
 9004                             	        shlq	$3, %rcx
 9005                             	        vpinsrq	$0x00, %rdx, %xmm0, %xmm0
 9006                             	        vpinsrq	$0x01, %rcx, %xmm0, %xmm0
 9007                             	        vpxor	%xmm0, %xmm6, %xmm6
 9008                             	        # ghash_gfmul_red_avx
 9009                             	        vpshufd	$0x4e, %xmm5, %xmm9
 9010                             	        vpshufd	$0x4e, %xmm6, %xmm10
 9011                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm11
 9012                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm8
 9013                             	        vpxor	%xmm5, %xmm9, %xmm9
 9014                             	        vpxor	%xmm6, %xmm10, %xmm10
 9015                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 9016                             	        vpxor	%xmm8, %xmm9, %xmm9
 9017                             	        vpxor	%xmm11, %xmm9, %xmm9
 9018                             	        vpslldq	$8, %xmm9, %xmm10
 9019                             	        vpsrldq	$8, %xmm9, %xmm9
 9020                             	        vpxor	%xmm10, %xmm8, %xmm8
 9021                             	        vpxor	%xmm9, %xmm11, %xmm6
 9022                             	        vpslld	$31, %xmm8, %xmm12
 9023                             	        vpslld	$30, %xmm8, %xmm13
 9024                             	        vpslld	$25, %xmm8, %xmm14
 9025                             	        vpxor	%xmm13, %xmm12, %xmm12
 9026                             	        vpxor	%xmm14, %xmm12, %xmm12
 9027                             	        vpsrldq	$4, %xmm12, %xmm13
 9028                             	        vpslldq	$12, %xmm12, %xmm12
 9029                             	        vpxor	%xmm12, %xmm8, %xmm8
 9030                             	        vpsrld	$0x01, %xmm8, %xmm14
 9031                             	        vpsrld	$2, %xmm8, %xmm10
 9032                             	        vpsrld	$7, %xmm8, %xmm9
 9033                             	        vpxor	%xmm10, %xmm14, %xmm14
 9034                             	        vpxor	%xmm9, %xmm14, %xmm14
 9035                             	        vpxor	%xmm13, %xmm14, %xmm14
 9036                             	        vpxor	%xmm8, %xmm14, %xmm14
 9037                             	        vpxor	%xmm14, %xmm6, %xmm6
 9038                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm6, %xmm6
 9039                             	        vpxor	144(%rsp), %xmm6, %xmm0
 9040                             	        cmpl	$16, %r14d
 9041                             	        je	L_AES_GCM_decrypt_avx1_cmp_tag_16
 9042                             	        subq	$16, %rsp
 9043                             	        xorq	%rcx, %rcx
 9044                             	        xorq	%rbx, %rbx
 9045                             	        vmovdqu	%xmm0, (%rsp)
 9046                             	L_AES_GCM_decrypt_avx1_cmp_tag_loop:
 9047                             	        movzbl	(%rsp,%rcx,1), %r13d
 9048                             	        xorb	(%r8,%rcx,1), %r13b
 9049                             	        orb	%r13b, %bl
 9050                             	        incl	%ecx
 9051                             	        cmpl	%r14d, %ecx
 9052                             	        jne	L_AES_GCM_decrypt_avx1_cmp_tag_loop
 9053                             	        cmpb	$0x00, %bl
 9054                             	        sete	%bl
 9055                             	        addq	$16, %rsp
 9056                             	        xorq	%rcx, %rcx
 9057                             	        jmp	L_AES_GCM_decrypt_avx1_cmp_tag_done
 9058                             	L_AES_GCM_decrypt_avx1_cmp_tag_16:
 9059                             	        vmovdqu	(%r8), %xmm1
 9060                             	        vpcmpeqb	%xmm1, %xmm0, %xmm0
 9061                             	        vpmovmskb	%xmm0, %rdx
 9062                             	        # %%edx == 0xFFFF then return 1 else => return 0
 9063                             	        xorl	%ebx, %ebx
 9064                             	        cmpl	$0xffff, %edx
 9065                             	        sete	%bl
 9066                             	L_AES_GCM_decrypt_avx1_cmp_tag_done:
 9067                             	        movl	%ebx, (%rbp)
 9068                             	        vzeroupper
 9069                             	        addq	$0xa8, %rsp
 9070                             	        popq	%rbp
 9071                             	        popq	%r15
 9072                             	        popq	%r14
 9073                             	        popq	%rbx
 9074                             	        popq	%r12
 9075                             	        popq	%r13
 9076                             	        repz retq
 9077                             	#ifndef __APPLE__
 9079                             	#endif /* __APPLE__ */
 9080                             	#ifdef WOLFSSL_AESGCM_STREAM
 9081                             	#ifndef __APPLE__
 9082                             	.text
 9083                             	.globl	AES_GCM_init_avx1
 9085                             	.align	16
 9086                             	AES_GCM_init_avx1:
 9087                             	#else
 9088                             	.section	__TEXT,__text
 9089                             	.globl	_AES_GCM_init_avx1
 9090                             	.p2align	4
 9091                             	_AES_GCM_init_avx1:
 9092                             	#endif /* __APPLE__ */
 9093                             	        pushq	%r12
 9094                             	        pushq	%r13
 9095                             	        movq	%rdx, %r10
 9096                             	        movl	%ecx, %r11d
 9097                             	        movq	24(%rsp), %rax
 9098                             	        subq	$16, %rsp
 9099                             	        vpxor	%xmm4, %xmm4, %xmm4
 9100                             	        movl	%r11d, %edx
 9101                             	        cmpl	$12, %edx
 9102                             	        jne	L_AES_GCM_init_avx1_iv_not_12
 9103                             	        # # Calculate values when IV is 12 bytes
 9104                             	        # Set counter based on IV
 9105                             	        movl	$0x1000000, %ecx
 9106                             	        vpinsrq	$0x00, (%r10), %xmm4, %xmm4
 9107                             	        vpinsrd	$2, 8(%r10), %xmm4, %xmm4
 9108                             	        vpinsrd	$3, %ecx, %xmm4, %xmm4
 9109                             	        # H = Encrypt X(=0) and T = Encrypt counter
 9110                             	        vmovdqa	(%rdi), %xmm5
 9111                             	        vpxor	%xmm5, %xmm4, %xmm1
 9112                             	        vmovdqa	16(%rdi), %xmm7
 9113                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9114                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9115                             	        vmovdqa	32(%rdi), %xmm7
 9116                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9117                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9118                             	        vmovdqa	48(%rdi), %xmm7
 9119                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9120                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9121                             	        vmovdqa	64(%rdi), %xmm7
 9122                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9123                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9124                             	        vmovdqa	80(%rdi), %xmm7
 9125                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9126                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9127                             	        vmovdqa	96(%rdi), %xmm7
 9128                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9129                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9130                             	        vmovdqa	112(%rdi), %xmm7
 9131                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9132                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9133                             	        vmovdqa	128(%rdi), %xmm7
 9134                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9135                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9136                             	        vmovdqa	144(%rdi), %xmm7
 9137                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9138                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9139                             	        cmpl	$11, %esi
 9140                             	        vmovdqa	160(%rdi), %xmm7
 9141                             	        jl	L_AES_GCM_init_avx1_calc_iv_12_last
 9142                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9143                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9144                             	        vmovdqa	176(%rdi), %xmm7
 9145                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9146                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9147                             	        cmpl	$13, %esi
 9148                             	        vmovdqa	192(%rdi), %xmm7
 9149                             	        jl	L_AES_GCM_init_avx1_calc_iv_12_last
 9150                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9151                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9152                             	        vmovdqa	208(%rdi), %xmm7
 9153                             	        vaesenc	%xmm7, %xmm5, %xmm5
 9154                             	        vaesenc	%xmm7, %xmm1, %xmm1
 9155                             	        vmovdqa	224(%rdi), %xmm7
 9156                             	L_AES_GCM_init_avx1_calc_iv_12_last:
 9157                             	        vaesenclast	%xmm7, %xmm5, %xmm5
 9158                             	        vaesenclast	%xmm7, %xmm1, %xmm1
 9159                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm5, %xmm5
 9160                             	        vmovdqa	%xmm1, %xmm15
 9161                             	        jmp	L_AES_GCM_init_avx1_iv_done
 9162                             	L_AES_GCM_init_avx1_iv_not_12:
 9163                             	        # Calculate values when IV is not 12 bytes
 9164                             	        # H = Encrypt X(=0)
 9165                             	        vmovdqa	(%rdi), %xmm5
 9166                             	        vaesenc	16(%rdi), %xmm5, %xmm5
 9167                             	        vaesenc	32(%rdi), %xmm5, %xmm5
 9168                             	        vaesenc	48(%rdi), %xmm5, %xmm5
 9169                             	        vaesenc	64(%rdi), %xmm5, %xmm5
 9170                             	        vaesenc	80(%rdi), %xmm5, %xmm5
 9171                             	        vaesenc	96(%rdi), %xmm5, %xmm5
 9172                             	        vaesenc	112(%rdi), %xmm5, %xmm5
 9173                             	        vaesenc	128(%rdi), %xmm5, %xmm5
 9174                             	        vaesenc	144(%rdi), %xmm5, %xmm5
 9175                             	        cmpl	$11, %esi
 9176                             	        vmovdqa	160(%rdi), %xmm9
 9177                             	        jl	L_AES_GCM_init_avx1_calc_iv_1_aesenc_avx_last
 9178                             	        vaesenc	%xmm9, %xmm5, %xmm5
 9179                             	        vaesenc	176(%rdi), %xmm5, %xmm5
 9180                             	        cmpl	$13, %esi
 9181                             	        vmovdqa	192(%rdi), %xmm9
 9182                             	        jl	L_AES_GCM_init_avx1_calc_iv_1_aesenc_avx_last
 9183                             	        vaesenc	%xmm9, %xmm5, %xmm5
 9184                             	        vaesenc	208(%rdi), %xmm5, %xmm5
 9185                             	        vmovdqa	224(%rdi), %xmm9
 9186                             	L_AES_GCM_init_avx1_calc_iv_1_aesenc_avx_last:
 9187                             	        vaesenclast	%xmm9, %xmm5, %xmm5
 9188                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm5, %xmm5
 9189                             	        # Calc counter
 9190                             	        # Initialization vector
 9191                             	        cmpl	$0x00, %edx
 9192                             	        movq	$0x00, %rcx
 9193                             	        je	L_AES_GCM_init_avx1_calc_iv_done
 9194                             	        cmpl	$16, %edx
 9195                             	        jl	L_AES_GCM_init_avx1_calc_iv_lt16
 9196                             	        andl	$0xfffffff0, %edx
 9197                             	L_AES_GCM_init_avx1_calc_iv_16_loop:
 9198                             	        vmovdqu	(%r10,%rcx,1), %xmm8
 9199                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 9200                             	        vpxor	%xmm8, %xmm4, %xmm4
 9201                             	        # ghash_gfmul_avx
 9202                             	        vpshufd	$0x4e, %xmm4, %xmm1
 9203                             	        vpshufd	$0x4e, %xmm5, %xmm2
 9204                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 9205                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 9206                             	        vpxor	%xmm4, %xmm1, %xmm1
 9207                             	        vpxor	%xmm5, %xmm2, %xmm2
 9208                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 9209                             	        vpxor	%xmm0, %xmm1, %xmm1
 9210                             	        vpxor	%xmm3, %xmm1, %xmm1
 9211                             	        vmovdqa	%xmm0, %xmm7
 9212                             	        vmovdqa	%xmm3, %xmm4
 9213                             	        vpslldq	$8, %xmm1, %xmm2
 9214                             	        vpsrldq	$8, %xmm1, %xmm1
 9215                             	        vpxor	%xmm2, %xmm7, %xmm7
 9216                             	        vpxor	%xmm1, %xmm4, %xmm4
 9217                             	        vpsrld	$31, %xmm7, %xmm0
 9218                             	        vpsrld	$31, %xmm4, %xmm1
 9219                             	        vpslld	$0x01, %xmm7, %xmm7
 9220                             	        vpslld	$0x01, %xmm4, %xmm4
 9221                             	        vpsrldq	$12, %xmm0, %xmm2
 9222                             	        vpslldq	$4, %xmm0, %xmm0
 9223                             	        vpslldq	$4, %xmm1, %xmm1
 9224                             	        vpor	%xmm2, %xmm4, %xmm4
 9225                             	        vpor	%xmm0, %xmm7, %xmm7
 9226                             	        vpor	%xmm1, %xmm4, %xmm4
 9227                             	        vpslld	$31, %xmm7, %xmm0
 9228                             	        vpslld	$30, %xmm7, %xmm1
 9229                             	        vpslld	$25, %xmm7, %xmm2
 9230                             	        vpxor	%xmm1, %xmm0, %xmm0
 9231                             	        vpxor	%xmm2, %xmm0, %xmm0
 9232                             	        vmovdqa	%xmm0, %xmm1
 9233                             	        vpsrldq	$4, %xmm1, %xmm1
 9234                             	        vpslldq	$12, %xmm0, %xmm0
 9235                             	        vpxor	%xmm0, %xmm7, %xmm7
 9236                             	        vpsrld	$0x01, %xmm7, %xmm2
 9237                             	        vpsrld	$2, %xmm7, %xmm3
 9238                             	        vpsrld	$7, %xmm7, %xmm0
 9239                             	        vpxor	%xmm3, %xmm2, %xmm2
 9240                             	        vpxor	%xmm0, %xmm2, %xmm2
 9241                             	        vpxor	%xmm1, %xmm2, %xmm2
 9242                             	        vpxor	%xmm7, %xmm2, %xmm2
 9243                             	        vpxor	%xmm2, %xmm4, %xmm4
 9244                             	        addl	$16, %ecx
 9245                             	        cmpl	%edx, %ecx
 9246                             	        jl	L_AES_GCM_init_avx1_calc_iv_16_loop
 9247                             	        movl	%r11d, %edx
 9248                             	        cmpl	%edx, %ecx
 9249                             	        je	L_AES_GCM_init_avx1_calc_iv_done
 9250                             	L_AES_GCM_init_avx1_calc_iv_lt16:
 9251                             	        subq	$16, %rsp
 9252                             	        vpxor	%xmm8, %xmm8, %xmm8
 9253                             	        xorl	%r13d, %r13d
 9254                             	        vmovdqu	%xmm8, (%rsp)
 9255                             	L_AES_GCM_init_avx1_calc_iv_loop:
 9256                             	        movzbl	(%r10,%rcx,1), %r12d
 9257                             	        movb	%r12b, (%rsp,%r13,1)
 9258                             	        incl	%ecx
 9259                             	        incl	%r13d
 9260                             	        cmpl	%edx, %ecx
 9261                             	        jl	L_AES_GCM_init_avx1_calc_iv_loop
 9262                             	        vmovdqu	(%rsp), %xmm8
 9263                             	        addq	$16, %rsp
 9264                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 9265                             	        vpxor	%xmm8, %xmm4, %xmm4
 9266                             	        # ghash_gfmul_avx
 9267                             	        vpshufd	$0x4e, %xmm4, %xmm1
 9268                             	        vpshufd	$0x4e, %xmm5, %xmm2
 9269                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 9270                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 9271                             	        vpxor	%xmm4, %xmm1, %xmm1
 9272                             	        vpxor	%xmm5, %xmm2, %xmm2
 9273                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 9274                             	        vpxor	%xmm0, %xmm1, %xmm1
 9275                             	        vpxor	%xmm3, %xmm1, %xmm1
 9276                             	        vmovdqa	%xmm0, %xmm7
 9277                             	        vmovdqa	%xmm3, %xmm4
 9278                             	        vpslldq	$8, %xmm1, %xmm2
 9279                             	        vpsrldq	$8, %xmm1, %xmm1
 9280                             	        vpxor	%xmm2, %xmm7, %xmm7
 9281                             	        vpxor	%xmm1, %xmm4, %xmm4
 9282                             	        vpsrld	$31, %xmm7, %xmm0
 9283                             	        vpsrld	$31, %xmm4, %xmm1
 9284                             	        vpslld	$0x01, %xmm7, %xmm7
 9285                             	        vpslld	$0x01, %xmm4, %xmm4
 9286                             	        vpsrldq	$12, %xmm0, %xmm2
 9287                             	        vpslldq	$4, %xmm0, %xmm0
 9288                             	        vpslldq	$4, %xmm1, %xmm1
 9289                             	        vpor	%xmm2, %xmm4, %xmm4
 9290                             	        vpor	%xmm0, %xmm7, %xmm7
 9291                             	        vpor	%xmm1, %xmm4, %xmm4
 9292                             	        vpslld	$31, %xmm7, %xmm0
 9293                             	        vpslld	$30, %xmm7, %xmm1
 9294                             	        vpslld	$25, %xmm7, %xmm2
 9295                             	        vpxor	%xmm1, %xmm0, %xmm0
 9296                             	        vpxor	%xmm2, %xmm0, %xmm0
 9297                             	        vmovdqa	%xmm0, %xmm1
 9298                             	        vpsrldq	$4, %xmm1, %xmm1
 9299                             	        vpslldq	$12, %xmm0, %xmm0
 9300                             	        vpxor	%xmm0, %xmm7, %xmm7
 9301                             	        vpsrld	$0x01, %xmm7, %xmm2
 9302                             	        vpsrld	$2, %xmm7, %xmm3
 9303                             	        vpsrld	$7, %xmm7, %xmm0
 9304                             	        vpxor	%xmm3, %xmm2, %xmm2
 9305                             	        vpxor	%xmm0, %xmm2, %xmm2
 9306                             	        vpxor	%xmm1, %xmm2, %xmm2
 9307                             	        vpxor	%xmm7, %xmm2, %xmm2
 9308                             	        vpxor	%xmm2, %xmm4, %xmm4
 9309                             	L_AES_GCM_init_avx1_calc_iv_done:
 9310                             	        # T = Encrypt counter
 9311                             	        vpxor	%xmm0, %xmm0, %xmm0
 9312                             	        shll	$3, %edx
 9313                             	        vpinsrq	$0x00, %rdx, %xmm0, %xmm0
 9314                             	        vpxor	%xmm0, %xmm4, %xmm4
 9315                             	        # ghash_gfmul_avx
 9316                             	        vpshufd	$0x4e, %xmm4, %xmm1
 9317                             	        vpshufd	$0x4e, %xmm5, %xmm2
 9318                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 9319                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 9320                             	        vpxor	%xmm4, %xmm1, %xmm1
 9321                             	        vpxor	%xmm5, %xmm2, %xmm2
 9322                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 9323                             	        vpxor	%xmm0, %xmm1, %xmm1
 9324                             	        vpxor	%xmm3, %xmm1, %xmm1
 9325                             	        vmovdqa	%xmm0, %xmm7
 9326                             	        vmovdqa	%xmm3, %xmm4
 9327                             	        vpslldq	$8, %xmm1, %xmm2
 9328                             	        vpsrldq	$8, %xmm1, %xmm1
 9329                             	        vpxor	%xmm2, %xmm7, %xmm7
 9330                             	        vpxor	%xmm1, %xmm4, %xmm4
 9331                             	        vpsrld	$31, %xmm7, %xmm0
 9332                             	        vpsrld	$31, %xmm4, %xmm1
 9333                             	        vpslld	$0x01, %xmm7, %xmm7
 9334                             	        vpslld	$0x01, %xmm4, %xmm4
 9335                             	        vpsrldq	$12, %xmm0, %xmm2
 9336                             	        vpslldq	$4, %xmm0, %xmm0
 9337                             	        vpslldq	$4, %xmm1, %xmm1
 9338                             	        vpor	%xmm2, %xmm4, %xmm4
 9339                             	        vpor	%xmm0, %xmm7, %xmm7
 9340                             	        vpor	%xmm1, %xmm4, %xmm4
 9341                             	        vpslld	$31, %xmm7, %xmm0
 9342                             	        vpslld	$30, %xmm7, %xmm1
 9343                             	        vpslld	$25, %xmm7, %xmm2
 9344                             	        vpxor	%xmm1, %xmm0, %xmm0
 9345                             	        vpxor	%xmm2, %xmm0, %xmm0
 9346                             	        vmovdqa	%xmm0, %xmm1
 9347                             	        vpsrldq	$4, %xmm1, %xmm1
 9348                             	        vpslldq	$12, %xmm0, %xmm0
 9349                             	        vpxor	%xmm0, %xmm7, %xmm7
 9350                             	        vpsrld	$0x01, %xmm7, %xmm2
 9351                             	        vpsrld	$2, %xmm7, %xmm3
 9352                             	        vpsrld	$7, %xmm7, %xmm0
 9353                             	        vpxor	%xmm3, %xmm2, %xmm2
 9354                             	        vpxor	%xmm0, %xmm2, %xmm2
 9355                             	        vpxor	%xmm1, %xmm2, %xmm2
 9356                             	        vpxor	%xmm7, %xmm2, %xmm2
 9357                             	        vpxor	%xmm2, %xmm4, %xmm4
 9358                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm4, %xmm4
 9359                             	        #   Encrypt counter
 9360                             	        vmovdqa	(%rdi), %xmm8
 9361                             	        vpxor	%xmm4, %xmm8, %xmm8
 9362                             	        vaesenc	16(%rdi), %xmm8, %xmm8
 9363                             	        vaesenc	32(%rdi), %xmm8, %xmm8
 9364                             	        vaesenc	48(%rdi), %xmm8, %xmm8
 9365                             	        vaesenc	64(%rdi), %xmm8, %xmm8
 9366                             	        vaesenc	80(%rdi), %xmm8, %xmm8
 9367                             	        vaesenc	96(%rdi), %xmm8, %xmm8
 9368                             	        vaesenc	112(%rdi), %xmm8, %xmm8
 9369                             	        vaesenc	128(%rdi), %xmm8, %xmm8
 9370                             	        vaesenc	144(%rdi), %xmm8, %xmm8
 9371                             	        cmpl	$11, %esi
 9372                             	        vmovdqa	160(%rdi), %xmm9
 9373                             	        jl	L_AES_GCM_init_avx1_calc_iv_2_aesenc_avx_last
 9374                             	        vaesenc	%xmm9, %xmm8, %xmm8
 9375                             	        vaesenc	176(%rdi), %xmm8, %xmm8
 9376                             	        cmpl	$13, %esi
 9377                             	        vmovdqa	192(%rdi), %xmm9
 9378                             	        jl	L_AES_GCM_init_avx1_calc_iv_2_aesenc_avx_last
 9379                             	        vaesenc	%xmm9, %xmm8, %xmm8
 9380                             	        vaesenc	208(%rdi), %xmm8, %xmm8
 9381                             	        vmovdqa	224(%rdi), %xmm9
 9382                             	L_AES_GCM_init_avx1_calc_iv_2_aesenc_avx_last:
 9383                             	        vaesenclast	%xmm9, %xmm8, %xmm8
 9384                             	        vmovdqa	%xmm8, %xmm15
 9385                             	L_AES_GCM_init_avx1_iv_done:
 9386                             	        vmovdqa	%xmm15, (%rax)
 9387                             	        vpshufb	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm4, %xmm4
 9388                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm4, %xmm4
 9389                             	        vmovdqa	%xmm5, (%r8)
 9390                             	        vmovdqa	%xmm4, (%r9)
 9391                             	        vzeroupper
 9392                             	        addq	$16, %rsp
 9393                             	        popq	%r13
 9394                             	        popq	%r12
 9395                             	        repz retq
 9396                             	#ifndef __APPLE__
 9398                             	#endif /* __APPLE__ */
 9399                             	#ifndef __APPLE__
 9400                             	.text
 9401                             	.globl	AES_GCM_aad_update_avx1
 9403                             	.align	16
 9404                             	AES_GCM_aad_update_avx1:
 9405                             	#else
 9406                             	.section	__TEXT,__text
 9407                             	.globl	_AES_GCM_aad_update_avx1
 9408                             	.p2align	4
 9409                             	_AES_GCM_aad_update_avx1:
 9410                             	#endif /* __APPLE__ */
 9411                             	        movq	%rcx, %rax
 9412                             	        vmovdqa	(%rdx), %xmm5
 9413                             	        vmovdqa	(%rax), %xmm6
 9414                             	        xorl	%ecx, %ecx
 9415                             	L_AES_GCM_aad_update_avx1_16_loop:
 9416                             	        vmovdqu	(%rdi,%rcx,1), %xmm8
 9417                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 9418                             	        vpxor	%xmm8, %xmm5, %xmm5
 9419                             	        # ghash_gfmul_avx
 9420                             	        vpshufd	$0x4e, %xmm5, %xmm1
 9421                             	        vpshufd	$0x4e, %xmm6, %xmm2
 9422                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm3
 9423                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm0
 9424                             	        vpxor	%xmm5, %xmm1, %xmm1
 9425                             	        vpxor	%xmm6, %xmm2, %xmm2
 9426                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 9427                             	        vpxor	%xmm0, %xmm1, %xmm1
 9428                             	        vpxor	%xmm3, %xmm1, %xmm1
 9429                             	        vmovdqa	%xmm0, %xmm4
 9430                             	        vmovdqa	%xmm3, %xmm5
 9431                             	        vpslldq	$8, %xmm1, %xmm2
 9432                             	        vpsrldq	$8, %xmm1, %xmm1
 9433                             	        vpxor	%xmm2, %xmm4, %xmm4
 9434                             	        vpxor	%xmm1, %xmm5, %xmm5
 9435                             	        vpsrld	$31, %xmm4, %xmm0
 9436                             	        vpsrld	$31, %xmm5, %xmm1
 9437                             	        vpslld	$0x01, %xmm4, %xmm4
 9438                             	        vpslld	$0x01, %xmm5, %xmm5
 9439                             	        vpsrldq	$12, %xmm0, %xmm2
 9440                             	        vpslldq	$4, %xmm0, %xmm0
 9441                             	        vpslldq	$4, %xmm1, %xmm1
 9442                             	        vpor	%xmm2, %xmm5, %xmm5
 9443                             	        vpor	%xmm0, %xmm4, %xmm4
 9444                             	        vpor	%xmm1, %xmm5, %xmm5
 9445                             	        vpslld	$31, %xmm4, %xmm0
 9446                             	        vpslld	$30, %xmm4, %xmm1
 9447                             	        vpslld	$25, %xmm4, %xmm2
 9448                             	        vpxor	%xmm1, %xmm0, %xmm0
 9449                             	        vpxor	%xmm2, %xmm0, %xmm0
 9450                             	        vmovdqa	%xmm0, %xmm1
 9451                             	        vpsrldq	$4, %xmm1, %xmm1
 9452                             	        vpslldq	$12, %xmm0, %xmm0
 9453                             	        vpxor	%xmm0, %xmm4, %xmm4
 9454                             	        vpsrld	$0x01, %xmm4, %xmm2
 9455                             	        vpsrld	$2, %xmm4, %xmm3
 9456                             	        vpsrld	$7, %xmm4, %xmm0
 9457                             	        vpxor	%xmm3, %xmm2, %xmm2
 9458                             	        vpxor	%xmm0, %xmm2, %xmm2
 9459                             	        vpxor	%xmm1, %xmm2, %xmm2
 9460                             	        vpxor	%xmm4, %xmm2, %xmm2
 9461                             	        vpxor	%xmm2, %xmm5, %xmm5
 9462                             	        addl	$16, %ecx
 9463                             	        cmpl	%esi, %ecx
 9464                             	        jl	L_AES_GCM_aad_update_avx1_16_loop
 9465                             	        vmovdqa	%xmm5, (%rdx)
 9466                             	        vzeroupper
 9467                             	        repz retq
 9468                             	#ifndef __APPLE__
 9470                             	#endif /* __APPLE__ */
 9471                             	#ifndef __APPLE__
 9472                             	.text
 9473                             	.globl	AES_GCM_encrypt_block_avx1
 9475                             	.align	16
 9476                             	AES_GCM_encrypt_block_avx1:
 9477                             	#else
 9478                             	.section	__TEXT,__text
 9479                             	.globl	_AES_GCM_encrypt_block_avx1
 9480                             	.p2align	4
 9481                             	_AES_GCM_encrypt_block_avx1:
 9482                             	#endif /* __APPLE__ */
 9483                             	        movq	%rdx, %r10
 9484                             	        movq	%rcx, %r11
 9485                             	        vmovdqa	(%r8), %xmm9
 9486                             	        vpshufb	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm9, %xmm8
 9487                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm9, %xmm9
 9488                             	        vmovdqa	%xmm9, (%r8)
 9489                             	        vpxor	(%rdi), %xmm8, %xmm8
 9490                             	        vaesenc	16(%rdi), %xmm8, %xmm8
 9491                             	        vaesenc	32(%rdi), %xmm8, %xmm8
 9492                             	        vaesenc	48(%rdi), %xmm8, %xmm8
 9493                             	        vaesenc	64(%rdi), %xmm8, %xmm8
 9494                             	        vaesenc	80(%rdi), %xmm8, %xmm8
 9495                             	        vaesenc	96(%rdi), %xmm8, %xmm8
 9496                             	        vaesenc	112(%rdi), %xmm8, %xmm8
 9497                             	        vaesenc	128(%rdi), %xmm8, %xmm8
 9498                             	        vaesenc	144(%rdi), %xmm8, %xmm8
 9499                             	        cmpl	$11, %esi
 9500                             	        vmovdqa	160(%rdi), %xmm9
 9501                             	        jl	L_AES_GCM_encrypt_block_avx1_aesenc_block_last
 9502                             	        vaesenc	%xmm9, %xmm8, %xmm8
 9503                             	        vaesenc	176(%rdi), %xmm8, %xmm8
 9504                             	        cmpl	$13, %esi
 9505                             	        vmovdqa	192(%rdi), %xmm9
 9506                             	        jl	L_AES_GCM_encrypt_block_avx1_aesenc_block_last
 9507                             	        vaesenc	%xmm9, %xmm8, %xmm8
 9508                             	        vaesenc	208(%rdi), %xmm8, %xmm8
 9509                             	        vmovdqa	224(%rdi), %xmm9
 9510                             	L_AES_GCM_encrypt_block_avx1_aesenc_block_last:
 9511                             	        vaesenclast	%xmm9, %xmm8, %xmm8
 9512                             	        vmovdqu	(%r11), %xmm9
 9513                             	        vpxor	%xmm9, %xmm8, %xmm8
 9514                             	        vmovdqu	%xmm8, (%r10)
 9515                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 9516                             	        vzeroupper
 9517                             	        repz retq
 9518                             	#ifndef __APPLE__
 9520                             	#endif /* __APPLE__ */
 9521                             	#ifndef __APPLE__
 9522                             	.text
 9523                             	.globl	AES_GCM_ghash_block_avx1
 9525                             	.align	16
 9526                             	AES_GCM_ghash_block_avx1:
 9527                             	#else
 9528                             	.section	__TEXT,__text
 9529                             	.globl	_AES_GCM_ghash_block_avx1
 9530                             	.p2align	4
 9531                             	_AES_GCM_ghash_block_avx1:
 9532                             	#endif /* __APPLE__ */
 9533                             	        vmovdqa	(%rsi), %xmm4
 9534                             	        vmovdqa	(%rdx), %xmm5
 9535                             	        vmovdqu	(%rdi), %xmm8
 9536                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 9537                             	        vpxor	%xmm8, %xmm4, %xmm4
 9538                             	        # ghash_gfmul_avx
 9539                             	        vpshufd	$0x4e, %xmm4, %xmm1
 9540                             	        vpshufd	$0x4e, %xmm5, %xmm2
 9541                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 9542                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 9543                             	        vpxor	%xmm4, %xmm1, %xmm1
 9544                             	        vpxor	%xmm5, %xmm2, %xmm2
 9545                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 9546                             	        vpxor	%xmm0, %xmm1, %xmm1
 9547                             	        vpxor	%xmm3, %xmm1, %xmm1
 9548                             	        vmovdqa	%xmm0, %xmm6
 9549                             	        vmovdqa	%xmm3, %xmm4
 9550                             	        vpslldq	$8, %xmm1, %xmm2
 9551                             	        vpsrldq	$8, %xmm1, %xmm1
 9552                             	        vpxor	%xmm2, %xmm6, %xmm6
 9553                             	        vpxor	%xmm1, %xmm4, %xmm4
 9554                             	        vpsrld	$31, %xmm6, %xmm0
 9555                             	        vpsrld	$31, %xmm4, %xmm1
 9556                             	        vpslld	$0x01, %xmm6, %xmm6
 9557                             	        vpslld	$0x01, %xmm4, %xmm4
 9558                             	        vpsrldq	$12, %xmm0, %xmm2
 9559                             	        vpslldq	$4, %xmm0, %xmm0
 9560                             	        vpslldq	$4, %xmm1, %xmm1
 9561                             	        vpor	%xmm2, %xmm4, %xmm4
 9562                             	        vpor	%xmm0, %xmm6, %xmm6
 9563                             	        vpor	%xmm1, %xmm4, %xmm4
 9564                             	        vpslld	$31, %xmm6, %xmm0
 9565                             	        vpslld	$30, %xmm6, %xmm1
 9566                             	        vpslld	$25, %xmm6, %xmm2
 9567                             	        vpxor	%xmm1, %xmm0, %xmm0
 9568                             	        vpxor	%xmm2, %xmm0, %xmm0
 9569                             	        vmovdqa	%xmm0, %xmm1
 9570                             	        vpsrldq	$4, %xmm1, %xmm1
 9571                             	        vpslldq	$12, %xmm0, %xmm0
 9572                             	        vpxor	%xmm0, %xmm6, %xmm6
 9573                             	        vpsrld	$0x01, %xmm6, %xmm2
 9574                             	        vpsrld	$2, %xmm6, %xmm3
 9575                             	        vpsrld	$7, %xmm6, %xmm0
 9576                             	        vpxor	%xmm3, %xmm2, %xmm2
 9577                             	        vpxor	%xmm0, %xmm2, %xmm2
 9578                             	        vpxor	%xmm1, %xmm2, %xmm2
 9579                             	        vpxor	%xmm6, %xmm2, %xmm2
 9580                             	        vpxor	%xmm2, %xmm4, %xmm4
 9581                             	        vmovdqa	%xmm4, (%rsi)
 9582                             	        vzeroupper
 9583                             	        repz retq
 9584                             	#ifndef __APPLE__
 9586                             	#endif /* __APPLE__ */
 9587                             	#ifndef __APPLE__
 9588                             	.text
 9589                             	.globl	AES_GCM_encrypt_update_avx1
 9591                             	.align	16
 9592                             	AES_GCM_encrypt_update_avx1:
 9593                             	#else
 9594                             	.section	__TEXT,__text
 9595                             	.globl	_AES_GCM_encrypt_update_avx1
 9596                             	.p2align	4
 9597                             	_AES_GCM_encrypt_update_avx1:
 9598                             	#endif /* __APPLE__ */
 9599                             	        pushq	%r13
 9600                             	        pushq	%r12
 9601                             	        pushq	%r14
 9602                             	        movq	%rdx, %r10
 9603                             	        movq	%rcx, %r11
 9604                             	        movq	32(%rsp), %rax
 9605                             	        movq	40(%rsp), %r12
 9606                             	        subq	$0xa0, %rsp
 9607                             	        vmovdqa	(%r9), %xmm6
 9608                             	        vmovdqa	(%rax), %xmm5
 9609                             	        vpsrlq	$63, %xmm5, %xmm9
 9610                             	        vpsllq	$0x01, %xmm5, %xmm8
 9611                             	        vpslldq	$8, %xmm9, %xmm9
 9612                             	        vpor	%xmm9, %xmm8, %xmm8
 9613                             	        vpshufd	$0xff, %xmm5, %xmm5
 9614                             	        vpsrad	$31, %xmm5, %xmm5
 9615                             	        vpand	L_avx1_aes_gcm_mod2_128(%rip), %xmm5, %xmm5
 9616                             	        vpxor	%xmm8, %xmm5, %xmm5
 9617                             	        xorl	%r14d, %r14d
 9618                             	        cmpl	$0x80, %r8d
 9619                             	        movl	%r8d, %r13d
 9620                             	        jl	L_AES_GCM_encrypt_update_avx1_done_128
 9621                             	        andl	$0xffffff80, %r13d
 9622                             	        vmovdqa	%xmm6, %xmm2
 9623                             	        # H ^ 1
 9624                             	        vmovdqa	%xmm5, (%rsp)
 9625                             	        # H ^ 2
 9626                             	        vpclmulqdq	$0x00, %xmm5, %xmm5, %xmm8
 9627                             	        vpclmulqdq	$0x11, %xmm5, %xmm5, %xmm0
 9628                             	        vpslld	$31, %xmm8, %xmm12
 9629                             	        vpslld	$30, %xmm8, %xmm13
 9630                             	        vpslld	$25, %xmm8, %xmm14
 9631                             	        vpxor	%xmm13, %xmm12, %xmm12
 9632                             	        vpxor	%xmm14, %xmm12, %xmm12
 9633                             	        vpsrldq	$4, %xmm12, %xmm13
 9634                             	        vpslldq	$12, %xmm12, %xmm12
 9635                             	        vpxor	%xmm12, %xmm8, %xmm8
 9636                             	        vpsrld	$0x01, %xmm8, %xmm14
 9637                             	        vpsrld	$2, %xmm8, %xmm10
 9638                             	        vpsrld	$7, %xmm8, %xmm9
 9639                             	        vpxor	%xmm10, %xmm14, %xmm14
 9640                             	        vpxor	%xmm9, %xmm14, %xmm14
 9641                             	        vpxor	%xmm13, %xmm14, %xmm14
 9642                             	        vpxor	%xmm8, %xmm14, %xmm14
 9643                             	        vpxor	%xmm14, %xmm0, %xmm0
 9644                             	        vmovdqa	%xmm0, 16(%rsp)
 9645                             	        # H ^ 3
 9646                             	        # ghash_gfmul_red_avx
 9647                             	        vpshufd	$0x4e, %xmm5, %xmm9
 9648                             	        vpshufd	$0x4e, %xmm0, %xmm10
 9649                             	        vpclmulqdq	$0x11, %xmm5, %xmm0, %xmm11
 9650                             	        vpclmulqdq	$0x00, %xmm5, %xmm0, %xmm8
 9651                             	        vpxor	%xmm5, %xmm9, %xmm9
 9652                             	        vpxor	%xmm0, %xmm10, %xmm10
 9653                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 9654                             	        vpxor	%xmm8, %xmm9, %xmm9
 9655                             	        vpxor	%xmm11, %xmm9, %xmm9
 9656                             	        vpslldq	$8, %xmm9, %xmm10
 9657                             	        vpsrldq	$8, %xmm9, %xmm9
 9658                             	        vpxor	%xmm10, %xmm8, %xmm8
 9659                             	        vpxor	%xmm9, %xmm11, %xmm1
 9660                             	        vpslld	$31, %xmm8, %xmm12
 9661                             	        vpslld	$30, %xmm8, %xmm13
 9662                             	        vpslld	$25, %xmm8, %xmm14
 9663                             	        vpxor	%xmm13, %xmm12, %xmm12
 9664                             	        vpxor	%xmm14, %xmm12, %xmm12
 9665                             	        vpsrldq	$4, %xmm12, %xmm13
 9666                             	        vpslldq	$12, %xmm12, %xmm12
 9667                             	        vpxor	%xmm12, %xmm8, %xmm8
 9668                             	        vpsrld	$0x01, %xmm8, %xmm14
 9669                             	        vpsrld	$2, %xmm8, %xmm10
 9670                             	        vpsrld	$7, %xmm8, %xmm9
 9671                             	        vpxor	%xmm10, %xmm14, %xmm14
 9672                             	        vpxor	%xmm9, %xmm14, %xmm14
 9673                             	        vpxor	%xmm13, %xmm14, %xmm14
 9674                             	        vpxor	%xmm8, %xmm14, %xmm14
 9675                             	        vpxor	%xmm14, %xmm1, %xmm1
 9676                             	        vmovdqa	%xmm1, 32(%rsp)
 9677                             	        # H ^ 4
 9678                             	        vpclmulqdq	$0x00, %xmm0, %xmm0, %xmm8
 9679                             	        vpclmulqdq	$0x11, %xmm0, %xmm0, %xmm3
 9680                             	        vpslld	$31, %xmm8, %xmm12
 9681                             	        vpslld	$30, %xmm8, %xmm13
 9682                             	        vpslld	$25, %xmm8, %xmm14
 9683                             	        vpxor	%xmm13, %xmm12, %xmm12
 9684                             	        vpxor	%xmm14, %xmm12, %xmm12
 9685                             	        vpsrldq	$4, %xmm12, %xmm13
 9686                             	        vpslldq	$12, %xmm12, %xmm12
 9687                             	        vpxor	%xmm12, %xmm8, %xmm8
 9688                             	        vpsrld	$0x01, %xmm8, %xmm14
 9689                             	        vpsrld	$2, %xmm8, %xmm10
 9690                             	        vpsrld	$7, %xmm8, %xmm9
 9691                             	        vpxor	%xmm10, %xmm14, %xmm14
 9692                             	        vpxor	%xmm9, %xmm14, %xmm14
 9693                             	        vpxor	%xmm13, %xmm14, %xmm14
 9694                             	        vpxor	%xmm8, %xmm14, %xmm14
 9695                             	        vpxor	%xmm14, %xmm3, %xmm3
 9696                             	        vmovdqa	%xmm3, 48(%rsp)
 9697                             	        # H ^ 5
 9698                             	        # ghash_gfmul_red_avx
 9699                             	        vpshufd	$0x4e, %xmm0, %xmm9
 9700                             	        vpshufd	$0x4e, %xmm1, %xmm10
 9701                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm11
 9702                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm8
 9703                             	        vpxor	%xmm0, %xmm9, %xmm9
 9704                             	        vpxor	%xmm1, %xmm10, %xmm10
 9705                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 9706                             	        vpxor	%xmm8, %xmm9, %xmm9
 9707                             	        vpxor	%xmm11, %xmm9, %xmm9
 9708                             	        vpslldq	$8, %xmm9, %xmm10
 9709                             	        vpsrldq	$8, %xmm9, %xmm9
 9710                             	        vpxor	%xmm10, %xmm8, %xmm8
 9711                             	        vpxor	%xmm9, %xmm11, %xmm7
 9712                             	        vpslld	$31, %xmm8, %xmm12
 9713                             	        vpslld	$30, %xmm8, %xmm13
 9714                             	        vpslld	$25, %xmm8, %xmm14
 9715                             	        vpxor	%xmm13, %xmm12, %xmm12
 9716                             	        vpxor	%xmm14, %xmm12, %xmm12
 9717                             	        vpsrldq	$4, %xmm12, %xmm13
 9718                             	        vpslldq	$12, %xmm12, %xmm12
 9719                             	        vpxor	%xmm12, %xmm8, %xmm8
 9720                             	        vpsrld	$0x01, %xmm8, %xmm14
 9721                             	        vpsrld	$2, %xmm8, %xmm10
 9722                             	        vpsrld	$7, %xmm8, %xmm9
 9723                             	        vpxor	%xmm10, %xmm14, %xmm14
 9724                             	        vpxor	%xmm9, %xmm14, %xmm14
 9725                             	        vpxor	%xmm13, %xmm14, %xmm14
 9726                             	        vpxor	%xmm8, %xmm14, %xmm14
 9727                             	        vpxor	%xmm14, %xmm7, %xmm7
 9728                             	        vmovdqa	%xmm7, 64(%rsp)
 9729                             	        # H ^ 6
 9730                             	        vpclmulqdq	$0x00, %xmm1, %xmm1, %xmm8
 9731                             	        vpclmulqdq	$0x11, %xmm1, %xmm1, %xmm7
 9732                             	        vpslld	$31, %xmm8, %xmm12
 9733                             	        vpslld	$30, %xmm8, %xmm13
 9734                             	        vpslld	$25, %xmm8, %xmm14
 9735                             	        vpxor	%xmm13, %xmm12, %xmm12
 9736                             	        vpxor	%xmm14, %xmm12, %xmm12
 9737                             	        vpsrldq	$4, %xmm12, %xmm13
 9738                             	        vpslldq	$12, %xmm12, %xmm12
 9739                             	        vpxor	%xmm12, %xmm8, %xmm8
 9740                             	        vpsrld	$0x01, %xmm8, %xmm14
 9741                             	        vpsrld	$2, %xmm8, %xmm10
 9742                             	        vpsrld	$7, %xmm8, %xmm9
 9743                             	        vpxor	%xmm10, %xmm14, %xmm14
 9744                             	        vpxor	%xmm9, %xmm14, %xmm14
 9745                             	        vpxor	%xmm13, %xmm14, %xmm14
 9746                             	        vpxor	%xmm8, %xmm14, %xmm14
 9747                             	        vpxor	%xmm14, %xmm7, %xmm7
 9748                             	        vmovdqa	%xmm7, 80(%rsp)
 9749                             	        # H ^ 7
 9750                             	        # ghash_gfmul_red_avx
 9751                             	        vpshufd	$0x4e, %xmm1, %xmm9
 9752                             	        vpshufd	$0x4e, %xmm3, %xmm10
 9753                             	        vpclmulqdq	$0x11, %xmm1, %xmm3, %xmm11
 9754                             	        vpclmulqdq	$0x00, %xmm1, %xmm3, %xmm8
 9755                             	        vpxor	%xmm1, %xmm9, %xmm9
 9756                             	        vpxor	%xmm3, %xmm10, %xmm10
 9757                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 9758                             	        vpxor	%xmm8, %xmm9, %xmm9
 9759                             	        vpxor	%xmm11, %xmm9, %xmm9
 9760                             	        vpslldq	$8, %xmm9, %xmm10
 9761                             	        vpsrldq	$8, %xmm9, %xmm9
 9762                             	        vpxor	%xmm10, %xmm8, %xmm8
 9763                             	        vpxor	%xmm9, %xmm11, %xmm7
 9764                             	        vpslld	$31, %xmm8, %xmm12
 9765                             	        vpslld	$30, %xmm8, %xmm13
 9766                             	        vpslld	$25, %xmm8, %xmm14
 9767                             	        vpxor	%xmm13, %xmm12, %xmm12
 9768                             	        vpxor	%xmm14, %xmm12, %xmm12
 9769                             	        vpsrldq	$4, %xmm12, %xmm13
 9770                             	        vpslldq	$12, %xmm12, %xmm12
 9771                             	        vpxor	%xmm12, %xmm8, %xmm8
 9772                             	        vpsrld	$0x01, %xmm8, %xmm14
 9773                             	        vpsrld	$2, %xmm8, %xmm10
 9774                             	        vpsrld	$7, %xmm8, %xmm9
 9775                             	        vpxor	%xmm10, %xmm14, %xmm14
 9776                             	        vpxor	%xmm9, %xmm14, %xmm14
 9777                             	        vpxor	%xmm13, %xmm14, %xmm14
 9778                             	        vpxor	%xmm8, %xmm14, %xmm14
 9779                             	        vpxor	%xmm14, %xmm7, %xmm7
 9780                             	        vmovdqa	%xmm7, 96(%rsp)
 9781                             	        # H ^ 8
 9782                             	        vpclmulqdq	$0x00, %xmm3, %xmm3, %xmm8
 9783                             	        vpclmulqdq	$0x11, %xmm3, %xmm3, %xmm7
 9784                             	        vpslld	$31, %xmm8, %xmm12
 9785                             	        vpslld	$30, %xmm8, %xmm13
 9786                             	        vpslld	$25, %xmm8, %xmm14
 9787                             	        vpxor	%xmm13, %xmm12, %xmm12
 9788                             	        vpxor	%xmm14, %xmm12, %xmm12
 9789                             	        vpsrldq	$4, %xmm12, %xmm13
 9790                             	        vpslldq	$12, %xmm12, %xmm12
 9791                             	        vpxor	%xmm12, %xmm8, %xmm8
 9792                             	        vpsrld	$0x01, %xmm8, %xmm14
 9793                             	        vpsrld	$2, %xmm8, %xmm10
 9794                             	        vpsrld	$7, %xmm8, %xmm9
 9795                             	        vpxor	%xmm10, %xmm14, %xmm14
 9796                             	        vpxor	%xmm9, %xmm14, %xmm14
 9797                             	        vpxor	%xmm13, %xmm14, %xmm14
 9798                             	        vpxor	%xmm8, %xmm14, %xmm14
 9799                             	        vpxor	%xmm14, %xmm7, %xmm7
 9800                             	        vmovdqa	%xmm7, 112(%rsp)
 9801                             	        # First 128 bytes of input
 9802                             	        vmovdqa	(%r12), %xmm0
 9803                             	        vmovdqa	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm1
 9804                             	        vpshufb	%xmm1, %xmm0, %xmm8
 9805                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm0, %xmm9
 9806                             	        vpshufb	%xmm1, %xmm9, %xmm9
 9807                             	        vpaddd	L_avx1_aes_gcm_two(%rip), %xmm0, %xmm10
 9808                             	        vpshufb	%xmm1, %xmm10, %xmm10
 9809                             	        vpaddd	L_avx1_aes_gcm_three(%rip), %xmm0, %xmm11
 9810                             	        vpshufb	%xmm1, %xmm11, %xmm11
 9811                             	        vpaddd	L_avx1_aes_gcm_four(%rip), %xmm0, %xmm12
 9812                             	        vpshufb	%xmm1, %xmm12, %xmm12
 9813                             	        vpaddd	L_avx1_aes_gcm_five(%rip), %xmm0, %xmm13
 9814                             	        vpshufb	%xmm1, %xmm13, %xmm13
 9815                             	        vpaddd	L_avx1_aes_gcm_six(%rip), %xmm0, %xmm14
 9816                             	        vpshufb	%xmm1, %xmm14, %xmm14
 9817                             	        vpaddd	L_avx1_aes_gcm_seven(%rip), %xmm0, %xmm15
 9818                             	        vpshufb	%xmm1, %xmm15, %xmm15
 9819                             	        vpaddd	L_avx1_aes_gcm_eight(%rip), %xmm0, %xmm0
 9820                             	        vmovdqa	(%rdi), %xmm7
 9821                             	        vmovdqa	%xmm0, (%r12)
 9822                             	        vpxor	%xmm7, %xmm8, %xmm8
 9823                             	        vpxor	%xmm7, %xmm9, %xmm9
 9824                             	        vpxor	%xmm7, %xmm10, %xmm10
 9825                             	        vpxor	%xmm7, %xmm11, %xmm11
 9826                             	        vpxor	%xmm7, %xmm12, %xmm12
 9827                             	        vpxor	%xmm7, %xmm13, %xmm13
 9828                             	        vpxor	%xmm7, %xmm14, %xmm14
 9829                             	        vpxor	%xmm7, %xmm15, %xmm15
 9830                             	        vmovdqa	16(%rdi), %xmm7
 9831                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9832                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9833                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9834                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9835                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9836                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9837                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9838                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9839                             	        vmovdqa	32(%rdi), %xmm7
 9840                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9841                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9842                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9843                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9844                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9845                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9846                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9847                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9848                             	        vmovdqa	48(%rdi), %xmm7
 9849                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9850                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9851                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9852                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9853                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9854                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9855                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9856                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9857                             	        vmovdqa	64(%rdi), %xmm7
 9858                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9859                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9860                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9861                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9862                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9863                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9864                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9865                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9866                             	        vmovdqa	80(%rdi), %xmm7
 9867                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9868                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9869                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9870                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9871                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9872                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9873                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9874                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9875                             	        vmovdqa	96(%rdi), %xmm7
 9876                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9877                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9878                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9879                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9880                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9881                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9882                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9883                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9884                             	        vmovdqa	112(%rdi), %xmm7
 9885                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9886                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9887                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9888                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9889                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9890                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9891                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9892                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9893                             	        vmovdqa	128(%rdi), %xmm7
 9894                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9895                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9896                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9897                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9898                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9899                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9900                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9901                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9902                             	        vmovdqa	144(%rdi), %xmm7
 9903                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9904                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9905                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9906                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9907                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9908                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9909                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9910                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9911                             	        cmpl	$11, %esi
 9912                             	        vmovdqa	160(%rdi), %xmm7
 9913                             	        jl	L_AES_GCM_encrypt_update_avx1_aesenc_128_enc_done
 9914                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9915                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9916                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9917                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9918                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9919                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9920                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9921                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9922                             	        vmovdqa	176(%rdi), %xmm7
 9923                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9924                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9925                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9926                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9927                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9928                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9929                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9930                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9931                             	        cmpl	$13, %esi
 9932                             	        vmovdqa	192(%rdi), %xmm7
 9933                             	        jl	L_AES_GCM_encrypt_update_avx1_aesenc_128_enc_done
 9934                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9935                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9936                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9937                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9938                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9939                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9940                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9941                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9942                             	        vmovdqa	208(%rdi), %xmm7
 9943                             	        vaesenc	%xmm7, %xmm8, %xmm8
 9944                             	        vaesenc	%xmm7, %xmm9, %xmm9
 9945                             	        vaesenc	%xmm7, %xmm10, %xmm10
 9946                             	        vaesenc	%xmm7, %xmm11, %xmm11
 9947                             	        vaesenc	%xmm7, %xmm12, %xmm12
 9948                             	        vaesenc	%xmm7, %xmm13, %xmm13
 9949                             	        vaesenc	%xmm7, %xmm14, %xmm14
 9950                             	        vaesenc	%xmm7, %xmm15, %xmm15
 9951                             	        vmovdqa	224(%rdi), %xmm7
 9952                             	L_AES_GCM_encrypt_update_avx1_aesenc_128_enc_done:
 9953                             	        vaesenclast	%xmm7, %xmm8, %xmm8
 9954                             	        vaesenclast	%xmm7, %xmm9, %xmm9
 9955                             	        vmovdqu	(%r11), %xmm0
 9956                             	        vmovdqu	16(%r11), %xmm1
 9957                             	        vpxor	%xmm0, %xmm8, %xmm8
 9958                             	        vpxor	%xmm1, %xmm9, %xmm9
 9959                             	        vmovdqu	%xmm8, (%r10)
 9960                             	        vmovdqu	%xmm9, 16(%r10)
 9961                             	        vaesenclast	%xmm7, %xmm10, %xmm10
 9962                             	        vaesenclast	%xmm7, %xmm11, %xmm11
 9963                             	        vmovdqu	32(%r11), %xmm0
 9964                             	        vmovdqu	48(%r11), %xmm1
 9965                             	        vpxor	%xmm0, %xmm10, %xmm10
 9966                             	        vpxor	%xmm1, %xmm11, %xmm11
 9967                             	        vmovdqu	%xmm10, 32(%r10)
 9968                             	        vmovdqu	%xmm11, 48(%r10)
 9969                             	        vaesenclast	%xmm7, %xmm12, %xmm12
 9970                             	        vaesenclast	%xmm7, %xmm13, %xmm13
 9971                             	        vmovdqu	64(%r11), %xmm0
 9972                             	        vmovdqu	80(%r11), %xmm1
 9973                             	        vpxor	%xmm0, %xmm12, %xmm12
 9974                             	        vpxor	%xmm1, %xmm13, %xmm13
 9975                             	        vmovdqu	%xmm12, 64(%r10)
 9976                             	        vmovdqu	%xmm13, 80(%r10)
 9977                             	        vaesenclast	%xmm7, %xmm14, %xmm14
 9978                             	        vaesenclast	%xmm7, %xmm15, %xmm15
 9979                             	        vmovdqu	96(%r11), %xmm0
 9980                             	        vmovdqu	112(%r11), %xmm1
 9981                             	        vpxor	%xmm0, %xmm14, %xmm14
 9982                             	        vpxor	%xmm1, %xmm15, %xmm15
 9983                             	        vmovdqu	%xmm14, 96(%r10)
 9984                             	        vmovdqu	%xmm15, 112(%r10)
 9985                             	        cmpl	$0x80, %r13d
 9986                             	        movl	$0x80, %r14d
 9987                             	        jle	L_AES_GCM_encrypt_update_avx1_end_128
 9988                             	        # More 128 bytes of input
 9989                             	L_AES_GCM_encrypt_update_avx1_ghash_128:
 9990                             	        leaq	(%r11,%r14,1), %rcx
 9991                             	        leaq	(%r10,%r14,1), %rdx
 9992                             	        vmovdqa	(%r12), %xmm0
 9993                             	        vmovdqa	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm1
 9994                             	        vpshufb	%xmm1, %xmm0, %xmm8
 9995                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm0, %xmm9
 9996                             	        vpshufb	%xmm1, %xmm9, %xmm9
 9997                             	        vpaddd	L_avx1_aes_gcm_two(%rip), %xmm0, %xmm10
 9998                             	        vpshufb	%xmm1, %xmm10, %xmm10
 9999                             	        vpaddd	L_avx1_aes_gcm_three(%rip), %xmm0, %xmm11
 10000                             	        vpshufb	%xmm1, %xmm11, %xmm11
 10001                             	        vpaddd	L_avx1_aes_gcm_four(%rip), %xmm0, %xmm12
 10002                             	        vpshufb	%xmm1, %xmm12, %xmm12
 10003                             	        vpaddd	L_avx1_aes_gcm_five(%rip), %xmm0, %xmm13
 10004                             	        vpshufb	%xmm1, %xmm13, %xmm13
 10005                             	        vpaddd	L_avx1_aes_gcm_six(%rip), %xmm0, %xmm14
 10006                             	        vpshufb	%xmm1, %xmm14, %xmm14
 10007                             	        vpaddd	L_avx1_aes_gcm_seven(%rip), %xmm0, %xmm15
 10008                             	        vpshufb	%xmm1, %xmm15, %xmm15
 10009                             	        vpaddd	L_avx1_aes_gcm_eight(%rip), %xmm0, %xmm0
 10010                             	        vmovdqa	(%rdi), %xmm7
 10011                             	        vmovdqa	%xmm0, (%r12)
 10012                             	        vpxor	%xmm7, %xmm8, %xmm8
 10013                             	        vpxor	%xmm7, %xmm9, %xmm9
 10014                             	        vpxor	%xmm7, %xmm10, %xmm10
 10015                             	        vpxor	%xmm7, %xmm11, %xmm11
 10016                             	        vpxor	%xmm7, %xmm12, %xmm12
 10017                             	        vpxor	%xmm7, %xmm13, %xmm13
 10018                             	        vpxor	%xmm7, %xmm14, %xmm14
 10019                             	        vpxor	%xmm7, %xmm15, %xmm15
 10020                             	        vmovdqa	112(%rsp), %xmm7
 10021                             	        vmovdqu	-128(%rdx), %xmm0
 10022                             	        vaesenc	16(%rdi), %xmm8, %xmm8
 10023                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 10024                             	        vpxor	%xmm2, %xmm0, %xmm0
 10025                             	        vpshufd	$0x4e, %xmm7, %xmm1
 10026                             	        vpshufd	$0x4e, %xmm0, %xmm5
 10027                             	        vpxor	%xmm7, %xmm1, %xmm1
 10028                             	        vpxor	%xmm0, %xmm5, %xmm5
 10029                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm3
 10030                             	        vaesenc	16(%rdi), %xmm9, %xmm9
 10031                             	        vaesenc	16(%rdi), %xmm10, %xmm10
 10032                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm2
 10033                             	        vaesenc	16(%rdi), %xmm11, %xmm11
 10034                             	        vaesenc	16(%rdi), %xmm12, %xmm12
 10035                             	        vpclmulqdq	$0x00, %xmm5, %xmm1, %xmm1
 10036                             	        vaesenc	16(%rdi), %xmm13, %xmm13
 10037                             	        vaesenc	16(%rdi), %xmm14, %xmm14
 10038                             	        vaesenc	16(%rdi), %xmm15, %xmm15
 10039                             	        vpxor	%xmm2, %xmm1, %xmm1
 10040                             	        vpxor	%xmm3, %xmm1, %xmm1
 10041                             	        vmovdqa	96(%rsp), %xmm7
 10042                             	        vmovdqu	-112(%rdx), %xmm0
 10043                             	        vpshufd	$0x4e, %xmm7, %xmm4
 10044                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 10045                             	        vaesenc	32(%rdi), %xmm8, %xmm8
 10046                             	        vpxor	%xmm7, %xmm4, %xmm4
 10047                             	        vpshufd	$0x4e, %xmm0, %xmm5
 10048                             	        vpxor	%xmm0, %xmm5, %xmm5
 10049                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 10050                             	        vaesenc	32(%rdi), %xmm9, %xmm9
 10051                             	        vaesenc	32(%rdi), %xmm10, %xmm10
 10052                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 10053                             	        vaesenc	32(%rdi), %xmm11, %xmm11
 10054                             	        vaesenc	32(%rdi), %xmm12, %xmm12
 10055                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 10056                             	        vaesenc	32(%rdi), %xmm13, %xmm13
 10057                             	        vaesenc	32(%rdi), %xmm14, %xmm14
 10058                             	        vaesenc	32(%rdi), %xmm15, %xmm15
 10059                             	        vpxor	%xmm7, %xmm1, %xmm1
 10060                             	        vpxor	%xmm7, %xmm2, %xmm2
 10061                             	        vpxor	%xmm6, %xmm1, %xmm1
 10062                             	        vpxor	%xmm6, %xmm3, %xmm3
 10063                             	        vpxor	%xmm4, %xmm1, %xmm1
 10064                             	        vmovdqa	80(%rsp), %xmm7
 10065                             	        vmovdqu	-96(%rdx), %xmm0
 10066                             	        vpshufd	$0x4e, %xmm7, %xmm4
 10067                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 10068                             	        vaesenc	48(%rdi), %xmm8, %xmm8
 10069                             	        vpxor	%xmm7, %xmm4, %xmm4
 10070                             	        vpshufd	$0x4e, %xmm0, %xmm5
 10071                             	        vpxor	%xmm0, %xmm5, %xmm5
 10072                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 10073                             	        vaesenc	48(%rdi), %xmm9, %xmm9
 10074                             	        vaesenc	48(%rdi), %xmm10, %xmm10
 10075                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 10076                             	        vaesenc	48(%rdi), %xmm11, %xmm11
 10077                             	        vaesenc	48(%rdi), %xmm12, %xmm12
 10078                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 10079                             	        vaesenc	48(%rdi), %xmm13, %xmm13
 10080                             	        vaesenc	48(%rdi), %xmm14, %xmm14
 10081                             	        vaesenc	48(%rdi), %xmm15, %xmm15
 10082                             	        vpxor	%xmm7, %xmm1, %xmm1
 10083                             	        vpxor	%xmm7, %xmm2, %xmm2
 10084                             	        vpxor	%xmm6, %xmm1, %xmm1
 10085                             	        vpxor	%xmm6, %xmm3, %xmm3
 10086                             	        vpxor	%xmm4, %xmm1, %xmm1
 10087                             	        vmovdqa	64(%rsp), %xmm7
 10088                             	        vmovdqu	-80(%rdx), %xmm0
 10089                             	        vpshufd	$0x4e, %xmm7, %xmm4
 10090                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 10091                             	        vaesenc	64(%rdi), %xmm8, %xmm8
 10092                             	        vpxor	%xmm7, %xmm4, %xmm4
 10093                             	        vpshufd	$0x4e, %xmm0, %xmm5
 10094                             	        vpxor	%xmm0, %xmm5, %xmm5
 10095                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 10096                             	        vaesenc	64(%rdi), %xmm9, %xmm9
 10097                             	        vaesenc	64(%rdi), %xmm10, %xmm10
 10098                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 10099                             	        vaesenc	64(%rdi), %xmm11, %xmm11
 10100                             	        vaesenc	64(%rdi), %xmm12, %xmm12
 10101                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 10102                             	        vaesenc	64(%rdi), %xmm13, %xmm13
 10103                             	        vaesenc	64(%rdi), %xmm14, %xmm14
 10104                             	        vaesenc	64(%rdi), %xmm15, %xmm15
 10105                             	        vpxor	%xmm7, %xmm1, %xmm1
 10106                             	        vpxor	%xmm7, %xmm2, %xmm2
 10107                             	        vpxor	%xmm6, %xmm1, %xmm1
 10108                             	        vpxor	%xmm6, %xmm3, %xmm3
 10109                             	        vpxor	%xmm4, %xmm1, %xmm1
 10110                             	        vmovdqa	48(%rsp), %xmm7
 10111                             	        vmovdqu	-64(%rdx), %xmm0
 10112                             	        vpshufd	$0x4e, %xmm7, %xmm4
 10113                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 10114                             	        vaesenc	80(%rdi), %xmm8, %xmm8
 10115                             	        vpxor	%xmm7, %xmm4, %xmm4
 10116                             	        vpshufd	$0x4e, %xmm0, %xmm5
 10117                             	        vpxor	%xmm0, %xmm5, %xmm5
 10118                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 10119                             	        vaesenc	80(%rdi), %xmm9, %xmm9
 10120                             	        vaesenc	80(%rdi), %xmm10, %xmm10
 10121                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 10122                             	        vaesenc	80(%rdi), %xmm11, %xmm11
 10123                             	        vaesenc	80(%rdi), %xmm12, %xmm12
 10124                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 10125                             	        vaesenc	80(%rdi), %xmm13, %xmm13
 10126                             	        vaesenc	80(%rdi), %xmm14, %xmm14
 10127                             	        vaesenc	80(%rdi), %xmm15, %xmm15
 10128                             	        vpxor	%xmm7, %xmm1, %xmm1
 10129                             	        vpxor	%xmm7, %xmm2, %xmm2
 10130                             	        vpxor	%xmm6, %xmm1, %xmm1
 10131                             	        vpxor	%xmm6, %xmm3, %xmm3
 10132                             	        vpxor	%xmm4, %xmm1, %xmm1
 10133                             	        vmovdqa	32(%rsp), %xmm7
 10134                             	        vmovdqu	-48(%rdx), %xmm0
 10135                             	        vpshufd	$0x4e, %xmm7, %xmm4
 10136                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 10137                             	        vaesenc	96(%rdi), %xmm8, %xmm8
 10138                             	        vpxor	%xmm7, %xmm4, %xmm4
 10139                             	        vpshufd	$0x4e, %xmm0, %xmm5
 10140                             	        vpxor	%xmm0, %xmm5, %xmm5
 10141                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 10142                             	        vaesenc	96(%rdi), %xmm9, %xmm9
 10143                             	        vaesenc	96(%rdi), %xmm10, %xmm10
 10144                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 10145                             	        vaesenc	96(%rdi), %xmm11, %xmm11
 10146                             	        vaesenc	96(%rdi), %xmm12, %xmm12
 10147                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 10148                             	        vaesenc	96(%rdi), %xmm13, %xmm13
 10149                             	        vaesenc	96(%rdi), %xmm14, %xmm14
 10150                             	        vaesenc	96(%rdi), %xmm15, %xmm15
 10151                             	        vpxor	%xmm7, %xmm1, %xmm1
 10152                             	        vpxor	%xmm7, %xmm2, %xmm2
 10153                             	        vpxor	%xmm6, %xmm1, %xmm1
 10154                             	        vpxor	%xmm6, %xmm3, %xmm3
 10155                             	        vpxor	%xmm4, %xmm1, %xmm1
 10156                             	        vmovdqa	16(%rsp), %xmm7
 10157                             	        vmovdqu	-32(%rdx), %xmm0
 10158                             	        vpshufd	$0x4e, %xmm7, %xmm4
 10159                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 10160                             	        vaesenc	112(%rdi), %xmm8, %xmm8
 10161                             	        vpxor	%xmm7, %xmm4, %xmm4
 10162                             	        vpshufd	$0x4e, %xmm0, %xmm5
 10163                             	        vpxor	%xmm0, %xmm5, %xmm5
 10164                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 10165                             	        vaesenc	112(%rdi), %xmm9, %xmm9
 10166                             	        vaesenc	112(%rdi), %xmm10, %xmm10
 10167                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 10168                             	        vaesenc	112(%rdi), %xmm11, %xmm11
 10169                             	        vaesenc	112(%rdi), %xmm12, %xmm12
 10170                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 10171                             	        vaesenc	112(%rdi), %xmm13, %xmm13
 10172                             	        vaesenc	112(%rdi), %xmm14, %xmm14
 10173                             	        vaesenc	112(%rdi), %xmm15, %xmm15
 10174                             	        vpxor	%xmm7, %xmm1, %xmm1
 10175                             	        vpxor	%xmm7, %xmm2, %xmm2
 10176                             	        vpxor	%xmm6, %xmm1, %xmm1
 10177                             	        vpxor	%xmm6, %xmm3, %xmm3
 10178                             	        vpxor	%xmm4, %xmm1, %xmm1
 10179                             	        vmovdqa	(%rsp), %xmm7
 10180                             	        vmovdqu	-16(%rdx), %xmm0
 10181                             	        vpshufd	$0x4e, %xmm7, %xmm4
 10182                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 10183                             	        vaesenc	128(%rdi), %xmm8, %xmm8
 10184                             	        vpxor	%xmm7, %xmm4, %xmm4
 10185                             	        vpshufd	$0x4e, %xmm0, %xmm5
 10186                             	        vpxor	%xmm0, %xmm5, %xmm5
 10187                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 10188                             	        vaesenc	128(%rdi), %xmm9, %xmm9
 10189                             	        vaesenc	128(%rdi), %xmm10, %xmm10
 10190                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 10191                             	        vaesenc	128(%rdi), %xmm11, %xmm11
 10192                             	        vaesenc	128(%rdi), %xmm12, %xmm12
 10193                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 10194                             	        vaesenc	128(%rdi), %xmm13, %xmm13
 10195                             	        vaesenc	128(%rdi), %xmm14, %xmm14
 10196                             	        vaesenc	128(%rdi), %xmm15, %xmm15
 10197                             	        vpxor	%xmm7, %xmm1, %xmm1
 10198                             	        vpxor	%xmm7, %xmm2, %xmm2
 10199                             	        vpxor	%xmm6, %xmm1, %xmm1
 10200                             	        vpxor	%xmm6, %xmm3, %xmm3
 10201                             	        vpxor	%xmm4, %xmm1, %xmm1
 10202                             	        vpslldq	$8, %xmm1, %xmm5
 10203                             	        vpsrldq	$8, %xmm1, %xmm1
 10204                             	        vaesenc	144(%rdi), %xmm8, %xmm8
 10205                             	        vpxor	%xmm5, %xmm2, %xmm2
 10206                             	        vpxor	%xmm1, %xmm3, %xmm3
 10207                             	        vaesenc	144(%rdi), %xmm9, %xmm9
 10208                             	        vpslld	$31, %xmm2, %xmm7
 10209                             	        vpslld	$30, %xmm2, %xmm4
 10210                             	        vpslld	$25, %xmm2, %xmm5
 10211                             	        vaesenc	144(%rdi), %xmm10, %xmm10
 10212                             	        vpxor	%xmm4, %xmm7, %xmm7
 10213                             	        vpxor	%xmm5, %xmm7, %xmm7
 10214                             	        vaesenc	144(%rdi), %xmm11, %xmm11
 10215                             	        vpsrldq	$4, %xmm7, %xmm4
 10216                             	        vpslldq	$12, %xmm7, %xmm7
 10217                             	        vaesenc	144(%rdi), %xmm12, %xmm12
 10218                             	        vpxor	%xmm7, %xmm2, %xmm2
 10219                             	        vpsrld	$0x01, %xmm2, %xmm5
 10220                             	        vaesenc	144(%rdi), %xmm13, %xmm13
 10221                             	        vpsrld	$2, %xmm2, %xmm1
 10222                             	        vpsrld	$7, %xmm2, %xmm0
 10223                             	        vaesenc	144(%rdi), %xmm14, %xmm14
 10224                             	        vpxor	%xmm1, %xmm5, %xmm5
 10225                             	        vpxor	%xmm0, %xmm5, %xmm5
 10226                             	        vaesenc	144(%rdi), %xmm15, %xmm15
 10227                             	        vpxor	%xmm4, %xmm5, %xmm5
 10228                             	        vpxor	%xmm5, %xmm2, %xmm2
 10229                             	        vpxor	%xmm3, %xmm2, %xmm2
 10230                             	        cmpl	$11, %esi
 10231                             	        vmovdqa	160(%rdi), %xmm7
 10232                             	        jl	L_AES_GCM_encrypt_update_avx1_aesenc_128_ghash_avx_done
 10233                             	        vaesenc	%xmm7, %xmm8, %xmm8
 10234                             	        vaesenc	%xmm7, %xmm9, %xmm9
 10235                             	        vaesenc	%xmm7, %xmm10, %xmm10
 10236                             	        vaesenc	%xmm7, %xmm11, %xmm11
 10237                             	        vaesenc	%xmm7, %xmm12, %xmm12
 10238                             	        vaesenc	%xmm7, %xmm13, %xmm13
 10239                             	        vaesenc	%xmm7, %xmm14, %xmm14
 10240                             	        vaesenc	%xmm7, %xmm15, %xmm15
 10241                             	        vmovdqa	176(%rdi), %xmm7
 10242                             	        vaesenc	%xmm7, %xmm8, %xmm8
 10243                             	        vaesenc	%xmm7, %xmm9, %xmm9
 10244                             	        vaesenc	%xmm7, %xmm10, %xmm10
 10245                             	        vaesenc	%xmm7, %xmm11, %xmm11
 10246                             	        vaesenc	%xmm7, %xmm12, %xmm12
 10247                             	        vaesenc	%xmm7, %xmm13, %xmm13
 10248                             	        vaesenc	%xmm7, %xmm14, %xmm14
 10249                             	        vaesenc	%xmm7, %xmm15, %xmm15
 10250                             	        cmpl	$13, %esi
 10251                             	        vmovdqa	192(%rdi), %xmm7
 10252                             	        jl	L_AES_GCM_encrypt_update_avx1_aesenc_128_ghash_avx_done
 10253                             	        vaesenc	%xmm7, %xmm8, %xmm8
 10254                             	        vaesenc	%xmm7, %xmm9, %xmm9
 10255                             	        vaesenc	%xmm7, %xmm10, %xmm10
 10256                             	        vaesenc	%xmm7, %xmm11, %xmm11
 10257                             	        vaesenc	%xmm7, %xmm12, %xmm12
 10258                             	        vaesenc	%xmm7, %xmm13, %xmm13
 10259                             	        vaesenc	%xmm7, %xmm14, %xmm14
 10260                             	        vaesenc	%xmm7, %xmm15, %xmm15
 10261                             	        vmovdqa	208(%rdi), %xmm7
 10262                             	        vaesenc	%xmm7, %xmm8, %xmm8
 10263                             	        vaesenc	%xmm7, %xmm9, %xmm9
 10264                             	        vaesenc	%xmm7, %xmm10, %xmm10
 10265                             	        vaesenc	%xmm7, %xmm11, %xmm11
 10266                             	        vaesenc	%xmm7, %xmm12, %xmm12
 10267                             	        vaesenc	%xmm7, %xmm13, %xmm13
 10268                             	        vaesenc	%xmm7, %xmm14, %xmm14
 10269                             	        vaesenc	%xmm7, %xmm15, %xmm15
 10270                             	        vmovdqa	224(%rdi), %xmm7
 10271                             	L_AES_GCM_encrypt_update_avx1_aesenc_128_ghash_avx_done:
 10272                             	        vaesenclast	%xmm7, %xmm8, %xmm8
 10273                             	        vaesenclast	%xmm7, %xmm9, %xmm9
 10274                             	        vmovdqu	(%rcx), %xmm0
 10275                             	        vmovdqu	16(%rcx), %xmm1
 10276                             	        vpxor	%xmm0, %xmm8, %xmm8
 10277                             	        vpxor	%xmm1, %xmm9, %xmm9
 10278                             	        vmovdqu	%xmm8, (%rdx)
 10279                             	        vmovdqu	%xmm9, 16(%rdx)
 10280                             	        vaesenclast	%xmm7, %xmm10, %xmm10
 10281                             	        vaesenclast	%xmm7, %xmm11, %xmm11
 10282                             	        vmovdqu	32(%rcx), %xmm0
 10283                             	        vmovdqu	48(%rcx), %xmm1
 10284                             	        vpxor	%xmm0, %xmm10, %xmm10
 10285                             	        vpxor	%xmm1, %xmm11, %xmm11
 10286                             	        vmovdqu	%xmm10, 32(%rdx)
 10287                             	        vmovdqu	%xmm11, 48(%rdx)
 10288                             	        vaesenclast	%xmm7, %xmm12, %xmm12
 10289                             	        vaesenclast	%xmm7, %xmm13, %xmm13
 10290                             	        vmovdqu	64(%rcx), %xmm0
 10291                             	        vmovdqu	80(%rcx), %xmm1
 10292                             	        vpxor	%xmm0, %xmm12, %xmm12
 10293                             	        vpxor	%xmm1, %xmm13, %xmm13
 10294                             	        vmovdqu	%xmm12, 64(%rdx)
 10295                             	        vmovdqu	%xmm13, 80(%rdx)
 10296                             	        vaesenclast	%xmm7, %xmm14, %xmm14
 10297                             	        vaesenclast	%xmm7, %xmm15, %xmm15
 10298                             	        vmovdqu	96(%rcx), %xmm0
 10299                             	        vmovdqu	112(%rcx), %xmm1
 10300                             	        vpxor	%xmm0, %xmm14, %xmm14
 10301                             	        vpxor	%xmm1, %xmm15, %xmm15
 10302                             	        vmovdqu	%xmm14, 96(%rdx)
 10303                             	        vmovdqu	%xmm15, 112(%rdx)
 10304                             	        addl	$0x80, %r14d
 10305                             	        cmpl	%r13d, %r14d
 10306                             	        jl	L_AES_GCM_encrypt_update_avx1_ghash_128
 10307                             	L_AES_GCM_encrypt_update_avx1_end_128:
 10308                             	        vmovdqa	L_avx1_aes_gcm_bswap_mask(%rip), %xmm4
 10309                             	        vpshufb	%xmm4, %xmm8, %xmm8
 10310                             	        vpshufb	%xmm4, %xmm9, %xmm9
 10311                             	        vpshufb	%xmm4, %xmm10, %xmm10
 10312                             	        vpshufb	%xmm4, %xmm11, %xmm11
 10313                             	        vpxor	%xmm2, %xmm8, %xmm8
 10314                             	        vpshufb	%xmm4, %xmm12, %xmm12
 10315                             	        vpshufb	%xmm4, %xmm13, %xmm13
 10316                             	        vpshufb	%xmm4, %xmm14, %xmm14
 10317                             	        vpshufb	%xmm4, %xmm15, %xmm15
 10318                             	        vmovdqa	(%rsp), %xmm7
 10319                             	        vmovdqa	16(%rsp), %xmm5
 10320                             	        # ghash_gfmul_avx
 10321                             	        vpshufd	$0x4e, %xmm15, %xmm1
 10322                             	        vpshufd	$0x4e, %xmm7, %xmm2
 10323                             	        vpclmulqdq	$0x11, %xmm15, %xmm7, %xmm3
 10324                             	        vpclmulqdq	$0x00, %xmm15, %xmm7, %xmm0
 10325                             	        vpxor	%xmm15, %xmm1, %xmm1
 10326                             	        vpxor	%xmm7, %xmm2, %xmm2
 10327                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 10328                             	        vpxor	%xmm0, %xmm1, %xmm1
 10329                             	        vpxor	%xmm3, %xmm1, %xmm1
 10330                             	        vmovdqa	%xmm0, %xmm4
 10331                             	        vmovdqa	%xmm3, %xmm6
 10332                             	        vpslldq	$8, %xmm1, %xmm2
 10333                             	        vpsrldq	$8, %xmm1, %xmm1
 10334                             	        vpxor	%xmm2, %xmm4, %xmm4
 10335                             	        vpxor	%xmm1, %xmm6, %xmm6
 10336                             	        # ghash_gfmul_xor_avx
 10337                             	        vpshufd	$0x4e, %xmm14, %xmm1
 10338                             	        vpshufd	$0x4e, %xmm5, %xmm2
 10339                             	        vpclmulqdq	$0x11, %xmm14, %xmm5, %xmm3
 10340                             	        vpclmulqdq	$0x00, %xmm14, %xmm5, %xmm0
 10341                             	        vpxor	%xmm14, %xmm1, %xmm1
 10342                             	        vpxor	%xmm5, %xmm2, %xmm2
 10343                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 10344                             	        vpxor	%xmm0, %xmm1, %xmm1
 10345                             	        vpxor	%xmm3, %xmm1, %xmm1
 10346                             	        vpxor	%xmm0, %xmm4, %xmm4
 10347                             	        vpxor	%xmm3, %xmm6, %xmm6
 10348                             	        vpslldq	$8, %xmm1, %xmm2
 10349                             	        vpsrldq	$8, %xmm1, %xmm1
 10350                             	        vpxor	%xmm2, %xmm4, %xmm4
 10351                             	        vpxor	%xmm1, %xmm6, %xmm6
 10352                             	        vmovdqa	32(%rsp), %xmm7
 10353                             	        vmovdqa	48(%rsp), %xmm5
 10354                             	        # ghash_gfmul_xor_avx
 10355                             	        vpshufd	$0x4e, %xmm13, %xmm1
 10356                             	        vpshufd	$0x4e, %xmm7, %xmm2
 10357                             	        vpclmulqdq	$0x11, %xmm13, %xmm7, %xmm3
 10358                             	        vpclmulqdq	$0x00, %xmm13, %xmm7, %xmm0
 10359                             	        vpxor	%xmm13, %xmm1, %xmm1
 10360                             	        vpxor	%xmm7, %xmm2, %xmm2
 10361                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 10362                             	        vpxor	%xmm0, %xmm1, %xmm1
 10363                             	        vpxor	%xmm3, %xmm1, %xmm1
 10364                             	        vpxor	%xmm0, %xmm4, %xmm4
 10365                             	        vpxor	%xmm3, %xmm6, %xmm6
 10366                             	        vpslldq	$8, %xmm1, %xmm2
 10367                             	        vpsrldq	$8, %xmm1, %xmm1
 10368                             	        vpxor	%xmm2, %xmm4, %xmm4
 10369                             	        vpxor	%xmm1, %xmm6, %xmm6
 10370                             	        # ghash_gfmul_xor_avx
 10371                             	        vpshufd	$0x4e, %xmm12, %xmm1
 10372                             	        vpshufd	$0x4e, %xmm5, %xmm2
 10373                             	        vpclmulqdq	$0x11, %xmm12, %xmm5, %xmm3
 10374                             	        vpclmulqdq	$0x00, %xmm12, %xmm5, %xmm0
 10375                             	        vpxor	%xmm12, %xmm1, %xmm1
 10376                             	        vpxor	%xmm5, %xmm2, %xmm2
 10377                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 10378                             	        vpxor	%xmm0, %xmm1, %xmm1
 10379                             	        vpxor	%xmm3, %xmm1, %xmm1
 10380                             	        vpxor	%xmm0, %xmm4, %xmm4
 10381                             	        vpxor	%xmm3, %xmm6, %xmm6
 10382                             	        vpslldq	$8, %xmm1, %xmm2
 10383                             	        vpsrldq	$8, %xmm1, %xmm1
 10384                             	        vpxor	%xmm2, %xmm4, %xmm4
 10385                             	        vpxor	%xmm1, %xmm6, %xmm6
 10386                             	        vmovdqa	64(%rsp), %xmm7
 10387                             	        vmovdqa	80(%rsp), %xmm5
 10388                             	        # ghash_gfmul_xor_avx
 10389                             	        vpshufd	$0x4e, %xmm11, %xmm1
 10390                             	        vpshufd	$0x4e, %xmm7, %xmm2
 10391                             	        vpclmulqdq	$0x11, %xmm11, %xmm7, %xmm3
 10392                             	        vpclmulqdq	$0x00, %xmm11, %xmm7, %xmm0
 10393                             	        vpxor	%xmm11, %xmm1, %xmm1
 10394                             	        vpxor	%xmm7, %xmm2, %xmm2
 10395                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 10396                             	        vpxor	%xmm0, %xmm1, %xmm1
 10397                             	        vpxor	%xmm3, %xmm1, %xmm1
 10398                             	        vpxor	%xmm0, %xmm4, %xmm4
 10399                             	        vpxor	%xmm3, %xmm6, %xmm6
 10400                             	        vpslldq	$8, %xmm1, %xmm2
 10401                             	        vpsrldq	$8, %xmm1, %xmm1
 10402                             	        vpxor	%xmm2, %xmm4, %xmm4
 10403                             	        vpxor	%xmm1, %xmm6, %xmm6
 10404                             	        # ghash_gfmul_xor_avx
 10405                             	        vpshufd	$0x4e, %xmm10, %xmm1
 10406                             	        vpshufd	$0x4e, %xmm5, %xmm2
 10407                             	        vpclmulqdq	$0x11, %xmm10, %xmm5, %xmm3
 10408                             	        vpclmulqdq	$0x00, %xmm10, %xmm5, %xmm0
 10409                             	        vpxor	%xmm10, %xmm1, %xmm1
 10410                             	        vpxor	%xmm5, %xmm2, %xmm2
 10411                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 10412                             	        vpxor	%xmm0, %xmm1, %xmm1
 10413                             	        vpxor	%xmm3, %xmm1, %xmm1
 10414                             	        vpxor	%xmm0, %xmm4, %xmm4
 10415                             	        vpxor	%xmm3, %xmm6, %xmm6
 10416                             	        vpslldq	$8, %xmm1, %xmm2
 10417                             	        vpsrldq	$8, %xmm1, %xmm1
 10418                             	        vpxor	%xmm2, %xmm4, %xmm4
 10419                             	        vpxor	%xmm1, %xmm6, %xmm6
 10420                             	        vmovdqa	96(%rsp), %xmm7
 10421                             	        vmovdqa	112(%rsp), %xmm5
 10422                             	        # ghash_gfmul_xor_avx
 10423                             	        vpshufd	$0x4e, %xmm9, %xmm1
 10424                             	        vpshufd	$0x4e, %xmm7, %xmm2
 10425                             	        vpclmulqdq	$0x11, %xmm9, %xmm7, %xmm3
 10426                             	        vpclmulqdq	$0x00, %xmm9, %xmm7, %xmm0
 10427                             	        vpxor	%xmm9, %xmm1, %xmm1
 10428                             	        vpxor	%xmm7, %xmm2, %xmm2
 10429                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 10430                             	        vpxor	%xmm0, %xmm1, %xmm1
 10431                             	        vpxor	%xmm3, %xmm1, %xmm1
 10432                             	        vpxor	%xmm0, %xmm4, %xmm4
 10433                             	        vpxor	%xmm3, %xmm6, %xmm6
 10434                             	        vpslldq	$8, %xmm1, %xmm2
 10435                             	        vpsrldq	$8, %xmm1, %xmm1
 10436                             	        vpxor	%xmm2, %xmm4, %xmm4
 10437                             	        vpxor	%xmm1, %xmm6, %xmm6
 10438                             	        # ghash_gfmul_xor_avx
 10439                             	        vpshufd	$0x4e, %xmm8, %xmm1
 10440                             	        vpshufd	$0x4e, %xmm5, %xmm2
 10441                             	        vpclmulqdq	$0x11, %xmm8, %xmm5, %xmm3
 10442                             	        vpclmulqdq	$0x00, %xmm8, %xmm5, %xmm0
 10443                             	        vpxor	%xmm8, %xmm1, %xmm1
 10444                             	        vpxor	%xmm5, %xmm2, %xmm2
 10445                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm1
 10446                             	        vpxor	%xmm0, %xmm1, %xmm1
 10447                             	        vpxor	%xmm3, %xmm1, %xmm1
 10448                             	        vpxor	%xmm0, %xmm4, %xmm4
 10449                             	        vpxor	%xmm3, %xmm6, %xmm6
 10450                             	        vpslldq	$8, %xmm1, %xmm2
 10451                             	        vpsrldq	$8, %xmm1, %xmm1
 10452                             	        vpxor	%xmm2, %xmm4, %xmm4
 10453                             	        vpxor	%xmm1, %xmm6, %xmm6
 10454                             	        vpslld	$31, %xmm4, %xmm0
 10455                             	        vpslld	$30, %xmm4, %xmm1
 10456                             	        vpslld	$25, %xmm4, %xmm2
 10457                             	        vpxor	%xmm1, %xmm0, %xmm0
 10458                             	        vpxor	%xmm2, %xmm0, %xmm0
 10459                             	        vmovdqa	%xmm0, %xmm1
 10460                             	        vpsrldq	$4, %xmm1, %xmm1
 10461                             	        vpslldq	$12, %xmm0, %xmm0
 10462                             	        vpxor	%xmm0, %xmm4, %xmm4
 10463                             	        vpsrld	$0x01, %xmm4, %xmm2
 10464                             	        vpsrld	$2, %xmm4, %xmm3
 10465                             	        vpsrld	$7, %xmm4, %xmm0
 10466                             	        vpxor	%xmm3, %xmm2, %xmm2
 10467                             	        vpxor	%xmm0, %xmm2, %xmm2
 10468                             	        vpxor	%xmm1, %xmm2, %xmm2
 10469                             	        vpxor	%xmm4, %xmm2, %xmm2
 10470                             	        vpxor	%xmm2, %xmm6, %xmm6
 10471                             	        vmovdqa	(%rsp), %xmm5
 10472                             	L_AES_GCM_encrypt_update_avx1_done_128:
 10473                             	        movl	%r8d, %edx
 10474                             	        cmpl	%edx, %r14d
 10475                             	        jge	L_AES_GCM_encrypt_update_avx1_done_enc
 10476                             	        movl	%r8d, %r13d
 10477                             	        andl	$0xfffffff0, %r13d
 10478                             	        cmpl	%r13d, %r14d
 10479                             	        jge	L_AES_GCM_encrypt_update_avx1_last_block_done
 10480                             	        vmovdqa	(%r12), %xmm9
 10481                             	        vpshufb	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm9, %xmm8
 10482                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm9, %xmm9
 10483                             	        vmovdqa	%xmm9, (%r12)
 10484                             	        vpxor	(%rdi), %xmm8, %xmm8
 10485                             	        vaesenc	16(%rdi), %xmm8, %xmm8
 10486                             	        vaesenc	32(%rdi), %xmm8, %xmm8
 10487                             	        vaesenc	48(%rdi), %xmm8, %xmm8
 10488                             	        vaesenc	64(%rdi), %xmm8, %xmm8
 10489                             	        vaesenc	80(%rdi), %xmm8, %xmm8
 10490                             	        vaesenc	96(%rdi), %xmm8, %xmm8
 10491                             	        vaesenc	112(%rdi), %xmm8, %xmm8
 10492                             	        vaesenc	128(%rdi), %xmm8, %xmm8
 10493                             	        vaesenc	144(%rdi), %xmm8, %xmm8
 10494                             	        cmpl	$11, %esi
 10495                             	        vmovdqa	160(%rdi), %xmm9
 10496                             	        jl	L_AES_GCM_encrypt_update_avx1_aesenc_block_last
 10497                             	        vaesenc	%xmm9, %xmm8, %xmm8
 10498                             	        vaesenc	176(%rdi), %xmm8, %xmm8
 10499                             	        cmpl	$13, %esi
 10500                             	        vmovdqa	192(%rdi), %xmm9
 10501                             	        jl	L_AES_GCM_encrypt_update_avx1_aesenc_block_last
 10502                             	        vaesenc	%xmm9, %xmm8, %xmm8
 10503                             	        vaesenc	208(%rdi), %xmm8, %xmm8
 10504                             	        vmovdqa	224(%rdi), %xmm9
 10505                             	L_AES_GCM_encrypt_update_avx1_aesenc_block_last:
 10506                             	        vaesenclast	%xmm9, %xmm8, %xmm8
 10507                             	        vmovdqu	(%r11,%r14,1), %xmm9
 10508                             	        vpxor	%xmm9, %xmm8, %xmm8
 10509                             	        vmovdqu	%xmm8, (%r10,%r14,1)
 10510                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 10511                             	        vpxor	%xmm8, %xmm6, %xmm6
 10512                             	        addl	$16, %r14d
 10513                             	        cmpl	%r13d, %r14d
 10514                             	        jge	L_AES_GCM_encrypt_update_avx1_last_block_ghash
 10515                             	L_AES_GCM_encrypt_update_avx1_last_block_start:
 10516                             	        vmovdqu	(%r11,%r14,1), %xmm13
 10517                             	        vmovdqa	(%r12), %xmm9
 10518                             	        vpshufb	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm9, %xmm8
 10519                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm9, %xmm9
 10520                             	        vmovdqa	%xmm9, (%r12)
 10521                             	        vpxor	(%rdi), %xmm8, %xmm8
 10522                             	        vpclmulqdq	$16, %xmm5, %xmm6, %xmm10
 10523                             	        vaesenc	16(%rdi), %xmm8, %xmm8
 10524                             	        vaesenc	32(%rdi), %xmm8, %xmm8
 10525                             	        vpclmulqdq	$0x01, %xmm5, %xmm6, %xmm11
 10526                             	        vaesenc	48(%rdi), %xmm8, %xmm8
 10527                             	        vaesenc	64(%rdi), %xmm8, %xmm8
 10528                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm12
 10529                             	        vaesenc	80(%rdi), %xmm8, %xmm8
 10530                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm1
 10531                             	        vaesenc	96(%rdi), %xmm8, %xmm8
 10532                             	        vpxor	%xmm11, %xmm10, %xmm10
 10533                             	        vpslldq	$8, %xmm10, %xmm2
 10534                             	        vpsrldq	$8, %xmm10, %xmm10
 10535                             	        vaesenc	112(%rdi), %xmm8, %xmm8
 10536                             	        vpxor	%xmm12, %xmm2, %xmm2
 10537                             	        vpxor	%xmm10, %xmm1, %xmm3
 10538                             	        vmovdqa	L_avx1_aes_gcm_mod2_128(%rip), %xmm0
 10539                             	        vpclmulqdq	$16, %xmm0, %xmm2, %xmm11
 10540                             	        vaesenc	128(%rdi), %xmm8, %xmm8
 10541                             	        vpshufd	$0x4e, %xmm2, %xmm10
 10542                             	        vpxor	%xmm11, %xmm10, %xmm10
 10543                             	        vpclmulqdq	$16, %xmm0, %xmm10, %xmm11
 10544                             	        vaesenc	144(%rdi), %xmm8, %xmm8
 10545                             	        vpshufd	$0x4e, %xmm10, %xmm10
 10546                             	        vpxor	%xmm11, %xmm10, %xmm10
 10547                             	        vpxor	%xmm3, %xmm10, %xmm6
 10548                             	        cmpl	$11, %esi
 10549                             	        vmovdqa	160(%rdi), %xmm9
 10550                             	        jl	L_AES_GCM_encrypt_update_avx1_aesenc_gfmul_last
 10551                             	        vaesenc	%xmm9, %xmm8, %xmm8
 10552                             	        vaesenc	176(%rdi), %xmm8, %xmm8
 10553                             	        cmpl	$13, %esi
 10554                             	        vmovdqa	192(%rdi), %xmm9
 10555                             	        jl	L_AES_GCM_encrypt_update_avx1_aesenc_gfmul_last
 10556                             	        vaesenc	%xmm9, %xmm8, %xmm8
 10557                             	        vaesenc	208(%rdi), %xmm8, %xmm8
 10558                             	        vmovdqa	224(%rdi), %xmm9
 10559                             	L_AES_GCM_encrypt_update_avx1_aesenc_gfmul_last:
 10560                             	        vaesenclast	%xmm9, %xmm8, %xmm8
 10561                             	        vmovdqa	%xmm13, %xmm0
 10562                             	        vpxor	%xmm0, %xmm8, %xmm8
 10563                             	        vmovdqu	%xmm8, (%r10,%r14,1)
 10564                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm8, %xmm8
 10565                             	        addl	$16, %r14d
 10566                             	        vpxor	%xmm8, %xmm6, %xmm6
 10567                             	        cmpl	%r13d, %r14d
 10568                             	        jl	L_AES_GCM_encrypt_update_avx1_last_block_start
 10569                             	L_AES_GCM_encrypt_update_avx1_last_block_ghash:
 10570                             	        # ghash_gfmul_red_avx
 10571                             	        vpshufd	$0x4e, %xmm5, %xmm9
 10572                             	        vpshufd	$0x4e, %xmm6, %xmm10
 10573                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm11
 10574                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm8
 10575                             	        vpxor	%xmm5, %xmm9, %xmm9
 10576                             	        vpxor	%xmm6, %xmm10, %xmm10
 10577                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 10578                             	        vpxor	%xmm8, %xmm9, %xmm9
 10579                             	        vpxor	%xmm11, %xmm9, %xmm9
 10580                             	        vpslldq	$8, %xmm9, %xmm10
 10581                             	        vpsrldq	$8, %xmm9, %xmm9
 10582                             	        vpxor	%xmm10, %xmm8, %xmm8
 10583                             	        vpxor	%xmm9, %xmm11, %xmm6
 10584                             	        vpslld	$31, %xmm8, %xmm12
 10585                             	        vpslld	$30, %xmm8, %xmm13
 10586                             	        vpslld	$25, %xmm8, %xmm14
 10587                             	        vpxor	%xmm13, %xmm12, %xmm12
 10588                             	        vpxor	%xmm14, %xmm12, %xmm12
 10589                             	        vpsrldq	$4, %xmm12, %xmm13
 10590                             	        vpslldq	$12, %xmm12, %xmm12
 10591                             	        vpxor	%xmm12, %xmm8, %xmm8
 10592                             	        vpsrld	$0x01, %xmm8, %xmm14
 10593                             	        vpsrld	$2, %xmm8, %xmm10
 10594                             	        vpsrld	$7, %xmm8, %xmm9
 10595                             	        vpxor	%xmm10, %xmm14, %xmm14
 10596                             	        vpxor	%xmm9, %xmm14, %xmm14
 10597                             	        vpxor	%xmm13, %xmm14, %xmm14
 10598                             	        vpxor	%xmm8, %xmm14, %xmm14
 10599                             	        vpxor	%xmm14, %xmm6, %xmm6
 10600                             	L_AES_GCM_encrypt_update_avx1_last_block_done:
 10601                             	L_AES_GCM_encrypt_update_avx1_done_enc:
 10602                             	        vmovdqa	%xmm6, (%r9)
 10603                             	        vzeroupper
 10604                             	        addq	$0xa0, %rsp
 10605                             	        popq	%r14
 10606                             	        popq	%r12
 10607                             	        popq	%r13
 10608                             	        repz retq
 10609                             	#ifndef __APPLE__
 10611                             	#endif /* __APPLE__ */
 10612                             	#ifndef __APPLE__
 10613                             	.text
 10614                             	.globl	AES_GCM_encrypt_final_avx1
 10616                             	.align	16
 10617                             	AES_GCM_encrypt_final_avx1:
 10618                             	#else
 10619                             	.section	__TEXT,__text
 10620                             	.globl	_AES_GCM_encrypt_final_avx1
 10621                             	.p2align	4
 10622                             	_AES_GCM_encrypt_final_avx1:
 10623                             	#endif /* __APPLE__ */
 10624                             	        pushq	%r13
 10625                             	        movq	%rdx, %rax
 10626                             	        movl	%ecx, %r10d
 10627                             	        movl	%r8d, %r11d
 10628                             	        movq	16(%rsp), %r8
 10629                             	        subq	$16, %rsp
 10630                             	        vmovdqa	(%rdi), %xmm4
 10631                             	        vmovdqa	(%r9), %xmm5
 10632                             	        vmovdqa	(%r8), %xmm6
 10633                             	        vpsrlq	$63, %xmm5, %xmm9
 10634                             	        vpsllq	$0x01, %xmm5, %xmm8
 10635                             	        vpslldq	$8, %xmm9, %xmm9
 10636                             	        vpor	%xmm9, %xmm8, %xmm8
 10637                             	        vpshufd	$0xff, %xmm5, %xmm5
 10638                             	        vpsrad	$31, %xmm5, %xmm5
 10639                             	        vpand	L_avx1_aes_gcm_mod2_128(%rip), %xmm5, %xmm5
 10640                             	        vpxor	%xmm8, %xmm5, %xmm5
 10641                             	        movl	%r10d, %edx
 10642                             	        movl	%r11d, %ecx
 10643                             	        shlq	$3, %rdx
 10644                             	        shlq	$3, %rcx
 10645                             	        vpinsrq	$0x00, %rdx, %xmm0, %xmm0
 10646                             	        vpinsrq	$0x01, %rcx, %xmm0, %xmm0
 10647                             	        vpxor	%xmm0, %xmm4, %xmm4
 10648                             	        # ghash_gfmul_red_avx
 10649                             	        vpshufd	$0x4e, %xmm5, %xmm9
 10650                             	        vpshufd	$0x4e, %xmm4, %xmm10
 10651                             	        vpclmulqdq	$0x11, %xmm5, %xmm4, %xmm11
 10652                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm8
 10653                             	        vpxor	%xmm5, %xmm9, %xmm9
 10654                             	        vpxor	%xmm4, %xmm10, %xmm10
 10655                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 10656                             	        vpxor	%xmm8, %xmm9, %xmm9
 10657                             	        vpxor	%xmm11, %xmm9, %xmm9
 10658                             	        vpslldq	$8, %xmm9, %xmm10
 10659                             	        vpsrldq	$8, %xmm9, %xmm9
 10660                             	        vpxor	%xmm10, %xmm8, %xmm8
 10661                             	        vpxor	%xmm9, %xmm11, %xmm4
 10662                             	        vpslld	$31, %xmm8, %xmm12
 10663                             	        vpslld	$30, %xmm8, %xmm13
 10664                             	        vpslld	$25, %xmm8, %xmm14
 10665                             	        vpxor	%xmm13, %xmm12, %xmm12
 10666                             	        vpxor	%xmm14, %xmm12, %xmm12
 10667                             	        vpsrldq	$4, %xmm12, %xmm13
 10668                             	        vpslldq	$12, %xmm12, %xmm12
 10669                             	        vpxor	%xmm12, %xmm8, %xmm8
 10670                             	        vpsrld	$0x01, %xmm8, %xmm14
 10671                             	        vpsrld	$2, %xmm8, %xmm10
 10672                             	        vpsrld	$7, %xmm8, %xmm9
 10673                             	        vpxor	%xmm10, %xmm14, %xmm14
 10674                             	        vpxor	%xmm9, %xmm14, %xmm14
 10675                             	        vpxor	%xmm13, %xmm14, %xmm14
 10676                             	        vpxor	%xmm8, %xmm14, %xmm14
 10677                             	        vpxor	%xmm14, %xmm4, %xmm4
 10678                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm4, %xmm4
 10679                             	        vpxor	%xmm6, %xmm4, %xmm0
 10680                             	        cmpl	$16, %eax
 10681                             	        je	L_AES_GCM_encrypt_final_avx1_store_tag_16
 10682                             	        xorq	%rcx, %rcx
 10683                             	        vmovdqu	%xmm0, (%rsp)
 10684                             	L_AES_GCM_encrypt_final_avx1_store_tag_loop:
 10685                             	        movzbl	(%rsp,%rcx,1), %r13d
 10686                             	        movb	%r13b, (%rsi,%rcx,1)
 10687                             	        incl	%ecx
 10688                             	        cmpl	%eax, %ecx
 10689                             	        jne	L_AES_GCM_encrypt_final_avx1_store_tag_loop
 10690                             	        jmp	L_AES_GCM_encrypt_final_avx1_store_tag_done
 10691                             	L_AES_GCM_encrypt_final_avx1_store_tag_16:
 10692                             	        vmovdqu	%xmm0, (%rsi)
 10693                             	L_AES_GCM_encrypt_final_avx1_store_tag_done:
 10694                             	        vzeroupper
 10695                             	        addq	$16, %rsp
 10696                             	        popq	%r13
 10697                             	        repz retq
 10698                             	#ifndef __APPLE__
 10700                             	#endif /* __APPLE__ */
 10701                             	#ifndef __APPLE__
 10702                             	.text
 10703                             	.globl	AES_GCM_decrypt_update_avx1
 10705                             	.align	16
 10706                             	AES_GCM_decrypt_update_avx1:
 10707                             	#else
 10708                             	.section	__TEXT,__text
 10709                             	.globl	_AES_GCM_decrypt_update_avx1
 10710                             	.p2align	4
 10711                             	_AES_GCM_decrypt_update_avx1:
 10712                             	#endif /* __APPLE__ */
 10713                             	        pushq	%r13
 10714                             	        pushq	%r12
 10715                             	        pushq	%r14
 10716                             	        movq	%rdx, %r10
 10717                             	        movq	%rcx, %r11
 10718                             	        movq	32(%rsp), %rax
 10719                             	        movq	40(%rsp), %r12
 10720                             	        subq	$0xa8, %rsp
 10721                             	        vmovdqa	(%r9), %xmm6
 10722                             	        vmovdqa	(%rax), %xmm5
 10723                             	        vpsrlq	$63, %xmm5, %xmm9
 10724                             	        vpsllq	$0x01, %xmm5, %xmm8
 10725                             	        vpslldq	$8, %xmm9, %xmm9
 10726                             	        vpor	%xmm9, %xmm8, %xmm8
 10727                             	        vpshufd	$0xff, %xmm5, %xmm5
 10728                             	        vpsrad	$31, %xmm5, %xmm5
 10729                             	        vpand	L_avx1_aes_gcm_mod2_128(%rip), %xmm5, %xmm5
 10730                             	        vpxor	%xmm8, %xmm5, %xmm5
 10731                             	        xorl	%r14d, %r14d
 10732                             	        cmpl	$0x80, %r8d
 10733                             	        movl	%r8d, %r13d
 10734                             	        jl	L_AES_GCM_decrypt_update_avx1_done_128
 10735                             	        andl	$0xffffff80, %r13d
 10736                             	        vmovdqa	%xmm6, %xmm2
 10737                             	        # H ^ 1
 10738                             	        vmovdqa	%xmm5, (%rsp)
 10739                             	        # H ^ 2
 10740                             	        vpclmulqdq	$0x00, %xmm5, %xmm5, %xmm8
 10741                             	        vpclmulqdq	$0x11, %xmm5, %xmm5, %xmm0
 10742                             	        vpslld	$31, %xmm8, %xmm12
 10743                             	        vpslld	$30, %xmm8, %xmm13
 10744                             	        vpslld	$25, %xmm8, %xmm14
 10745                             	        vpxor	%xmm13, %xmm12, %xmm12
 10746                             	        vpxor	%xmm14, %xmm12, %xmm12
 10747                             	        vpsrldq	$4, %xmm12, %xmm13
 10748                             	        vpslldq	$12, %xmm12, %xmm12
 10749                             	        vpxor	%xmm12, %xmm8, %xmm8
 10750                             	        vpsrld	$0x01, %xmm8, %xmm14
 10751                             	        vpsrld	$2, %xmm8, %xmm10
 10752                             	        vpsrld	$7, %xmm8, %xmm9
 10753                             	        vpxor	%xmm10, %xmm14, %xmm14
 10754                             	        vpxor	%xmm9, %xmm14, %xmm14
 10755                             	        vpxor	%xmm13, %xmm14, %xmm14
 10756                             	        vpxor	%xmm8, %xmm14, %xmm14
 10757                             	        vpxor	%xmm14, %xmm0, %xmm0
 10758                             	        vmovdqa	%xmm0, 16(%rsp)
 10759                             	        # H ^ 3
 10760                             	        # ghash_gfmul_red_avx
 10761                             	        vpshufd	$0x4e, %xmm5, %xmm9
 10762                             	        vpshufd	$0x4e, %xmm0, %xmm10
 10763                             	        vpclmulqdq	$0x11, %xmm5, %xmm0, %xmm11
 10764                             	        vpclmulqdq	$0x00, %xmm5, %xmm0, %xmm8
 10765                             	        vpxor	%xmm5, %xmm9, %xmm9
 10766                             	        vpxor	%xmm0, %xmm10, %xmm10
 10767                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 10768                             	        vpxor	%xmm8, %xmm9, %xmm9
 10769                             	        vpxor	%xmm11, %xmm9, %xmm9
 10770                             	        vpslldq	$8, %xmm9, %xmm10
 10771                             	        vpsrldq	$8, %xmm9, %xmm9
 10772                             	        vpxor	%xmm10, %xmm8, %xmm8
 10773                             	        vpxor	%xmm9, %xmm11, %xmm1
 10774                             	        vpslld	$31, %xmm8, %xmm12
 10775                             	        vpslld	$30, %xmm8, %xmm13
 10776                             	        vpslld	$25, %xmm8, %xmm14
 10777                             	        vpxor	%xmm13, %xmm12, %xmm12
 10778                             	        vpxor	%xmm14, %xmm12, %xmm12
 10779                             	        vpsrldq	$4, %xmm12, %xmm13
 10780                             	        vpslldq	$12, %xmm12, %xmm12
 10781                             	        vpxor	%xmm12, %xmm8, %xmm8
 10782                             	        vpsrld	$0x01, %xmm8, %xmm14
 10783                             	        vpsrld	$2, %xmm8, %xmm10
 10784                             	        vpsrld	$7, %xmm8, %xmm9
 10785                             	        vpxor	%xmm10, %xmm14, %xmm14
 10786                             	        vpxor	%xmm9, %xmm14, %xmm14
 10787                             	        vpxor	%xmm13, %xmm14, %xmm14
 10788                             	        vpxor	%xmm8, %xmm14, %xmm14
 10789                             	        vpxor	%xmm14, %xmm1, %xmm1
 10790                             	        vmovdqa	%xmm1, 32(%rsp)
 10791                             	        # H ^ 4
 10792                             	        vpclmulqdq	$0x00, %xmm0, %xmm0, %xmm8
 10793                             	        vpclmulqdq	$0x11, %xmm0, %xmm0, %xmm3
 10794                             	        vpslld	$31, %xmm8, %xmm12
 10795                             	        vpslld	$30, %xmm8, %xmm13
 10796                             	        vpslld	$25, %xmm8, %xmm14
 10797                             	        vpxor	%xmm13, %xmm12, %xmm12
 10798                             	        vpxor	%xmm14, %xmm12, %xmm12
 10799                             	        vpsrldq	$4, %xmm12, %xmm13
 10800                             	        vpslldq	$12, %xmm12, %xmm12
 10801                             	        vpxor	%xmm12, %xmm8, %xmm8
 10802                             	        vpsrld	$0x01, %xmm8, %xmm14
 10803                             	        vpsrld	$2, %xmm8, %xmm10
 10804                             	        vpsrld	$7, %xmm8, %xmm9
 10805                             	        vpxor	%xmm10, %xmm14, %xmm14
 10806                             	        vpxor	%xmm9, %xmm14, %xmm14
 10807                             	        vpxor	%xmm13, %xmm14, %xmm14
 10808                             	        vpxor	%xmm8, %xmm14, %xmm14
 10809                             	        vpxor	%xmm14, %xmm3, %xmm3
 10810                             	        vmovdqa	%xmm3, 48(%rsp)
 10811                             	        # H ^ 5
 10812                             	        # ghash_gfmul_red_avx
 10813                             	        vpshufd	$0x4e, %xmm0, %xmm9
 10814                             	        vpshufd	$0x4e, %xmm1, %xmm10
 10815                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm11
 10816                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm8
 10817                             	        vpxor	%xmm0, %xmm9, %xmm9
 10818                             	        vpxor	%xmm1, %xmm10, %xmm10
 10819                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 10820                             	        vpxor	%xmm8, %xmm9, %xmm9
 10821                             	        vpxor	%xmm11, %xmm9, %xmm9
 10822                             	        vpslldq	$8, %xmm9, %xmm10
 10823                             	        vpsrldq	$8, %xmm9, %xmm9
 10824                             	        vpxor	%xmm10, %xmm8, %xmm8
 10825                             	        vpxor	%xmm9, %xmm11, %xmm7
 10826                             	        vpslld	$31, %xmm8, %xmm12
 10827                             	        vpslld	$30, %xmm8, %xmm13
 10828                             	        vpslld	$25, %xmm8, %xmm14
 10829                             	        vpxor	%xmm13, %xmm12, %xmm12
 10830                             	        vpxor	%xmm14, %xmm12, %xmm12
 10831                             	        vpsrldq	$4, %xmm12, %xmm13
 10832                             	        vpslldq	$12, %xmm12, %xmm12
 10833                             	        vpxor	%xmm12, %xmm8, %xmm8
 10834                             	        vpsrld	$0x01, %xmm8, %xmm14
 10835                             	        vpsrld	$2, %xmm8, %xmm10
 10836                             	        vpsrld	$7, %xmm8, %xmm9
 10837                             	        vpxor	%xmm10, %xmm14, %xmm14
 10838                             	        vpxor	%xmm9, %xmm14, %xmm14
 10839                             	        vpxor	%xmm13, %xmm14, %xmm14
 10840                             	        vpxor	%xmm8, %xmm14, %xmm14
 10841                             	        vpxor	%xmm14, %xmm7, %xmm7
 10842                             	        vmovdqa	%xmm7, 64(%rsp)
 10843                             	        # H ^ 6
 10844                             	        vpclmulqdq	$0x00, %xmm1, %xmm1, %xmm8
 10845                             	        vpclmulqdq	$0x11, %xmm1, %xmm1, %xmm7
 10846                             	        vpslld	$31, %xmm8, %xmm12
 10847                             	        vpslld	$30, %xmm8, %xmm13
 10848                             	        vpslld	$25, %xmm8, %xmm14
 10849                             	        vpxor	%xmm13, %xmm12, %xmm12
 10850                             	        vpxor	%xmm14, %xmm12, %xmm12
 10851                             	        vpsrldq	$4, %xmm12, %xmm13
 10852                             	        vpslldq	$12, %xmm12, %xmm12
 10853                             	        vpxor	%xmm12, %xmm8, %xmm8
 10854                             	        vpsrld	$0x01, %xmm8, %xmm14
 10855                             	        vpsrld	$2, %xmm8, %xmm10
 10856                             	        vpsrld	$7, %xmm8, %xmm9
 10857                             	        vpxor	%xmm10, %xmm14, %xmm14
 10858                             	        vpxor	%xmm9, %xmm14, %xmm14
 10859                             	        vpxor	%xmm13, %xmm14, %xmm14
 10860                             	        vpxor	%xmm8, %xmm14, %xmm14
 10861                             	        vpxor	%xmm14, %xmm7, %xmm7
 10862                             	        vmovdqa	%xmm7, 80(%rsp)
 10863                             	        # H ^ 7
 10864                             	        # ghash_gfmul_red_avx
 10865                             	        vpshufd	$0x4e, %xmm1, %xmm9
 10866                             	        vpshufd	$0x4e, %xmm3, %xmm10
 10867                             	        vpclmulqdq	$0x11, %xmm1, %xmm3, %xmm11
 10868                             	        vpclmulqdq	$0x00, %xmm1, %xmm3, %xmm8
 10869                             	        vpxor	%xmm1, %xmm9, %xmm9
 10870                             	        vpxor	%xmm3, %xmm10, %xmm10
 10871                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 10872                             	        vpxor	%xmm8, %xmm9, %xmm9
 10873                             	        vpxor	%xmm11, %xmm9, %xmm9
 10874                             	        vpslldq	$8, %xmm9, %xmm10
 10875                             	        vpsrldq	$8, %xmm9, %xmm9
 10876                             	        vpxor	%xmm10, %xmm8, %xmm8
 10877                             	        vpxor	%xmm9, %xmm11, %xmm7
 10878                             	        vpslld	$31, %xmm8, %xmm12
 10879                             	        vpslld	$30, %xmm8, %xmm13
 10880                             	        vpslld	$25, %xmm8, %xmm14
 10881                             	        vpxor	%xmm13, %xmm12, %xmm12
 10882                             	        vpxor	%xmm14, %xmm12, %xmm12
 10883                             	        vpsrldq	$4, %xmm12, %xmm13
 10884                             	        vpslldq	$12, %xmm12, %xmm12
 10885                             	        vpxor	%xmm12, %xmm8, %xmm8
 10886                             	        vpsrld	$0x01, %xmm8, %xmm14
 10887                             	        vpsrld	$2, %xmm8, %xmm10
 10888                             	        vpsrld	$7, %xmm8, %xmm9
 10889                             	        vpxor	%xmm10, %xmm14, %xmm14
 10890                             	        vpxor	%xmm9, %xmm14, %xmm14
 10891                             	        vpxor	%xmm13, %xmm14, %xmm14
 10892                             	        vpxor	%xmm8, %xmm14, %xmm14
 10893                             	        vpxor	%xmm14, %xmm7, %xmm7
 10894                             	        vmovdqa	%xmm7, 96(%rsp)
 10895                             	        # H ^ 8
 10896                             	        vpclmulqdq	$0x00, %xmm3, %xmm3, %xmm8
 10897                             	        vpclmulqdq	$0x11, %xmm3, %xmm3, %xmm7
 10898                             	        vpslld	$31, %xmm8, %xmm12
 10899                             	        vpslld	$30, %xmm8, %xmm13
 10900                             	        vpslld	$25, %xmm8, %xmm14
 10901                             	        vpxor	%xmm13, %xmm12, %xmm12
 10902                             	        vpxor	%xmm14, %xmm12, %xmm12
 10903                             	        vpsrldq	$4, %xmm12, %xmm13
 10904                             	        vpslldq	$12, %xmm12, %xmm12
 10905                             	        vpxor	%xmm12, %xmm8, %xmm8
 10906                             	        vpsrld	$0x01, %xmm8, %xmm14
 10907                             	        vpsrld	$2, %xmm8, %xmm10
 10908                             	        vpsrld	$7, %xmm8, %xmm9
 10909                             	        vpxor	%xmm10, %xmm14, %xmm14
 10910                             	        vpxor	%xmm9, %xmm14, %xmm14
 10911                             	        vpxor	%xmm13, %xmm14, %xmm14
 10912                             	        vpxor	%xmm8, %xmm14, %xmm14
 10913                             	        vpxor	%xmm14, %xmm7, %xmm7
 10914                             	        vmovdqa	%xmm7, 112(%rsp)
 10915                             	L_AES_GCM_decrypt_update_avx1_ghash_128:
 10916                             	        leaq	(%r11,%r14,1), %rcx
 10917                             	        leaq	(%r10,%r14,1), %rdx
 10918                             	        vmovdqa	(%r12), %xmm0
 10919                             	        vmovdqa	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm1
 10920                             	        vpshufb	%xmm1, %xmm0, %xmm8
 10921                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm0, %xmm9
 10922                             	        vpshufb	%xmm1, %xmm9, %xmm9
 10923                             	        vpaddd	L_avx1_aes_gcm_two(%rip), %xmm0, %xmm10
 10924                             	        vpshufb	%xmm1, %xmm10, %xmm10
 10925                             	        vpaddd	L_avx1_aes_gcm_three(%rip), %xmm0, %xmm11
 10926                             	        vpshufb	%xmm1, %xmm11, %xmm11
 10927                             	        vpaddd	L_avx1_aes_gcm_four(%rip), %xmm0, %xmm12
 10928                             	        vpshufb	%xmm1, %xmm12, %xmm12
 10929                             	        vpaddd	L_avx1_aes_gcm_five(%rip), %xmm0, %xmm13
 10930                             	        vpshufb	%xmm1, %xmm13, %xmm13
 10931                             	        vpaddd	L_avx1_aes_gcm_six(%rip), %xmm0, %xmm14
 10932                             	        vpshufb	%xmm1, %xmm14, %xmm14
 10933                             	        vpaddd	L_avx1_aes_gcm_seven(%rip), %xmm0, %xmm15
 10934                             	        vpshufb	%xmm1, %xmm15, %xmm15
 10935                             	        vpaddd	L_avx1_aes_gcm_eight(%rip), %xmm0, %xmm0
 10936                             	        vmovdqa	(%rdi), %xmm7
 10937                             	        vmovdqa	%xmm0, (%r12)
 10938                             	        vpxor	%xmm7, %xmm8, %xmm8
 10939                             	        vpxor	%xmm7, %xmm9, %xmm9
 10940                             	        vpxor	%xmm7, %xmm10, %xmm10
 10941                             	        vpxor	%xmm7, %xmm11, %xmm11
 10942                             	        vpxor	%xmm7, %xmm12, %xmm12
 10943                             	        vpxor	%xmm7, %xmm13, %xmm13
 10944                             	        vpxor	%xmm7, %xmm14, %xmm14
 10945                             	        vpxor	%xmm7, %xmm15, %xmm15
 10946                             	        vmovdqa	112(%rsp), %xmm7
 10947                             	        vmovdqu	(%rcx), %xmm0
 10948                             	        vaesenc	16(%rdi), %xmm8, %xmm8
 10949                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 10950                             	        vpxor	%xmm2, %xmm0, %xmm0
 10951                             	        vpshufd	$0x4e, %xmm7, %xmm1
 10952                             	        vpshufd	$0x4e, %xmm0, %xmm5
 10953                             	        vpxor	%xmm7, %xmm1, %xmm1
 10954                             	        vpxor	%xmm0, %xmm5, %xmm5
 10955                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm3
 10956                             	        vaesenc	16(%rdi), %xmm9, %xmm9
 10957                             	        vaesenc	16(%rdi), %xmm10, %xmm10
 10958                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm2
 10959                             	        vaesenc	16(%rdi), %xmm11, %xmm11
 10960                             	        vaesenc	16(%rdi), %xmm12, %xmm12
 10961                             	        vpclmulqdq	$0x00, %xmm5, %xmm1, %xmm1
 10962                             	        vaesenc	16(%rdi), %xmm13, %xmm13
 10963                             	        vaesenc	16(%rdi), %xmm14, %xmm14
 10964                             	        vaesenc	16(%rdi), %xmm15, %xmm15
 10965                             	        vpxor	%xmm2, %xmm1, %xmm1
 10966                             	        vpxor	%xmm3, %xmm1, %xmm1
 10967                             	        vmovdqa	96(%rsp), %xmm7
 10968                             	        vmovdqu	16(%rcx), %xmm0
 10969                             	        vpshufd	$0x4e, %xmm7, %xmm4
 10970                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 10971                             	        vaesenc	32(%rdi), %xmm8, %xmm8
 10972                             	        vpxor	%xmm7, %xmm4, %xmm4
 10973                             	        vpshufd	$0x4e, %xmm0, %xmm5
 10974                             	        vpxor	%xmm0, %xmm5, %xmm5
 10975                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 10976                             	        vaesenc	32(%rdi), %xmm9, %xmm9
 10977                             	        vaesenc	32(%rdi), %xmm10, %xmm10
 10978                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 10979                             	        vaesenc	32(%rdi), %xmm11, %xmm11
 10980                             	        vaesenc	32(%rdi), %xmm12, %xmm12
 10981                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 10982                             	        vaesenc	32(%rdi), %xmm13, %xmm13
 10983                             	        vaesenc	32(%rdi), %xmm14, %xmm14
 10984                             	        vaesenc	32(%rdi), %xmm15, %xmm15
 10985                             	        vpxor	%xmm7, %xmm1, %xmm1
 10986                             	        vpxor	%xmm7, %xmm2, %xmm2
 10987                             	        vpxor	%xmm6, %xmm1, %xmm1
 10988                             	        vpxor	%xmm6, %xmm3, %xmm3
 10989                             	        vpxor	%xmm4, %xmm1, %xmm1
 10990                             	        vmovdqa	80(%rsp), %xmm7
 10991                             	        vmovdqu	32(%rcx), %xmm0
 10992                             	        vpshufd	$0x4e, %xmm7, %xmm4
 10993                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 10994                             	        vaesenc	48(%rdi), %xmm8, %xmm8
 10995                             	        vpxor	%xmm7, %xmm4, %xmm4
 10996                             	        vpshufd	$0x4e, %xmm0, %xmm5
 10997                             	        vpxor	%xmm0, %xmm5, %xmm5
 10998                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 10999                             	        vaesenc	48(%rdi), %xmm9, %xmm9
 11000                             	        vaesenc	48(%rdi), %xmm10, %xmm10
 11001                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 11002                             	        vaesenc	48(%rdi), %xmm11, %xmm11
 11003                             	        vaesenc	48(%rdi), %xmm12, %xmm12
 11004                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 11005                             	        vaesenc	48(%rdi), %xmm13, %xmm13
 11006                             	        vaesenc	48(%rdi), %xmm14, %xmm14
 11007                             	        vaesenc	48(%rdi), %xmm15, %xmm15
 11008                             	        vpxor	%xmm7, %xmm1, %xmm1
 11009                             	        vpxor	%xmm7, %xmm2, %xmm2
 11010                             	        vpxor	%xmm6, %xmm1, %xmm1
 11011                             	        vpxor	%xmm6, %xmm3, %xmm3
 11012                             	        vpxor	%xmm4, %xmm1, %xmm1
 11013                             	        vmovdqa	64(%rsp), %xmm7
 11014                             	        vmovdqu	48(%rcx), %xmm0
 11015                             	        vpshufd	$0x4e, %xmm7, %xmm4
 11016                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 11017                             	        vaesenc	64(%rdi), %xmm8, %xmm8
 11018                             	        vpxor	%xmm7, %xmm4, %xmm4
 11019                             	        vpshufd	$0x4e, %xmm0, %xmm5
 11020                             	        vpxor	%xmm0, %xmm5, %xmm5
 11021                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 11022                             	        vaesenc	64(%rdi), %xmm9, %xmm9
 11023                             	        vaesenc	64(%rdi), %xmm10, %xmm10
 11024                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 11025                             	        vaesenc	64(%rdi), %xmm11, %xmm11
 11026                             	        vaesenc	64(%rdi), %xmm12, %xmm12
 11027                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 11028                             	        vaesenc	64(%rdi), %xmm13, %xmm13
 11029                             	        vaesenc	64(%rdi), %xmm14, %xmm14
 11030                             	        vaesenc	64(%rdi), %xmm15, %xmm15
 11031                             	        vpxor	%xmm7, %xmm1, %xmm1
 11032                             	        vpxor	%xmm7, %xmm2, %xmm2
 11033                             	        vpxor	%xmm6, %xmm1, %xmm1
 11034                             	        vpxor	%xmm6, %xmm3, %xmm3
 11035                             	        vpxor	%xmm4, %xmm1, %xmm1
 11036                             	        vmovdqa	48(%rsp), %xmm7
 11037                             	        vmovdqu	64(%rcx), %xmm0
 11038                             	        vpshufd	$0x4e, %xmm7, %xmm4
 11039                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 11040                             	        vaesenc	80(%rdi), %xmm8, %xmm8
 11041                             	        vpxor	%xmm7, %xmm4, %xmm4
 11042                             	        vpshufd	$0x4e, %xmm0, %xmm5
 11043                             	        vpxor	%xmm0, %xmm5, %xmm5
 11044                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 11045                             	        vaesenc	80(%rdi), %xmm9, %xmm9
 11046                             	        vaesenc	80(%rdi), %xmm10, %xmm10
 11047                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 11048                             	        vaesenc	80(%rdi), %xmm11, %xmm11
 11049                             	        vaesenc	80(%rdi), %xmm12, %xmm12
 11050                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 11051                             	        vaesenc	80(%rdi), %xmm13, %xmm13
 11052                             	        vaesenc	80(%rdi), %xmm14, %xmm14
 11053                             	        vaesenc	80(%rdi), %xmm15, %xmm15
 11054                             	        vpxor	%xmm7, %xmm1, %xmm1
 11055                             	        vpxor	%xmm7, %xmm2, %xmm2
 11056                             	        vpxor	%xmm6, %xmm1, %xmm1
 11057                             	        vpxor	%xmm6, %xmm3, %xmm3
 11058                             	        vpxor	%xmm4, %xmm1, %xmm1
 11059                             	        vmovdqa	32(%rsp), %xmm7
 11060                             	        vmovdqu	80(%rcx), %xmm0
 11061                             	        vpshufd	$0x4e, %xmm7, %xmm4
 11062                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 11063                             	        vaesenc	96(%rdi), %xmm8, %xmm8
 11064                             	        vpxor	%xmm7, %xmm4, %xmm4
 11065                             	        vpshufd	$0x4e, %xmm0, %xmm5
 11066                             	        vpxor	%xmm0, %xmm5, %xmm5
 11067                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 11068                             	        vaesenc	96(%rdi), %xmm9, %xmm9
 11069                             	        vaesenc	96(%rdi), %xmm10, %xmm10
 11070                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 11071                             	        vaesenc	96(%rdi), %xmm11, %xmm11
 11072                             	        vaesenc	96(%rdi), %xmm12, %xmm12
 11073                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 11074                             	        vaesenc	96(%rdi), %xmm13, %xmm13
 11075                             	        vaesenc	96(%rdi), %xmm14, %xmm14
 11076                             	        vaesenc	96(%rdi), %xmm15, %xmm15
 11077                             	        vpxor	%xmm7, %xmm1, %xmm1
 11078                             	        vpxor	%xmm7, %xmm2, %xmm2
 11079                             	        vpxor	%xmm6, %xmm1, %xmm1
 11080                             	        vpxor	%xmm6, %xmm3, %xmm3
 11081                             	        vpxor	%xmm4, %xmm1, %xmm1
 11082                             	        vmovdqa	16(%rsp), %xmm7
 11083                             	        vmovdqu	96(%rcx), %xmm0
 11084                             	        vpshufd	$0x4e, %xmm7, %xmm4
 11085                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 11086                             	        vaesenc	112(%rdi), %xmm8, %xmm8
 11087                             	        vpxor	%xmm7, %xmm4, %xmm4
 11088                             	        vpshufd	$0x4e, %xmm0, %xmm5
 11089                             	        vpxor	%xmm0, %xmm5, %xmm5
 11090                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 11091                             	        vaesenc	112(%rdi), %xmm9, %xmm9
 11092                             	        vaesenc	112(%rdi), %xmm10, %xmm10
 11093                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 11094                             	        vaesenc	112(%rdi), %xmm11, %xmm11
 11095                             	        vaesenc	112(%rdi), %xmm12, %xmm12
 11096                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 11097                             	        vaesenc	112(%rdi), %xmm13, %xmm13
 11098                             	        vaesenc	112(%rdi), %xmm14, %xmm14
 11099                             	        vaesenc	112(%rdi), %xmm15, %xmm15
 11100                             	        vpxor	%xmm7, %xmm1, %xmm1
 11101                             	        vpxor	%xmm7, %xmm2, %xmm2
 11102                             	        vpxor	%xmm6, %xmm1, %xmm1
 11103                             	        vpxor	%xmm6, %xmm3, %xmm3
 11104                             	        vpxor	%xmm4, %xmm1, %xmm1
 11105                             	        vmovdqa	(%rsp), %xmm7
 11106                             	        vmovdqu	112(%rcx), %xmm0
 11107                             	        vpshufd	$0x4e, %xmm7, %xmm4
 11108                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 11109                             	        vaesenc	128(%rdi), %xmm8, %xmm8
 11110                             	        vpxor	%xmm7, %xmm4, %xmm4
 11111                             	        vpshufd	$0x4e, %xmm0, %xmm5
 11112                             	        vpxor	%xmm0, %xmm5, %xmm5
 11113                             	        vpclmulqdq	$0x11, %xmm7, %xmm0, %xmm6
 11114                             	        vaesenc	128(%rdi), %xmm9, %xmm9
 11115                             	        vaesenc	128(%rdi), %xmm10, %xmm10
 11116                             	        vpclmulqdq	$0x00, %xmm7, %xmm0, %xmm7
 11117                             	        vaesenc	128(%rdi), %xmm11, %xmm11
 11118                             	        vaesenc	128(%rdi), %xmm12, %xmm12
 11119                             	        vpclmulqdq	$0x00, %xmm5, %xmm4, %xmm4
 11120                             	        vaesenc	128(%rdi), %xmm13, %xmm13
 11121                             	        vaesenc	128(%rdi), %xmm14, %xmm14
 11122                             	        vaesenc	128(%rdi), %xmm15, %xmm15
 11123                             	        vpxor	%xmm7, %xmm1, %xmm1
 11124                             	        vpxor	%xmm7, %xmm2, %xmm2
 11125                             	        vpxor	%xmm6, %xmm1, %xmm1
 11126                             	        vpxor	%xmm6, %xmm3, %xmm3
 11127                             	        vpxor	%xmm4, %xmm1, %xmm1
 11128                             	        vpslldq	$8, %xmm1, %xmm5
 11129                             	        vpsrldq	$8, %xmm1, %xmm1
 11130                             	        vaesenc	144(%rdi), %xmm8, %xmm8
 11131                             	        vpxor	%xmm5, %xmm2, %xmm2
 11132                             	        vpxor	%xmm1, %xmm3, %xmm3
 11133                             	        vaesenc	144(%rdi), %xmm9, %xmm9
 11134                             	        vpslld	$31, %xmm2, %xmm7
 11135                             	        vpslld	$30, %xmm2, %xmm4
 11136                             	        vpslld	$25, %xmm2, %xmm5
 11137                             	        vaesenc	144(%rdi), %xmm10, %xmm10
 11138                             	        vpxor	%xmm4, %xmm7, %xmm7
 11139                             	        vpxor	%xmm5, %xmm7, %xmm7
 11140                             	        vaesenc	144(%rdi), %xmm11, %xmm11
 11141                             	        vpsrldq	$4, %xmm7, %xmm4
 11142                             	        vpslldq	$12, %xmm7, %xmm7
 11143                             	        vaesenc	144(%rdi), %xmm12, %xmm12
 11144                             	        vpxor	%xmm7, %xmm2, %xmm2
 11145                             	        vpsrld	$0x01, %xmm2, %xmm5
 11146                             	        vaesenc	144(%rdi), %xmm13, %xmm13
 11147                             	        vpsrld	$2, %xmm2, %xmm1
 11148                             	        vpsrld	$7, %xmm2, %xmm0
 11149                             	        vaesenc	144(%rdi), %xmm14, %xmm14
 11150                             	        vpxor	%xmm1, %xmm5, %xmm5
 11151                             	        vpxor	%xmm0, %xmm5, %xmm5
 11152                             	        vaesenc	144(%rdi), %xmm15, %xmm15
 11153                             	        vpxor	%xmm4, %xmm5, %xmm5
 11154                             	        vpxor	%xmm5, %xmm2, %xmm2
 11155                             	        vpxor	%xmm3, %xmm2, %xmm2
 11156                             	        cmpl	$11, %esi
 11157                             	        vmovdqa	160(%rdi), %xmm7
 11158                             	        jl	L_AES_GCM_decrypt_update_avx1_aesenc_128_ghash_avx_done
 11159                             	        vaesenc	%xmm7, %xmm8, %xmm8
 11160                             	        vaesenc	%xmm7, %xmm9, %xmm9
 11161                             	        vaesenc	%xmm7, %xmm10, %xmm10
 11162                             	        vaesenc	%xmm7, %xmm11, %xmm11
 11163                             	        vaesenc	%xmm7, %xmm12, %xmm12
 11164                             	        vaesenc	%xmm7, %xmm13, %xmm13
 11165                             	        vaesenc	%xmm7, %xmm14, %xmm14
 11166                             	        vaesenc	%xmm7, %xmm15, %xmm15
 11167                             	        vmovdqa	176(%rdi), %xmm7
 11168                             	        vaesenc	%xmm7, %xmm8, %xmm8
 11169                             	        vaesenc	%xmm7, %xmm9, %xmm9
 11170                             	        vaesenc	%xmm7, %xmm10, %xmm10
 11171                             	        vaesenc	%xmm7, %xmm11, %xmm11
 11172                             	        vaesenc	%xmm7, %xmm12, %xmm12
 11173                             	        vaesenc	%xmm7, %xmm13, %xmm13
 11174                             	        vaesenc	%xmm7, %xmm14, %xmm14
 11175                             	        vaesenc	%xmm7, %xmm15, %xmm15
 11176                             	        cmpl	$13, %esi
 11177                             	        vmovdqa	192(%rdi), %xmm7
 11178                             	        jl	L_AES_GCM_decrypt_update_avx1_aesenc_128_ghash_avx_done
 11179                             	        vaesenc	%xmm7, %xmm8, %xmm8
 11180                             	        vaesenc	%xmm7, %xmm9, %xmm9
 11181                             	        vaesenc	%xmm7, %xmm10, %xmm10
 11182                             	        vaesenc	%xmm7, %xmm11, %xmm11
 11183                             	        vaesenc	%xmm7, %xmm12, %xmm12
 11184                             	        vaesenc	%xmm7, %xmm13, %xmm13
 11185                             	        vaesenc	%xmm7, %xmm14, %xmm14
 11186                             	        vaesenc	%xmm7, %xmm15, %xmm15
 11187                             	        vmovdqa	208(%rdi), %xmm7
 11188                             	        vaesenc	%xmm7, %xmm8, %xmm8
 11189                             	        vaesenc	%xmm7, %xmm9, %xmm9
 11190                             	        vaesenc	%xmm7, %xmm10, %xmm10
 11191                             	        vaesenc	%xmm7, %xmm11, %xmm11
 11192                             	        vaesenc	%xmm7, %xmm12, %xmm12
 11193                             	        vaesenc	%xmm7, %xmm13, %xmm13
 11194                             	        vaesenc	%xmm7, %xmm14, %xmm14
 11195                             	        vaesenc	%xmm7, %xmm15, %xmm15
 11196                             	        vmovdqa	224(%rdi), %xmm7
 11197                             	L_AES_GCM_decrypt_update_avx1_aesenc_128_ghash_avx_done:
 11198                             	        vaesenclast	%xmm7, %xmm8, %xmm8
 11199                             	        vaesenclast	%xmm7, %xmm9, %xmm9
 11200                             	        vmovdqu	(%rcx), %xmm0
 11201                             	        vmovdqu	16(%rcx), %xmm1
 11202                             	        vpxor	%xmm0, %xmm8, %xmm8
 11203                             	        vpxor	%xmm1, %xmm9, %xmm9
 11204                             	        vmovdqu	%xmm8, (%rdx)
 11205                             	        vmovdqu	%xmm9, 16(%rdx)
 11206                             	        vaesenclast	%xmm7, %xmm10, %xmm10
 11207                             	        vaesenclast	%xmm7, %xmm11, %xmm11
 11208                             	        vmovdqu	32(%rcx), %xmm0
 11209                             	        vmovdqu	48(%rcx), %xmm1
 11210                             	        vpxor	%xmm0, %xmm10, %xmm10
 11211                             	        vpxor	%xmm1, %xmm11, %xmm11
 11212                             	        vmovdqu	%xmm10, 32(%rdx)
 11213                             	        vmovdqu	%xmm11, 48(%rdx)
 11214                             	        vaesenclast	%xmm7, %xmm12, %xmm12
 11215                             	        vaesenclast	%xmm7, %xmm13, %xmm13
 11216                             	        vmovdqu	64(%rcx), %xmm0
 11217                             	        vmovdqu	80(%rcx), %xmm1
 11218                             	        vpxor	%xmm0, %xmm12, %xmm12
 11219                             	        vpxor	%xmm1, %xmm13, %xmm13
 11220                             	        vmovdqu	%xmm12, 64(%rdx)
 11221                             	        vmovdqu	%xmm13, 80(%rdx)
 11222                             	        vaesenclast	%xmm7, %xmm14, %xmm14
 11223                             	        vaesenclast	%xmm7, %xmm15, %xmm15
 11224                             	        vmovdqu	96(%rcx), %xmm0
 11225                             	        vmovdqu	112(%rcx), %xmm1
 11226                             	        vpxor	%xmm0, %xmm14, %xmm14
 11227                             	        vpxor	%xmm1, %xmm15, %xmm15
 11228                             	        vmovdqu	%xmm14, 96(%rdx)
 11229                             	        vmovdqu	%xmm15, 112(%rdx)
 11230                             	        addl	$0x80, %r14d
 11231                             	        cmpl	%r13d, %r14d
 11232                             	        jl	L_AES_GCM_decrypt_update_avx1_ghash_128
 11233                             	        vmovdqa	%xmm2, %xmm6
 11234                             	        vmovdqa	(%rsp), %xmm5
 11235                             	L_AES_GCM_decrypt_update_avx1_done_128:
 11236                             	        movl	%r8d, %edx
 11237                             	        cmpl	%edx, %r14d
 11238                             	        jge	L_AES_GCM_decrypt_update_avx1_done_dec
 11239                             	        movl	%r8d, %r13d
 11240                             	        andl	$0xfffffff0, %r13d
 11241                             	        cmpl	%r13d, %r14d
 11242                             	        jge	L_AES_GCM_decrypt_update_avx1_last_block_done
 11243                             	L_AES_GCM_decrypt_update_avx1_last_block_start:
 11244                             	        vmovdqu	(%r11,%r14,1), %xmm13
 11245                             	        vmovdqa	%xmm5, %xmm0
 11246                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm13, %xmm1
 11247                             	        vpxor	%xmm6, %xmm1, %xmm1
 11248                             	        vmovdqa	(%r12), %xmm9
 11249                             	        vpshufb	L_avx1_aes_gcm_bswap_epi64(%rip), %xmm9, %xmm8
 11250                             	        vpaddd	L_avx1_aes_gcm_one(%rip), %xmm9, %xmm9
 11251                             	        vmovdqa	%xmm9, (%r12)
 11252                             	        vpxor	(%rdi), %xmm8, %xmm8
 11253                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm10
 11254                             	        vaesenc	16(%rdi), %xmm8, %xmm8
 11255                             	        vaesenc	32(%rdi), %xmm8, %xmm8
 11256                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm11
 11257                             	        vaesenc	48(%rdi), %xmm8, %xmm8
 11258                             	        vaesenc	64(%rdi), %xmm8, %xmm8
 11259                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm12
 11260                             	        vaesenc	80(%rdi), %xmm8, %xmm8
 11261                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 11262                             	        vaesenc	96(%rdi), %xmm8, %xmm8
 11263                             	        vpxor	%xmm11, %xmm10, %xmm10
 11264                             	        vpslldq	$8, %xmm10, %xmm2
 11265                             	        vpsrldq	$8, %xmm10, %xmm10
 11266                             	        vaesenc	112(%rdi), %xmm8, %xmm8
 11267                             	        vpxor	%xmm12, %xmm2, %xmm2
 11268                             	        vpxor	%xmm10, %xmm1, %xmm3
 11269                             	        vmovdqa	L_avx1_aes_gcm_mod2_128(%rip), %xmm0
 11270                             	        vpclmulqdq	$16, %xmm0, %xmm2, %xmm11
 11271                             	        vaesenc	128(%rdi), %xmm8, %xmm8
 11272                             	        vpshufd	$0x4e, %xmm2, %xmm10
 11273                             	        vpxor	%xmm11, %xmm10, %xmm10
 11274                             	        vpclmulqdq	$16, %xmm0, %xmm10, %xmm11
 11275                             	        vaesenc	144(%rdi), %xmm8, %xmm8
 11276                             	        vpshufd	$0x4e, %xmm10, %xmm10
 11277                             	        vpxor	%xmm11, %xmm10, %xmm10
 11278                             	        vpxor	%xmm3, %xmm10, %xmm6
 11279                             	        cmpl	$11, %esi
 11280                             	        vmovdqa	160(%rdi), %xmm9
 11281                             	        jl	L_AES_GCM_decrypt_update_avx1_aesenc_gfmul_last
 11282                             	        vaesenc	%xmm9, %xmm8, %xmm8
 11283                             	        vaesenc	176(%rdi), %xmm8, %xmm8
 11284                             	        cmpl	$13, %esi
 11285                             	        vmovdqa	192(%rdi), %xmm9
 11286                             	        jl	L_AES_GCM_decrypt_update_avx1_aesenc_gfmul_last
 11287                             	        vaesenc	%xmm9, %xmm8, %xmm8
 11288                             	        vaesenc	208(%rdi), %xmm8, %xmm8
 11289                             	        vmovdqa	224(%rdi), %xmm9
 11290                             	L_AES_GCM_decrypt_update_avx1_aesenc_gfmul_last:
 11291                             	        vaesenclast	%xmm9, %xmm8, %xmm8
 11292                             	        vmovdqa	%xmm13, %xmm0
 11293                             	        vpxor	%xmm0, %xmm8, %xmm8
 11294                             	        vmovdqu	%xmm8, (%r10,%r14,1)
 11295                             	        addl	$16, %r14d
 11296                             	        cmpl	%r13d, %r14d
 11297                             	        jl	L_AES_GCM_decrypt_update_avx1_last_block_start
 11298                             	L_AES_GCM_decrypt_update_avx1_last_block_done:
 11299                             	L_AES_GCM_decrypt_update_avx1_done_dec:
 11300                             	        vmovdqa	%xmm6, (%r9)
 11301                             	        vzeroupper
 11302                             	        addq	$0xa8, %rsp
 11303                             	        popq	%r14
 11304                             	        popq	%r12
 11305                             	        popq	%r13
 11306                             	        repz retq
 11307                             	#ifndef __APPLE__
 11309                             	#endif /* __APPLE__ */
 11310                             	#ifndef __APPLE__
 11311                             	.text
 11312                             	.globl	AES_GCM_decrypt_final_avx1
 11314                             	.align	16
 11315                             	AES_GCM_decrypt_final_avx1:
 11316                             	#else
 11317                             	.section	__TEXT,__text
 11318                             	.globl	_AES_GCM_decrypt_final_avx1
 11319                             	.p2align	4
 11320                             	_AES_GCM_decrypt_final_avx1:
 11321                             	#endif /* __APPLE__ */
 11322                             	        pushq	%r13
 11323                             	        pushq	%rbp
 11324                             	        pushq	%r12
 11325                             	        movq	%rdx, %rax
 11326                             	        movl	%ecx, %r10d
 11327                             	        movl	%r8d, %r11d
 11328                             	        movq	32(%rsp), %r8
 11329                             	        movq	40(%rsp), %rbp
 11330                             	        subq	$16, %rsp
 11331                             	        vmovdqa	(%rdi), %xmm6
 11332                             	        vmovdqa	(%r9), %xmm5
 11333                             	        vmovdqa	(%r8), %xmm15
 11334                             	        vpsrlq	$63, %xmm5, %xmm9
 11335                             	        vpsllq	$0x01, %xmm5, %xmm8
 11336                             	        vpslldq	$8, %xmm9, %xmm9
 11337                             	        vpor	%xmm9, %xmm8, %xmm8
 11338                             	        vpshufd	$0xff, %xmm5, %xmm5
 11339                             	        vpsrad	$31, %xmm5, %xmm5
 11340                             	        vpand	L_avx1_aes_gcm_mod2_128(%rip), %xmm5, %xmm5
 11341                             	        vpxor	%xmm8, %xmm5, %xmm5
 11342                             	        movl	%r10d, %edx
 11343                             	        movl	%r11d, %ecx
 11344                             	        shlq	$3, %rdx
 11345                             	        shlq	$3, %rcx
 11346                             	        vpinsrq	$0x00, %rdx, %xmm0, %xmm0
 11347                             	        vpinsrq	$0x01, %rcx, %xmm0, %xmm0
 11348                             	        vpxor	%xmm0, %xmm6, %xmm6
 11349                             	        # ghash_gfmul_red_avx
 11350                             	        vpshufd	$0x4e, %xmm5, %xmm9
 11351                             	        vpshufd	$0x4e, %xmm6, %xmm10
 11352                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm11
 11353                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm8
 11354                             	        vpxor	%xmm5, %xmm9, %xmm9
 11355                             	        vpxor	%xmm6, %xmm10, %xmm10
 11356                             	        vpclmulqdq	$0x00, %xmm10, %xmm9, %xmm9
 11357                             	        vpxor	%xmm8, %xmm9, %xmm9
 11358                             	        vpxor	%xmm11, %xmm9, %xmm9
 11359                             	        vpslldq	$8, %xmm9, %xmm10
 11360                             	        vpsrldq	$8, %xmm9, %xmm9
 11361                             	        vpxor	%xmm10, %xmm8, %xmm8
 11362                             	        vpxor	%xmm9, %xmm11, %xmm6
 11363                             	        vpslld	$31, %xmm8, %xmm12
 11364                             	        vpslld	$30, %xmm8, %xmm13
 11365                             	        vpslld	$25, %xmm8, %xmm14
 11366                             	        vpxor	%xmm13, %xmm12, %xmm12
 11367                             	        vpxor	%xmm14, %xmm12, %xmm12
 11368                             	        vpsrldq	$4, %xmm12, %xmm13
 11369                             	        vpslldq	$12, %xmm12, %xmm12
 11370                             	        vpxor	%xmm12, %xmm8, %xmm8
 11371                             	        vpsrld	$0x01, %xmm8, %xmm14
 11372                             	        vpsrld	$2, %xmm8, %xmm10
 11373                             	        vpsrld	$7, %xmm8, %xmm9
 11374                             	        vpxor	%xmm10, %xmm14, %xmm14
 11375                             	        vpxor	%xmm9, %xmm14, %xmm14
 11376                             	        vpxor	%xmm13, %xmm14, %xmm14
 11377                             	        vpxor	%xmm8, %xmm14, %xmm14
 11378                             	        vpxor	%xmm14, %xmm6, %xmm6
 11379                             	        vpshufb	L_avx1_aes_gcm_bswap_mask(%rip), %xmm6, %xmm6
 11380                             	        vpxor	%xmm15, %xmm6, %xmm0
 11381                             	        cmpl	$16, %eax
 11382                             	        je	L_AES_GCM_decrypt_final_avx1_cmp_tag_16
 11383                             	        subq	$16, %rsp
 11384                             	        xorq	%rcx, %rcx
 11385                             	        xorq	%r12, %r12
 11386                             	        vmovdqu	%xmm0, (%rsp)
 11387                             	L_AES_GCM_decrypt_final_avx1_cmp_tag_loop:
 11388                             	        movzbl	(%rsp,%rcx,1), %r13d
 11389                             	        xorb	(%rsi,%rcx,1), %r13b
 11390                             	        orb	%r13b, %r12b
 11391                             	        incl	%ecx
 11392                             	        cmpl	%eax, %ecx
 11393                             	        jne	L_AES_GCM_decrypt_final_avx1_cmp_tag_loop
 11394                             	        cmpb	$0x00, %r12b
 11395                             	        sete	%r12b
 11396                             	        addq	$16, %rsp
 11397                             	        xorq	%rcx, %rcx
 11398                             	        jmp	L_AES_GCM_decrypt_final_avx1_cmp_tag_done
 11399                             	L_AES_GCM_decrypt_final_avx1_cmp_tag_16:
 11400                             	        vmovdqu	(%rsi), %xmm1
 11401                             	        vpcmpeqb	%xmm1, %xmm0, %xmm0
 11402                             	        vpmovmskb	%xmm0, %rdx
 11403                             	        # %%edx == 0xFFFF then return 1 else => return 0
 11404                             	        xorl	%r12d, %r12d
 11405                             	        cmpl	$0xffff, %edx
 11406                             	        sete	%r12b
 11407                             	L_AES_GCM_decrypt_final_avx1_cmp_tag_done:
 11408                             	        movl	%r12d, (%rbp)
 11409                             	        vzeroupper
 11410                             	        addq	$16, %rsp
 11411                             	        popq	%r12
 11412                             	        popq	%rbp
 11413                             	        popq	%r13
 11414                             	        repz retq
 11415                             	#ifndef __APPLE__
 11417                             	#endif /* __APPLE__ */
 11418                             	#endif /* WOLFSSL_AESGCM_STREAM */
 11419                             	#endif /* HAVE_INTEL_AVX1 */
 11420                             	#ifdef HAVE_INTEL_AVX2
 11421                             	#ifndef __APPLE__
 11422                             	.data
 11423                             	#else
 11424                             	.section	__DATA,__data
 11425                             	#endif /* __APPLE__ */
 11426                             	#ifndef __APPLE__
 11427                             	.align	16
 11428                             	#else
 11429                             	.p2align	4
 11430                             	#endif /* __APPLE__ */
 11431                             	L_avx2_aes_gcm_one:
 11432 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x1
 11432      01 00 00 00 00 00 00 00 
 11433                             	#ifndef __APPLE__
 11434                             	.data
 11435                             	#else
 11436                             	.section	__DATA,__data
 11437                             	#endif /* __APPLE__ */
 11438                             	#ifndef __APPLE__
 11439                             	.align	16
 11440                             	#else
 11441                             	.p2align	4
 11442                             	#endif /* __APPLE__ */
 11443                             	L_avx2_aes_gcm_two:
 11444 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x2
 11444      02 00 00 00 00 00 00 00 
 11445                             	#ifndef __APPLE__
 11446                             	.data
 11447                             	#else
 11448                             	.section	__DATA,__data
 11449                             	#endif /* __APPLE__ */
 11450                             	#ifndef __APPLE__
 11451                             	.align	16
 11452                             	#else
 11453                             	.p2align	4
 11454                             	#endif /* __APPLE__ */
 11455                             	L_avx2_aes_gcm_three:
 11456 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x3
 11456      03 00 00 00 00 00 00 00 
 11457                             	#ifndef __APPLE__
 11458                             	.data
 11459                             	#else
 11460                             	.section	__DATA,__data
 11461                             	#endif /* __APPLE__ */
 11462                             	#ifndef __APPLE__
 11463                             	.align	16
 11464                             	#else
 11465                             	.p2align	4
 11466                             	#endif /* __APPLE__ */
 11467                             	L_avx2_aes_gcm_four:
 11468 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x4
 11468      04 00 00 00 00 00 00 00 
 11469                             	#ifndef __APPLE__
 11470                             	.data
 11471                             	#else
 11472                             	.section	__DATA,__data
 11473                             	#endif /* __APPLE__ */
 11474                             	#ifndef __APPLE__
 11475                             	.align	16
 11476                             	#else
 11477                             	.p2align	4
 11478                             	#endif /* __APPLE__ */
 11479                             	L_avx2_aes_gcm_five:
 11480 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x5
 11480      05 00 00 00 00 00 00 00 
 11481                             	#ifndef __APPLE__
 11482                             	.data
 11483                             	#else
 11484                             	.section	__DATA,__data
 11485                             	#endif /* __APPLE__ */
 11486                             	#ifndef __APPLE__
 11487                             	.align	16
 11488                             	#else
 11489                             	.p2align	4
 11490                             	#endif /* __APPLE__ */
 11491                             	L_avx2_aes_gcm_six:
 11492 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x6
 11492      06 00 00 00 00 00 00 00 
 11493                             	#ifndef __APPLE__
 11494                             	.data
 11495                             	#else
 11496                             	.section	__DATA,__data
 11497                             	#endif /* __APPLE__ */
 11498                             	#ifndef __APPLE__
 11499                             	.align	16
 11500                             	#else
 11501                             	.p2align	4
 11502                             	#endif /* __APPLE__ */
 11503                             	L_avx2_aes_gcm_seven:
 11504 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x7
 11504      07 00 00 00 00 00 00 00 
 11505                             	#ifndef __APPLE__
 11506                             	.data
 11507                             	#else
 11508                             	.section	__DATA,__data
 11509                             	#endif /* __APPLE__ */
 11510                             	#ifndef __APPLE__
 11511                             	.align	16
 11512                             	#else
 11513                             	.p2align	4
 11514                             	#endif /* __APPLE__ */
 11515                             	L_avx2_aes_gcm_eight:
 11516 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x8
 11516      08 00 00 00 00 00 00 00 
 11517                             	#ifndef __APPLE__
 11518                             	.data
 11519                             	#else
 11520                             	.section	__DATA,__data
 11521                             	#endif /* __APPLE__ */
 11522                             	#ifndef __APPLE__
 11523                             	.align	16
 11524                             	#else
 11525                             	.p2align	4
 11526                             	#endif /* __APPLE__ */
 11527                             	L_avx2_aes_gcm_bswap_one:
 11528 ???? 00 00 00 00 00 00 00 00 	.quad	0x0, 0x100000000000000
 11528      00 00 00 00 00 00 00 01 
 11529                             	#ifndef __APPLE__
 11530                             	.data
 11531                             	#else
 11532                             	.section	__DATA,__data
 11533                             	#endif /* __APPLE__ */
 11534                             	#ifndef __APPLE__
 11535                             	.align	16
 11536                             	#else
 11537                             	.p2align	4
 11538                             	#endif /* __APPLE__ */
 11539                             	L_avx2_aes_gcm_bswap_epi64:
 11540 ???? 07 06 05 04 03 02 01 00 	.quad	0x1020304050607, 0x8090a0b0c0d0e0f
 11540      0F 0E 0D 0C 0B 0A 09 08 
 11541                             	#ifndef __APPLE__
 11542                             	.data
 11543                             	#else
 11544                             	.section	__DATA,__data
 11545                             	#endif /* __APPLE__ */
 11546                             	#ifndef __APPLE__
 11547                             	.align	16
 11548                             	#else
 11549                             	.p2align	4
 11550                             	#endif /* __APPLE__ */
 11551                             	L_avx2_aes_gcm_bswap_mask:
 11552 ???? 0F 0E 0D 0C 0B 0A 09 08 	.quad	0x8090a0b0c0d0e0f, 0x1020304050607
 11552      07 06 05 04 03 02 01 00 
 11553                             	#ifndef __APPLE__
 11554                             	.data
 11555                             	#else
 11556                             	.section	__DATA,__data
 11557                             	#endif /* __APPLE__ */
 11558                             	#ifndef __APPLE__
 11559                             	.align	16
 11560                             	#else
 11561                             	.p2align	4
 11562                             	#endif /* __APPLE__ */
 11563                             	L_avx2_aes_gcm_mod2_128:
 11564 ???? 01 00 00 00 00 00 00 00 	.quad	0x1, 0xc200000000000000
 11564      00 00 00 00 00 00 00 C2 
 11565                             	#ifndef __APPLE__
 11566                             	.text
 11567                             	.globl	AES_GCM_encrypt_avx2
 11569                             	.align	16
 11570                             	AES_GCM_encrypt_avx2:
 11571                             	#else
 11572                             	.section	__TEXT,__text
 11573                             	.globl	_AES_GCM_encrypt_avx2
 11574                             	.p2align	4
 11575                             	_AES_GCM_encrypt_avx2:
 11576                             	#endif /* __APPLE__ */
 11577                             	        pushq	%r13
 11578                             	        pushq	%r12
 11579                             	        pushq	%r15
 11580                             	        pushq	%rbx
 11581                             	        pushq	%r14
 11582                             	        movq	%rdx, %r12
 11583                             	        movq	%rcx, %rax
 11584                             	        movq	%r8, %r15
 11585                             	        movq	%rsi, %r8
 11586                             	        movl	%r9d, %r10d
 11587                             	        movl	48(%rsp), %r11d
 11588                             	        movl	56(%rsp), %ebx
 11589                             	        movl	64(%rsp), %r14d
 11590                             	        movq	72(%rsp), %rsi
 11591                             	        movl	80(%rsp), %r9d
 11592                             	        subq	$0xa0, %rsp
 11593                             	        vpxor	%xmm4, %xmm4, %xmm4
 11594                             	        vpxor	%xmm6, %xmm6, %xmm6
 11595                             	        movl	%ebx, %edx
 11596                             	        cmpl	$12, %edx
 11597                             	        je	L_AES_GCM_encrypt_avx2_iv_12
 11598                             	        # Calculate values when IV is not 12 bytes
 11599                             	        # H = Encrypt X(=0)
 11600                             	        vmovdqu	(%rsi), %xmm5
 11601                             	        vaesenc	16(%rsi), %xmm5, %xmm5
 11602                             	        vaesenc	32(%rsi), %xmm5, %xmm5
 11603                             	        vaesenc	48(%rsi), %xmm5, %xmm5
 11604                             	        vaesenc	64(%rsi), %xmm5, %xmm5
 11605                             	        vaesenc	80(%rsi), %xmm5, %xmm5
 11606                             	        vaesenc	96(%rsi), %xmm5, %xmm5
 11607                             	        vaesenc	112(%rsi), %xmm5, %xmm5
 11608                             	        vaesenc	128(%rsi), %xmm5, %xmm5
 11609                             	        vaesenc	144(%rsi), %xmm5, %xmm5
 11610                             	        cmpl	$11, %r9d
 11611                             	        vmovdqu	160(%rsi), %xmm0
 11612                             	        jl	L_AES_GCM_encrypt_avx2_calc_iv_1_aesenc_avx_last
 11613                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11614                             	        vaesenc	176(%rsi), %xmm5, %xmm5
 11615                             	        cmpl	$13, %r9d
 11616                             	        vmovdqu	192(%rsi), %xmm0
 11617                             	        jl	L_AES_GCM_encrypt_avx2_calc_iv_1_aesenc_avx_last
 11618                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11619                             	        vaesenc	208(%rsi), %xmm5, %xmm5
 11620                             	        vmovdqu	224(%rsi), %xmm0
 11621                             	L_AES_GCM_encrypt_avx2_calc_iv_1_aesenc_avx_last:
 11622                             	        vaesenclast	%xmm0, %xmm5, %xmm5
 11623                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm5, %xmm5
 11624                             	        # Calc counter
 11625                             	        # Initialization vector
 11626                             	        cmpl	$0x00, %edx
 11627                             	        movq	$0x00, %rcx
 11628                             	        je	L_AES_GCM_encrypt_avx2_calc_iv_done
 11629                             	        cmpl	$16, %edx
 11630                             	        jl	L_AES_GCM_encrypt_avx2_calc_iv_lt16
 11631                             	        andl	$0xfffffff0, %edx
 11632                             	L_AES_GCM_encrypt_avx2_calc_iv_16_loop:
 11633                             	        vmovdqu	(%rax,%rcx,1), %xmm0
 11634                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 11635                             	        vpxor	%xmm0, %xmm4, %xmm4
 11636                             	        # ghash_gfmul_avx
 11637                             	        vpclmulqdq	$16, %xmm4, %xmm5, %xmm2
 11638                             	        vpclmulqdq	$0x01, %xmm4, %xmm5, %xmm1
 11639                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 11640                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 11641                             	        vpxor	%xmm1, %xmm2, %xmm2
 11642                             	        vpslldq	$8, %xmm2, %xmm1
 11643                             	        vpsrldq	$8, %xmm2, %xmm2
 11644                             	        vpxor	%xmm1, %xmm0, %xmm7
 11645                             	        vpxor	%xmm2, %xmm3, %xmm4
 11646                             	        # ghash_mid
 11647                             	        vpsrld	$31, %xmm7, %xmm0
 11648                             	        vpsrld	$31, %xmm4, %xmm1
 11649                             	        vpslld	$0x01, %xmm7, %xmm7
 11650                             	        vpslld	$0x01, %xmm4, %xmm4
 11651                             	        vpsrldq	$12, %xmm0, %xmm2
 11652                             	        vpslldq	$4, %xmm0, %xmm0
 11653                             	        vpslldq	$4, %xmm1, %xmm1
 11654                             	        vpor	%xmm2, %xmm4, %xmm4
 11655                             	        vpor	%xmm0, %xmm7, %xmm7
 11656                             	        vpor	%xmm1, %xmm4, %xmm4
 11657                             	        # ghash_red
 11658                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm2
 11659                             	        vpclmulqdq	$16, %xmm2, %xmm7, %xmm0
 11660                             	        vpshufd	$0x4e, %xmm7, %xmm1
 11661                             	        vpxor	%xmm0, %xmm1, %xmm1
 11662                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm0
 11663                             	        vpshufd	$0x4e, %xmm1, %xmm1
 11664                             	        vpxor	%xmm0, %xmm1, %xmm1
 11665                             	        vpxor	%xmm1, %xmm4, %xmm4
 11666                             	        addl	$16, %ecx
 11667                             	        cmpl	%edx, %ecx
 11668                             	        jl	L_AES_GCM_encrypt_avx2_calc_iv_16_loop
 11669                             	        movl	%ebx, %edx
 11670                             	        cmpl	%edx, %ecx
 11671                             	        je	L_AES_GCM_encrypt_avx2_calc_iv_done
 11672                             	L_AES_GCM_encrypt_avx2_calc_iv_lt16:
 11673                             	        vpxor	%xmm0, %xmm0, %xmm0
 11674                             	        xorl	%ebx, %ebx
 11675                             	        vmovdqu	%xmm0, (%rsp)
 11676                             	L_AES_GCM_encrypt_avx2_calc_iv_loop:
 11677                             	        movzbl	(%rax,%rcx,1), %r13d
 11678                             	        movb	%r13b, (%rsp,%rbx,1)
 11679                             	        incl	%ecx
 11680                             	        incl	%ebx
 11681                             	        cmpl	%edx, %ecx
 11682                             	        jl	L_AES_GCM_encrypt_avx2_calc_iv_loop
 11683                             	        vmovdqu	(%rsp), %xmm0
 11684                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 11685                             	        vpxor	%xmm0, %xmm4, %xmm4
 11686                             	        # ghash_gfmul_avx
 11687                             	        vpclmulqdq	$16, %xmm4, %xmm5, %xmm2
 11688                             	        vpclmulqdq	$0x01, %xmm4, %xmm5, %xmm1
 11689                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 11690                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 11691                             	        vpxor	%xmm1, %xmm2, %xmm2
 11692                             	        vpslldq	$8, %xmm2, %xmm1
 11693                             	        vpsrldq	$8, %xmm2, %xmm2
 11694                             	        vpxor	%xmm1, %xmm0, %xmm7
 11695                             	        vpxor	%xmm2, %xmm3, %xmm4
 11696                             	        # ghash_mid
 11697                             	        vpsrld	$31, %xmm7, %xmm0
 11698                             	        vpsrld	$31, %xmm4, %xmm1
 11699                             	        vpslld	$0x01, %xmm7, %xmm7
 11700                             	        vpslld	$0x01, %xmm4, %xmm4
 11701                             	        vpsrldq	$12, %xmm0, %xmm2
 11702                             	        vpslldq	$4, %xmm0, %xmm0
 11703                             	        vpslldq	$4, %xmm1, %xmm1
 11704                             	        vpor	%xmm2, %xmm4, %xmm4
 11705                             	        vpor	%xmm0, %xmm7, %xmm7
 11706                             	        vpor	%xmm1, %xmm4, %xmm4
 11707                             	        # ghash_red
 11708                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm2
 11709                             	        vpclmulqdq	$16, %xmm2, %xmm7, %xmm0
 11710                             	        vpshufd	$0x4e, %xmm7, %xmm1
 11711                             	        vpxor	%xmm0, %xmm1, %xmm1
 11712                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm0
 11713                             	        vpshufd	$0x4e, %xmm1, %xmm1
 11714                             	        vpxor	%xmm0, %xmm1, %xmm1
 11715                             	        vpxor	%xmm1, %xmm4, %xmm4
 11716                             	L_AES_GCM_encrypt_avx2_calc_iv_done:
 11717                             	        # T = Encrypt counter
 11718                             	        vpxor	%xmm0, %xmm0, %xmm0
 11719                             	        shll	$3, %edx
 11720                             	        vpinsrq	$0x00, %rdx, %xmm0, %xmm0
 11721                             	        vpxor	%xmm0, %xmm4, %xmm4
 11722                             	        # ghash_gfmul_avx
 11723                             	        vpclmulqdq	$16, %xmm4, %xmm5, %xmm2
 11724                             	        vpclmulqdq	$0x01, %xmm4, %xmm5, %xmm1
 11725                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 11726                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 11727                             	        vpxor	%xmm1, %xmm2, %xmm2
 11728                             	        vpslldq	$8, %xmm2, %xmm1
 11729                             	        vpsrldq	$8, %xmm2, %xmm2
 11730                             	        vpxor	%xmm1, %xmm0, %xmm7
 11731                             	        vpxor	%xmm2, %xmm3, %xmm4
 11732                             	        # ghash_mid
 11733                             	        vpsrld	$31, %xmm7, %xmm0
 11734                             	        vpsrld	$31, %xmm4, %xmm1
 11735                             	        vpslld	$0x01, %xmm7, %xmm7
 11736                             	        vpslld	$0x01, %xmm4, %xmm4
 11737                             	        vpsrldq	$12, %xmm0, %xmm2
 11738                             	        vpslldq	$4, %xmm0, %xmm0
 11739                             	        vpslldq	$4, %xmm1, %xmm1
 11740                             	        vpor	%xmm2, %xmm4, %xmm4
 11741                             	        vpor	%xmm0, %xmm7, %xmm7
 11742                             	        vpor	%xmm1, %xmm4, %xmm4
 11743                             	        # ghash_red
 11744                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm2
 11745                             	        vpclmulqdq	$16, %xmm2, %xmm7, %xmm0
 11746                             	        vpshufd	$0x4e, %xmm7, %xmm1
 11747                             	        vpxor	%xmm0, %xmm1, %xmm1
 11748                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm0
 11749                             	        vpshufd	$0x4e, %xmm1, %xmm1
 11750                             	        vpxor	%xmm0, %xmm1, %xmm1
 11751                             	        vpxor	%xmm1, %xmm4, %xmm4
 11752                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm4, %xmm4
 11753                             	        #   Encrypt counter
 11754                             	        vmovdqu	(%rsi), %xmm15
 11755                             	        vpxor	%xmm4, %xmm15, %xmm15
 11756                             	        vaesenc	16(%rsi), %xmm15, %xmm15
 11757                             	        vaesenc	32(%rsi), %xmm15, %xmm15
 11758                             	        vaesenc	48(%rsi), %xmm15, %xmm15
 11759                             	        vaesenc	64(%rsi), %xmm15, %xmm15
 11760                             	        vaesenc	80(%rsi), %xmm15, %xmm15
 11761                             	        vaesenc	96(%rsi), %xmm15, %xmm15
 11762                             	        vaesenc	112(%rsi), %xmm15, %xmm15
 11763                             	        vaesenc	128(%rsi), %xmm15, %xmm15
 11764                             	        vaesenc	144(%rsi), %xmm15, %xmm15
 11765                             	        cmpl	$11, %r9d
 11766                             	        vmovdqu	160(%rsi), %xmm0
 11767                             	        jl	L_AES_GCM_encrypt_avx2_calc_iv_2_aesenc_avx_last
 11768                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11769                             	        vaesenc	176(%rsi), %xmm15, %xmm15
 11770                             	        cmpl	$13, %r9d
 11771                             	        vmovdqu	192(%rsi), %xmm0
 11772                             	        jl	L_AES_GCM_encrypt_avx2_calc_iv_2_aesenc_avx_last
 11773                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11774                             	        vaesenc	208(%rsi), %xmm15, %xmm15
 11775                             	        vmovdqu	224(%rsi), %xmm0
 11776                             	L_AES_GCM_encrypt_avx2_calc_iv_2_aesenc_avx_last:
 11777                             	        vaesenclast	%xmm0, %xmm15, %xmm15
 11778                             	        jmp	L_AES_GCM_encrypt_avx2_iv_done
 11779                             	L_AES_GCM_encrypt_avx2_iv_12:
 11780                             	        # # Calculate values when IV is 12 bytes
 11781                             	        # Set counter based on IV
 11782                             	        vmovdqu	L_avx2_aes_gcm_bswap_one(%rip), %xmm4
 11783                             	        vmovdqu	(%rsi), %xmm5
 11784                             	        vpblendd	$7, (%rax), %xmm4, %xmm4
 11785                             	        # H = Encrypt X(=0) and T = Encrypt counter
 11786                             	        vmovdqu	16(%rsi), %xmm7
 11787                             	        vpxor	%xmm5, %xmm4, %xmm15
 11788                             	        vaesenc	%xmm7, %xmm5, %xmm5
 11789                             	        vaesenc	%xmm7, %xmm15, %xmm15
 11790                             	        vmovdqu	32(%rsi), %xmm0
 11791                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11792                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11793                             	        vmovdqu	48(%rsi), %xmm0
 11794                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11795                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11796                             	        vmovdqu	64(%rsi), %xmm0
 11797                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11798                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11799                             	        vmovdqu	80(%rsi), %xmm0
 11800                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11801                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11802                             	        vmovdqu	96(%rsi), %xmm0
 11803                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11804                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11805                             	        vmovdqu	112(%rsi), %xmm0
 11806                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11807                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11808                             	        vmovdqu	128(%rsi), %xmm0
 11809                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11810                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11811                             	        vmovdqu	144(%rsi), %xmm0
 11812                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11813                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11814                             	        cmpl	$11, %r9d
 11815                             	        vmovdqu	160(%rsi), %xmm0
 11816                             	        jl	L_AES_GCM_encrypt_avx2_calc_iv_12_last
 11817                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11818                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11819                             	        vmovdqu	176(%rsi), %xmm0
 11820                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11821                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11822                             	        cmpl	$13, %r9d
 11823                             	        vmovdqu	192(%rsi), %xmm0
 11824                             	        jl	L_AES_GCM_encrypt_avx2_calc_iv_12_last
 11825                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11826                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11827                             	        vmovdqu	208(%rsi), %xmm0
 11828                             	        vaesenc	%xmm0, %xmm5, %xmm5
 11829                             	        vaesenc	%xmm0, %xmm15, %xmm15
 11830                             	        vmovdqu	224(%rsi), %xmm0
 11831                             	L_AES_GCM_encrypt_avx2_calc_iv_12_last:
 11832                             	        vaesenclast	%xmm0, %xmm5, %xmm5
 11833                             	        vaesenclast	%xmm0, %xmm15, %xmm15
 11834                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm5, %xmm5
 11835                             	L_AES_GCM_encrypt_avx2_iv_done:
 11836                             	        # Additional authentication data
 11837                             	        movl	%r11d, %edx
 11838                             	        cmpl	$0x00, %edx
 11839                             	        je	L_AES_GCM_encrypt_avx2_calc_aad_done
 11840                             	        xorl	%ecx, %ecx
 11841                             	        cmpl	$16, %edx
 11842                             	        jl	L_AES_GCM_encrypt_avx2_calc_aad_lt16
 11843                             	        andl	$0xfffffff0, %edx
 11844                             	L_AES_GCM_encrypt_avx2_calc_aad_16_loop:
 11845                             	        vmovdqu	(%r12,%rcx,1), %xmm0
 11846                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 11847                             	        vpxor	%xmm0, %xmm6, %xmm6
 11848                             	        # ghash_gfmul_avx
 11849                             	        vpclmulqdq	$16, %xmm6, %xmm5, %xmm2
 11850                             	        vpclmulqdq	$0x01, %xmm6, %xmm5, %xmm1
 11851                             	        vpclmulqdq	$0x00, %xmm6, %xmm5, %xmm0
 11852                             	        vpclmulqdq	$0x11, %xmm6, %xmm5, %xmm3
 11853                             	        vpxor	%xmm1, %xmm2, %xmm2
 11854                             	        vpslldq	$8, %xmm2, %xmm1
 11855                             	        vpsrldq	$8, %xmm2, %xmm2
 11856                             	        vpxor	%xmm1, %xmm0, %xmm7
 11857                             	        vpxor	%xmm2, %xmm3, %xmm6
 11858                             	        # ghash_mid
 11859                             	        vpsrld	$31, %xmm7, %xmm0
 11860                             	        vpsrld	$31, %xmm6, %xmm1
 11861                             	        vpslld	$0x01, %xmm7, %xmm7
 11862                             	        vpslld	$0x01, %xmm6, %xmm6
 11863                             	        vpsrldq	$12, %xmm0, %xmm2
 11864                             	        vpslldq	$4, %xmm0, %xmm0
 11865                             	        vpslldq	$4, %xmm1, %xmm1
 11866                             	        vpor	%xmm2, %xmm6, %xmm6
 11867                             	        vpor	%xmm0, %xmm7, %xmm7
 11868                             	        vpor	%xmm1, %xmm6, %xmm6
 11869                             	        # ghash_red
 11870                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm2
 11871                             	        vpclmulqdq	$16, %xmm2, %xmm7, %xmm0
 11872                             	        vpshufd	$0x4e, %xmm7, %xmm1
 11873                             	        vpxor	%xmm0, %xmm1, %xmm1
 11874                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm0
 11875                             	        vpshufd	$0x4e, %xmm1, %xmm1
 11876                             	        vpxor	%xmm0, %xmm1, %xmm1
 11877                             	        vpxor	%xmm1, %xmm6, %xmm6
 11878                             	        addl	$16, %ecx
 11879                             	        cmpl	%edx, %ecx
 11880                             	        jl	L_AES_GCM_encrypt_avx2_calc_aad_16_loop
 11881                             	        movl	%r11d, %edx
 11882                             	        cmpl	%edx, %ecx
 11883                             	        je	L_AES_GCM_encrypt_avx2_calc_aad_done
 11884                             	L_AES_GCM_encrypt_avx2_calc_aad_lt16:
 11885                             	        vpxor	%xmm0, %xmm0, %xmm0
 11886                             	        xorl	%ebx, %ebx
 11887                             	        vmovdqu	%xmm0, (%rsp)
 11888                             	L_AES_GCM_encrypt_avx2_calc_aad_loop:
 11889                             	        movzbl	(%r12,%rcx,1), %r13d
 11890                             	        movb	%r13b, (%rsp,%rbx,1)
 11891                             	        incl	%ecx
 11892                             	        incl	%ebx
 11893                             	        cmpl	%edx, %ecx
 11894                             	        jl	L_AES_GCM_encrypt_avx2_calc_aad_loop
 11895                             	        vmovdqu	(%rsp), %xmm0
 11896                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 11897                             	        vpxor	%xmm0, %xmm6, %xmm6
 11898                             	        # ghash_gfmul_avx
 11899                             	        vpclmulqdq	$16, %xmm6, %xmm5, %xmm2
 11900                             	        vpclmulqdq	$0x01, %xmm6, %xmm5, %xmm1
 11901                             	        vpclmulqdq	$0x00, %xmm6, %xmm5, %xmm0
 11902                             	        vpclmulqdq	$0x11, %xmm6, %xmm5, %xmm3
 11903                             	        vpxor	%xmm1, %xmm2, %xmm2
 11904                             	        vpslldq	$8, %xmm2, %xmm1
 11905                             	        vpsrldq	$8, %xmm2, %xmm2
 11906                             	        vpxor	%xmm1, %xmm0, %xmm7
 11907                             	        vpxor	%xmm2, %xmm3, %xmm6
 11908                             	        # ghash_mid
 11909                             	        vpsrld	$31, %xmm7, %xmm0
 11910                             	        vpsrld	$31, %xmm6, %xmm1
 11911                             	        vpslld	$0x01, %xmm7, %xmm7
 11912                             	        vpslld	$0x01, %xmm6, %xmm6
 11913                             	        vpsrldq	$12, %xmm0, %xmm2
 11914                             	        vpslldq	$4, %xmm0, %xmm0
 11915                             	        vpslldq	$4, %xmm1, %xmm1
 11916                             	        vpor	%xmm2, %xmm6, %xmm6
 11917                             	        vpor	%xmm0, %xmm7, %xmm7
 11918                             	        vpor	%xmm1, %xmm6, %xmm6
 11919                             	        # ghash_red
 11920                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm2
 11921                             	        vpclmulqdq	$16, %xmm2, %xmm7, %xmm0
 11922                             	        vpshufd	$0x4e, %xmm7, %xmm1
 11923                             	        vpxor	%xmm0, %xmm1, %xmm1
 11924                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm0
 11925                             	        vpshufd	$0x4e, %xmm1, %xmm1
 11926                             	        vpxor	%xmm0, %xmm1, %xmm1
 11927                             	        vpxor	%xmm1, %xmm6, %xmm6
 11928                             	L_AES_GCM_encrypt_avx2_calc_aad_done:
 11929                             	        # Calculate counter and H
 11930                             	        vpsrlq	$63, %xmm5, %xmm1
 11931                             	        vpsllq	$0x01, %xmm5, %xmm0
 11932                             	        vpslldq	$8, %xmm1, %xmm1
 11933                             	        vpor	%xmm1, %xmm0, %xmm0
 11934                             	        vpshufd	$0xff, %xmm5, %xmm5
 11935                             	        vpsrad	$31, %xmm5, %xmm5
 11936                             	        vpshufb	L_avx2_aes_gcm_bswap_epi64(%rip), %xmm4, %xmm4
 11937                             	        vpand	L_avx2_aes_gcm_mod2_128(%rip), %xmm5, %xmm5
 11938                             	        vpaddd	L_avx2_aes_gcm_one(%rip), %xmm4, %xmm4
 11939                             	        vpxor	%xmm0, %xmm5, %xmm5
 11940                             	        xorl	%ebx, %ebx
 11941                             	        cmpl	$0x80, %r10d
 11942                             	        movl	%r10d, %r13d
 11943                             	        jl	L_AES_GCM_encrypt_avx2_done_128
 11944                             	        andl	$0xffffff80, %r13d
 11945                             	        vmovdqu	%xmm4, 128(%rsp)
 11946                             	        vmovdqu	%xmm15, 144(%rsp)
 11947                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm3
 11948                             	        # H ^ 1 and H ^ 2
 11949                             	        vpclmulqdq	$0x00, %xmm5, %xmm5, %xmm9
 11950                             	        vpclmulqdq	$0x11, %xmm5, %xmm5, %xmm10
 11951                             	        vpclmulqdq	$16, %xmm3, %xmm9, %xmm8
 11952                             	        vpshufd	$0x4e, %xmm9, %xmm9
 11953                             	        vpxor	%xmm8, %xmm9, %xmm9
 11954                             	        vpclmulqdq	$16, %xmm3, %xmm9, %xmm8
 11955                             	        vpshufd	$0x4e, %xmm9, %xmm9
 11956                             	        vpxor	%xmm8, %xmm9, %xmm9
 11957                             	        vpxor	%xmm9, %xmm10, %xmm0
 11958                             	        vmovdqu	%xmm5, (%rsp)
 11959                             	        vmovdqu	%xmm0, 16(%rsp)
 11960                             	        # H ^ 3 and H ^ 4
 11961                             	        vpclmulqdq	$16, %xmm5, %xmm0, %xmm11
 11962                             	        vpclmulqdq	$0x01, %xmm5, %xmm0, %xmm10
 11963                             	        vpclmulqdq	$0x00, %xmm5, %xmm0, %xmm9
 11964                             	        vpclmulqdq	$0x11, %xmm5, %xmm0, %xmm12
 11965                             	        vpclmulqdq	$0x00, %xmm0, %xmm0, %xmm13
 11966                             	        vpclmulqdq	$0x11, %xmm0, %xmm0, %xmm14
 11967                             	        vpxor	%xmm10, %xmm11, %xmm11
 11968                             	        vpslldq	$8, %xmm11, %xmm10
 11969                             	        vpsrldq	$8, %xmm11, %xmm11
 11970                             	        vpxor	%xmm9, %xmm10, %xmm10
 11971                             	        vpclmulqdq	$16, %xmm3, %xmm13, %xmm8
 11972                             	        vpclmulqdq	$16, %xmm3, %xmm10, %xmm9
 11973                             	        vpshufd	$0x4e, %xmm10, %xmm10
 11974                             	        vpshufd	$0x4e, %xmm13, %xmm13
 11975                             	        vpxor	%xmm9, %xmm10, %xmm10
 11976                             	        vpxor	%xmm8, %xmm13, %xmm13
 11977                             	        vpclmulqdq	$16, %xmm3, %xmm10, %xmm9
 11978                             	        vpclmulqdq	$16, %xmm3, %xmm13, %xmm8
 11979                             	        vpshufd	$0x4e, %xmm10, %xmm10
 11980                             	        vpshufd	$0x4e, %xmm13, %xmm13
 11981                             	        vpxor	%xmm11, %xmm12, %xmm12
 11982                             	        vpxor	%xmm8, %xmm13, %xmm13
 11983                             	        vpxor	%xmm12, %xmm10, %xmm10
 11984                             	        vpxor	%xmm14, %xmm13, %xmm2
 11985                             	        vpxor	%xmm9, %xmm10, %xmm1
 11986                             	        vmovdqu	%xmm1, 32(%rsp)
 11987                             	        vmovdqu	%xmm2, 48(%rsp)
 11988                             	        # H ^ 5 and H ^ 6
 11989                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm11
 11990                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm10
 11991                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm9
 11992                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm12
 11993                             	        vpclmulqdq	$0x00, %xmm1, %xmm1, %xmm13
 11994                             	        vpclmulqdq	$0x11, %xmm1, %xmm1, %xmm14
 11995                             	        vpxor	%xmm10, %xmm11, %xmm11
 11996                             	        vpslldq	$8, %xmm11, %xmm10
 11997                             	        vpsrldq	$8, %xmm11, %xmm11
 11998                             	        vpxor	%xmm9, %xmm10, %xmm10
 11999                             	        vpclmulqdq	$16, %xmm3, %xmm13, %xmm8
 12000                             	        vpclmulqdq	$16, %xmm3, %xmm10, %xmm9
 12001                             	        vpshufd	$0x4e, %xmm10, %xmm10
 12002                             	        vpshufd	$0x4e, %xmm13, %xmm13
 12003                             	        vpxor	%xmm9, %xmm10, %xmm10
 12004                             	        vpxor	%xmm8, %xmm13, %xmm13
 12005                             	        vpclmulqdq	$16, %xmm3, %xmm10, %xmm9
 12006                             	        vpclmulqdq	$16, %xmm3, %xmm13, %xmm8
 12007                             	        vpshufd	$0x4e, %xmm10, %xmm10
 12008                             	        vpshufd	$0x4e, %xmm13, %xmm13
 12009                             	        vpxor	%xmm11, %xmm12, %xmm12
 12010                             	        vpxor	%xmm8, %xmm13, %xmm13
 12011                             	        vpxor	%xmm12, %xmm10, %xmm10
 12012                             	        vpxor	%xmm14, %xmm13, %xmm0
 12013                             	        vpxor	%xmm9, %xmm10, %xmm7
 12014                             	        vmovdqu	%xmm7, 64(%rsp)
 12015                             	        vmovdqu	%xmm0, 80(%rsp)
 12016                             	        # H ^ 7 and H ^ 8
 12017                             	        vpclmulqdq	$16, %xmm1, %xmm2, %xmm11
 12018                             	        vpclmulqdq	$0x01, %xmm1, %xmm2, %xmm10
 12019                             	        vpclmulqdq	$0x00, %xmm1, %xmm2, %xmm9
 12020                             	        vpclmulqdq	$0x11, %xmm1, %xmm2, %xmm12
 12021                             	        vpclmulqdq	$0x00, %xmm2, %xmm2, %xmm13
 12022                             	        vpclmulqdq	$0x11, %xmm2, %xmm2, %xmm14
 12023                             	        vpxor	%xmm10, %xmm11, %xmm11
 12024                             	        vpslldq	$8, %xmm11, %xmm10
 12025                             	        vpsrldq	$8, %xmm11, %xmm11
 12026                             	        vpxor	%xmm9, %xmm10, %xmm10
 12027                             	        vpclmulqdq	$16, %xmm3, %xmm13, %xmm8
 12028                             	        vpclmulqdq	$16, %xmm3, %xmm10, %xmm9
 12029                             	        vpshufd	$0x4e, %xmm10, %xmm10
 12030                             	        vpshufd	$0x4e, %xmm13, %xmm13
 12031                             	        vpxor	%xmm9, %xmm10, %xmm10
 12032                             	        vpxor	%xmm8, %xmm13, %xmm13
 12033                             	        vpclmulqdq	$16, %xmm3, %xmm10, %xmm9
 12034                             	        vpclmulqdq	$16, %xmm3, %xmm13, %xmm8
 12035                             	        vpshufd	$0x4e, %xmm10, %xmm10
 12036                             	        vpshufd	$0x4e, %xmm13, %xmm13
 12037                             	        vpxor	%xmm11, %xmm12, %xmm12
 12038                             	        vpxor	%xmm8, %xmm13, %xmm13
 12039                             	        vpxor	%xmm12, %xmm10, %xmm10
 12040                             	        vpxor	%xmm14, %xmm13, %xmm0
 12041                             	        vpxor	%xmm9, %xmm10, %xmm7
 12042                             	        vmovdqu	%xmm7, 96(%rsp)
 12043                             	        vmovdqu	%xmm0, 112(%rsp)
 12044                             	        # First 128 bytes of input
 12045                             	        # aesenc_128
 12046                             	        # aesenc_ctr
 12047                             	        vmovdqu	128(%rsp), %xmm0
 12048                             	        vmovdqu	L_avx2_aes_gcm_bswap_epi64(%rip), %xmm1
 12049                             	        vpaddd	L_avx2_aes_gcm_one(%rip), %xmm0, %xmm9
 12050                             	        vpshufb	%xmm1, %xmm0, %xmm8
 12051                             	        vpaddd	L_avx2_aes_gcm_two(%rip), %xmm0, %xmm10
 12052                             	        vpshufb	%xmm1, %xmm9, %xmm9
 12053                             	        vpaddd	L_avx2_aes_gcm_three(%rip), %xmm0, %xmm11
 12054                             	        vpshufb	%xmm1, %xmm10, %xmm10
 12055                             	        vpaddd	L_avx2_aes_gcm_four(%rip), %xmm0, %xmm12
 12056                             	        vpshufb	%xmm1, %xmm11, %xmm11
 12057                             	        vpaddd	L_avx2_aes_gcm_five(%rip), %xmm0, %xmm13
 12058                             	        vpshufb	%xmm1, %xmm12, %xmm12
 12059                             	        vpaddd	L_avx2_aes_gcm_six(%rip), %xmm0, %xmm14
 12060                             	        vpshufb	%xmm1, %xmm13, %xmm13
 12061                             	        vpaddd	L_avx2_aes_gcm_seven(%rip), %xmm0, %xmm15
 12062                             	        vpshufb	%xmm1, %xmm14, %xmm14
 12063                             	        vpaddd	L_avx2_aes_gcm_eight(%rip), %xmm0, %xmm0
 12064                             	        vpshufb	%xmm1, %xmm15, %xmm15
 12065                             	        # aesenc_xor
 12066                             	        vmovdqu	(%rsi), %xmm7
 12067                             	        vmovdqu	%xmm0, 128(%rsp)
 12068                             	        vpxor	%xmm7, %xmm8, %xmm8
 12069                             	        vpxor	%xmm7, %xmm9, %xmm9
 12070                             	        vpxor	%xmm7, %xmm10, %xmm10
 12071                             	        vpxor	%xmm7, %xmm11, %xmm11
 12072                             	        vpxor	%xmm7, %xmm12, %xmm12
 12073                             	        vpxor	%xmm7, %xmm13, %xmm13
 12074                             	        vpxor	%xmm7, %xmm14, %xmm14
 12075                             	        vpxor	%xmm7, %xmm15, %xmm15
 12076                             	        vmovdqu	16(%rsi), %xmm7
 12077                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12078                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12079                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12080                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12081                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12082                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12083                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12084                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12085                             	        vmovdqu	32(%rsi), %xmm7
 12086                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12087                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12088                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12089                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12090                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12091                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12092                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12093                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12094                             	        vmovdqu	48(%rsi), %xmm7
 12095                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12096                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12097                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12098                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12099                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12100                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12101                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12102                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12103                             	        vmovdqu	64(%rsi), %xmm7
 12104                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12105                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12106                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12107                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12108                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12109                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12110                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12111                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12112                             	        vmovdqu	80(%rsi), %xmm7
 12113                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12114                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12115                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12116                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12117                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12118                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12119                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12120                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12121                             	        vmovdqu	96(%rsi), %xmm7
 12122                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12123                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12124                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12125                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12126                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12127                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12128                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12129                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12130                             	        vmovdqu	112(%rsi), %xmm7
 12131                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12132                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12133                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12134                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12135                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12136                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12137                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12138                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12139                             	        vmovdqu	128(%rsi), %xmm7
 12140                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12141                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12142                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12143                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12144                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12145                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12146                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12147                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12148                             	        vmovdqu	144(%rsi), %xmm7
 12149                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12150                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12151                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12152                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12153                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12154                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12155                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12156                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12157                             	        cmpl	$11, %r9d
 12158                             	        vmovdqu	160(%rsi), %xmm7
 12159                             	        jl	L_AES_GCM_encrypt_avx2_aesenc_128_enc_done
 12160                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12161                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12162                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12163                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12164                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12165                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12166                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12167                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12168                             	        vmovdqu	176(%rsi), %xmm7
 12169                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12170                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12171                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12172                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12173                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12174                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12175                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12176                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12177                             	        cmpl	$13, %r9d
 12178                             	        vmovdqu	192(%rsi), %xmm7
 12179                             	        jl	L_AES_GCM_encrypt_avx2_aesenc_128_enc_done
 12180                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12181                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12182                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12183                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12184                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12185                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12186                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12187                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12188                             	        vmovdqu	208(%rsi), %xmm7
 12189                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12190                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12191                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12192                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12193                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12194                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12195                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12196                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12197                             	        vmovdqu	224(%rsi), %xmm7
 12198                             	L_AES_GCM_encrypt_avx2_aesenc_128_enc_done:
 12199                             	        # aesenc_last
 12200                             	        vaesenclast	%xmm7, %xmm8, %xmm8
 12201                             	        vaesenclast	%xmm7, %xmm9, %xmm9
 12202                             	        vaesenclast	%xmm7, %xmm10, %xmm10
 12203                             	        vaesenclast	%xmm7, %xmm11, %xmm11
 12204                             	        vmovdqu	(%rdi), %xmm0
 12205                             	        vmovdqu	16(%rdi), %xmm1
 12206                             	        vmovdqu	32(%rdi), %xmm2
 12207                             	        vmovdqu	48(%rdi), %xmm3
 12208                             	        vpxor	%xmm0, %xmm8, %xmm8
 12209                             	        vpxor	%xmm1, %xmm9, %xmm9
 12210                             	        vpxor	%xmm2, %xmm10, %xmm10
 12211                             	        vpxor	%xmm3, %xmm11, %xmm11
 12212                             	        vmovdqu	%xmm8, (%r8)
 12213                             	        vmovdqu	%xmm9, 16(%r8)
 12214                             	        vmovdqu	%xmm10, 32(%r8)
 12215                             	        vmovdqu	%xmm11, 48(%r8)
 12216                             	        vaesenclast	%xmm7, %xmm12, %xmm12
 12217                             	        vaesenclast	%xmm7, %xmm13, %xmm13
 12218                             	        vaesenclast	%xmm7, %xmm14, %xmm14
 12219                             	        vaesenclast	%xmm7, %xmm15, %xmm15
 12220                             	        vmovdqu	64(%rdi), %xmm0
 12221                             	        vmovdqu	80(%rdi), %xmm1
 12222                             	        vmovdqu	96(%rdi), %xmm2
 12223                             	        vmovdqu	112(%rdi), %xmm3
 12224                             	        vpxor	%xmm0, %xmm12, %xmm12
 12225                             	        vpxor	%xmm1, %xmm13, %xmm13
 12226                             	        vpxor	%xmm2, %xmm14, %xmm14
 12227                             	        vpxor	%xmm3, %xmm15, %xmm15
 12228                             	        vmovdqu	%xmm12, 64(%r8)
 12229                             	        vmovdqu	%xmm13, 80(%r8)
 12230                             	        vmovdqu	%xmm14, 96(%r8)
 12231                             	        vmovdqu	%xmm15, 112(%r8)
 12232                             	        cmpl	$0x80, %r13d
 12233                             	        movl	$0x80, %ebx
 12234                             	        jle	L_AES_GCM_encrypt_avx2_end_128
 12235                             	        # More 128 bytes of input
 12236                             	L_AES_GCM_encrypt_avx2_ghash_128:
 12237                             	        # aesenc_128_ghash
 12238                             	        leaq	(%rdi,%rbx,1), %rcx
 12239                             	        leaq	(%r8,%rbx,1), %rdx
 12240                             	        # aesenc_ctr
 12241                             	        vmovdqu	128(%rsp), %xmm0
 12242                             	        vmovdqu	L_avx2_aes_gcm_bswap_epi64(%rip), %xmm1
 12243                             	        vpaddd	L_avx2_aes_gcm_one(%rip), %xmm0, %xmm9
 12244                             	        vpshufb	%xmm1, %xmm0, %xmm8
 12245                             	        vpaddd	L_avx2_aes_gcm_two(%rip), %xmm0, %xmm10
 12246                             	        vpshufb	%xmm1, %xmm9, %xmm9
 12247                             	        vpaddd	L_avx2_aes_gcm_three(%rip), %xmm0, %xmm11
 12248                             	        vpshufb	%xmm1, %xmm10, %xmm10
 12249                             	        vpaddd	L_avx2_aes_gcm_four(%rip), %xmm0, %xmm12
 12250                             	        vpshufb	%xmm1, %xmm11, %xmm11
 12251                             	        vpaddd	L_avx2_aes_gcm_five(%rip), %xmm0, %xmm13
 12252                             	        vpshufb	%xmm1, %xmm12, %xmm12
 12253                             	        vpaddd	L_avx2_aes_gcm_six(%rip), %xmm0, %xmm14
 12254                             	        vpshufb	%xmm1, %xmm13, %xmm13
 12255                             	        vpaddd	L_avx2_aes_gcm_seven(%rip), %xmm0, %xmm15
 12256                             	        vpshufb	%xmm1, %xmm14, %xmm14
 12257                             	        vpaddd	L_avx2_aes_gcm_eight(%rip), %xmm0, %xmm0
 12258                             	        vpshufb	%xmm1, %xmm15, %xmm15
 12259                             	        # aesenc_xor
 12260                             	        vmovdqu	(%rsi), %xmm7
 12261                             	        vmovdqu	%xmm0, 128(%rsp)
 12262                             	        vpxor	%xmm7, %xmm8, %xmm8
 12263                             	        vpxor	%xmm7, %xmm9, %xmm9
 12264                             	        vpxor	%xmm7, %xmm10, %xmm10
 12265                             	        vpxor	%xmm7, %xmm11, %xmm11
 12266                             	        vpxor	%xmm7, %xmm12, %xmm12
 12267                             	        vpxor	%xmm7, %xmm13, %xmm13
 12268                             	        vpxor	%xmm7, %xmm14, %xmm14
 12269                             	        vpxor	%xmm7, %xmm15, %xmm15
 12270                             	        # aesenc_pclmul_1
 12271                             	        vmovdqu	-128(%rdx), %xmm1
 12272                             	        vmovdqu	16(%rsi), %xmm0
 12273                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 12274                             	        vmovdqu	112(%rsp), %xmm2
 12275                             	        vpxor	%xmm6, %xmm1, %xmm1
 12276                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm5
 12277                             	        vpclmulqdq	$0x01, %xmm2, %xmm1, %xmm3
 12278                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm6
 12279                             	        vpclmulqdq	$0x11, %xmm2, %xmm1, %xmm7
 12280                             	        vaesenc	%xmm0, %xmm8, %xmm8
 12281                             	        vaesenc	%xmm0, %xmm9, %xmm9
 12282                             	        vaesenc	%xmm0, %xmm10, %xmm10
 12283                             	        vaesenc	%xmm0, %xmm11, %xmm11
 12284                             	        vaesenc	%xmm0, %xmm12, %xmm12
 12285                             	        vaesenc	%xmm0, %xmm13, %xmm13
 12286                             	        vaesenc	%xmm0, %xmm14, %xmm14
 12287                             	        vaesenc	%xmm0, %xmm15, %xmm15
 12288                             	        # aesenc_pclmul_2
 12289                             	        vmovdqu	-112(%rdx), %xmm1
 12290                             	        vmovdqu	96(%rsp), %xmm0
 12291                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 12292                             	        vpxor	%xmm3, %xmm5, %xmm5
 12293                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 12294                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 12295                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 12296                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 12297                             	        vmovdqu	32(%rsi), %xmm0
 12298                             	        vpxor	%xmm1, %xmm7, %xmm7
 12299                             	        vaesenc	%xmm0, %xmm8, %xmm8
 12300                             	        vaesenc	%xmm0, %xmm9, %xmm9
 12301                             	        vaesenc	%xmm0, %xmm10, %xmm10
 12302                             	        vaesenc	%xmm0, %xmm11, %xmm11
 12303                             	        vaesenc	%xmm0, %xmm12, %xmm12
 12304                             	        vaesenc	%xmm0, %xmm13, %xmm13
 12305                             	        vaesenc	%xmm0, %xmm14, %xmm14
 12306                             	        vaesenc	%xmm0, %xmm15, %xmm15
 12307                             	        # aesenc_pclmul_n
 12308                             	        vmovdqu	-96(%rdx), %xmm1
 12309                             	        vmovdqu	80(%rsp), %xmm0
 12310                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 12311                             	        vpxor	%xmm2, %xmm5, %xmm5
 12312                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 12313                             	        vpxor	%xmm3, %xmm5, %xmm5
 12314                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 12315                             	        vpxor	%xmm4, %xmm6, %xmm6
 12316                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 12317                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 12318                             	        vmovdqu	48(%rsi), %xmm0
 12319                             	        vpxor	%xmm1, %xmm7, %xmm7
 12320                             	        vaesenc	%xmm0, %xmm8, %xmm8
 12321                             	        vaesenc	%xmm0, %xmm9, %xmm9
 12322                             	        vaesenc	%xmm0, %xmm10, %xmm10
 12323                             	        vaesenc	%xmm0, %xmm11, %xmm11
 12324                             	        vaesenc	%xmm0, %xmm12, %xmm12
 12325                             	        vaesenc	%xmm0, %xmm13, %xmm13
 12326                             	        vaesenc	%xmm0, %xmm14, %xmm14
 12327                             	        vaesenc	%xmm0, %xmm15, %xmm15
 12328                             	        # aesenc_pclmul_n
 12329                             	        vmovdqu	-80(%rdx), %xmm1
 12330                             	        vmovdqu	64(%rsp), %xmm0
 12331                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 12332                             	        vpxor	%xmm2, %xmm5, %xmm5
 12333                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 12334                             	        vpxor	%xmm3, %xmm5, %xmm5
 12335                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 12336                             	        vpxor	%xmm4, %xmm6, %xmm6
 12337                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 12338                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 12339                             	        vmovdqu	64(%rsi), %xmm0
 12340                             	        vpxor	%xmm1, %xmm7, %xmm7
 12341                             	        vaesenc	%xmm0, %xmm8, %xmm8
 12342                             	        vaesenc	%xmm0, %xmm9, %xmm9
 12343                             	        vaesenc	%xmm0, %xmm10, %xmm10
 12344                             	        vaesenc	%xmm0, %xmm11, %xmm11
 12345                             	        vaesenc	%xmm0, %xmm12, %xmm12
 12346                             	        vaesenc	%xmm0, %xmm13, %xmm13
 12347                             	        vaesenc	%xmm0, %xmm14, %xmm14
 12348                             	        vaesenc	%xmm0, %xmm15, %xmm15
 12349                             	        # aesenc_pclmul_n
 12350                             	        vmovdqu	-64(%rdx), %xmm1
 12351                             	        vmovdqu	48(%rsp), %xmm0
 12352                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 12353                             	        vpxor	%xmm2, %xmm5, %xmm5
 12354                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 12355                             	        vpxor	%xmm3, %xmm5, %xmm5
 12356                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 12357                             	        vpxor	%xmm4, %xmm6, %xmm6
 12358                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 12359                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 12360                             	        vmovdqu	80(%rsi), %xmm0
 12361                             	        vpxor	%xmm1, %xmm7, %xmm7
 12362                             	        vaesenc	%xmm0, %xmm8, %xmm8
 12363                             	        vaesenc	%xmm0, %xmm9, %xmm9
 12364                             	        vaesenc	%xmm0, %xmm10, %xmm10
 12365                             	        vaesenc	%xmm0, %xmm11, %xmm11
 12366                             	        vaesenc	%xmm0, %xmm12, %xmm12
 12367                             	        vaesenc	%xmm0, %xmm13, %xmm13
 12368                             	        vaesenc	%xmm0, %xmm14, %xmm14
 12369                             	        vaesenc	%xmm0, %xmm15, %xmm15
 12370                             	        # aesenc_pclmul_n
 12371                             	        vmovdqu	-48(%rdx), %xmm1
 12372                             	        vmovdqu	32(%rsp), %xmm0
 12373                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 12374                             	        vpxor	%xmm2, %xmm5, %xmm5
 12375                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 12376                             	        vpxor	%xmm3, %xmm5, %xmm5
 12377                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 12378                             	        vpxor	%xmm4, %xmm6, %xmm6
 12379                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 12380                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 12381                             	        vmovdqu	96(%rsi), %xmm0
 12382                             	        vpxor	%xmm1, %xmm7, %xmm7
 12383                             	        vaesenc	%xmm0, %xmm8, %xmm8
 12384                             	        vaesenc	%xmm0, %xmm9, %xmm9
 12385                             	        vaesenc	%xmm0, %xmm10, %xmm10
 12386                             	        vaesenc	%xmm0, %xmm11, %xmm11
 12387                             	        vaesenc	%xmm0, %xmm12, %xmm12
 12388                             	        vaesenc	%xmm0, %xmm13, %xmm13
 12389                             	        vaesenc	%xmm0, %xmm14, %xmm14
 12390                             	        vaesenc	%xmm0, %xmm15, %xmm15
 12391                             	        # aesenc_pclmul_n
 12392                             	        vmovdqu	-32(%rdx), %xmm1
 12393                             	        vmovdqu	16(%rsp), %xmm0
 12394                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 12395                             	        vpxor	%xmm2, %xmm5, %xmm5
 12396                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 12397                             	        vpxor	%xmm3, %xmm5, %xmm5
 12398                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 12399                             	        vpxor	%xmm4, %xmm6, %xmm6
 12400                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 12401                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 12402                             	        vmovdqu	112(%rsi), %xmm0
 12403                             	        vpxor	%xmm1, %xmm7, %xmm7
 12404                             	        vaesenc	%xmm0, %xmm8, %xmm8
 12405                             	        vaesenc	%xmm0, %xmm9, %xmm9
 12406                             	        vaesenc	%xmm0, %xmm10, %xmm10
 12407                             	        vaesenc	%xmm0, %xmm11, %xmm11
 12408                             	        vaesenc	%xmm0, %xmm12, %xmm12
 12409                             	        vaesenc	%xmm0, %xmm13, %xmm13
 12410                             	        vaesenc	%xmm0, %xmm14, %xmm14
 12411                             	        vaesenc	%xmm0, %xmm15, %xmm15
 12412                             	        # aesenc_pclmul_n
 12413                             	        vmovdqu	-16(%rdx), %xmm1
 12414                             	        vmovdqu	(%rsp), %xmm0
 12415                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 12416                             	        vpxor	%xmm2, %xmm5, %xmm5
 12417                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 12418                             	        vpxor	%xmm3, %xmm5, %xmm5
 12419                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 12420                             	        vpxor	%xmm4, %xmm6, %xmm6
 12421                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 12422                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 12423                             	        vmovdqu	128(%rsi), %xmm0
 12424                             	        vpxor	%xmm1, %xmm7, %xmm7
 12425                             	        vaesenc	%xmm0, %xmm8, %xmm8
 12426                             	        vaesenc	%xmm0, %xmm9, %xmm9
 12427                             	        vaesenc	%xmm0, %xmm10, %xmm10
 12428                             	        vaesenc	%xmm0, %xmm11, %xmm11
 12429                             	        vaesenc	%xmm0, %xmm12, %xmm12
 12430                             	        vaesenc	%xmm0, %xmm13, %xmm13
 12431                             	        vaesenc	%xmm0, %xmm14, %xmm14
 12432                             	        vaesenc	%xmm0, %xmm15, %xmm15
 12433                             	        # aesenc_pclmul_l
 12434                             	        vpxor	%xmm2, %xmm5, %xmm5
 12435                             	        vpxor	%xmm4, %xmm6, %xmm6
 12436                             	        vpxor	%xmm3, %xmm5, %xmm5
 12437                             	        vpslldq	$8, %xmm5, %xmm1
 12438                             	        vpsrldq	$8, %xmm5, %xmm5
 12439                             	        vmovdqu	144(%rsi), %xmm4
 12440                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm0
 12441                             	        vaesenc	%xmm4, %xmm8, %xmm8
 12442                             	        vpxor	%xmm1, %xmm6, %xmm6
 12443                             	        vpxor	%xmm5, %xmm7, %xmm7
 12444                             	        vpclmulqdq	$16, %xmm0, %xmm6, %xmm3
 12445                             	        vaesenc	%xmm4, %xmm9, %xmm9
 12446                             	        vaesenc	%xmm4, %xmm10, %xmm10
 12447                             	        vaesenc	%xmm4, %xmm11, %xmm11
 12448                             	        vpshufd	$0x4e, %xmm6, %xmm6
 12449                             	        vpxor	%xmm3, %xmm6, %xmm6
 12450                             	        vpclmulqdq	$16, %xmm0, %xmm6, %xmm3
 12451                             	        vaesenc	%xmm4, %xmm12, %xmm12
 12452                             	        vaesenc	%xmm4, %xmm13, %xmm13
 12453                             	        vaesenc	%xmm4, %xmm14, %xmm14
 12454                             	        vpshufd	$0x4e, %xmm6, %xmm6
 12455                             	        vpxor	%xmm3, %xmm6, %xmm6
 12456                             	        vpxor	%xmm7, %xmm6, %xmm6
 12457                             	        vaesenc	%xmm4, %xmm15, %xmm15
 12458                             	        cmpl	$11, %r9d
 12459                             	        vmovdqu	160(%rsi), %xmm7
 12460                             	        jl	L_AES_GCM_encrypt_avx2_aesenc_128_ghash_avx_done
 12461                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12462                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12463                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12464                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12465                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12466                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12467                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12468                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12469                             	        vmovdqu	176(%rsi), %xmm7
 12470                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12471                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12472                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12473                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12474                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12475                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12476                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12477                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12478                             	        cmpl	$13, %r9d
 12479                             	        vmovdqu	192(%rsi), %xmm7
 12480                             	        jl	L_AES_GCM_encrypt_avx2_aesenc_128_ghash_avx_done
 12481                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12482                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12483                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12484                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12485                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12486                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12487                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12488                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12489                             	        vmovdqu	208(%rsi), %xmm7
 12490                             	        vaesenc	%xmm7, %xmm8, %xmm8
 12491                             	        vaesenc	%xmm7, %xmm9, %xmm9
 12492                             	        vaesenc	%xmm7, %xmm10, %xmm10
 12493                             	        vaesenc	%xmm7, %xmm11, %xmm11
 12494                             	        vaesenc	%xmm7, %xmm12, %xmm12
 12495                             	        vaesenc	%xmm7, %xmm13, %xmm13
 12496                             	        vaesenc	%xmm7, %xmm14, %xmm14
 12497                             	        vaesenc	%xmm7, %xmm15, %xmm15
 12498                             	        vmovdqu	224(%rsi), %xmm7
 12499                             	L_AES_GCM_encrypt_avx2_aesenc_128_ghash_avx_done:
 12500                             	        # aesenc_last
 12501                             	        vaesenclast	%xmm7, %xmm8, %xmm8
 12502                             	        vaesenclast	%xmm7, %xmm9, %xmm9
 12503                             	        vaesenclast	%xmm7, %xmm10, %xmm10
 12504                             	        vaesenclast	%xmm7, %xmm11, %xmm11
 12505                             	        vmovdqu	(%rcx), %xmm0
 12506                             	        vmovdqu	16(%rcx), %xmm1
 12507                             	        vmovdqu	32(%rcx), %xmm2
 12508                             	        vmovdqu	48(%rcx), %xmm3
 12509                             	        vpxor	%xmm0, %xmm8, %xmm8
 12510                             	        vpxor	%xmm1, %xmm9, %xmm9
 12511                             	        vpxor	%xmm2, %xmm10, %xmm10
 12512                             	        vpxor	%xmm3, %xmm11, %xmm11
 12513                             	        vmovdqu	%xmm8, (%rdx)
 12514                             	        vmovdqu	%xmm9, 16(%rdx)
 12515                             	        vmovdqu	%xmm10, 32(%rdx)
 12516                             	        vmovdqu	%xmm11, 48(%rdx)
 12517                             	        vaesenclast	%xmm7, %xmm12, %xmm12
 12518                             	        vaesenclast	%xmm7, %xmm13, %xmm13
 12519                             	        vaesenclast	%xmm7, %xmm14, %xmm14
 12520                             	        vaesenclast	%xmm7, %xmm15, %xmm15
 12521                             	        vmovdqu	64(%rcx), %xmm0
 12522                             	        vmovdqu	80(%rcx), %xmm1
 12523                             	        vmovdqu	96(%rcx), %xmm2
 12524                             	        vmovdqu	112(%rcx), %xmm3
 12525                             	        vpxor	%xmm0, %xmm12, %xmm12
 12526                             	        vpxor	%xmm1, %xmm13, %xmm13
 12527                             	        vpxor	%xmm2, %xmm14, %xmm14
 12528                             	        vpxor	%xmm3, %xmm15, %xmm15
 12529                             	        vmovdqu	%xmm12, 64(%rdx)
 12530                             	        vmovdqu	%xmm13, 80(%rdx)
 12531                             	        vmovdqu	%xmm14, 96(%rdx)
 12532                             	        vmovdqu	%xmm15, 112(%rdx)
 12533                             	        # aesenc_128_ghash - end
 12534                             	        addl	$0x80, %ebx
 12535                             	        cmpl	%r13d, %ebx
 12536                             	        jl	L_AES_GCM_encrypt_avx2_ghash_128
 12537                             	L_AES_GCM_encrypt_avx2_end_128:
 12538                             	        vmovdqu	L_avx2_aes_gcm_bswap_mask(%rip), %xmm4
 12539                             	        vpshufb	%xmm4, %xmm8, %xmm8
 12540                             	        vpshufb	%xmm4, %xmm9, %xmm9
 12541                             	        vpshufb	%xmm4, %xmm10, %xmm10
 12542                             	        vpshufb	%xmm4, %xmm11, %xmm11
 12543                             	        vpshufb	%xmm4, %xmm12, %xmm12
 12544                             	        vpshufb	%xmm4, %xmm13, %xmm13
 12545                             	        vpshufb	%xmm4, %xmm14, %xmm14
 12546                             	        vpshufb	%xmm4, %xmm15, %xmm15
 12547                             	        vpxor	%xmm6, %xmm8, %xmm8
 12548                             	        vmovdqu	(%rsp), %xmm7
 12549                             	        vpclmulqdq	$16, %xmm15, %xmm7, %xmm5
 12550                             	        vpclmulqdq	$0x01, %xmm15, %xmm7, %xmm1
 12551                             	        vpclmulqdq	$0x00, %xmm15, %xmm7, %xmm4
 12552                             	        vpclmulqdq	$0x11, %xmm15, %xmm7, %xmm6
 12553                             	        vpxor	%xmm1, %xmm5, %xmm5
 12554                             	        vmovdqu	16(%rsp), %xmm7
 12555                             	        vpclmulqdq	$16, %xmm14, %xmm7, %xmm2
 12556                             	        vpclmulqdq	$0x01, %xmm14, %xmm7, %xmm1
 12557                             	        vpclmulqdq	$0x00, %xmm14, %xmm7, %xmm0
 12558                             	        vpclmulqdq	$0x11, %xmm14, %xmm7, %xmm3
 12559                             	        vpxor	%xmm1, %xmm2, %xmm2
 12560                             	        vpxor	%xmm3, %xmm6, %xmm6
 12561                             	        vpxor	%xmm2, %xmm5, %xmm5
 12562                             	        vpxor	%xmm0, %xmm4, %xmm4
 12563                             	        vmovdqu	32(%rsp), %xmm15
 12564                             	        vmovdqu	48(%rsp), %xmm7
 12565                             	        vpclmulqdq	$16, %xmm13, %xmm15, %xmm2
 12566                             	        vpclmulqdq	$0x01, %xmm13, %xmm15, %xmm1
 12567                             	        vpclmulqdq	$0x00, %xmm13, %xmm15, %xmm0
 12568                             	        vpclmulqdq	$0x11, %xmm13, %xmm15, %xmm3
 12569                             	        vpxor	%xmm1, %xmm2, %xmm2
 12570                             	        vpxor	%xmm3, %xmm6, %xmm6
 12571                             	        vpxor	%xmm2, %xmm5, %xmm5
 12572                             	        vpxor	%xmm0, %xmm4, %xmm4
 12573                             	        vpclmulqdq	$16, %xmm12, %xmm7, %xmm2
 12574                             	        vpclmulqdq	$0x01, %xmm12, %xmm7, %xmm1
 12575                             	        vpclmulqdq	$0x00, %xmm12, %xmm7, %xmm0
 12576                             	        vpclmulqdq	$0x11, %xmm12, %xmm7, %xmm3
 12577                             	        vpxor	%xmm1, %xmm2, %xmm2
 12578                             	        vpxor	%xmm3, %xmm6, %xmm6
 12579                             	        vpxor	%xmm2, %xmm5, %xmm5
 12580                             	        vpxor	%xmm0, %xmm4, %xmm4
 12581                             	        vmovdqu	64(%rsp), %xmm15
 12582                             	        vmovdqu	80(%rsp), %xmm7
 12583                             	        vpclmulqdq	$16, %xmm11, %xmm15, %xmm2
 12584                             	        vpclmulqdq	$0x01, %xmm11, %xmm15, %xmm1
 12585                             	        vpclmulqdq	$0x00, %xmm11, %xmm15, %xmm0
 12586                             	        vpclmulqdq	$0x11, %xmm11, %xmm15, %xmm3
 12587                             	        vpxor	%xmm1, %xmm2, %xmm2
 12588                             	        vpxor	%xmm3, %xmm6, %xmm6
 12589                             	        vpxor	%xmm2, %xmm5, %xmm5
 12590                             	        vpxor	%xmm0, %xmm4, %xmm4
 12591                             	        vpclmulqdq	$16, %xmm10, %xmm7, %xmm2
 12592                             	        vpclmulqdq	$0x01, %xmm10, %xmm7, %xmm1
 12593                             	        vpclmulqdq	$0x00, %xmm10, %xmm7, %xmm0
 12594                             	        vpclmulqdq	$0x11, %xmm10, %xmm7, %xmm3
 12595                             	        vpxor	%xmm1, %xmm2, %xmm2
 12596                             	        vpxor	%xmm3, %xmm6, %xmm6
 12597                             	        vpxor	%xmm2, %xmm5, %xmm5
 12598                             	        vpxor	%xmm0, %xmm4, %xmm4
 12599                             	        vmovdqu	96(%rsp), %xmm15
 12600                             	        vmovdqu	112(%rsp), %xmm7
 12601                             	        vpclmulqdq	$16, %xmm9, %xmm15, %xmm2
 12602                             	        vpclmulqdq	$0x01, %xmm9, %xmm15, %xmm1
 12603                             	        vpclmulqdq	$0x00, %xmm9, %xmm15, %xmm0
 12604                             	        vpclmulqdq	$0x11, %xmm9, %xmm15, %xmm3
 12605                             	        vpxor	%xmm1, %xmm2, %xmm2
 12606                             	        vpxor	%xmm3, %xmm6, %xmm6
 12607                             	        vpxor	%xmm2, %xmm5, %xmm5
 12608                             	        vpxor	%xmm0, %xmm4, %xmm4
 12609                             	        vpclmulqdq	$16, %xmm8, %xmm7, %xmm2
 12610                             	        vpclmulqdq	$0x01, %xmm8, %xmm7, %xmm1
 12611                             	        vpclmulqdq	$0x00, %xmm8, %xmm7, %xmm0
 12612                             	        vpclmulqdq	$0x11, %xmm8, %xmm7, %xmm3
 12613                             	        vpxor	%xmm1, %xmm2, %xmm2
 12614                             	        vpxor	%xmm3, %xmm6, %xmm6
 12615                             	        vpxor	%xmm2, %xmm5, %xmm5
 12616                             	        vpxor	%xmm0, %xmm4, %xmm4
 12617                             	        vpslldq	$8, %xmm5, %xmm7
 12618                             	        vpsrldq	$8, %xmm5, %xmm5
 12619                             	        vpxor	%xmm7, %xmm4, %xmm4
 12620                             	        vpxor	%xmm5, %xmm6, %xmm6
 12621                             	        # ghash_red
 12622                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm2
 12623                             	        vpclmulqdq	$16, %xmm2, %xmm4, %xmm0
 12624                             	        vpshufd	$0x4e, %xmm4, %xmm1
 12625                             	        vpxor	%xmm0, %xmm1, %xmm1
 12626                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm0
 12627                             	        vpshufd	$0x4e, %xmm1, %xmm1
 12628                             	        vpxor	%xmm0, %xmm1, %xmm1
 12629                             	        vpxor	%xmm1, %xmm6, %xmm6
 12630                             	        vmovdqu	(%rsp), %xmm5
 12631                             	        vmovdqu	128(%rsp), %xmm4
 12632                             	        vmovdqu	144(%rsp), %xmm15
 12633                             	L_AES_GCM_encrypt_avx2_done_128:
 12634                             	        cmpl	%r10d, %ebx
 12635                             	        je	L_AES_GCM_encrypt_avx2_done_enc
 12636                             	        movl	%r10d, %r13d
 12637                             	        andl	$0xfffffff0, %r13d
 12638                             	        cmpl	%r13d, %ebx
 12639                             	        jge	L_AES_GCM_encrypt_avx2_last_block_done
 12640                             	        # aesenc_block
 12641                             	        vmovdqu	%xmm4, %xmm1
 12642                             	        vpshufb	L_avx2_aes_gcm_bswap_epi64(%rip), %xmm1, %xmm0
 12643                             	        vpaddd	L_avx2_aes_gcm_one(%rip), %xmm1, %xmm1
 12644                             	        vpxor	(%rsi), %xmm0, %xmm0
 12645                             	        vmovdqu	16(%rsi), %xmm2
 12646                             	        vaesenc	%xmm2, %xmm0, %xmm0
 12647                             	        vmovdqu	32(%rsi), %xmm2
 12648                             	        vaesenc	%xmm2, %xmm0, %xmm0
 12649                             	        vmovdqu	48(%rsi), %xmm2
 12650                             	        vaesenc	%xmm2, %xmm0, %xmm0
 12651                             	        vmovdqu	64(%rsi), %xmm2
 12652                             	        vaesenc	%xmm2, %xmm0, %xmm0
 12653                             	        vmovdqu	80(%rsi), %xmm2
 12654                             	        vaesenc	%xmm2, %xmm0, %xmm0
 12655                             	        vmovdqu	96(%rsi), %xmm2
 12656                             	        vaesenc	%xmm2, %xmm0, %xmm0
 12657                             	        vmovdqu	112(%rsi), %xmm2
 12658                             	        vaesenc	%xmm2, %xmm0, %xmm0
 12659                             	        vmovdqu	128(%rsi), %xmm2
 12660                             	        vaesenc	%xmm2, %xmm0, %xmm0
 12661                             	        vmovdqu	144(%rsi), %xmm2
 12662                             	        vaesenc	%xmm2, %xmm0, %xmm0
 12663                             	        vmovdqu	%xmm1, %xmm4
 12664                             	        cmpl	$11, %r9d
 12665                             	        vmovdqu	160(%rsi), %xmm1
 12666                             	        jl	L_AES_GCM_encrypt_avx2_aesenc_block_last
 12667                             	        vaesenc	%xmm1, %xmm0, %xmm0
 12668                             	        vmovdqu	176(%rsi), %xmm2
 12669                             	        vaesenc	%xmm2, %xmm0, %xmm0
 12670                             	        cmpl	$13, %r9d
 12671                             	        vmovdqu	192(%rsi), %xmm1
 12672                             	        jl	L_AES_GCM_encrypt_avx2_aesenc_block_last
 12673                             	        vaesenc	%xmm1, %xmm0, %xmm0
 12674                             	        vmovdqu	208(%rsi), %xmm2
 12675                             	        vaesenc	%xmm2, %xmm0, %xmm0
 12676                             	        vmovdqu	224(%rsi), %xmm1
 12677                             	L_AES_GCM_encrypt_avx2_aesenc_block_last:
 12678                             	        vaesenclast	%xmm1, %xmm0, %xmm0
 12679                             	        vmovdqu	(%rdi,%rbx,1), %xmm1
 12680                             	        vpxor	%xmm1, %xmm0, %xmm0
 12681                             	        vmovdqu	%xmm0, (%r8,%rbx,1)
 12682                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 12683                             	        vpxor	%xmm0, %xmm6, %xmm6
 12684                             	        addl	$16, %ebx
 12685                             	        cmpl	%r13d, %ebx
 12686                             	        jge	L_AES_GCM_encrypt_avx2_last_block_ghash
 12687                             	L_AES_GCM_encrypt_avx2_last_block_start:
 12688                             	        vmovdqu	(%rdi,%rbx,1), %xmm12
 12689                             	        vpshufb	L_avx2_aes_gcm_bswap_epi64(%rip), %xmm4, %xmm11
 12690                             	        vpaddd	L_avx2_aes_gcm_one(%rip), %xmm4, %xmm4
 12691                             	        # aesenc_gfmul_sb
 12692                             	        vpclmulqdq	$0x01, %xmm5, %xmm6, %xmm2
 12693                             	        vpclmulqdq	$16, %xmm5, %xmm6, %xmm3
 12694                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm1
 12695                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm8
 12696                             	        vpxor	(%rsi), %xmm11, %xmm11
 12697                             	        vaesenc	16(%rsi), %xmm11, %xmm11
 12698                             	        vpxor	%xmm2, %xmm3, %xmm3
 12699                             	        vpslldq	$8, %xmm3, %xmm2
 12700                             	        vpsrldq	$8, %xmm3, %xmm3
 12701                             	        vaesenc	32(%rsi), %xmm11, %xmm11
 12702                             	        vpxor	%xmm1, %xmm2, %xmm2
 12703                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm2, %xmm1
 12704                             	        vaesenc	48(%rsi), %xmm11, %xmm11
 12705                             	        vaesenc	64(%rsi), %xmm11, %xmm11
 12706                             	        vaesenc	80(%rsi), %xmm11, %xmm11
 12707                             	        vpshufd	$0x4e, %xmm2, %xmm2
 12708                             	        vpxor	%xmm1, %xmm2, %xmm2
 12709                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm2, %xmm1
 12710                             	        vaesenc	96(%rsi), %xmm11, %xmm11
 12711                             	        vaesenc	112(%rsi), %xmm11, %xmm11
 12712                             	        vaesenc	128(%rsi), %xmm11, %xmm11
 12713                             	        vpshufd	$0x4e, %xmm2, %xmm2
 12714                             	        vaesenc	144(%rsi), %xmm11, %xmm11
 12715                             	        vpxor	%xmm3, %xmm8, %xmm8
 12716                             	        vpxor	%xmm8, %xmm2, %xmm2
 12717                             	        vmovdqu	160(%rsi), %xmm0
 12718                             	        cmpl	$11, %r9d
 12719                             	        jl	L_AES_GCM_encrypt_avx2_aesenc_gfmul_sb_last
 12720                             	        vaesenc	%xmm0, %xmm11, %xmm11
 12721                             	        vaesenc	176(%rsi), %xmm11, %xmm11
 12722                             	        vmovdqu	192(%rsi), %xmm0
 12723                             	        cmpl	$13, %r9d
 12724                             	        jl	L_AES_GCM_encrypt_avx2_aesenc_gfmul_sb_last
 12725                             	        vaesenc	%xmm0, %xmm11, %xmm11
 12726                             	        vaesenc	208(%rsi), %xmm11, %xmm11
 12727                             	        vmovdqu	224(%rsi), %xmm0
 12728                             	L_AES_GCM_encrypt_avx2_aesenc_gfmul_sb_last:
 12729                             	        vaesenclast	%xmm0, %xmm11, %xmm11
 12730                             	        vpxor	%xmm1, %xmm2, %xmm6
 12731                             	        vpxor	%xmm12, %xmm11, %xmm11
 12732                             	        vmovdqu	%xmm11, (%r8,%rbx,1)
 12733                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm11, %xmm11
 12734                             	        vpxor	%xmm11, %xmm6, %xmm6
 12735                             	        addl	$16, %ebx
 12736                             	        cmpl	%r13d, %ebx
 12737                             	        jl	L_AES_GCM_encrypt_avx2_last_block_start
 12738                             	L_AES_GCM_encrypt_avx2_last_block_ghash:
 12739                             	        # ghash_gfmul_red
 12740                             	        vpclmulqdq	$16, %xmm5, %xmm6, %xmm10
 12741                             	        vpclmulqdq	$0x01, %xmm5, %xmm6, %xmm9
 12742                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm8
 12743                             	        vpxor	%xmm9, %xmm10, %xmm10
 12744                             	        vpslldq	$8, %xmm10, %xmm9
 12745                             	        vpsrldq	$8, %xmm10, %xmm10
 12746                             	        vpxor	%xmm8, %xmm9, %xmm9
 12747                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm6
 12748                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm9, %xmm8
 12749                             	        vpshufd	$0x4e, %xmm9, %xmm9
 12750                             	        vpxor	%xmm8, %xmm9, %xmm9
 12751                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm9, %xmm8
 12752                             	        vpshufd	$0x4e, %xmm9, %xmm9
 12753                             	        vpxor	%xmm10, %xmm6, %xmm6
 12754                             	        vpxor	%xmm9, %xmm6, %xmm6
 12755                             	        vpxor	%xmm8, %xmm6, %xmm6
 12756                             	L_AES_GCM_encrypt_avx2_last_block_done:
 12757                             	        movl	%r10d, %ecx
 12758                             	        movl	%r10d, %edx
 12759                             	        andl	$15, %ecx
 12760                             	        jz	L_AES_GCM_encrypt_avx2_done_enc
 12761                             	        # aesenc_last15_enc
 12762                             	        vpshufb	L_avx2_aes_gcm_bswap_epi64(%rip), %xmm4, %xmm4
 12763                             	        vpxor	(%rsi), %xmm4, %xmm4
 12764                             	        vaesenc	16(%rsi), %xmm4, %xmm4
 12765                             	        vaesenc	32(%rsi), %xmm4, %xmm4
 12766                             	        vaesenc	48(%rsi), %xmm4, %xmm4
 12767                             	        vaesenc	64(%rsi), %xmm4, %xmm4
 12768                             	        vaesenc	80(%rsi), %xmm4, %xmm4
 12769                             	        vaesenc	96(%rsi), %xmm4, %xmm4
 12770                             	        vaesenc	112(%rsi), %xmm4, %xmm4
 12771                             	        vaesenc	128(%rsi), %xmm4, %xmm4
 12772                             	        vaesenc	144(%rsi), %xmm4, %xmm4
 12773                             	        cmpl	$11, %r9d
 12774                             	        vmovdqu	160(%rsi), %xmm0
 12775                             	        jl	L_AES_GCM_encrypt_avx2_aesenc_last15_enc_avx_aesenc_avx_last
 12776                             	        vaesenc	%xmm0, %xmm4, %xmm4
 12777                             	        vaesenc	176(%rsi), %xmm4, %xmm4
 12778                             	        cmpl	$13, %r9d
 12779                             	        vmovdqu	192(%rsi), %xmm0
 12780                             	        jl	L_AES_GCM_encrypt_avx2_aesenc_last15_enc_avx_aesenc_avx_last
 12781                             	        vaesenc	%xmm0, %xmm4, %xmm4
 12782                             	        vaesenc	208(%rsi), %xmm4, %xmm4
 12783                             	        vmovdqu	224(%rsi), %xmm0
 12784                             	L_AES_GCM_encrypt_avx2_aesenc_last15_enc_avx_aesenc_avx_last:
 12785                             	        vaesenclast	%xmm0, %xmm4, %xmm4
 12786                             	        xorl	%ecx, %ecx
 12787                             	        vpxor	%xmm0, %xmm0, %xmm0
 12788                             	        vmovdqu	%xmm4, (%rsp)
 12789                             	        vmovdqu	%xmm0, 16(%rsp)
 12790                             	L_AES_GCM_encrypt_avx2_aesenc_last15_enc_avx_loop:
 12791                             	        movzbl	(%rdi,%rbx,1), %r13d
 12792                             	        xorb	(%rsp,%rcx,1), %r13b
 12793                             	        movb	%r13b, 16(%rsp,%rcx,1)
 12794                             	        movb	%r13b, (%r8,%rbx,1)
 12795                             	        incl	%ebx
 12796                             	        incl	%ecx
 12797                             	        cmpl	%edx, %ebx
 12798                             	        jl	L_AES_GCM_encrypt_avx2_aesenc_last15_enc_avx_loop
 12799                             	L_AES_GCM_encrypt_avx2_aesenc_last15_enc_avx_finish_enc:
 12800                             	        vmovdqu	16(%rsp), %xmm4
 12801                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm4, %xmm4
 12802                             	        vpxor	%xmm4, %xmm6, %xmm6
 12803                             	        # ghash_gfmul_red
 12804                             	        vpclmulqdq	$16, %xmm5, %xmm6, %xmm2
 12805                             	        vpclmulqdq	$0x01, %xmm5, %xmm6, %xmm1
 12806                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm0
 12807                             	        vpxor	%xmm1, %xmm2, %xmm2
 12808                             	        vpslldq	$8, %xmm2, %xmm1
 12809                             	        vpsrldq	$8, %xmm2, %xmm2
 12810                             	        vpxor	%xmm0, %xmm1, %xmm1
 12811                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm6
 12812                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm1, %xmm0
 12813                             	        vpshufd	$0x4e, %xmm1, %xmm1
 12814                             	        vpxor	%xmm0, %xmm1, %xmm1
 12815                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm1, %xmm0
 12816                             	        vpshufd	$0x4e, %xmm1, %xmm1
 12817                             	        vpxor	%xmm2, %xmm6, %xmm6
 12818                             	        vpxor	%xmm1, %xmm6, %xmm6
 12819                             	        vpxor	%xmm0, %xmm6, %xmm6
 12820                             	L_AES_GCM_encrypt_avx2_done_enc:
 12821                             	        # calc_tag
 12822                             	        shlq	$3, %r10
 12823                             	        vpinsrq	$0x00, %r10, %xmm0, %xmm0
 12824                             	        shlq	$3, %r11
 12825                             	        vpinsrq	$0x01, %r11, %xmm1, %xmm1
 12826                             	        vpblendd	$12, %xmm1, %xmm0, %xmm0
 12827                             	        vpxor	%xmm6, %xmm0, %xmm0
 12828                             	        # ghash_gfmul_red
 12829                             	        vpclmulqdq	$16, %xmm5, %xmm0, %xmm4
 12830                             	        vpclmulqdq	$0x01, %xmm5, %xmm0, %xmm3
 12831                             	        vpclmulqdq	$0x00, %xmm5, %xmm0, %xmm2
 12832                             	        vpxor	%xmm3, %xmm4, %xmm4
 12833                             	        vpslldq	$8, %xmm4, %xmm3
 12834                             	        vpsrldq	$8, %xmm4, %xmm4
 12835                             	        vpxor	%xmm2, %xmm3, %xmm3
 12836                             	        vpclmulqdq	$0x11, %xmm5, %xmm0, %xmm0
 12837                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm3, %xmm2
 12838                             	        vpshufd	$0x4e, %xmm3, %xmm3
 12839                             	        vpxor	%xmm2, %xmm3, %xmm3
 12840                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm3, %xmm2
 12841                             	        vpshufd	$0x4e, %xmm3, %xmm3
 12842                             	        vpxor	%xmm4, %xmm0, %xmm0
 12843                             	        vpxor	%xmm3, %xmm0, %xmm0
 12844                             	        vpxor	%xmm2, %xmm0, %xmm0
 12845                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 12846                             	        vpxor	%xmm15, %xmm0, %xmm0
 12847                             	        # store_tag
 12848                             	        cmpl	$16, %r14d
 12849                             	        je	L_AES_GCM_encrypt_avx2_store_tag_16
 12850                             	        xorq	%rcx, %rcx
 12851                             	        vmovdqu	%xmm0, (%rsp)
 12852                             	L_AES_GCM_encrypt_avx2_store_tag_loop:
 12853                             	        movzbl	(%rsp,%rcx,1), %r13d
 12854                             	        movb	%r13b, (%r15,%rcx,1)
 12855                             	        incl	%ecx
 12856                             	        cmpl	%r14d, %ecx
 12857                             	        jne	L_AES_GCM_encrypt_avx2_store_tag_loop
 12858                             	        jmp	L_AES_GCM_encrypt_avx2_store_tag_done
 12859                             	L_AES_GCM_encrypt_avx2_store_tag_16:
 12860                             	        vmovdqu	%xmm0, (%r15)
 12861                             	L_AES_GCM_encrypt_avx2_store_tag_done:
 12862                             	        vzeroupper
 12863                             	        addq	$0xa0, %rsp
 12864                             	        popq	%r14
 12865                             	        popq	%rbx
 12866                             	        popq	%r15
 12867                             	        popq	%r12
 12868                             	        popq	%r13
 12869                             	        repz retq
 12870                             	#ifndef __APPLE__
 12872                             	#endif /* __APPLE__ */
 12873                             	#ifndef __APPLE__
 12874                             	.text
 12875                             	.globl	AES_GCM_decrypt_avx2
 12877                             	.align	16
 12878                             	AES_GCM_decrypt_avx2:
 12879                             	#else
 12880                             	.section	__TEXT,__text
 12881                             	.globl	_AES_GCM_decrypt_avx2
 12882                             	.p2align	4
 12883                             	_AES_GCM_decrypt_avx2:
 12884                             	#endif /* __APPLE__ */
 12885                             	        pushq	%r13
 12886                             	        pushq	%r12
 12887                             	        pushq	%r14
 12888                             	        pushq	%rbx
 12889                             	        pushq	%r15
 12890                             	        pushq	%rbp
 12891                             	        movq	%rdx, %r12
 12892                             	        movq	%rcx, %rax
 12893                             	        movq	%r8, %r14
 12894                             	        movq	%rsi, %r8
 12895                             	        movl	%r9d, %r10d
 12896                             	        movl	56(%rsp), %r11d
 12897                             	        movl	64(%rsp), %ebx
 12898                             	        movl	72(%rsp), %r15d
 12899                             	        movq	80(%rsp), %rsi
 12900                             	        movl	88(%rsp), %r9d
 12901                             	        movq	96(%rsp), %rbp
 12902                             	        subq	$0xa8, %rsp
 12903                             	        vpxor	%xmm4, %xmm4, %xmm4
 12904                             	        vpxor	%xmm6, %xmm6, %xmm6
 12905                             	        movl	%ebx, %edx
 12906                             	        cmpl	$12, %edx
 12907                             	        je	L_AES_GCM_decrypt_avx2_iv_12
 12908                             	        # Calculate values when IV is not 12 bytes
 12909                             	        # H = Encrypt X(=0)
 12910                             	        vmovdqu	(%rsi), %xmm5
 12911                             	        vaesenc	16(%rsi), %xmm5, %xmm5
 12912                             	        vaesenc	32(%rsi), %xmm5, %xmm5
 12913                             	        vaesenc	48(%rsi), %xmm5, %xmm5
 12914                             	        vaesenc	64(%rsi), %xmm5, %xmm5
 12915                             	        vaesenc	80(%rsi), %xmm5, %xmm5
 12916                             	        vaesenc	96(%rsi), %xmm5, %xmm5
 12917                             	        vaesenc	112(%rsi), %xmm5, %xmm5
 12918                             	        vaesenc	128(%rsi), %xmm5, %xmm5
 12919                             	        vaesenc	144(%rsi), %xmm5, %xmm5
 12920                             	        cmpl	$11, %r9d
 12921                             	        vmovdqu	160(%rsi), %xmm0
 12922                             	        jl	L_AES_GCM_decrypt_avx2_calc_iv_1_aesenc_avx_last
 12923                             	        vaesenc	%xmm0, %xmm5, %xmm5
 12924                             	        vaesenc	176(%rsi), %xmm5, %xmm5
 12925                             	        cmpl	$13, %r9d
 12926                             	        vmovdqu	192(%rsi), %xmm0
 12927                             	        jl	L_AES_GCM_decrypt_avx2_calc_iv_1_aesenc_avx_last
 12928                             	        vaesenc	%xmm0, %xmm5, %xmm5
 12929                             	        vaesenc	208(%rsi), %xmm5, %xmm5
 12930                             	        vmovdqu	224(%rsi), %xmm0
 12931                             	L_AES_GCM_decrypt_avx2_calc_iv_1_aesenc_avx_last:
 12932                             	        vaesenclast	%xmm0, %xmm5, %xmm5
 12933                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm5, %xmm5
 12934                             	        # Calc counter
 12935                             	        # Initialization vector
 12936                             	        cmpl	$0x00, %edx
 12937                             	        movq	$0x00, %rcx
 12938                             	        je	L_AES_GCM_decrypt_avx2_calc_iv_done
 12939                             	        cmpl	$16, %edx
 12940                             	        jl	L_AES_GCM_decrypt_avx2_calc_iv_lt16
 12941                             	        andl	$0xfffffff0, %edx
 12942                             	L_AES_GCM_decrypt_avx2_calc_iv_16_loop:
 12943                             	        vmovdqu	(%rax,%rcx,1), %xmm0
 12944                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 12945                             	        vpxor	%xmm0, %xmm4, %xmm4
 12946                             	        # ghash_gfmul_avx
 12947                             	        vpclmulqdq	$16, %xmm4, %xmm5, %xmm2
 12948                             	        vpclmulqdq	$0x01, %xmm4, %xmm5, %xmm1
 12949                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 12950                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 12951                             	        vpxor	%xmm1, %xmm2, %xmm2
 12952                             	        vpslldq	$8, %xmm2, %xmm1
 12953                             	        vpsrldq	$8, %xmm2, %xmm2
 12954                             	        vpxor	%xmm1, %xmm0, %xmm7
 12955                             	        vpxor	%xmm2, %xmm3, %xmm4
 12956                             	        # ghash_mid
 12957                             	        vpsrld	$31, %xmm7, %xmm0
 12958                             	        vpsrld	$31, %xmm4, %xmm1
 12959                             	        vpslld	$0x01, %xmm7, %xmm7
 12960                             	        vpslld	$0x01, %xmm4, %xmm4
 12961                             	        vpsrldq	$12, %xmm0, %xmm2
 12962                             	        vpslldq	$4, %xmm0, %xmm0
 12963                             	        vpslldq	$4, %xmm1, %xmm1
 12964                             	        vpor	%xmm2, %xmm4, %xmm4
 12965                             	        vpor	%xmm0, %xmm7, %xmm7
 12966                             	        vpor	%xmm1, %xmm4, %xmm4
 12967                             	        # ghash_red
 12968                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm2
 12969                             	        vpclmulqdq	$16, %xmm2, %xmm7, %xmm0
 12970                             	        vpshufd	$0x4e, %xmm7, %xmm1
 12971                             	        vpxor	%xmm0, %xmm1, %xmm1
 12972                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm0
 12973                             	        vpshufd	$0x4e, %xmm1, %xmm1
 12974                             	        vpxor	%xmm0, %xmm1, %xmm1
 12975                             	        vpxor	%xmm1, %xmm4, %xmm4
 12976                             	        addl	$16, %ecx
 12977                             	        cmpl	%edx, %ecx
 12978                             	        jl	L_AES_GCM_decrypt_avx2_calc_iv_16_loop
 12979                             	        movl	%ebx, %edx
 12980                             	        cmpl	%edx, %ecx
 12981                             	        je	L_AES_GCM_decrypt_avx2_calc_iv_done
 12982                             	L_AES_GCM_decrypt_avx2_calc_iv_lt16:
 12983                             	        vpxor	%xmm0, %xmm0, %xmm0
 12984                             	        xorl	%ebx, %ebx
 12985                             	        vmovdqu	%xmm0, (%rsp)
 12986                             	L_AES_GCM_decrypt_avx2_calc_iv_loop:
 12987                             	        movzbl	(%rax,%rcx,1), %r13d
 12988                             	        movb	%r13b, (%rsp,%rbx,1)
 12989                             	        incl	%ecx
 12990                             	        incl	%ebx
 12991                             	        cmpl	%edx, %ecx
 12992                             	        jl	L_AES_GCM_decrypt_avx2_calc_iv_loop
 12993                             	        vmovdqu	(%rsp), %xmm0
 12994                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 12995                             	        vpxor	%xmm0, %xmm4, %xmm4
 12996                             	        # ghash_gfmul_avx
 12997                             	        vpclmulqdq	$16, %xmm4, %xmm5, %xmm2
 12998                             	        vpclmulqdq	$0x01, %xmm4, %xmm5, %xmm1
 12999                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 13000                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 13001                             	        vpxor	%xmm1, %xmm2, %xmm2
 13002                             	        vpslldq	$8, %xmm2, %xmm1
 13003                             	        vpsrldq	$8, %xmm2, %xmm2
 13004                             	        vpxor	%xmm1, %xmm0, %xmm7
 13005                             	        vpxor	%xmm2, %xmm3, %xmm4
 13006                             	        # ghash_mid
 13007                             	        vpsrld	$31, %xmm7, %xmm0
 13008                             	        vpsrld	$31, %xmm4, %xmm1
 13009                             	        vpslld	$0x01, %xmm7, %xmm7
 13010                             	        vpslld	$0x01, %xmm4, %xmm4
 13011                             	        vpsrldq	$12, %xmm0, %xmm2
 13012                             	        vpslldq	$4, %xmm0, %xmm0
 13013                             	        vpslldq	$4, %xmm1, %xmm1
 13014                             	        vpor	%xmm2, %xmm4, %xmm4
 13015                             	        vpor	%xmm0, %xmm7, %xmm7
 13016                             	        vpor	%xmm1, %xmm4, %xmm4
 13017                             	        # ghash_red
 13018                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm2
 13019                             	        vpclmulqdq	$16, %xmm2, %xmm7, %xmm0
 13020                             	        vpshufd	$0x4e, %xmm7, %xmm1
 13021                             	        vpxor	%xmm0, %xmm1, %xmm1
 13022                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm0
 13023                             	        vpshufd	$0x4e, %xmm1, %xmm1
 13024                             	        vpxor	%xmm0, %xmm1, %xmm1
 13025                             	        vpxor	%xmm1, %xmm4, %xmm4
 13026                             	L_AES_GCM_decrypt_avx2_calc_iv_done:
 13027                             	        # T = Encrypt counter
 13028                             	        vpxor	%xmm0, %xmm0, %xmm0
 13029                             	        shll	$3, %edx
 13030                             	        vpinsrq	$0x00, %rdx, %xmm0, %xmm0
 13031                             	        vpxor	%xmm0, %xmm4, %xmm4
 13032                             	        # ghash_gfmul_avx
 13033                             	        vpclmulqdq	$16, %xmm4, %xmm5, %xmm2
 13034                             	        vpclmulqdq	$0x01, %xmm4, %xmm5, %xmm1
 13035                             	        vpclmulqdq	$0x00, %xmm4, %xmm5, %xmm0
 13036                             	        vpclmulqdq	$0x11, %xmm4, %xmm5, %xmm3
 13037                             	        vpxor	%xmm1, %xmm2, %xmm2
 13038                             	        vpslldq	$8, %xmm2, %xmm1
 13039                             	        vpsrldq	$8, %xmm2, %xmm2
 13040                             	        vpxor	%xmm1, %xmm0, %xmm7
 13041                             	        vpxor	%xmm2, %xmm3, %xmm4
 13042                             	        # ghash_mid
 13043                             	        vpsrld	$31, %xmm7, %xmm0
 13044                             	        vpsrld	$31, %xmm4, %xmm1
 13045                             	        vpslld	$0x01, %xmm7, %xmm7
 13046                             	        vpslld	$0x01, %xmm4, %xmm4
 13047                             	        vpsrldq	$12, %xmm0, %xmm2
 13048                             	        vpslldq	$4, %xmm0, %xmm0
 13049                             	        vpslldq	$4, %xmm1, %xmm1
 13050                             	        vpor	%xmm2, %xmm4, %xmm4
 13051                             	        vpor	%xmm0, %xmm7, %xmm7
 13052                             	        vpor	%xmm1, %xmm4, %xmm4
 13053                             	        # ghash_red
 13054                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm2
 13055                             	        vpclmulqdq	$16, %xmm2, %xmm7, %xmm0
 13056                             	        vpshufd	$0x4e, %xmm7, %xmm1
 13057                             	        vpxor	%xmm0, %xmm1, %xmm1
 13058                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm0
 13059                             	        vpshufd	$0x4e, %xmm1, %xmm1
 13060                             	        vpxor	%xmm0, %xmm1, %xmm1
 13061                             	        vpxor	%xmm1, %xmm4, %xmm4
 13062                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm4, %xmm4
 13063                             	        #   Encrypt counter
 13064                             	        vmovdqu	(%rsi), %xmm15
 13065                             	        vpxor	%xmm4, %xmm15, %xmm15
 13066                             	        vaesenc	16(%rsi), %xmm15, %xmm15
 13067                             	        vaesenc	32(%rsi), %xmm15, %xmm15
 13068                             	        vaesenc	48(%rsi), %xmm15, %xmm15
 13069                             	        vaesenc	64(%rsi), %xmm15, %xmm15
 13070                             	        vaesenc	80(%rsi), %xmm15, %xmm15
 13071                             	        vaesenc	96(%rsi), %xmm15, %xmm15
 13072                             	        vaesenc	112(%rsi), %xmm15, %xmm15
 13073                             	        vaesenc	128(%rsi), %xmm15, %xmm15
 13074                             	        vaesenc	144(%rsi), %xmm15, %xmm15
 13075                             	        cmpl	$11, %r9d
 13076                             	        vmovdqu	160(%rsi), %xmm0
 13077                             	        jl	L_AES_GCM_decrypt_avx2_calc_iv_2_aesenc_avx_last
 13078                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13079                             	        vaesenc	176(%rsi), %xmm15, %xmm15
 13080                             	        cmpl	$13, %r9d
 13081                             	        vmovdqu	192(%rsi), %xmm0
 13082                             	        jl	L_AES_GCM_decrypt_avx2_calc_iv_2_aesenc_avx_last
 13083                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13084                             	        vaesenc	208(%rsi), %xmm15, %xmm15
 13085                             	        vmovdqu	224(%rsi), %xmm0
 13086                             	L_AES_GCM_decrypt_avx2_calc_iv_2_aesenc_avx_last:
 13087                             	        vaesenclast	%xmm0, %xmm15, %xmm15
 13088                             	        jmp	L_AES_GCM_decrypt_avx2_iv_done
 13089                             	L_AES_GCM_decrypt_avx2_iv_12:
 13090                             	        # # Calculate values when IV is 12 bytes
 13091                             	        # Set counter based on IV
 13092                             	        vmovdqu	L_avx2_aes_gcm_bswap_one(%rip), %xmm4
 13093                             	        vmovdqu	(%rsi), %xmm5
 13094                             	        vpblendd	$7, (%rax), %xmm4, %xmm4
 13095                             	        # H = Encrypt X(=0) and T = Encrypt counter
 13096                             	        vmovdqu	16(%rsi), %xmm7
 13097                             	        vpxor	%xmm5, %xmm4, %xmm15
 13098                             	        vaesenc	%xmm7, %xmm5, %xmm5
 13099                             	        vaesenc	%xmm7, %xmm15, %xmm15
 13100                             	        vmovdqu	32(%rsi), %xmm0
 13101                             	        vaesenc	%xmm0, %xmm5, %xmm5
 13102                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13103                             	        vmovdqu	48(%rsi), %xmm0
 13104                             	        vaesenc	%xmm0, %xmm5, %xmm5
 13105                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13106                             	        vmovdqu	64(%rsi), %xmm0
 13107                             	        vaesenc	%xmm0, %xmm5, %xmm5
 13108                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13109                             	        vmovdqu	80(%rsi), %xmm0
 13110                             	        vaesenc	%xmm0, %xmm5, %xmm5
 13111                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13112                             	        vmovdqu	96(%rsi), %xmm0
 13113                             	        vaesenc	%xmm0, %xmm5, %xmm5
 13114                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13115                             	        vmovdqu	112(%rsi), %xmm0
 13116                             	        vaesenc	%xmm0, %xmm5, %xmm5
 13117                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13118                             	        vmovdqu	128(%rsi), %xmm0
 13119                             	        vaesenc	%xmm0, %xmm5, %xmm5
 13120                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13121                             	        vmovdqu	144(%rsi), %xmm0
 13122                             	        vaesenc	%xmm0, %xmm5, %xmm5
 13123                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13124                             	        cmpl	$11, %r9d
 13125                             	        vmovdqu	160(%rsi), %xmm0
 13126                             	        jl	L_AES_GCM_decrypt_avx2_calc_iv_12_last
 13127                             	        vaesenc	%xmm0, %xmm5, %xmm5
 13128                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13129                             	        vmovdqu	176(%rsi), %xmm0
 13130                             	        vaesenc	%xmm0, %xmm5, %xmm5
 13131                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13132                             	        cmpl	$13, %r9d
 13133                             	        vmovdqu	192(%rsi), %xmm0
 13134                             	        jl	L_AES_GCM_decrypt_avx2_calc_iv_12_last
 13135                             	        vaesenc	%xmm0, %xmm5, %xmm5
 13136                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13137                             	        vmovdqu	208(%rsi), %xmm0
 13138                             	        vaesenc	%xmm0, %xmm5, %xmm5
 13139                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13140                             	        vmovdqu	224(%rsi), %xmm0
 13141                             	L_AES_GCM_decrypt_avx2_calc_iv_12_last:
 13142                             	        vaesenclast	%xmm0, %xmm5, %xmm5
 13143                             	        vaesenclast	%xmm0, %xmm15, %xmm15
 13144                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm5, %xmm5
 13145                             	L_AES_GCM_decrypt_avx2_iv_done:
 13146                             	        # Additional authentication data
 13147                             	        movl	%r11d, %edx
 13148                             	        cmpl	$0x00, %edx
 13149                             	        je	L_AES_GCM_decrypt_avx2_calc_aad_done
 13150                             	        xorl	%ecx, %ecx
 13151                             	        cmpl	$16, %edx
 13152                             	        jl	L_AES_GCM_decrypt_avx2_calc_aad_lt16
 13153                             	        andl	$0xfffffff0, %edx
 13154                             	L_AES_GCM_decrypt_avx2_calc_aad_16_loop:
 13155                             	        vmovdqu	(%r12,%rcx,1), %xmm0
 13156                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 13157                             	        vpxor	%xmm0, %xmm6, %xmm6
 13158                             	        # ghash_gfmul_avx
 13159                             	        vpclmulqdq	$16, %xmm6, %xmm5, %xmm2
 13160                             	        vpclmulqdq	$0x01, %xmm6, %xmm5, %xmm1
 13161                             	        vpclmulqdq	$0x00, %xmm6, %xmm5, %xmm0
 13162                             	        vpclmulqdq	$0x11, %xmm6, %xmm5, %xmm3
 13163                             	        vpxor	%xmm1, %xmm2, %xmm2
 13164                             	        vpslldq	$8, %xmm2, %xmm1
 13165                             	        vpsrldq	$8, %xmm2, %xmm2
 13166                             	        vpxor	%xmm1, %xmm0, %xmm7
 13167                             	        vpxor	%xmm2, %xmm3, %xmm6
 13168                             	        # ghash_mid
 13169                             	        vpsrld	$31, %xmm7, %xmm0
 13170                             	        vpsrld	$31, %xmm6, %xmm1
 13171                             	        vpslld	$0x01, %xmm7, %xmm7
 13172                             	        vpslld	$0x01, %xmm6, %xmm6
 13173                             	        vpsrldq	$12, %xmm0, %xmm2
 13174                             	        vpslldq	$4, %xmm0, %xmm0
 13175                             	        vpslldq	$4, %xmm1, %xmm1
 13176                             	        vpor	%xmm2, %xmm6, %xmm6
 13177                             	        vpor	%xmm0, %xmm7, %xmm7
 13178                             	        vpor	%xmm1, %xmm6, %xmm6
 13179                             	        # ghash_red
 13180                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm2
 13181                             	        vpclmulqdq	$16, %xmm2, %xmm7, %xmm0
 13182                             	        vpshufd	$0x4e, %xmm7, %xmm1
 13183                             	        vpxor	%xmm0, %xmm1, %xmm1
 13184                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm0
 13185                             	        vpshufd	$0x4e, %xmm1, %xmm1
 13186                             	        vpxor	%xmm0, %xmm1, %xmm1
 13187                             	        vpxor	%xmm1, %xmm6, %xmm6
 13188                             	        addl	$16, %ecx
 13189                             	        cmpl	%edx, %ecx
 13190                             	        jl	L_AES_GCM_decrypt_avx2_calc_aad_16_loop
 13191                             	        movl	%r11d, %edx
 13192                             	        cmpl	%edx, %ecx
 13193                             	        je	L_AES_GCM_decrypt_avx2_calc_aad_done
 13194                             	L_AES_GCM_decrypt_avx2_calc_aad_lt16:
 13195                             	        vpxor	%xmm0, %xmm0, %xmm0
 13196                             	        xorl	%ebx, %ebx
 13197                             	        vmovdqu	%xmm0, (%rsp)
 13198                             	L_AES_GCM_decrypt_avx2_calc_aad_loop:
 13199                             	        movzbl	(%r12,%rcx,1), %r13d
 13200                             	        movb	%r13b, (%rsp,%rbx,1)
 13201                             	        incl	%ecx
 13202                             	        incl	%ebx
 13203                             	        cmpl	%edx, %ecx
 13204                             	        jl	L_AES_GCM_decrypt_avx2_calc_aad_loop
 13205                             	        vmovdqu	(%rsp), %xmm0
 13206                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 13207                             	        vpxor	%xmm0, %xmm6, %xmm6
 13208                             	        # ghash_gfmul_avx
 13209                             	        vpclmulqdq	$16, %xmm6, %xmm5, %xmm2
 13210                             	        vpclmulqdq	$0x01, %xmm6, %xmm5, %xmm1
 13211                             	        vpclmulqdq	$0x00, %xmm6, %xmm5, %xmm0
 13212                             	        vpclmulqdq	$0x11, %xmm6, %xmm5, %xmm3
 13213                             	        vpxor	%xmm1, %xmm2, %xmm2
 13214                             	        vpslldq	$8, %xmm2, %xmm1
 13215                             	        vpsrldq	$8, %xmm2, %xmm2
 13216                             	        vpxor	%xmm1, %xmm0, %xmm7
 13217                             	        vpxor	%xmm2, %xmm3, %xmm6
 13218                             	        # ghash_mid
 13219                             	        vpsrld	$31, %xmm7, %xmm0
 13220                             	        vpsrld	$31, %xmm6, %xmm1
 13221                             	        vpslld	$0x01, %xmm7, %xmm7
 13222                             	        vpslld	$0x01, %xmm6, %xmm6
 13223                             	        vpsrldq	$12, %xmm0, %xmm2
 13224                             	        vpslldq	$4, %xmm0, %xmm0
 13225                             	        vpslldq	$4, %xmm1, %xmm1
 13226                             	        vpor	%xmm2, %xmm6, %xmm6
 13227                             	        vpor	%xmm0, %xmm7, %xmm7
 13228                             	        vpor	%xmm1, %xmm6, %xmm6
 13229                             	        # ghash_red
 13230                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm2
 13231                             	        vpclmulqdq	$16, %xmm2, %xmm7, %xmm0
 13232                             	        vpshufd	$0x4e, %xmm7, %xmm1
 13233                             	        vpxor	%xmm0, %xmm1, %xmm1
 13234                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm0
 13235                             	        vpshufd	$0x4e, %xmm1, %xmm1
 13236                             	        vpxor	%xmm0, %xmm1, %xmm1
 13237                             	        vpxor	%xmm1, %xmm6, %xmm6
 13238                             	L_AES_GCM_decrypt_avx2_calc_aad_done:
 13239                             	        # Calculate counter and H
 13240                             	        vpsrlq	$63, %xmm5, %xmm1
 13241                             	        vpsllq	$0x01, %xmm5, %xmm0
 13242                             	        vpslldq	$8, %xmm1, %xmm1
 13243                             	        vpor	%xmm1, %xmm0, %xmm0
 13244                             	        vpshufd	$0xff, %xmm5, %xmm5
 13245                             	        vpsrad	$31, %xmm5, %xmm5
 13246                             	        vpshufb	L_avx2_aes_gcm_bswap_epi64(%rip), %xmm4, %xmm4
 13247                             	        vpand	L_avx2_aes_gcm_mod2_128(%rip), %xmm5, %xmm5
 13248                             	        vpaddd	L_avx2_aes_gcm_one(%rip), %xmm4, %xmm4
 13249                             	        vpxor	%xmm0, %xmm5, %xmm5
 13250                             	        xorl	%ebx, %ebx
 13251                             	        cmpl	$0x80, %r10d
 13252                             	        movl	%r10d, %r13d
 13253                             	        jl	L_AES_GCM_decrypt_avx2_done_128
 13254                             	        andl	$0xffffff80, %r13d
 13255                             	        vmovdqu	%xmm4, 128(%rsp)
 13256                             	        vmovdqu	%xmm15, 144(%rsp)
 13257                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm3
 13258                             	        # H ^ 1 and H ^ 2
 13259                             	        vpclmulqdq	$0x00, %xmm5, %xmm5, %xmm9
 13260                             	        vpclmulqdq	$0x11, %xmm5, %xmm5, %xmm10
 13261                             	        vpclmulqdq	$16, %xmm3, %xmm9, %xmm8
 13262                             	        vpshufd	$0x4e, %xmm9, %xmm9
 13263                             	        vpxor	%xmm8, %xmm9, %xmm9
 13264                             	        vpclmulqdq	$16, %xmm3, %xmm9, %xmm8
 13265                             	        vpshufd	$0x4e, %xmm9, %xmm9
 13266                             	        vpxor	%xmm8, %xmm9, %xmm9
 13267                             	        vpxor	%xmm9, %xmm10, %xmm0
 13268                             	        vmovdqu	%xmm5, (%rsp)
 13269                             	        vmovdqu	%xmm0, 16(%rsp)
 13270                             	        # H ^ 3 and H ^ 4
 13271                             	        vpclmulqdq	$16, %xmm5, %xmm0, %xmm11
 13272                             	        vpclmulqdq	$0x01, %xmm5, %xmm0, %xmm10
 13273                             	        vpclmulqdq	$0x00, %xmm5, %xmm0, %xmm9
 13274                             	        vpclmulqdq	$0x11, %xmm5, %xmm0, %xmm12
 13275                             	        vpclmulqdq	$0x00, %xmm0, %xmm0, %xmm13
 13276                             	        vpclmulqdq	$0x11, %xmm0, %xmm0, %xmm14
 13277                             	        vpxor	%xmm10, %xmm11, %xmm11
 13278                             	        vpslldq	$8, %xmm11, %xmm10
 13279                             	        vpsrldq	$8, %xmm11, %xmm11
 13280                             	        vpxor	%xmm9, %xmm10, %xmm10
 13281                             	        vpclmulqdq	$16, %xmm3, %xmm13, %xmm8
 13282                             	        vpclmulqdq	$16, %xmm3, %xmm10, %xmm9
 13283                             	        vpshufd	$0x4e, %xmm10, %xmm10
 13284                             	        vpshufd	$0x4e, %xmm13, %xmm13
 13285                             	        vpxor	%xmm9, %xmm10, %xmm10
 13286                             	        vpxor	%xmm8, %xmm13, %xmm13
 13287                             	        vpclmulqdq	$16, %xmm3, %xmm10, %xmm9
 13288                             	        vpclmulqdq	$16, %xmm3, %xmm13, %xmm8
 13289                             	        vpshufd	$0x4e, %xmm10, %xmm10
 13290                             	        vpshufd	$0x4e, %xmm13, %xmm13
 13291                             	        vpxor	%xmm11, %xmm12, %xmm12
 13292                             	        vpxor	%xmm8, %xmm13, %xmm13
 13293                             	        vpxor	%xmm12, %xmm10, %xmm10
 13294                             	        vpxor	%xmm14, %xmm13, %xmm2
 13295                             	        vpxor	%xmm9, %xmm10, %xmm1
 13296                             	        vmovdqu	%xmm1, 32(%rsp)
 13297                             	        vmovdqu	%xmm2, 48(%rsp)
 13298                             	        # H ^ 5 and H ^ 6
 13299                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm11
 13300                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm10
 13301                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm9
 13302                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm12
 13303                             	        vpclmulqdq	$0x00, %xmm1, %xmm1, %xmm13
 13304                             	        vpclmulqdq	$0x11, %xmm1, %xmm1, %xmm14
 13305                             	        vpxor	%xmm10, %xmm11, %xmm11
 13306                             	        vpslldq	$8, %xmm11, %xmm10
 13307                             	        vpsrldq	$8, %xmm11, %xmm11
 13308                             	        vpxor	%xmm9, %xmm10, %xmm10
 13309                             	        vpclmulqdq	$16, %xmm3, %xmm13, %xmm8
 13310                             	        vpclmulqdq	$16, %xmm3, %xmm10, %xmm9
 13311                             	        vpshufd	$0x4e, %xmm10, %xmm10
 13312                             	        vpshufd	$0x4e, %xmm13, %xmm13
 13313                             	        vpxor	%xmm9, %xmm10, %xmm10
 13314                             	        vpxor	%xmm8, %xmm13, %xmm13
 13315                             	        vpclmulqdq	$16, %xmm3, %xmm10, %xmm9
 13316                             	        vpclmulqdq	$16, %xmm3, %xmm13, %xmm8
 13317                             	        vpshufd	$0x4e, %xmm10, %xmm10
 13318                             	        vpshufd	$0x4e, %xmm13, %xmm13
 13319                             	        vpxor	%xmm11, %xmm12, %xmm12
 13320                             	        vpxor	%xmm8, %xmm13, %xmm13
 13321                             	        vpxor	%xmm12, %xmm10, %xmm10
 13322                             	        vpxor	%xmm14, %xmm13, %xmm0
 13323                             	        vpxor	%xmm9, %xmm10, %xmm7
 13324                             	        vmovdqu	%xmm7, 64(%rsp)
 13325                             	        vmovdqu	%xmm0, 80(%rsp)
 13326                             	        # H ^ 7 and H ^ 8
 13327                             	        vpclmulqdq	$16, %xmm1, %xmm2, %xmm11
 13328                             	        vpclmulqdq	$0x01, %xmm1, %xmm2, %xmm10
 13329                             	        vpclmulqdq	$0x00, %xmm1, %xmm2, %xmm9
 13330                             	        vpclmulqdq	$0x11, %xmm1, %xmm2, %xmm12
 13331                             	        vpclmulqdq	$0x00, %xmm2, %xmm2, %xmm13
 13332                             	        vpclmulqdq	$0x11, %xmm2, %xmm2, %xmm14
 13333                             	        vpxor	%xmm10, %xmm11, %xmm11
 13334                             	        vpslldq	$8, %xmm11, %xmm10
 13335                             	        vpsrldq	$8, %xmm11, %xmm11
 13336                             	        vpxor	%xmm9, %xmm10, %xmm10
 13337                             	        vpclmulqdq	$16, %xmm3, %xmm13, %xmm8
 13338                             	        vpclmulqdq	$16, %xmm3, %xmm10, %xmm9
 13339                             	        vpshufd	$0x4e, %xmm10, %xmm10
 13340                             	        vpshufd	$0x4e, %xmm13, %xmm13
 13341                             	        vpxor	%xmm9, %xmm10, %xmm10
 13342                             	        vpxor	%xmm8, %xmm13, %xmm13
 13343                             	        vpclmulqdq	$16, %xmm3, %xmm10, %xmm9
 13344                             	        vpclmulqdq	$16, %xmm3, %xmm13, %xmm8
 13345                             	        vpshufd	$0x4e, %xmm10, %xmm10
 13346                             	        vpshufd	$0x4e, %xmm13, %xmm13
 13347                             	        vpxor	%xmm11, %xmm12, %xmm12
 13348                             	        vpxor	%xmm8, %xmm13, %xmm13
 13349                             	        vpxor	%xmm12, %xmm10, %xmm10
 13350                             	        vpxor	%xmm14, %xmm13, %xmm0
 13351                             	        vpxor	%xmm9, %xmm10, %xmm7
 13352                             	        vmovdqu	%xmm7, 96(%rsp)
 13353                             	        vmovdqu	%xmm0, 112(%rsp)
 13354                             	L_AES_GCM_decrypt_avx2_ghash_128:
 13355                             	        # aesenc_128_ghash
 13356                             	        leaq	(%rdi,%rbx,1), %rcx
 13357                             	        leaq	(%r8,%rbx,1), %rdx
 13358                             	        # aesenc_ctr
 13359                             	        vmovdqu	128(%rsp), %xmm0
 13360                             	        vmovdqu	L_avx2_aes_gcm_bswap_epi64(%rip), %xmm1
 13361                             	        vpaddd	L_avx2_aes_gcm_one(%rip), %xmm0, %xmm9
 13362                             	        vpshufb	%xmm1, %xmm0, %xmm8
 13363                             	        vpaddd	L_avx2_aes_gcm_two(%rip), %xmm0, %xmm10
 13364                             	        vpshufb	%xmm1, %xmm9, %xmm9
 13365                             	        vpaddd	L_avx2_aes_gcm_three(%rip), %xmm0, %xmm11
 13366                             	        vpshufb	%xmm1, %xmm10, %xmm10
 13367                             	        vpaddd	L_avx2_aes_gcm_four(%rip), %xmm0, %xmm12
 13368                             	        vpshufb	%xmm1, %xmm11, %xmm11
 13369                             	        vpaddd	L_avx2_aes_gcm_five(%rip), %xmm0, %xmm13
 13370                             	        vpshufb	%xmm1, %xmm12, %xmm12
 13371                             	        vpaddd	L_avx2_aes_gcm_six(%rip), %xmm0, %xmm14
 13372                             	        vpshufb	%xmm1, %xmm13, %xmm13
 13373                             	        vpaddd	L_avx2_aes_gcm_seven(%rip), %xmm0, %xmm15
 13374                             	        vpshufb	%xmm1, %xmm14, %xmm14
 13375                             	        vpaddd	L_avx2_aes_gcm_eight(%rip), %xmm0, %xmm0
 13376                             	        vpshufb	%xmm1, %xmm15, %xmm15
 13377                             	        # aesenc_xor
 13378                             	        vmovdqu	(%rsi), %xmm7
 13379                             	        vmovdqu	%xmm0, 128(%rsp)
 13380                             	        vpxor	%xmm7, %xmm8, %xmm8
 13381                             	        vpxor	%xmm7, %xmm9, %xmm9
 13382                             	        vpxor	%xmm7, %xmm10, %xmm10
 13383                             	        vpxor	%xmm7, %xmm11, %xmm11
 13384                             	        vpxor	%xmm7, %xmm12, %xmm12
 13385                             	        vpxor	%xmm7, %xmm13, %xmm13
 13386                             	        vpxor	%xmm7, %xmm14, %xmm14
 13387                             	        vpxor	%xmm7, %xmm15, %xmm15
 13388                             	        # aesenc_pclmul_1
 13389                             	        vmovdqu	(%rcx), %xmm1
 13390                             	        vmovdqu	16(%rsi), %xmm0
 13391                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 13392                             	        vmovdqu	112(%rsp), %xmm2
 13393                             	        vpxor	%xmm6, %xmm1, %xmm1
 13394                             	        vpclmulqdq	$16, %xmm2, %xmm1, %xmm5
 13395                             	        vpclmulqdq	$0x01, %xmm2, %xmm1, %xmm3
 13396                             	        vpclmulqdq	$0x00, %xmm2, %xmm1, %xmm6
 13397                             	        vpclmulqdq	$0x11, %xmm2, %xmm1, %xmm7
 13398                             	        vaesenc	%xmm0, %xmm8, %xmm8
 13399                             	        vaesenc	%xmm0, %xmm9, %xmm9
 13400                             	        vaesenc	%xmm0, %xmm10, %xmm10
 13401                             	        vaesenc	%xmm0, %xmm11, %xmm11
 13402                             	        vaesenc	%xmm0, %xmm12, %xmm12
 13403                             	        vaesenc	%xmm0, %xmm13, %xmm13
 13404                             	        vaesenc	%xmm0, %xmm14, %xmm14
 13405                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13406                             	        # aesenc_pclmul_2
 13407                             	        vmovdqu	16(%rcx), %xmm1
 13408                             	        vmovdqu	96(%rsp), %xmm0
 13409                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 13410                             	        vpxor	%xmm3, %xmm5, %xmm5
 13411                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 13412                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 13413                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 13414                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 13415                             	        vmovdqu	32(%rsi), %xmm0
 13416                             	        vpxor	%xmm1, %xmm7, %xmm7
 13417                             	        vaesenc	%xmm0, %xmm8, %xmm8
 13418                             	        vaesenc	%xmm0, %xmm9, %xmm9
 13419                             	        vaesenc	%xmm0, %xmm10, %xmm10
 13420                             	        vaesenc	%xmm0, %xmm11, %xmm11
 13421                             	        vaesenc	%xmm0, %xmm12, %xmm12
 13422                             	        vaesenc	%xmm0, %xmm13, %xmm13
 13423                             	        vaesenc	%xmm0, %xmm14, %xmm14
 13424                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13425                             	        # aesenc_pclmul_n
 13426                             	        vmovdqu	32(%rcx), %xmm1
 13427                             	        vmovdqu	80(%rsp), %xmm0
 13428                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 13429                             	        vpxor	%xmm2, %xmm5, %xmm5
 13430                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 13431                             	        vpxor	%xmm3, %xmm5, %xmm5
 13432                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 13433                             	        vpxor	%xmm4, %xmm6, %xmm6
 13434                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 13435                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 13436                             	        vmovdqu	48(%rsi), %xmm0
 13437                             	        vpxor	%xmm1, %xmm7, %xmm7
 13438                             	        vaesenc	%xmm0, %xmm8, %xmm8
 13439                             	        vaesenc	%xmm0, %xmm9, %xmm9
 13440                             	        vaesenc	%xmm0, %xmm10, %xmm10
 13441                             	        vaesenc	%xmm0, %xmm11, %xmm11
 13442                             	        vaesenc	%xmm0, %xmm12, %xmm12
 13443                             	        vaesenc	%xmm0, %xmm13, %xmm13
 13444                             	        vaesenc	%xmm0, %xmm14, %xmm14
 13445                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13446                             	        # aesenc_pclmul_n
 13447                             	        vmovdqu	48(%rcx), %xmm1
 13448                             	        vmovdqu	64(%rsp), %xmm0
 13449                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 13450                             	        vpxor	%xmm2, %xmm5, %xmm5
 13451                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 13452                             	        vpxor	%xmm3, %xmm5, %xmm5
 13453                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 13454                             	        vpxor	%xmm4, %xmm6, %xmm6
 13455                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 13456                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 13457                             	        vmovdqu	64(%rsi), %xmm0
 13458                             	        vpxor	%xmm1, %xmm7, %xmm7
 13459                             	        vaesenc	%xmm0, %xmm8, %xmm8
 13460                             	        vaesenc	%xmm0, %xmm9, %xmm9
 13461                             	        vaesenc	%xmm0, %xmm10, %xmm10
 13462                             	        vaesenc	%xmm0, %xmm11, %xmm11
 13463                             	        vaesenc	%xmm0, %xmm12, %xmm12
 13464                             	        vaesenc	%xmm0, %xmm13, %xmm13
 13465                             	        vaesenc	%xmm0, %xmm14, %xmm14
 13466                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13467                             	        # aesenc_pclmul_n
 13468                             	        vmovdqu	64(%rcx), %xmm1
 13469                             	        vmovdqu	48(%rsp), %xmm0
 13470                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 13471                             	        vpxor	%xmm2, %xmm5, %xmm5
 13472                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 13473                             	        vpxor	%xmm3, %xmm5, %xmm5
 13474                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 13475                             	        vpxor	%xmm4, %xmm6, %xmm6
 13476                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 13477                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 13478                             	        vmovdqu	80(%rsi), %xmm0
 13479                             	        vpxor	%xmm1, %xmm7, %xmm7
 13480                             	        vaesenc	%xmm0, %xmm8, %xmm8
 13481                             	        vaesenc	%xmm0, %xmm9, %xmm9
 13482                             	        vaesenc	%xmm0, %xmm10, %xmm10
 13483                             	        vaesenc	%xmm0, %xmm11, %xmm11
 13484                             	        vaesenc	%xmm0, %xmm12, %xmm12
 13485                             	        vaesenc	%xmm0, %xmm13, %xmm13
 13486                             	        vaesenc	%xmm0, %xmm14, %xmm14
 13487                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13488                             	        # aesenc_pclmul_n
 13489                             	        vmovdqu	80(%rcx), %xmm1
 13490                             	        vmovdqu	32(%rsp), %xmm0
 13491                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 13492                             	        vpxor	%xmm2, %xmm5, %xmm5
 13493                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 13494                             	        vpxor	%xmm3, %xmm5, %xmm5
 13495                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 13496                             	        vpxor	%xmm4, %xmm6, %xmm6
 13497                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 13498                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 13499                             	        vmovdqu	96(%rsi), %xmm0
 13500                             	        vpxor	%xmm1, %xmm7, %xmm7
 13501                             	        vaesenc	%xmm0, %xmm8, %xmm8
 13502                             	        vaesenc	%xmm0, %xmm9, %xmm9
 13503                             	        vaesenc	%xmm0, %xmm10, %xmm10
 13504                             	        vaesenc	%xmm0, %xmm11, %xmm11
 13505                             	        vaesenc	%xmm0, %xmm12, %xmm12
 13506                             	        vaesenc	%xmm0, %xmm13, %xmm13
 13507                             	        vaesenc	%xmm0, %xmm14, %xmm14
 13508                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13509                             	        # aesenc_pclmul_n
 13510                             	        vmovdqu	96(%rcx), %xmm1
 13511                             	        vmovdqu	16(%rsp), %xmm0
 13512                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 13513                             	        vpxor	%xmm2, %xmm5, %xmm5
 13514                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 13515                             	        vpxor	%xmm3, %xmm5, %xmm5
 13516                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 13517                             	        vpxor	%xmm4, %xmm6, %xmm6
 13518                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 13519                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 13520                             	        vmovdqu	112(%rsi), %xmm0
 13521                             	        vpxor	%xmm1, %xmm7, %xmm7
 13522                             	        vaesenc	%xmm0, %xmm8, %xmm8
 13523                             	        vaesenc	%xmm0, %xmm9, %xmm9
 13524                             	        vaesenc	%xmm0, %xmm10, %xmm10
 13525                             	        vaesenc	%xmm0, %xmm11, %xmm11
 13526                             	        vaesenc	%xmm0, %xmm12, %xmm12
 13527                             	        vaesenc	%xmm0, %xmm13, %xmm13
 13528                             	        vaesenc	%xmm0, %xmm14, %xmm14
 13529                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13530                             	        # aesenc_pclmul_n
 13531                             	        vmovdqu	112(%rcx), %xmm1
 13532                             	        vmovdqu	(%rsp), %xmm0
 13533                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm1, %xmm1
 13534                             	        vpxor	%xmm2, %xmm5, %xmm5
 13535                             	        vpclmulqdq	$16, %xmm0, %xmm1, %xmm2
 13536                             	        vpxor	%xmm3, %xmm5, %xmm5
 13537                             	        vpclmulqdq	$0x01, %xmm0, %xmm1, %xmm3
 13538                             	        vpxor	%xmm4, %xmm6, %xmm6
 13539                             	        vpclmulqdq	$0x00, %xmm0, %xmm1, %xmm4
 13540                             	        vpclmulqdq	$0x11, %xmm0, %xmm1, %xmm1
 13541                             	        vmovdqu	128(%rsi), %xmm0
 13542                             	        vpxor	%xmm1, %xmm7, %xmm7
 13543                             	        vaesenc	%xmm0, %xmm8, %xmm8
 13544                             	        vaesenc	%xmm0, %xmm9, %xmm9
 13545                             	        vaesenc	%xmm0, %xmm10, %xmm10
 13546                             	        vaesenc	%xmm0, %xmm11, %xmm11
 13547                             	        vaesenc	%xmm0, %xmm12, %xmm12
 13548                             	        vaesenc	%xmm0, %xmm13, %xmm13
 13549                             	        vaesenc	%xmm0, %xmm14, %xmm14
 13550                             	        vaesenc	%xmm0, %xmm15, %xmm15
 13551                             	        # aesenc_pclmul_l
 13552                             	        vpxor	%xmm2, %xmm5, %xmm5
 13553                             	        vpxor	%xmm4, %xmm6, %xmm6
 13554                             	        vpxor	%xmm3, %xmm5, %xmm5
 13555                             	        vpslldq	$8, %xmm5, %xmm1
 13556                             	        vpsrldq	$8, %xmm5, %xmm5
 13557                             	        vmovdqu	144(%rsi), %xmm4
 13558                             	        vmovdqu	L_avx2_aes_gcm_mod2_128(%rip), %xmm0
 13559                             	        vaesenc	%xmm4, %xmm8, %xmm8
 13560                             	        vpxor	%xmm1, %xmm6, %xmm6
 13561                             	        vpxor	%xmm5, %xmm7, %xmm7
 13562                             	        vpclmulqdq	$16, %xmm0, %xmm6, %xmm3
 13563                             	        vaesenc	%xmm4, %xmm9, %xmm9
 13564                             	        vaesenc	%xmm4, %xmm10, %xmm10
 13565                             	        vaesenc	%xmm4, %xmm11, %xmm11
 13566                             	        vpshufd	$0x4e, %xmm6, %xmm6
 13567                             	        vpxor	%xmm3, %xmm6, %xmm6
 13568                             	        vpclmulqdq	$16, %xmm0, %xmm6, %xmm3
 13569                             	        vaesenc	%xmm4, %xmm12, %xmm12
 13570                             	        vaesenc	%xmm4, %xmm13, %xmm13
 13571                             	        vaesenc	%xmm4, %xmm14, %xmm14
 13572                             	        vpshufd	$0x4e, %xmm6, %xmm6
 13573                             	        vpxor	%xmm3, %xmm6, %xmm6
 13574                             	        vpxor	%xmm7, %xmm6, %xmm6
 13575                             	        vaesenc	%xmm4, %xmm15, %xmm15
 13576                             	        cmpl	$11, %r9d
 13577                             	        vmovdqu	160(%rsi), %xmm7
 13578                             	        jl	L_AES_GCM_decrypt_avx2_aesenc_128_ghash_avx_done
 13579                             	        vaesenc	%xmm7, %xmm8, %xmm8
 13580                             	        vaesenc	%xmm7, %xmm9, %xmm9
 13581                             	        vaesenc	%xmm7, %xmm10, %xmm10
 13582                             	        vaesenc	%xmm7, %xmm11, %xmm11
 13583                             	        vaesenc	%xmm7, %xmm12, %xmm12
 13584                             	        vaesenc	%xmm7, %xmm13, %xmm13
 13585                             	        vaesenc	%xmm7, %xmm14, %xmm14
 13586                             	        vaesenc	%xmm7, %xmm15, %xmm15
 13587                             	        vmovdqu	176(%rsi), %xmm7
 13588                             	        vaesenc	%xmm7, %xmm8, %xmm8
 13589                             	        vaesenc	%xmm7, %xmm9, %xmm9
 13590                             	        vaesenc	%xmm7, %xmm10, %xmm10
 13591                             	        vaesenc	%xmm7, %xmm11, %xmm11
 13592                             	        vaesenc	%xmm7, %xmm12, %xmm12
 13593                             	        vaesenc	%xmm7, %xmm13, %xmm13
 13594                             	        vaesenc	%xmm7, %xmm14, %xmm14
 13595                             	        vaesenc	%xmm7, %xmm15, %xmm15
 13596                             	        cmpl	$13, %r9d
 13597                             	        vmovdqu	192(%rsi), %xmm7
 13598                             	        jl	L_AES_GCM_decrypt_avx2_aesenc_128_ghash_avx_done
 13599                             	        vaesenc	%xmm7, %xmm8, %xmm8
 13600                             	        vaesenc	%xmm7, %xmm9, %xmm9
 13601                             	        vaesenc	%xmm7, %xmm10, %xmm10
 13602                             	        vaesenc	%xmm7, %xmm11, %xmm11
 13603                             	        vaesenc	%xmm7, %xmm12, %xmm12
 13604                             	        vaesenc	%xmm7, %xmm13, %xmm13
 13605                             	        vaesenc	%xmm7, %xmm14, %xmm14
 13606                             	        vaesenc	%xmm7, %xmm15, %xmm15
 13607                             	        vmovdqu	208(%rsi), %xmm7
 13608                             	        vaesenc	%xmm7, %xmm8, %xmm8
 13609                             	        vaesenc	%xmm7, %xmm9, %xmm9
 13610                             	        vaesenc	%xmm7, %xmm10, %xmm10
 13611                             	        vaesenc	%xmm7, %xmm11, %xmm11
 13612                             	        vaesenc	%xmm7, %xmm12, %xmm12
 13613                             	        vaesenc	%xmm7, %xmm13, %xmm13
 13614                             	        vaesenc	%xmm7, %xmm14, %xmm14
 13615                             	        vaesenc	%xmm7, %xmm15, %xmm15
 13616                             	        vmovdqu	224(%rsi), %xmm7
 13617                             	L_AES_GCM_decrypt_avx2_aesenc_128_ghash_avx_done:
 13618                             	        # aesenc_last
 13619                             	        vaesenclast	%xmm7, %xmm8, %xmm8
 13620                             	        vaesenclast	%xmm7, %xmm9, %xmm9
 13621                             	        vaesenclast	%xmm7, %xmm10, %xmm10
 13622                             	        vaesenclast	%xmm7, %xmm11, %xmm11
 13623                             	        vmovdqu	(%rcx), %xmm0
 13624                             	        vmovdqu	16(%rcx), %xmm1
 13625                             	        vmovdqu	32(%rcx), %xmm2
 13626                             	        vmovdqu	48(%rcx), %xmm3
 13627                             	        vpxor	%xmm0, %xmm8, %xmm8
 13628                             	        vpxor	%xmm1, %xmm9, %xmm9
 13629                             	        vpxor	%xmm2, %xmm10, %xmm10
 13630                             	        vpxor	%xmm3, %xmm11, %xmm11
 13631                             	        vmovdqu	%xmm8, (%rdx)
 13632                             	        vmovdqu	%xmm9, 16(%rdx)
 13633                             	        vmovdqu	%xmm10, 32(%rdx)
 13634                             	        vmovdqu	%xmm11, 48(%rdx)
 13635                             	        vaesenclast	%xmm7, %xmm12, %xmm12
 13636                             	        vaesenclast	%xmm7, %xmm13, %xmm13
 13637                             	        vaesenclast	%xmm7, %xmm14, %xmm14
 13638                             	        vaesenclast	%xmm7, %xmm15, %xmm15
 13639                             	        vmovdqu	64(%rcx), %xmm0
 13640                             	        vmovdqu	80(%rcx), %xmm1
 13641                             	        vmovdqu	96(%rcx), %xmm2
 13642                             	        vmovdqu	112(%rcx), %xmm3
 13643                             	        vpxor	%xmm0, %xmm12, %xmm12
 13644                             	        vpxor	%xmm1, %xmm13, %xmm13
 13645                             	        vpxor	%xmm2, %xmm14, %xmm14
 13646                             	        vpxor	%xmm3, %xmm15, %xmm15
 13647                             	        vmovdqu	%xmm12, 64(%rdx)
 13648                             	        vmovdqu	%xmm13, 80(%rdx)
 13649                             	        vmovdqu	%xmm14, 96(%rdx)
 13650                             	        vmovdqu	%xmm15, 112(%rdx)
 13651                             	        # aesenc_128_ghash - end
 13652                             	        addl	$0x80, %ebx
 13653                             	        cmpl	%r13d, %ebx
 13654                             	        jl	L_AES_GCM_decrypt_avx2_ghash_128
 13655                             	        vmovdqu	(%rsp), %xmm5
 13656                             	        vmovdqu	128(%rsp), %xmm4
 13657                             	        vmovdqu	144(%rsp), %xmm15
 13658                             	L_AES_GCM_decrypt_avx2_done_128:
 13659                             	        cmpl	%r10d, %ebx
 13660                             	        jge	L_AES_GCM_decrypt_avx2_done_dec
 13661                             	        movl	%r10d, %r13d
 13662                             	        andl	$0xfffffff0, %r13d
 13663                             	        cmpl	%r13d, %ebx
 13664                             	        jge	L_AES_GCM_decrypt_avx2_last_block_done
 13665                             	L_AES_GCM_decrypt_avx2_last_block_start:
 13666                             	        vmovdqu	(%rdi,%rbx,1), %xmm11
 13667                             	        vpshufb	L_avx2_aes_gcm_bswap_epi64(%rip), %xmm4, %xmm10
 13668                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm11, %xmm12
 13669                             	        vpaddd	L_avx2_aes_gcm_one(%rip), %xmm4, %xmm4
 13670                             	        vpxor	%xmm6, %xmm12, %xmm12
 13671                             	        # aesenc_gfmul_sb
 13672                             	        vpclmulqdq	$0x01, %xmm5, %xmm12, %xmm2
 13673                             	        vpclmulqdq	$16, %xmm5, %xmm12, %xmm3
 13674                             	        vpclmulqdq	$0x00, %xmm5, %xmm12, %xmm1
 13675                             	        vpclmulqdq	$0x11, %xmm5, %xmm12, %xmm8
 13676                             	        vpxor	(%rsi), %xmm10, %xmm10
 13677                             	        vaesenc	16(%rsi), %xmm10, %xmm10
 13678                             	        vpxor	%xmm2, %xmm3, %xmm3
 13679                             	        vpslldq	$8, %xmm3, %xmm2
 13680                             	        vpsrldq	$8, %xmm3, %xmm3
 13681                             	        vaesenc	32(%rsi), %xmm10, %xmm10
 13682                             	        vpxor	%xmm1, %xmm2, %xmm2
 13683                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm2, %xmm1
 13684                             	        vaesenc	48(%rsi), %xmm10, %xmm10
 13685                             	        vaesenc	64(%rsi), %xmm10, %xmm10
 13686                             	        vaesenc	80(%rsi), %xmm10, %xmm10
 13687                             	        vpshufd	$0x4e, %xmm2, %xmm2
 13688                             	        vpxor	%xmm1, %xmm2, %xmm2
 13689                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm2, %xmm1
 13690                             	        vaesenc	96(%rsi), %xmm10, %xmm10
 13691                             	        vaesenc	112(%rsi), %xmm10, %xmm10
 13692                             	        vaesenc	128(%rsi), %xmm10, %xmm10
 13693                             	        vpshufd	$0x4e, %xmm2, %xmm2
 13694                             	        vaesenc	144(%rsi), %xmm10, %xmm10
 13695                             	        vpxor	%xmm3, %xmm8, %xmm8
 13696                             	        vpxor	%xmm8, %xmm2, %xmm2
 13697                             	        vmovdqu	160(%rsi), %xmm0
 13698                             	        cmpl	$11, %r9d
 13699                             	        jl	L_AES_GCM_decrypt_avx2_aesenc_gfmul_sb_last
 13700                             	        vaesenc	%xmm0, %xmm10, %xmm10
 13701                             	        vaesenc	176(%rsi), %xmm10, %xmm10
 13702                             	        vmovdqu	192(%rsi), %xmm0
 13703                             	        cmpl	$13, %r9d
 13704                             	        jl	L_AES_GCM_decrypt_avx2_aesenc_gfmul_sb_last
 13705                             	        vaesenc	%xmm0, %xmm10, %xmm10
 13706                             	        vaesenc	208(%rsi), %xmm10, %xmm10
 13707                             	        vmovdqu	224(%rsi), %xmm0
 13708                             	L_AES_GCM_decrypt_avx2_aesenc_gfmul_sb_last:
 13709                             	        vaesenclast	%xmm0, %xmm10, %xmm10
 13710                             	        vpxor	%xmm1, %xmm2, %xmm6
 13711                             	        vpxor	%xmm11, %xmm10, %xmm10
 13712                             	        vmovdqu	%xmm10, (%r8,%rbx,1)
 13713                             	        addl	$16, %ebx
 13714                             	        cmpl	%r13d, %ebx
 13715                             	        jl	L_AES_GCM_decrypt_avx2_last_block_start
 13716                             	L_AES_GCM_decrypt_avx2_last_block_done:
 13717                             	        movl	%r10d, %ecx
 13718                             	        movl	%r10d, %edx
 13719                             	        andl	$15, %ecx
 13720                             	        jz	L_AES_GCM_decrypt_avx2_done_dec
 13721                             	        # aesenc_last15_dec
 13722                             	        vpshufb	L_avx2_aes_gcm_bswap_epi64(%rip), %xmm4, %xmm4
 13723                             	        vpxor	(%rsi), %xmm4, %xmm4
 13724                             	        vaesenc	16(%rsi), %xmm4, %xmm4
 13725                             	        vaesenc	32(%rsi), %xmm4, %xmm4
 13726                             	        vaesenc	48(%rsi), %xmm4, %xmm4
 13727                             	        vaesenc	64(%rsi), %xmm4, %xmm4
 13728                             	        vaesenc	80(%rsi), %xmm4, %xmm4
 13729                             	        vaesenc	96(%rsi), %xmm4, %xmm4
 13730                             	        vaesenc	112(%rsi), %xmm4, %xmm4
 13731                             	        vaesenc	128(%rsi), %xmm4, %xmm4
 13732                             	        vaesenc	144(%rsi), %xmm4, %xmm4
 13733                             	        cmpl	$11, %r9d
 13734                             	        vmovdqu	160(%rsi), %xmm1
 13735                             	        jl	L_AES_GCM_decrypt_avx2_aesenc_last15_dec_avx_aesenc_avx_last
 13736                             	        vaesenc	%xmm1, %xmm4, %xmm4
 13737                             	        vaesenc	176(%rsi), %xmm4, %xmm4
 13738                             	        cmpl	$13, %r9d
 13739                             	        vmovdqu	192(%rsi), %xmm1
 13740                             	        jl	L_AES_GCM_decrypt_avx2_aesenc_last15_dec_avx_aesenc_avx_last
 13741                             	        vaesenc	%xmm1, %xmm4, %xmm4
 13742                             	        vaesenc	208(%rsi), %xmm4, %xmm4
 13743                             	        vmovdqu	224(%rsi), %xmm1
 13744                             	L_AES_GCM_decrypt_avx2_aesenc_last15_dec_avx_aesenc_avx_last:
 13745                             	        vaesenclast	%xmm1, %xmm4, %xmm4
 13746                             	        xorl	%ecx, %ecx
 13747                             	        vpxor	%xmm0, %xmm0, %xmm0
 13748                             	        vmovdqu	%xmm4, (%rsp)
 13749                             	        vmovdqu	%xmm0, 16(%rsp)
 13750                             	L_AES_GCM_decrypt_avx2_aesenc_last15_dec_avx_loop:
 13751                             	        movzbl	(%rdi,%rbx,1), %r13d
 13752                             	        movb	%r13b, 16(%rsp,%rcx,1)
 13753                             	        xorb	(%rsp,%rcx,1), %r13b
 13754                             	        movb	%r13b, (%r8,%rbx,1)
 13755                             	        incl	%ebx
 13756                             	        incl	%ecx
 13757                             	        cmpl	%edx, %ebx
 13758                             	        jl	L_AES_GCM_decrypt_avx2_aesenc_last15_dec_avx_loop
 13759                             	        vmovdqu	16(%rsp), %xmm4
 13760                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm4, %xmm4
 13761                             	        vpxor	%xmm4, %xmm6, %xmm6
 13762                             	        # ghash_gfmul_red
 13763                             	        vpclmulqdq	$16, %xmm5, %xmm6, %xmm2
 13764                             	        vpclmulqdq	$0x01, %xmm5, %xmm6, %xmm1
 13765                             	        vpclmulqdq	$0x00, %xmm5, %xmm6, %xmm0
 13766                             	        vpxor	%xmm1, %xmm2, %xmm2
 13767                             	        vpslldq	$8, %xmm2, %xmm1
 13768                             	        vpsrldq	$8, %xmm2, %xmm2
 13769                             	        vpxor	%xmm0, %xmm1, %xmm1
 13770                             	        vpclmulqdq	$0x11, %xmm5, %xmm6, %xmm6
 13771                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm1, %xmm0
 13772                             	        vpshufd	$0x4e, %xmm1, %xmm1
 13773                             	        vpxor	%xmm0, %xmm1, %xmm1
 13774                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm1, %xmm0
 13775                             	        vpshufd	$0x4e, %xmm1, %xmm1
 13776                             	        vpxor	%xmm2, %xmm6, %xmm6
 13777                             	        vpxor	%xmm1, %xmm6, %xmm6
 13778                             	        vpxor	%xmm0, %xmm6, %xmm6
 13779                             	L_AES_GCM_decrypt_avx2_done_dec:
 13780                             	        # calc_tag
 13781                             	        shlq	$3, %r10
 13782                             	        vpinsrq	$0x00, %r10, %xmm0, %xmm0
 13783                             	        shlq	$3, %r11
 13784                             	        vpinsrq	$0x01, %r11, %xmm1, %xmm1
 13785                             	        vpblendd	$12, %xmm1, %xmm0, %xmm0
 13786                             	        vpxor	%xmm6, %xmm0, %xmm0
 13787                             	        # ghash_gfmul_red
 13788                             	        vpclmulqdq	$16, %xmm5, %xmm0, %xmm4
 13789                             	        vpclmulqdq	$0x01, %xmm5, %xmm0, %xmm3
 13790                             	        vpclmulqdq	$0x00, %xmm5, %xmm0, %xmm2
 13791                             	        vpxor	%xmm3, %xmm4, %xmm4
 13792                             	        vpslldq	$8, %xmm4, %xmm3
 13793                             	        vpsrldq	$8, %xmm4, %xmm4
 13794                             	        vpxor	%xmm2, %xmm3, %xmm3
 13795                             	        vpclmulqdq	$0x11, %xmm5, %xmm0, %xmm0
 13796                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm3, %xmm2
 13797                             	        vpshufd	$0x4e, %xmm3, %xmm3
 13798                             	        vpxor	%xmm2, %xmm3, %xmm3
 13799                             	        vpclmulqdq	$16, L_avx2_aes_gcm_mod2_128(%rip), %xmm3, %xmm2
 13800                             	        vpshufd	$0x4e, %xmm3, %xmm3
 13801                             	        vpxor	%xmm4, %xmm0, %xmm0
 13802                             	        vpxor	%xmm3, %xmm0, %xmm0
 13803                             	        vpxor	%xmm2, %xmm0, %xmm0
 13804                             	        vpshufb	L_avx2_aes_gcm_bswap_mask(%rip), %xmm0, %xmm0
 13805                             	        vpxor	%xmm15, %xmm0, %xmm0
 13806                             	        # cmp_tag
 13807                             	        cmpl	$16, %r15d
 13808                             	        je	L_AES_GCM_decrypt_avx2_cmp_tag_16
 13809                             	        xorq	%rdx, %rdx
 13810                             	        xorq	%rax, %rax
 13811                             	        vmovdqu	%xmm0, (%rsp)
 13812                             	L_AES_GCM_decrypt_avx2_cmp_tag_loop:
 13813                             	        movzbl	(%rsp,%rdx,1), %r13d
 13814                             	        xorb	(%r14,%rdx,1), %r13b
 13815                             	        orb	%r13b, %al
 13816                             	        incl	%edx
 13817                             	        cmpl	%r15d, %edx
 13818                             	        jne	L_AES_GCM_decrypt_avx2_cmp_tag_loop
 13819                             	        cmpb	$0x00, %al
 13820                             	        sete	%al
 13821                             	        jmp	L_AES_GCM_decrypt_avx2_cmp_tag_done
 13822                             	L_AES_GCM_decrypt_avx2_cmp_tag_16:
 13823                             	        vmovdqu	(%r14), %xmm1
 13824                             	        vpcmpeqb	%xmm1, %xmm0, %xmm0
 13825                             	        vpmovmskb	%xmm0, %rdx
 13826                             	        # %%edx == 0xFFFF then return 1 else => return 0
 13827                             	        xorl	%eax, %eax
 13828                             	        cmpl	$0xffff, %edx
 13829                             	        sete	%al
 13830                             	L_AES_GCM_decrypt_avx2_cmp_tag_done:
 13831                             	        movl	%eax, (%rbp)
 13832                             	        vzeroupper
 13833                             	        addq	$0xa8, %rsp
 13834                             	        popq	%rbp
 13835                             	        popq	%r15
 13836                             	        popq	%rbx
 13837                             	        popq	%r14
 13838                             	        popq	%r12
 13839                             	        popq	%r13
 13840                             	        repz retq
 13841                             	#ifndef __APPLE__
